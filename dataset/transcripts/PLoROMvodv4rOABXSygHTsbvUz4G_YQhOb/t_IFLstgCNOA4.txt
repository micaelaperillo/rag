so hello everyone and welcome for the last lecture of cs2 30 deep learning so it's been 10 weeks and then you've been studying deep learning all around starting with fully connected networks understanding how to boost these networks and make them better and then using recurrent neural networks in the last part and convolutional neural networks in the fourth part to build models for imaging and text and other applications so today is the class wrap-up and the lecture might be slightly shorter than usual but we're going to go over a small case study on conversional assistants to start with which is a neutral topic we will do a small quiz competition with Monty and the fastest person who has the best answer will win 400 hours of GPU credits on Amazon so you guys can can start can start working on it we will see some class project advice because you guys have about two weeks less than two weeks before the the poster presentation and the final project due date will also go over some of the next steps after CS 2:30 what have our students done over the past year and what we think are good next steps and closing remarks to finish I'll by you if you have a clicker with with bathroom please can you bring it to me okay so let's get started with how to build a chatbot to help students find or and enroll in the right course so this is going to be a pretty simple case of a chat bot because chat BOTS and commercial conversational assistants in general have been very hard to build and have an initial topic there are some places where academia has helped the chat pods improvements and here we're going to see how we can take all our algorithms what we've learned in this class and plug it in in a conversional setting that sounds good so let me give you an example students might write to the chat bot hi I want to enroll in CS 106a for winter 2019 to learn coding the chat bot can answer for sure I just enrolled you so that would be one goal of the chat bot a second example might be finding information about classes hi what are the undergraduate level history classes offered in spring 2019 then the chat bot can get back to the students and said here's the list of history classes offered in spring 2008 so we're making a small assumption here we're building a chat bot for a very restricted area in general and a lot of time chat BOTS which work very well our super goal-oriented or transactional and the state of possible or requests from users is small smaller than what you could expect in other industrial settings so here we're making the assumption that the students will only try to find information about a course or will try to enroll in the course so I want you guys to pair in groups of two or three and try to come up with ideas of what methods that we've seen together can be used in order to implement such a chatbot okay so take a minute introduce yourself to your mates and and try to figure out which methods can be leveraged in this case okay let's see what we have here parlance for natural language processing transfer learning and let's seem to pick out important words from inputs based on those input triggers output some predefined information from storage yeah so this seems to to say that there is going to be one learning part where we need to have probably record neural networks helping out and one other knowledge base or storage part where we can retrieve some information we're going to see that some attention models is true that today a lot of natural language processing models are built with attention models Arnon for speech recognition and speech generation so we didn't talk about the speech parts so far we assume that the congressional assistant is text-based but later on we will see what happens if we want to add speech to it fancy methods or reinforcement learning for making decisions about responses that's interesting so why do you guys think we would need reinforcement learning yes of different states and you also have like a volume associated the music is very goal-oriented and so you could sort of have a dress in that fashion yeah that's good so just to repeat it's important to keep a notion of context and also we have a sequence of utterances from the user and the commercial assistance and probably the outcome of the conversation would come far along the way and not at every step so that's true reinforcement learning has been a research topic for commercial assistants as well and often time we will try to learn a policy for the chat pod which given a state will tell us what action to take next this can be done using Q learning which is the method we've seen together or some time with policy gradients okay word and coding so I word embedding probably okay cool so I agree there's many ways to to plug in a deep learning algorithm in this chat pod setting we're going to see a few of them first I'd like to introduce some vocabulary which is commonly used when talking about commercial assistance conversational assistance an utterance is you can think of it as a user input so if I say the student utterance it's the sentence that was written by the students for the chat pod the assistant utterance is the one coming from the chat pod site the intent denotes the intention of the user so in our case we will have two intents which is very limited the user either wants to find information from for a course or the user wants to enroll in a class these are two different intentions that are probably to be detected early on the conversation and then you have something called slots slots are used to gather multiple information from the user on a specific intent that the user has so let's say the students wants to enroll in a class in order to enroll the students in a class you need to fill in several slots you need to understand probably which class the student is talking about which quarter the student wants to enroll in the class which year is the student talking about and eventually you want to know this su su ID of the students but probably we can assume that the su ID is already encoded in a conversation on the environment we're in so these are three vocabulary and we're also going to talk about turns four conversational assistance so so single turn conversation is when there is just a user returns and a response and multi turn is when there is several user utterances and conversational assistant utterances and you understand that Multi multi utterance conversations are harder to understand because we need to track context our assumption today will be that we work in an environment with limited intents and slots it means we can define to intents and for each of these two intents there are several slots that we want to fill in is going to make our life easier of course in practice you can have multi myriads of intents and slots and you the task becomes more complicated when you have more of those so my first question would be how to detect the intent based on the user utterance can you talk about what kind of data set you need to build in order to train a model to detect the intent or what type of network you need there is not a single good answer so go for it it's your brain zone so I think there's there's gonna be two options obviously because we have a we have a sequence coming in which is the user input we might want to use the recurrent neural network to encode long term dependencies or you might wanna use a convolutional net work actually convolutional networks have some benefits that's recurrent neural networks don't have and they they might work better for example if the intent we're looking for is always encoded in a small number of words somewhere in the input sequence because you will have a filter scanning that and the filter can detect the intent so if you have a filter that was trained in order to detect the intent inform another filter trained to detect the intent and roll then these two filter will detect the word enroll or the word I'm looking for and so on in order to detect the intent okay in terms of data what you probably need is pairs of user utterances along with the intent of the user so you would need to label the datasets like this one with X and input I want to so it's padded I want to enroll in CS 106a for winter 2019 to learn coding and this you will label it as enroll and notice that enroll here is a function so the label is actually noted as a function and the reason is because we can call this function in order to issue information another example is hi what are the undergraduate level history classes offered in spring 2018 and this would be label as in form so it's probably a two class classification or three classes if you want to add a third class that corresponds to other intents a user might want to use this chat bot for another intent that the chat bar wasn't built for so these are the classes enroll in inform and what's interesting is that if we identify that the intent of the user is enroll we probably want to call an API or to request information from another server and in this case it might be access because the the platform we use to enroll in classes is access and same to retrieve information in order to help the user about their classes we can probably call explore courses assuming that these these services have api's these surfaces have api's does that make sense and now the interesting part is that the unroll function might request some inputs that you have to identify those will be the slots same for the inform function okay so we could train a sequence classifier either convolutional or record and this we're not going to go into the details you've learnt it in the sequence models class how to detect the slots now so in terms of data it's going to look very similar to the previous one but we will have a sequence to sequence problem now where the user utterance will be a sequence of words and the slots tag will also be a sequence so for example show me the Tuesday fifth of December flights from paris to kuala lumpur if you were to build conversational assistance for flights booking then the label you want to have is probably something like that doesn't have to be exactly this but why they note zero for some of the words the sequence is be day by day or be dep be are are what do you think these correspond to and why do we need that we've probably you've probably seen that in in the sections a few weeks back so why do we denote these labels in a certain format and then the other one yeah yeah correct so I agree with what you said for day they departure arrival arrival so these words are encoding they departure and arrival how about the B and the I and the O someone has an idea is a beginning sometimes these things are moving more water yeah exactly please be be the notes beginning while I the notes in or inside and O out or output general so what happens here is that sometimes you would have a slot which might be filled by several words and not a single word and you want to be able to detect this entire chunk it's called chunking so you would use a special encoding in order to identify if this word is the beginning of a word that you want to fill in the slot or is the end or inside or out of the word you want to fill in the slot and then they departure and arrival or three possible slots that we want to fill in in order to be able to book the flight if you don't receive these slots you might want to have your chat BOTS request these slots later okay so another example in classes here can be daily departure arrival class like you want to travel in echo or business a number of passenger that you want to have on your flight if we were for our chat but here it would be hi I want to enroll in CS 106 3a for winter 2019 to learn coding and we will encode it by the beginning of the code of the class beginning of the quarter and beginning of the year that would be a possible encoding and then you will train using a probably a recurrent neural network and algorithm to predict all the tags that make sense so now we have already two models that are running on our chat bots one that is for the intense and one that is for the tags what do you think about joint training you think it's something we could do here and what do I mean by joint training yep trading on all the different codes like training for the defect border year and class are training in separate can work to each of themselves like the joint element of the Train not training for different codes no I was talking more about training for different tasks so infants and intent for enrolling intent from intent and and and slots tagging and here we have one intent classifier which takes an input sequence and outputs a single class and we have a slot tiger which takes the same input exactly the same input and tags every single word in the sequence so probably we can use joint training in order to train one network that might be able to do both and this network will be jointly trained with two different last functions one for the intent and one for the slaughter it's usually helpful to jointly train two networks especially in the earlier layers because you end up learning the same type of features that's that's interesting for natural language processing there is it yes boss function for them is it calculate both losses and something together or is there a trade-off between five minutes versus five seasons so the question is how would you describe the loss function in this joint training it was actually some two loss functions the two loss functions you are using you would just sum them and hope that's the backpropagation will train actually both networks and the networks will probably have a common base and then we'd be separated after so let's say you have a first lsdm layer that encode some information about your user utterance then this will give we give its output to two different networks which will will be trained separately okay and classes here are codes for the class quarter your NSU ID assuming su ID is already in the environment we will not need to request it so can you tell me how to acquire this data now that we've seen it so take take about a minute to discuss with your mates how to acquire that type of data and then answer on Monty okay so let's go over some of the answers Mechanical Turk have people manually collect annotate the data that's true so as we discussed earlier in the quarter this would be the method which is probably the more rigorous when it's applied with a specific labeling process and data collection process it will take more time so you would have to build a UI user interface for them to be able to label all these data which is not trivial in general Amazon Mechanical Turk a large number of Stanford students that works have a human chat assistant service user and enter the data in hand labeled data the ITU can start with hand labeling probably can also generate some data by substituting dead courses quarter and other tags oh that's a good idea so who wrote that someone wants to comment yeah that's a good idea so I repeat for the SCPD students we already have a bunch of possible dates we can easily find a list of dates you've done it in one assignment right where you were using the neural machine translation to transfer for human readable dates to machine readable dates so we have data sets of dates so we could use that we also have a list of courses that we can probably find on explore courses we know that they're not too many quarters and and we are we have probably databases for any other tagil at least of possible su ideas or like seven figures something like that so all numbers of seven figures hopefully and then we can have sentences with like blank spots where we insert this and we can generate a lot of data using this insertion scheme automated and every time we insert we can label we're going to see that I like this idea as well use a part of speech tagger identity recognition model to identify examples requests that are found elsewhere so one thing we discussed in section is that you have available models to do part of speech tagging right so why don't we use them these are trained really well and we could give our user utterances that we collected online and tagged them automatically using these good models of course it's not going to be perfect but we can at least get started with that and leverage a model that someone else has built to tag and label our dataset okay good ideas here so let's see the data generation process which is the most strategy to start with I would say we would have talking about the flight booking Virtual Assistants we would have a database of all the departure locations so whatever Paris London Kuala Lumpur and a lot of arrivals as well so these are lists of cities that have airports probably in the world and we will have a list of way to write days and also class business echo echo plus premium I don't know whatever you want and user occurrences and then what we will do is that we will pull a user a trends from the database such as this one I would like to book a flight from depth to arrival for in in business class let's say in class for this day and then we can plug in from dataset randomly the slots that make sense we can generate a lot of data using this process so this user utterance can be augmented in virtually tens or hundreds of different combinations so that's one way to augment your data set automatically and label it but you also need hand labelled data because you don't want your model to overfit to this specific type of user utterances okay and so on so same for our virtual assistant for the for the university hi I want to enroll in code for a quarter year and then we can insert from the database the quarter the year and the code of different classes so that we can train our network on that does this state augmentation make sense so these are common tricks you would seen in various papers and this is an example of one of them okay so we can label automatically when inserting and we can train a sequence to sequence model in order to fill in the slots okay so let's go on menti and start the competition which is the the most fun okay so let's get back to to to where we were we have a chat bot that is able to answer for sure I just enrolled you the way it does that is that it receives the user a chance I want to enroll in CS 106 a winter 2019 to learn coding it identifies the intent of the user using sequence classifier same type of network as you've built for the mo GFI assignment and then it also runs another algorithm which will fill in the slots and here we have all the slots needed we have the code for the class we have the quarter and we have the year thus unit ID is implicitly given so we're able to enroll to enroll the students by calling access with all these slots done now let's make it a little more complicated let's say the students say hi I want to enroll in CS 106 a 2 to learn coding so the difference between these utterance and the previous one example one is that you don't have all the slots you identify with your slots tagger that's CS 106 a is the coder of the class but you don't know the culture you don't know the year so you probably want your chat bot to get back to the to the student and say for which quarter would you like to enroll right and the student would hopefully say winter 2019 or winter and then you have to ask for the year 2019 and finally you can say for sure I just enrolled you so we're not making any assumption here on natural language generation you've worked on a Shakespeare assignment where you generate Shakespeare like sentences in fact a good shot boat would have this feature of generating language but for our purpose which can just hard code that when you're able to enroll the students you just say I just enrolled you when you were able to retrieve information from the students you would just write here is some information and you would plug in whatever the explore course is API sent back in a JSON okay so here the idea is this student utterance cannot be understood without context there is no way to understand winter 2019 if you don't have a context management system does it make sense so we want to build that context management system and then the question is how to handle context so there is a there's many there are many ways to do that and people are still searching for the best ways one way is to handle it with reinforcement learning as you mentioned earlier another way which is quite intuitive and and closer to what we've seen together in sequence model in the module in the module five is this type of architectures which is which is taken from Chen a tall and twin memory networks with knowledge carryover for multi turn spoken language understanding so now you're able to understand what multi-turn means and end-to-end memory network so what happens here just to describe it is we will save all the history occurrences it means from the beginning of the conversation we will record all the utterances and messages exchanged between the user and the assistant we will keep it in a storage that will be call history utterances see is the current accounts so let's say the student says winter 2019 this is the utterance of the student at this point we will run this see and of course like it's its these entrants will be run into an RNN and we will get back to an encoding of this sentence so there's all the like word embedding stuff that I don't describe but your guys are used to it so we use word embeddings we run it - we run it to an RNN and we get back the encoding of the user utterance and this encoding will then be compared to what we have in memory so all the user utterances that we had in memory are also going to be run in an RNN that will encode their information in vectors these vectors are going to be put in a memory representation and are you will be directly inner product we will have an inner product from ru with all the memories and this pooled into a soft Max will give us a vector of attention that you guys should be used to now a knowledge attention distribution telling us what's the relation where should we put our attention in the memory for this specific utterance that make sense so simple inner product soft max gives us a series of weights here ok then we get awaited sum of all these attention weights multiplied by the memory and it gives us a vector that encodes the relevance of the memory regarding our current utterance this is then summed and run into a simple matrix multiplication to get an output vector which would be run in a slot stagnant sequence and usually it's experimental but they pass also the current utterance to the RN and tiger and the orion tiger comes up with a slot tagging so using that you can understand that winter 2019 is actually the target for the slots quarter and here because you have this memory network does it make sense so this is another type of attention models you want to use and this memory network sim can be used to manage some contexts for the slots tagger okay so just to recap we have our example hi I want to enroll in a class and we detect the intents which is enrolled we also detect that there are some slots missing because we know we know that the enroll function needs the court earlier and the class in order to be able to be called so we have to ask for those so we probably hard-coded the fact that if you don't have the quarter the Year and the class you probably want to first ask for the class or the quarter or the year then you can you can get back to the person by asking which class you want to enroll in the person would get back to you you will use your memory network to understand that CS 230 is a slots for the enroll in tenth you would fill it in so now we have our intent with the class equals CS 230 and we have our slots quarter in year which are to be filled the chat BOTS get bags for which quarter and hopefully the student gives you the year at the same time and you can fill in the slots and then you are enrolled in CS 234 winter 2019 yeah should be spring yeah this shot boy is not trained very well okay any questions on that so this is a very simple case of a conversational assistant just to give you some ideas there are some paper listed in the presentation that you can go to in order to get more advanced research insights but the idea here is that we're limited to a specific intent to two specific intents and a few slots what do you think we would need if we didn't restrict ourselves to specific intents and slots [Applause] it's a very complicated tough one industrial way to do it is to use a knowledge graph what it means is let's say you're an e-commerce platform you probably have from your platform a knowledge graph oil of all the items on the platform with connections among them like let's say color off let's say you have a shoe a shoe is a slot that might be the object for the intents I want to buy something right the shoe can have several attributes like color or size or men or women like gender and all these are connected together in a gem in in in a gigantic knowledge graph and you will follow the path of this knowledge graph following some probabilistic probabilities so when we detect the intent of the user which is by something we could identify the object I want to buy a shoe and then based on our knowledge graph it says that the next question that we should ask or the next slots that we need to feel is which brand do you want your shoe to be and so the knowledge graph is going to tell you with 60% probability go to brand and ask about the brand once you're there what other information you need in order to be able to retrieve five results for the user to review and so on so the knowledge graph is something in this field that can be used in order to have multiple intense multiple slots for every intent okay and at the end we can make an API call here with CS 2:30 quarter winter 2019 quarter winter year 2019 and the Sui D okay another question I had for you I've had for you I have for you is how to evaluate the performance of a chat bot what do you think of that so there are common ways to to evaluate several part of your pipeline like how is your slot Tiger doing how is your intent classifier do you can use metrics such as precision and recall f1 score for the mix of both and report those in order to compare how this module is doing for the chat bot but ultimately you want to understand how good is your chat bot overall so some experiments are done and this is a paper of a deep reinforcement learning chat bot built in 2017 by the millah serve an adult and what they did is that they used Mechanical Turk in order to evaluate their chat BOTS and also build a scoring system for their reinforcement learning chat bot so I'm reading for you the instructions you will be presented with a conversation between two speakers speaker a and B you will also be presented with four potential responses from one of the speakers for this dialogue and the task is for you to rate each of the responses between one inappropriate doesn't make sense to five highly appropriate and interesting based on how appropriate the response is to continue the conversation three is neutral and if two responses are equally appropriate you should give them the same score and if you see response that is not in English please give a one score so here is what happens from a user perspective you would have a conversation you need to work on your English why do you say that about me well your English is very poor so this is the conversation and then the response one is but English is my native language response to is what other reasons come to mind response three is here is a funny fact go is the shortest complete sentence in the English language and then the fourth response is by doggy so obviously you have to you have to score you have to score these these responses according to what you think how relevant they are and then and then these scores will be used either for the scoring system of the deep reinforcement learning chat bot or it can be used to evaluate how good is your chat bot compare to other channels so maybe each of these responds come from a different model does that make sense so these are a few ways there another way which is asking for the opinion of the user on different responses so let's say you you're a user and you are you are comparing to chat BOTS you can give your opinion on which one you think is more natural and you would ask a lot of users to do that to compare two or three chat BOTS together and also compare them to natural language from a human and then by doing a lot of mean opinion score experiments you can evaluate which chat bots are better than the others just comparing them one-on-one okay now getting back to one thing that the student mentioned earlier is what if we want to have a vocal assistant so right now our assistant is not vocal it's just text what other things do we need to build in order to make it a vocal assistant we're not going to go into in the details but roughly you would need a speech-to-text system which will take the voice of a user convert it into a text and this as you've seen in the sequence model class has different step in the pipeline and the speech to text so any text to speech that takes the text from the chat pod and convert it into a voice so that's how you have like virtual assistants talking to us is because they have a text-to-speech system running and these are three papers the first one is this speech - from values team which built an end-to-end speech recognition in English and Mandarin and the two others are text to speech synthesis so one came up in February 2018 which is the tacit Ron - and the second one is wavenet which is a very popular generative models and these are these are far beyond the scope of the class but you can study them in other classes at Stanford which are more specific to speech okay class project advice so this Friday we're going to go over again the rubrics of what we look at when we when we great projects and here is the list of things we would look at so make sure you have a very good problem description when you read papers you see that there is a very good abstract we expect you to give us a very good abstract so that when we read it we get a good understanding of the paper hyper parameter tuning always report what you do you don't need to to be very exhaustive but but you can just tell us what hyper parameters you've been choosing and which ones you've been testing and why they didn't work the right thing we look for typos this is common in the grading scheme typos a clear language so review it peer review your paper explanation of choice in this unit this is a very important part we expect you to explain the decisions you're making so we don't want you to to tell us I've taken I've made that decision just without explaining but rather tell us there is this paper that mentioned that this architecture worked well on that specific task I've tried three architectures here are my hyper parameters and results that's why I'm gonna I'm going to dig more into that one and so on data cleaning and pre-processing if applicable to your project explain it how much code you wrote on your own it's important to us and please submit your github or privately to the TAS when you submit your projects gonna make it easier for us to review the code insights and discussions include the next steps what would you have done if you had more time and also interpret your results don't just give results without explanation but rather try to extract information from these results and you can also drive your next steps explanation results are important but if you don't have the results you expected it's fine we will look at how much work you've done and some tasks are very complicated we don't expect you to beat state-of-the-art on every single as some of you are going to be state-of-the-art hopefully but those of you who didn't still report all your results and explained why it didn't work give references and also penalty for more than five pages so if you're working on a on a theoretical project you can add additional pages as appendix you can also add appendix for your project but the core has to be five pages and for the final poster presentation which will happen not this Friday next one we will ask you to pitch your project in three minutes so not everyone in the group has to talk but at least one person has to talk in and we prefer if several of you talk in the project but you have three minutes to pitch your project so prepare the pitch in advance and you will have two minutes of questions from the TA which are also part of the grade okay finally what's next after CS 2:30 so there's a ton of class at Stanford we're in a good learning environment which is just super next steps can be in the university classes you can take in natural language processing and computer vision but also classes from different departments deep generative models is a good way to learn about text to speech for example or gans probably see graphical models is also very important class in the industry s Department of course if you haven't taken it yet CS two to nine machine learning or CS two to nine a applied machine learning or to go to to learn machine learning reinforcement learning is a class where you can you can delve more into Q learning policy gradients and all these methods that sometime use deep learning so we're going to publish that list in case you want to check it but these are examples of classes you can take and of course there are other classes that tournament not mention here that might be relevant to pursue your learning in in deep deep learning in machine learning okay that said I'm going to to give the microphone to Andrew for closing remarks and yeah good luck on your projects so we'll see you on Friday for the discussion sections and next week for the final project charlie microphone oh so all right here we are at the end of this class nearly at the end of this class um you know dear new Rip's conference is taking place right now formerly the nips conference of a renamed to new ribs and I remember it was ten years ago that at that time a piece tune in the diet bridge a trainer presents the paper workshop paper at nips telling people hey consider using GPUs and crew there which is a new thing that Nvidia I just published to train your networks and we've done that work on a GPU server that Ian could fellow the creator of Gans have built in his dorm room when he was an undergrad at Stanford so our first few server was built in the Stanford undergrads dorm and I remember sitting down with Jeff Fenton and food called and saying hey check out the screw the thing and Jeff said no but GPU program is really hard but then but then but but oh maybe this crew the thing looks promising and I tell the story because I want you to know as Stanford students that your work can matter right when younger fellow built that GPU server in his dorm room I had no idea if he realized that a decade later you know someone would be winning several hundred hours of AWS credits to try and bigger deep learning algorithms but I think as Stanford here at Stanford University were very much at the heart of the technology world I think Silicon Valley is here to a large pot because Stanford University is here and we live in a world where with the superpowers that you now have you have a lot of opportunities to do new and exciting work which may or may not seem like your mats in the short run maybe even seem constant in the short run be concerti have a huge impact in the long run as a couple weekends ago so um my wife we roast coffee beans at home right my wife buys raw coffee beans and then we actually roast them and camera or my wife tends to roast em and its really cheap popcorn popper that we have right now so I don't know I don't have much coffee you guys drink I drink a lot of coffee and so you know so Carol byesies being coffee bean see she puts them in this like cheap popcorn popper which is made for popping popcorn not made for rose and coffee beans this is one of the standard cheap ways to roast coffee beans and and I love my wife I drink the coffee she makes but sometimes she burns the coffee beans so I found this article on the internet from a former student that written an article and how they use machine learning to roast to optimize the roasting of coffee beans as I forwarded to the Carol she wasn't very happy about that and but I raised this is another example of how all of you you know I would never have thought of applying machine learning to roasting coffee beans it's just I mean you know I like my coffee but it had never occurred to me to do that but someone taking a machine learning class like you guys are go ahead and come up with a better way of roasting coffee beans using learning algorithms and again I think you I don't know this picker person that wrote this blog post was thinking building a business all of it I I don't know there might be a business that they might not or it might be just a fun personal hope you actually don't know but all of you with these skills have that opportunity and then again earlier this week was it Monday night a group of us we were actually in the gates building where a bunch of students actually room the yeah for health care boot camp that can alluded to yeah we're going over some to final projects for the students and they you have to healthcare boot camp where we're working on and I think and I think actually met several people including Artie right when she first participates in a much earlier version of that okay bootcamp secret you can also RT of others what you interested but they're um one of the masses students I was working with patients in primary record I think you guys been in this cause he was demoing an app where you could pull up an x-ray film and take a picture with your cell phone upload the picture to a website and have a website you know read the x-ray and suggest the diagnosis for our patients most of planning today has insufficient access to radiology services there are many countries where it costs you three months of salary to go and get an x-ray taken and then maybe try to find the radiologist to read it but most the planet billions of people on this planet do not have sufficient services and radiology services and while the standards in AI for healthcare bootcamp is still a research project actually you record on the checks net paper won't you answer yeah right yes I'll do a shared co-author on all these papers it is a game maybe work done here at Stanford that you know is taking the first steps to what maybe if we can improve the deep learning algorithms posterity hurdles you know proof safety maybe that type of work happening here at Stanford doing that for health care maybe that would have a transformative effect on how healthcare is run around the world so um the skills that you guys now have are very unique set of skills they're not that many people on the planet today that can apply learning algorithms and deep learning arms the way that you can and you can tell a lot idea as you learned in this class where you know invented in the last year or two so this is just not yet been time for these ideas even become widespread and if I look at a lot of the most pressing problems facing society be a lack of access to health care or um science I spent a lot of times think about climate change and I think if you look at the the can we improve access to education can we just make whole society run more efficiently I think that all of you have the skills to do very unique projects and I hope that as you graduate from this class I'm sure some of you will great businesses may make all the money that's great and and I hope that all of you will also take the unique skills you have to work on projects that matter the most to other people that that help other people because if one of you does not take your skills to do something meaningful then there's probably some very meaningful project that just no one is working on because I think the number of meaningful projects I think actually greatly exceeds the number of people in the world today that are skilled at deep learning which is why all of you have a unique opportunity to take these algorithms that you now know about to apply to anything from developing novel chatbots to improving healthcare - I guess my team at landing a is improving manufacturing agriculture also some healthcare to maybe helping with climate change to helping with global education and any other problems that that really matter so I hope I hope maybe I hope that all of you go on to to do work that matters and then one last story you know a few a few months ago now um I got to drive a tractor right it was very big a little bit scary it feels like a bigger machine then I should be qualified to drive it's a huge factor and and it turns out that when you drive a tractor so it turns out when you drive a normal car you know is really clear which way is up on the steering wheel right here you point the Siringo up and you know your car drives forward for the tractor that I got to drive this huge tractor it turns out that dumb as this giant steering wheel and to drive straight the giant steering wheel was just oriented at some weird angle and to turn right you turn the clockwise to turn left you turn anti-clockwise and that was that right so there's a lot of fun and maybe in addition to and and it was just fun you know I drove a tractor made a u-turn drove back to where started did not hit anyone you know there's no accident and they're like down off this giant rafter and maybe I tell that story because I hope that even while you are doing this important may be beneficial to other people sense of work I hope I hope you also have fun I think that I feel really privileged that is a machine learning engineer um I some days I get to go drive a tractor right and and I hope that and one of the most exciting things you know I feel like um a lot of the best a lot of biggest untapped opportunities for AI like outside the software industry I'm very proud of the work that helped you you know leaving the Google rain team being AI do and I think more people should do that type of work and I think that here in Silicon Valley many of you will get jobs in the tech sector and that's great we need more people to do that and I also think that if you look at all of human activity the majority of human activity is actually outside the software industry the majority of global GDP growth or global GDP is actually outside the software industry and I would just urge you as you are considering what is the most meaningful work so consider the software industry but also look outside the software industry because I think really the biggest untapped opportunities for AI lie outside I think lie outside the software industry and and we can't have everyone doing the same thing right there's actually not a healthy plan and if everyone you know works on improved web search or improve or even improved healthcare I think we need a world where all of you have these skills share these skills teach other people why should learn and go out to do this work that hopefully affects the software industry affects other industries affects profit nonprofit affects government but uses these AI capabilities to lift up the whole human race and then finally the last thing wants to say on behalf of Ken and me and the whole teaching team is I wanted to thank you for your hard work on this cause I know that you know watching the videos doing the homeworks on the website me to the tears section you know that many of you have put a lot of work in this cause and it wasn't so long ago I guess when I was a student you know staying at home doing this homework or trying to derive that math thing I'd also take some online courses myself so it's actually not so long ago that you know I was sitting a computer much like you kind of watch some Coursera videos and then click on this click on that and answer things online and and and III appreciate Ken and I and the whole teaching team appreciate all the hard work you put into this and I hope also that you got a lot out of your hard work and that you will take these rare and unique skills you now have to go on and and when you drive from Stanford or further oh or for the whole viewers I guess for those are home viewers as was for the in costume viewers that you take these Rascals you now have them ain't going to do work that matters and go on to do working cause other people so with that I look forward to seeing all of your projects at the poster session and apologize in advance we won't be the really get a deep understanding in three minutes we don't worry we do read your project reports but I look forward to seeing hope you are looking forward also to seeing everyone else's work on the poster session boom with that let me just say on behalf of the enemy and the whole teaching team thank you all very much [Applause]