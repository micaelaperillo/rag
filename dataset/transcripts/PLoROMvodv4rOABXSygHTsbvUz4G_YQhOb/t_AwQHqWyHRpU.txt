hello everyone welcome to the second lecture for yes 2:30 so as I as I said earlier you can go on Monty comm from your smartphones or your computers and enter this code 84 5709 we will use this tool for interactive questions during the lecture and we will also use it to to track attendance I'll add it at the end of the lecture but if you have time do it now let's start the lecture or you guys are doing that ok so today's lecture is going to be about deep learning intuition and the goal is to give you a systematic way to think about projects everything related to deep learning it includes how to collect your data how to label your data how to choose an architecture but also how to design a proper loss function to optimize so all these decisions are decisions you are going to have to do during your projects and will try to give you here an overview of this systematic way of thinking for different projects it's going to be high-level more than other lectures but we hope it gives you a good start for your project we'll start with a 10 minute recap on what you've seen in the two first in the first week about neural networks so as you know you can think of machine learning deep learning in general as modeling a function that takes an input that can be an image a speech a natural language or a CSV file give it to a box and get an output that can be classification is it a cat's zero is there a cat on this image output one or is there no cat on this image output zero and I think a good way to remember what is the model is to define it as architecture plus parameters architecture is the design that you choose so logistic regression is the first one you've seen you will see shallow neural networks deep neural networks then you will see convolutional neural networks and recurrent neural networks so these are all types of architectures and you can choose to make them deeper or shallower parameters are the core parts there numbers that major function take these cats as input and convert it to an output so these are millions of numbers and the goal of machine learning deep learning is to find all these numbers so we're all trying hard to find numbers basically millions of numbers in matrices if you give this cat and you forward propagated so we propagate it through the model to get an output you will have to compare this output to the ground truth the function used to do so is called the loss function you've seen an example of a loss function this week that is the logistic loss function we will see more loss functions later on computing the gradient of this loss function is going to tell you how much should I move my parameters in order to update in order in order to make the loss go down so in order to make this function recognize cats better than before you do that many many times until you find the right parameters to plug in your architecture you can then give your cats and get an output what is very interesting in the printing is that many things can change you can change the input we talked about natural language speech structure and structured data in general you can change the output it can be a classification algorithm it can be a multi-class algorithm I can ask you give me the breed of the cat instead of asking you give me just the cat which makes the problem more complicated it can also be a regression problem I give you the cat and I ask you give me the age of the cat she's much more complicated again does that make sense ok another thing that can change is your architecture we talked about it earlier and finally the last function I think the last function is something that that people struggle with to understand what cost function to to choose for a specific project and we're going to put a huge emphasis on that today ok and of course in the architecture you can change the activation functions in this optimization loop you can choose a specific optimizers we're going to see in about three weeks all the optimizers that can be atom stochastic gradient descent batch gradient descent rmsprop and momentum and finally all the hyper parameters what is the learning rate of this loop what is the that I'm using for my optimization we're going to see all that together but there's a bunch of things that can change in this scheme any questions on that in general so far so good okay so let's take the first architecture that we've seen together logistic regression as we know an image in computer science can be represented by 3d matrix each matrix represent a certain color RGB red green blue we can take all these numbers from these 3d matrix and put it in a vector we flatten it in order to give it to our logistic regression we for propagate it we multiply it by W which is our parameter and B which is our bias give it to a sigmoid function and get an output if the network is trained properly we should get a number that is more than 0.5 here to tell us that there is a cut in this image so this is the basic scale now my question for you is if I want to do the same thing but I want to have a classifier that can classify several animals so on the image there could be a giraffe there could be an elephant or there could be a cat how would you modify this architecture yes exactly so that's a good point we could add several units so several neurons one for each animal and we will call it multi logistic regression so it could be something like that so we have a fully connection here before we were all all the inputs were connected to this neuron and now we added two neurons and each neuron is going to be responsible for one animal how do we know which neuron is responsible for which animal is the network going to figure it out on its own or do we have to help it exactly the label is important so what is going to tell your model this neuron should focus on cat dispersion through focus on elephant dissonance which works on giraffe is the way you label your data so how should we label this data now if we were to do this specific task any ideas one harvester okay so one hard vector means a vector with all zeros and one one any other ideas one two three so I assume you you say that each integer would correspond to a circle anymore okay any other ideas modifying the last function you want to put more weight on one anymore so you modify the last function I see we don't want hard concretely so I agree with the one hot encoding I think there's a downside to do one hot encoding what is the downside of the one cuts on Cody yes so you're saying that the daytime though if we have a lot of animals the data the labels only contain zero and one one so there's a huge imbalance I don't think that's an issue because these neurons are independent from each other right now so yeah it could run into an issue if you have really a lot of animals that's true but there is another problem with it the problem is that do you think if you want if you want hot and code your labels you would be able to detect an image with a giraffe and an elephant on the image you will not be able to do so you need the multi hots encoding so in this case if there is a cat on image I will use a one hot I would say 0 1 0 as my label but if I have a dog and a cat on the image I would say 1 1 0 okay the one hot encoding works very well when you have the constraint of having only one animal per image and in this case you would not use an activation function called sigmoid you would use another one which is softmax yeah the softmax function we're going to see together and for those of you - 2 to 9 you probably heard of it ok so what I wanted to explain here is the way you choose your labeling is very important and it's a decision you should make prior to start the project ok in terms of notation in the in this class we're going to use the following a square bracket 1 we denote all the activations of the first layer so the square bracket we denote the layer and the lower script we denote the index of the neuron in the layer ok and of course you can stack this neuron on top of each other to make the network more complex and depending on the task you're solving ok now the concept I wanted to introduce in this recap was the concept of encoding you probably some of you have probably seen this image before if you have a network that is not too shallow you would notice that what the first neurons see were very precise representation of the data so there are pixel level representations of the data x3i is probably one of the three channels of the 3d matrix just one number so what this neuron sees is going to be a pixel level representation of the image okay what this neuron see is the second layer the one in the hidden layer is going to see the representation outputted by all the neurons in the first layer these are going to be more high-level more complex because the first neurons will see pixels they're gonna output a little more detailed information like I found an edge here I found an edge there and so on give it to the second layer the second layer is going to see more complex information it's going to give it to the third layer which is going to assemble some high-level complex features that could be eyes nose mouth depending on what network you've been training so this is an extraction of what's happening in each layer when the network was trained on face recognition yes yes oh I see like give you a fully connected network but that's true this type of visuals are more observed in convolutional neural networks because these are filters but this happens also in this type of network is just harder to visualize okay so this is what we call an encoding it means if I extract the information from this layer so all the numbers that are coming out of these edges I extract them I will have a complex representation of my input data if I extract the numbers that are at the end of the first layer I will have a lower level representation of my data that might be edges okay we're going to use these encoding throughout this lecture any questions on that okay so let's build intuition on concrete applications we're going to start with a short warm-up with the day-night classification and then quickly move to face verification and face recognition and after that we'll do some art generation and finish with a trigger word detection if we have time we will talk about how to ship a model which is shipping architecture plus parameters okay we're done fascist as I said on the architecture that lost the training strategy to help you make decisions during your project so let's start with the first game we're given an image and we have to build a network that tells us if the image is taken during the day label zero or was taken at night label one so first question is what data set do we need to collect okay labeled image is captured during the day and during the night I agree so probably oh yeah let me ask the question how many images that was wrong acting how many images like how do you get this number can someone give me an estimate of how many images you need in order to solve this problem and explain how you get this s true so you're saying a number similar to a number of parameters you have in the network so I think it's better to think of it the other way around the network comes after so right now you don't know what network you will use so you cannot decide the number of data points based on your parameters later on based on how your network is flexible you can add more data and that's probably what you meant but at first you want to get you want to get a number yeah more images than pixels within an image I I don't think that that that has anything to do with the pixels in image you can have a very simple task like you have only images that are red and green and you want to classify red and green the image can be giant you can have a lot of pixels it's not gonna change the number of data points in it okay so you're talking about computation resources so the more images we have probably the more computation resources we will need so to me yeah there's something like that I think in general you want to try to gauge the complexity of the task so let's say we did a problem that was cat recognition detective there is a cat on an image or not in this problem we remember that with 10,000 images we managed to train a pretty good classifier how do you compare this problem to the cat's problem you think it's easier or harder easier yeah I agree that's probably easier so in terms of complexity this task looks less complex than the cat recognition task so you will probably need less data that's a rule of thumb the second rule of thumb and why I get to this image is what do we exactly want to do do we want to classify pictures that were taken outside which seems even easier or do we want also the network to classify complicated pictures what what do I mean by complicated pictures inside your house so like let's say on a picture you have a window on the right side a human will be able to say it's the day because I see the window but for the network is going to take a much longer to learn that much longer than for pictures taken outside what else what are other complicated don't I like sunrise sunset in general it's complicated because you have to define it and you have to teach your network what what does that mean is it night or day okay so depending on what task you want to solve it's going to tell you if you need more data or less data I think for this task if you take outside pictures 10,000 images is going to be enough but if you want the network to detect indoor as well you probably need a hundred thousand images something and this is based on comparing with projects you did in the past so it's going to come with experience now as you know when you have a dataset you need to split it between trained validation and test sets some of you have heard that we're going to sit together even more you need to train your network on a specific set and test another one how do you think you should split these 10,000 images 50/50 between training tests 8020 I think we would go more towards 8020 because the test set is made for analyzed to analyze if your network is doing well on real-world data or not I think 2,000 images is enough to get that sense probably and you want to put complicated examples in this data set this way so I would go towards 8020 and the bigger the data set the more I would put in the train set so if I have 1 million images I would put even more like 98% maybe in the train set and 2% to test my model okay now I wrote bias here what do I mean by bias yes you need to correct balance between classes you don't want to give 9000 dart images in 1,000 day images you want a balance between these two to teach your network to recognize both classes okay what should be the input of your network [Music] the pixel image yeah so this is an example of a pixel image it's the Louvre Museum during the day harder question what should be the resolution of this image and why do we care that's great so she said just move it to for SCPD students as well as low as you can in order to achieve good results why do we want low resolution is because in terms of computation is going to be better remember if I have a 32 by 32 image how many pixels there are if it's color I have 32 times 32 times 3 if I have 400 by 400 I have 400 by 400 by 3 it's a lot more so I want to minimize the resolution in order to still be able to achieve good performance so what does it mean to still achieve good performance how do I get this number okay similar resolution as you expect the algorithm in real life to work on yet probably I agree what else what other rule of thumb can you use in order to choose this resolution great idea compared to human performance so what I do so there's one way to do it which is the brute force way I would say we will train models on different resolutions and then compare their results or you can be smart and use human performance as a comparison so I will print this image or several images like this in different resolutions on paper and I would go see humans and say classify those classify those and classify those and I would compare human performance on all these three types of resolution in order to decide what's the minimum resolution that I can use in order to get perfect human performance so by doing that I got that 64 by 64 by 3 was enough resolution for a human to detect if an image is taken during the day or during the night and this is a pretty small resolution in imaging but it seems like a small like an easy task if you have to find a breed of a cat you probably need more because some cats are very look very alike and you need a high resolution to distinguish them and maybe training for the human as well I know only three bits of cat so I wouldn't be able to do it anyway what should be the output of the model labels so y equals zero for day y call one for night I agree what should be the last activation of the network the last function sigmoid we saw that see mo it takes a number between plus infinity minus infinity and plus infinity puts it between 0 and 1 so that we can interpret it as a probability what architecture would you use fully connected or convolutional I think later this quarter you will see that convolutional perform well in imaging so we would directly use a convolution writing a shallow Network fully connected or convolutional would do the job pretty well you don't need a deep network because you gauge the complexity of this task and what should be the loss function finally the log likelihoods it's also called the logistic loss that's the one you're talking about so the way you get this number and you'll prove it in in CS two to nine we're not going to prove it here but basically you interpret your data in a probabilistic way and you take the maximum likelihood estimation of the data which gives you this formula for those of you who did the math you can ask in office hours days are going to help you understand it more properly okay and of course this means that if y equals zero what y hat the prediction to be close to zero if y call one we want Y hat the prediction to be close to one okay so this was the warm now we're going to delve into face verification any question on the inline classification yes so your the question is about how you choose the size of the test set versus the train set in general you would first say how many images do I need or data points in order to be able to understand what my model do in the real world this can depend on the task like if I talk about if I if I tell you about speech recognition you want to figure out if your model is doing well for all accents in the world so your test set might be very big and very distributed in this case you might have a few examples that are during the day few during the night and a few at dawn and sunset sunrise and also indoor few of those is going to give you a number so there's no good number there is like you have to gauge it okay one more question yeah that's a good question so how do you choose the last function we're going to see in the next in the next slides how to choose loss functions but for this one specifically you choose this one because it's a it's a convex function for classification problem it's easier to optimize than other loss functions so there is a proof but but I will not go over it here if you know the l1 loss that compares Y to Y hat this one is harder to optimize for a classification problem we would use it for regression problems okay so our new game is the school wants to use face verification to validate student IDs in facilities like the gym so you know when you enter the gym you swipe your ID and then I guess the person sees your face on the screen based on this ID and looks at your face in real and compares let's say so now we want to put a camera and have you swipe and the camera is going to compare this image to the image in the database does that make sense to let you in or not so what what data set do we need to solve this problem what should we collect okay between the ID and the image yeah so probably schools have databases because when you enter the school you submit your image and you're sorry given a card an ID so you have this mapping okay what else doing it so pictures of every student label with their names that's what you say so this is a picture of birth home is the picture when he was younger and that's the one he gave to the school when he arrived what should be the input of our model is it this picture more photos of him I'm asking just like the input of the model like we probably need more photos of him as well but what's what's going to be the image we give to the model exactly the person standing in front of the camera when entering the gym so this is the entrance of the gym and Bergeron's trying to enter the gym so it's him okay what should be the resolution those of you who have done projects in imaging what do you think should be the resolution in 256 by 256 and your other idea for free size I think in general you will go over 400 so 400 by 400 what's the reason why do we need 64 for 4 day night and and 400 for face verification yeah yeah there's more details to detect so like distance between the eyes probably size of the nose mouth general general features of the face these are harder to detect for a 64 by 64 image and you can test it you can go outside and show two pictures of people that look like each other and ask people can you differentiate those two person or not and you'll see that with less than that sometimes it's people are struggling is color important that's a good question we should have talked about it in day and night actually is color important because if you remove the color you basically divide by three the number of pixels right so if we could do it without color we would do it without color in this case color is going to be important because probably you want your camera to work in different settings day/night as well so the luminosity is different the brightness and also we all have different colors and we need to all be detected compared to each other I might go somewhere in an island and come back you know full of color but but I still want to be able to access the gym outputs what should be the output I think if you have unlimited computational power you will take more resolution but that's the trade-off between computational results so output is going to be 1 if it's you and 0 if it's not you in which case they would not let you in okay now the question is what architecture should be used to solve this problem now that we collected the data set of mapping between student IDs and images you know how do you know how many images you need to train the network you don't know you can find an estimate it's going to depend on your architecture but in general the more complex the task the more data you will need and we will see something called error analysis in about 4 weeks which is once your network works you're going to give it a lot of examples detect which examples are misclassified by your network and you're going to add more of these in the training set so you're going to boost your data set ok talking about the architecture if I ask you what's the easiest way to compare two images what would you do like these two images the database image and the input image some sort of hash value means I have chickens standardized functional ok taking him take this run it into a specific function take this run it into a specific function and comparator 2 values that's correct that's a good idea and the more basic one is just computed distance between the pixels just compute the distance between the pixels and you get if it's the same person or not it doesn't work and a few reasons are the background lighting can be different and so if I do this - this this pixel which is let's say dark is going to have a value of 0 this pixel which is white is going to have a value of 255 the distance is gigantic but it's still the same person it's a problem person can wear makeup I can grow there can be younger on a picture the ID can be outdated so it doesn't work to just compare these two pictures together we need to find a function that we will apply this these two images to and will give us a more a better representation of the image so that's what we're going to do now what we're going to do is that will encode information use the encoding that we talked about of the picture in the vector so we want a vector that would represent features like distance between eyes nose mouth color all these type of stuff hair in a vector so this is the picture of weft Hong from the ID we would run it to a network and we hopefully can find a good encoding of this network then we will run the picture of Beth home add the facility run it in the deep network get another vector and hopefully if we train the network properly these two vector should be close to each other let's say we have a threshold that is 0.5 0.4 is the distance between these two it's less than the threshold so I would say about how is the right person it's you does this scheme make chain make sense what does the 128th vector below so the question can I say that the third entry corresponds to something specific it's complicated to say but depending on what network you choose and the training process you choose it will give you a different network a different vector so that's what we're going to talk about now the question is how do I know that this vector is good like right now if I take a random network I give my image to it it's gonna output around a vector this vector is not going to contain any useful information I want to make sure that this information is useful and that's how I will design my loss function ok so just to recap with the other all student faces encoding in a database once we have this and given a new picture we compute the distance between between the new picture and all the vectors in the database if we find a match oh sorry we compare this vector of the input image with the vector corresponding to the ID image if it's small we consider that is the same person ok now talking about the loss and the training to figure out is this vector corresponds to something meaningful first we need more data because we need our model to understand in general the features of the face and a university that has a thousand students is probably not going to be enough to have a thousand image in order to push a model to understand all the features of the face instead we will go online find open datasets with millions of pictures of faces and help the model learned from these faces to then use it inside the facility was a question in the back like we did with Andrea but every student is a one that's another option so the question is why can't you use the one hot encoding we could be the classifier that has n output neurons and corresponding to the number of students in the school and you take an image you run it to the network is going to tell you which student it is what's the issue with that every year students enter the school you will have to modify your network every year because you have more students and you need a higher output vector a larger output vector we don't want to retrain all the time our networks okay so what's what what we really want if we want to put it in words is that oh there's a mistake here what we really want is if I give you two pictures of the same person I want a similar encoding I want the vector to be similar if I give you two pictures of different persons I want different encodings I want the vector to be very different and we're going to rely on these two assumptions and these two dots in order to generate our last function by giving it triplets triplets means three pictures one that we call anchor that is the person a person one that we call positive that is the same person as the anchor but a different picture of that person and the third one that we call negative that is a picture of someone else and now what we want to do is to minimize the encoding distance between the anchor and the positive and maximize the encoding distance between the anchor of the neck and the negative thus these two thoughts make sense so now my question for you is what should be the loss function what should be the loss function so please go on menti and enter the code and there are three options here a B and C choose which of these you think should be the right loss function to use for this problem now you have it on your phone as well like issue it small on the screen but you can see it on on it's cut off it's better here [Music] eight four five seven zero nine can you see it on your phone so by end of a I mean the encoding vector of the anchor my anchor fee I mean the including vector of the positive image after you run them to the network okay 30 more seconds okay all right 20 more seconds okay let's see what we have okay so two thirds of the people think that that it's the first answer a so I read it for everyone the last is equal to the l2 distance between the encoding of a and the encoding of P minus the l2 distance between the encoding of a and the encoding of n so someone who has answered this do you want to give a an explanation yes we're trying to minimize the first difference between a and the positive and you tend to maximize difference between a and the negative when you subtract so the second part can be responsible to love minimize minimize yes that's correct so what you said I repeat it for this video students we want to maximize the distance between the encoding of a and the encoding of the negative that's why we have a minus sign here because we want the loss to go down and to go down we put a minus sign and we maximize this term and on the other hand we want to minimize the other term because it's a positive term okay so are you agree we don't sir okay that was the first time you use this tool it's gonna be quicker next time okay so we have we have figured out what the last function should be and now think about it now that we designed our last function we're able to use an optimization algorithm run an image in the network sorry run run three images in the network like that gets three outputs encoding of a encoding of T encoding of n compute the loss take the gradients of the loss and update the parameters in order to minimize the loss hopefully after doing that many times we would get an encoding that represents features of the face because the network will have to figure out who are the same people who are different people does that make sense this is called the triplet loss and I cheated a little bit in the in the quiz I didn't write this alpha the true loss function contains a small alpha you know why yes so you don't have negative loss yeah that that's not exactly the role of the Alpha in order to not have negative loss what you can do is to use a maximum of the loss and zero and train on the maximum of the loss and zero but there is another reason why we have this alpha yes which one you prefer based on false negative unfortunate if no it's not about that so sometimes you have an alpha in loss function to put a weight on some classes but this is an additional alpha it's not a multiplicative alpha so it has nothing to do with that yeah to penalize large weights are you talking about generalization if we had weights in this formula next to the Alpha like alpha times the norm of the weights this would be regularization but here this term doesn't penalize weight it's not going to affect the gradient it's not going to affect it's not gonna affect the weights but the reason we have it here is because let's say the encoding function is let's say the encoding function is just a function 0 what we're going to have is that we're going to have encoding of a equals 0 minus 0 and here zero minus zero and so we will have basically a perfect loss of zero and we still didn't train our network we just learned affection No so this alpha is called the margin and it pushes your network to learn something meaningful in order to to to stabilize itself on on zeros okay yeah so it also has to do with the initializations but because we didn't talk about initialization yet we only saw zero initialization I think and constellation two together another way to to avoid the network to stabilize to become stable on zero is to change the initialization scheme and in two weeks we're going to see different initialization schemes together so the question is how do we know that this network is going to be robust to rotations of the image or scaling of the image or translation of the image we know it's because in the data set we're going to give let's say your picture and your picture scale and we're going to tell the network this is the same person so the network will have to learn that the scale doesn't mean it's not the same person you have to learn this feature ok one more question and then we move on yeah so good question why is it a problem to stay it to stabilize at zero is because it's common to ships and the loss function is positive and in the paper that you can find its face net paper they don't train exactly this loss they train the maximum of this loss and zero okay so you train and you get the right function now let's make the problem a little more complicated what we did so far was face verification we're going to do face recognition what's the difference the difference is there is no more ID so now you just have a camera in the facility you enter the camera looks at you and finds you how would you design this new network yes in the back you've added in an element now of recognition as well because now before you'd search stand in front of it and you that every picture had a face now it needs to detect the face okay so you're saying maybe we need to add an element to the pipeline that is the detection detection element that's true in general for face recognition let's say you have a picture that is quite big you want to use the first Network that identifies the face like finds it on the picture detects it and then crop the face and give it to another network that's true that could also be used in verification as well great so the difference may be weak and what you're saying is maybe we can use or verification algorithm that you trained when instead of looking one-to-one comparison we look at 1 to n comparison so we have the pictures of all the students in the database what we can do is run all these data based pictures in the model get a vector that represents them right to get the vectors now you enter the facility we get your picture we run it through the model we get your vector and we can compare this vector to all the vectors in the database to identify you what's the complexity of this it's the number of students you have for every prediction to go over the whole database and a common network like model that you can use to do that is chain your neighbors so of course if you have only one picture per students it's not going to be very precise but if you collect three pictures per student and you run a two nearest neighbors algorithm you would decide that if the two pictures are the same it's likely that this person is the same as the two person on the picture ok now let's make it a little more complicated you probably saw that on your on your phones sometimes you take a picture and it recognizes that it's your grandmother or your grandfather or your mother and father what's happening behind is that there is some clustering happening it means we have a bunch of images and we want to cluster them together so this is also another algorithm that you seen here 2 to 9 and CF 2 to 9 a which is k-means algorithm and this is a clustering algorithm by taking all the vectors that we have in the database we can find let's say sorry you haven't you have a phone you have thousands of pictures of let's say 20 different people what you want is to cluster all the pictures of the same person separately what you will do is that you will encode all the pictures in vectors and then you will run a clustering algorithm like k-means in order to cluster those into groups these are the vectors that look like each other these are the vectors that look like each other ok and then you can simply give folders to the users with all the pictures of your mom all the pictures of your dad and so good question how do you define the cake so someone has an idea actually so one one way is to as you said to try different values trainer clustering algorithm and look at a certain loss to define our small ADIZ there's actually an algorithm called X means that is used X means we might search for that each one to find to find the K there is also a method called the elbow method and that you want to search for as well to deter grout the K okay and as you said maybe we need to detect the face first and then crop and give it to the algorithm one more question on face verification so you can use the music louder do you need to use the vector that you trained for classification sorry idea I don't understand so you mean oh so where is the encoding coming from that's what you mean in the network okay good question so you have a deep network and you want to decide where should you take the encoding from in this case the more complex the task the deeper you would go but for face verification what you want and you know it as a human you want to know features like distance between eyes nose and stuff and so you have to go deeper you need the first layers to figure out the edges give the edges to the second layer the second layer to figure out the nose the eyes give it to the third layer the third layer to figure out the distances between the eyes the distance in between years so you would go deeper and get the encoding deeper because you know that you want high level features okay our generation even a picture make it look beautiful as usual data what do we need it's a little complicated because we have to define what beautiful is so data some beautiful pictures I know maybe my concept of beautifully defended they timed a certain style let's go that's a good point so we might say that beautiful means paintings like paintings are usually beautiful so you wanna have a sigh kind of a style yeah that's true so let's say we have any data that we we want what we're going to do and the way we define this problem is let's take an image that we call the content image and here again you have the Louvre Museum and let's take an image that we call the style image and this is a painting that we find beautiful what we want is to generate an image that looks like it's the content of the content image but painted by the painter of the style so this style image is a clone Monet and here we have the Louvre painted by Claude Monet even if he was dead when this pyramid was created so that's our goal and this is what we would call our generation there are other methods but this is one so how do we do that what architectures do we need and please try to use what we've seen in the past two applications together what training scheme what application what what architecture one wants to try you're saying we take some spy images give it as input to a network and the network outputs yes or no like one or zero generate we want to generate an image probably so what you're proposing is we get an image that is the content image and we have a network that is a style style network which will style this image and we will get the content but style version of the content so use certain feature of this type and change this style according to what the network is not so this is actually done this is one method that's not the one we'll see today but this method which is a small issue is that you have to train your network to learn one style network learns one style you give the content it gives you the constant with the specific style of the model what we want to do is to have no model that is restricted to a specific style I want to be able to give a painting of Picasso and get this picture painted by Picasso so the difference here is that we're not we're not going to learn parameters of a network like we did for face verification or for the in a classification we're going to learn an image so remember when we talked about back propagation of the gradient to the parameters we're not going to do that we're going to back propagate all the way back to the image let's see how it works so first we have to understand what content means and what style means to do that we're going to use encoding we're going to to use the ideas that we talked about later giving the content image to a network that is very good will allow us to extract some information about the content of this image we specifically sew together that earlier layers we detect the edges the edges are usually a good representation of the content so I might have a very good Network give my contents image extract the information from the first layer this information is going to be the content of the image now the question is how do I get the style I want to give my style image and find a way to extract the style that's what we're going to learn later in this course it's a technique called Graham matrix and the important thing to remember is that the style is non localized information if I show you the pictures in the previous slide sorry here you see that in the generated picture although on the style image there was a tree on the left side there is no tree on the generated image it means when I extracted the style I just extracted non localized information what's the technique that Claude Monet has used to paint I didn't want to extract this tree that was on the style image don't want a content okay so we're going to take a network that understands images very well and they're common online you can find image net classification networks online that were trained to recognize more than thousand thousands of objects this network is going to understand basically anything you give it if I give it the Louvre Museum it's going to find all the edges very easily it's going to figure out that there is it's during the day it's going to figure out their buildings on the sides and all the features of the image because it was trained for months on thousands of classes let's say we have this network we give our content image to it and we extract information from the first few layers this information we call it content see content of the content image does that make sense now I give the styl image and I will use another method that is called the grain matrix to extract style s style of the style image okay and now the question is what should be the loss function so let's go on menti so same code as usual just open it if you want to repeat you can repeat the code if you want eight four five seven zero nine and these are the three proposals for the last function so reminder content C means content of the contents image style s means style of the styl image style G means style of the generated image content G means content of the generated image take like a minute it's too small on the code up eight four five seven zero nine what so just repeating the question why do we need to use imagenet because we we don't really need to classify any image and it's gonna waste time the reason we need image net is because image net understands our pictures so if if you give the contents image to a network that doesn't understand pictures very well you're not going to get the edges very well so you want a network that you don't care about the classification output you just cut the network in the middle extract the layers in the middle okay let's see what the answers are according to you guys so yeah I repeat we're not training anything here we're getting a model that exists and we use this model we're going to talk about the training after okay someone who has answered the second question and I will read it out loud the loss is the l2 difference between the style of the style image and the generated style plus the l2 distance between the generate the generators content and the contents content yeah so yeah we want to minimize both terms here so we want the content of the content image to look like the content of the generated image so we want to minimize the L to this s of these two and the reason we use a plus is because we also want to minimize the difference of styles between the generated in the style image so you see we don't have any terms that says style of the content image - style of the generated image is minimized this is the loss we want okay up below okay so just going over the architecture again so the last function we're going to use will be the one we saw and so one thing that I want to emphasize here is we're not training the network there's no parameter that we trained the parameters are in the image net classification network we use them we don't train them what we will train is the image so you get an image and you start with white noise you run this image through the classification network but you don't care about the classification of this image image net is going to give a random class to this image totally random instead you will extract content G and tile G okay so from this image you run it and you extract information from this network using the same techniques that you've used to extract content C and stylist so contents the N stylist you have it you have it you able to compute the last function because now you have the four terms of the class function you compute the derivatives instead of stopping in the network you go all the way back to the pixels of the image and you decide how much should I move the pixels in order to make this loss go down and you do that many times if you add many times and the more you do that the more this is going to look like the content of the content image and the style of the style image yeah yeah so the downside of this network is although it has the flexibility with any style any content every time you want to generate an image you have to do this training loop while the other network that you talked about doesn't need that because the model is trained to to convert the content to a style you just give it which network you talked about this network yeah so do we need to train this network on Mona images usually not this network is trained on millions of images it's basically seen everything you can imagine what do you mean back propagate properly here you're not training the network you're giving this image computing the back propagation and going back to the image only updating the image you don't update the network it comes from contents en stylist it comes from the stylist so the loss function you bake the baseline is you have content C and style s because you've chosen a Content picture in a style picture and now every at every step you will find the new content G in style G back propagate updates give it again get the new content G and style G update again and so on no did the art never touch it just one time the arts image just touches one time the neural network you can you extract style s and then that's all you don't use it again ok let's do one more question yeah good question why do you start with white nose instead of the content or this time actually do you think it's better to start with the content or this time probably the style I think probably the content because the the edges at least look like the content is going to to help the network converge quicker yeah that's true you don't have to start with white noise in generally the baseline is start with white noise so that anything can happen if you give it the content to start with is going to have a bias towards the content but if you train longer issues okay one more question and then we can image doesn't understand what's content and style but imagenet finds the edges on the image and so you can give the contents image and extract the few first layers to get information about them because when it was trained on classification it needed to find the edges to find that a dog is a dog you first need to find the edges of the log so it's it's trying to do so and for the style it's complicated to understand the style but the network finds all the features on the image and then we use a post processing technique that is called the Graham matrix in order to extract what we call styler it's basically a cross correlation of all the features of the network we will learn it together later okay let's move on to the next application because we don't have too much time so this is the one I prefer given a 10 second audio speech detect the word activate so you know we talked about trigger word detection and there are many companies that have this wake word thing where you have a device at home and when you say a certain word it activates itself so here's the same thing for the word activate what data do we need do we need a lot or not probably a lot because there are many accents and one thing that is counterintuitive is that if two humans like let's say let's say - two women speak as a human you would say these voices are are pretty similar right you can detect the word what the network's is is a list of numbers that are totally different from one person to another because the frequencies we use in our voices are totally different from each other so the numbers are very different although as a human we feel that it's very similar so we need a lot of ten-second audio clips that's it what should be the distribution it should contain as many accents as you can as many female male voices kid adults and so on what should be the input of the network it should be a 10 sec that we can represent like that the 10-second audio clip is going to contain some positive words in green positive word is activate and it's also going to contain negative words in pink like kitchen lion whatever words that are not activated and we want only to detect the positive word what should be the sample rate again same question you would test on humans you would you would you would also talk to an expert in space regard mission to know what's the best sample rate to use for speech processing what should be the output any ideas okay any other classification yes/no so 0 or 1 actually let's make your test let's do it this so we have 3 audio speech here speech 1 speech to speech 3 3 I don't know if we have this sound here do we have the sound maybe we'll have it now okay let's try Maria everybody in the last quality possibly unable to be manager so this is labeled one nobody speaks Italian in the second one to the European team of the apes' engine but then after a shopping openness and a spacer okay what's the way quark has anybody found what was the the trigger word we need more so you know what's fun is this is a right scheme to label like it's definitely possible but it seems that even for humans this labeling scheme is super hard we're not able to find what's what's happening like I don't know even if I did this slide I don't even remember no today now let's try something else okay so now we have a different labeling scheme that tells us also where the wake word is happening let's hear it again Maria already in the last koala t possibly label took manager look at our culture somewhat cynically amici Esther could you tell him I'm being pacified with humility put in the shopping open a service pizza ok what's the trigger word for Murray Joe yeah Paul Mary Jo means afternoon in Italian so you see what I'm trying to illustrate is compare the human to the computer and you will get what's the right labeling scheme to use and of course the labeling scheme here is going to be better for the model rather than the first one and we just proved it the important thing is to know that the first one would also work we just need a ton of data we need a lot more data to make the first labeling scheme work than we need for the second one does that make sense so yeah we will use something like that good question actually this is not the best labeling scheme as you said should the one come before or after the word was said what do you guys think before after yeah you will see that recurrent neural networks are going basically to look at the data just as human do like temporally from the beginning to the end and in this case you need to hear the word in order to detect it so we're going to put the one right after the word was set another issue that we have with this is that there are too many zeros it's highly unbalanced so the network is pushed to always predict zeros so what we do as a hack and there's a lot of hacks like that happening in papers if you read them we're going to add several ones after the word we'll say I would add twenty ones basically okay so this is our labeling scheme now what should be the last activation of our network sigmoid function yeah sigmoid but sequential for every time step you would use a sigmoid to output 0 or 1 basically don't worry if you don't understand specifically what networks were using you're going to learn it in a few weeks so the architecture should should be like a recurrent neural network probably convolutional networks might work as well we'll see it later on in the course and the loss function should be the same as before but we should make it sequential for every time step we should use the loss function like that and we should sum them over all the time step sounds good so another insight on this project I'll take it out is what was critical to the success of this project I think there are two things that are really critical when you when you build such a project the first one is to have a straight strategic data acquisition pipeline so let's talk more about that we said that our data should be 10-second audio clips that contain positive and negative words from many different accents how would you collect this data right yes you said you paid people to give you ten seconds of their voice but yes I think you you can take your phone go around campus and that's actually how we did it we took our phones we went around campus and we got some audio recordings so one way to do it is that's to go and get ten second audio recordings from different people with a large distribution of accents and then what do you do you label you label by hand that's one method is it long or short is it is it quick or not it's super slow yeah oh subtitles in movies alright that's a good idea actually you could like based on the licensing of the movie you could like take an audio from a movie and you get the subtitles and you're looking for activate and every time the subtitles they activate you could label your data that's super fun that's a pretty good actually you could label automatically using that yeah so that's a good idea I think there's another way to do it that is closer to that which is we're going to collect three databases the first one is going to be the positive word database the second one is going to be the negative word database the third one is going to be the background noise database so I take the background ten seconds I insert randomly from one two three negative words and I insert randomly from one two three positive words making sure it doesn't overlap with a negative word okay what's the main advantage of this method programmatic generation examples yeah programmatic generation of samples and automated labeling I tend label I know where I inserted my positive words so I just add ones where I inserted it I can generate millions of data examples like that just because I found the right strategy to to create data you see the difference between the two methods the one where you have to go out and collect data and the one where you just go out collect positive words negative words and then find background noise on YouTube or wherever you have the right license to use it's it's a big difference and this can make can make a company succeed compared to another company it's very common okay so I would go on campus take one second audio clips of positive words put it in the database in green take one second audio clips of negative words of the same people as well put it in the pink database and get background noise from anywhere I can find it it's very cheap and then create the synthetic data label it automatically and you know with like five plus Z words five negative words five backgrounds you can create a lot of data points okay so this is an important technique that you might want to think about in your project the second thing that is important for the success of such a project is the architecture search and hyper parameter tuning so all of you you will have complicated projects where you would be lost regarding the inker architecture to use at first it's a complicated process to find the architecture but you should not give up and the first thing I would say is talk to the experts so let me tell you the story of this project first I I started like looking at the literature and figuring out what network I could use for this project and I ended up using that for the beginning part I use a Fourier transform to extract features from the speech who's familiar with spectrograms or Fourier transforms so for the others think about audio speech as a 1d signal but every one this signal can be decomposed in a sum of sines and cosines with a specific frequency and amplitude for each of these and so I can convert a 1d signal into a matrix for with with with basically basically one axis that is the frequency one axis that is the time going from going from zero to ten seconds and I will get the value of all the the amplitude of this frequency so maybe this one is a strong frequency this one is a strong frequency this one is a low one and so on for every time step this is a spectrogram of an audio speech you're going to learn a little bit more about that so after I got the spectrogram which is better than the 1d signal for the network I would use an LSD M which is a recurrent neural network and add a sigmoid layer after it to get probabilities between zero and one I would threshold them everything be more than 0.5 I would consider that it's a 1 everything last to zero I tried for a long time fitting this network on the data it didn't work but one day I was working on campus and I I found a friend that was an expert in speech recognition he has worked a lot on all these problems and he exactly knew that this was not going to work he could told me he could have told me so he told me there are several issues with this network the first one is your hyper parameters in the Fourier transform they're wrong go on my github you will find what hyper parameters are used for this Fourier transform you will find specifically what sample rate what's window size what frequencies are used so that was better then he said one issue is that your record neural network is too big it's super hard to train instead you should reduce it so I've used so he told me to use a convolution to reduce the number of time steps of my audio clip you will learn about all these layers later and also use batch noir which is a specific type of layer that that makes the training easier and finally you get your sigmoid layer and you output zeros and ones but because the output time steps is smaller than the input you have to expand it so you need an expansion algorithm just a script that expands every zero in two zeros let's say every one in two ones and so on and now I get another architecture that I managed to train within a day and this was all because I was lucky enough to find the experts and get advice from this person so I think you will run into the same problems as I run into during your projects the important thing is spend more time figuring out who is the expert and who can tell you the answer rather than trying out random things I think this is a an important thing to think about okay so don't give up and also use error analysis which we're going to see later we have two more minutes so I'm not going to go over this one I'm just going to talk about it quickly there's another way to solve a word detection and the other way is to use the triplet loss algorithm instead of using anchor positive and negative faces you can use audio speech of one second anchor is the word activate positive is the word activate said differently and negative is another word you will train your network to encode activate in a certain vector and then compare the distance between vectors to figure out this activate is present or not okay we have about two more minutes so I'm going to my back does that me so just to finish with two more slides now that you've seen some last function I want to show you another one and I want you to tell me what application does this beautiful Las correspond to this one of the most beautiful la sigh I've seen in my life so someone can tell me what's the application what problem are we trying to solve if we use this loss function speech recognition no it's not looking good try yes regression that's true it's a regression problem but it's a specific regression problem bounding box good bounding boxes object detection this is object detection so I put the paper here you can check it out but how do you know that it's subject detection oh you've done it before okay so this is the loss function of a network called Yolo and the reason you can find out these bounding boxes is because if you look at the first term you would see that it's comparing X 2 through X predicted X 2 print 2 through X predicted Y 2 true Y this is the center of a bounding box X Y second term is W and H w ni H stands for width and height of a bounding box and it's trying to minimize the distance between the true bounding box and the predicted bounding box basically the third term has an idle indicator function with objects it's saying if there is an object you should have a high probability of object miss the fourth term is saying that if there is no object you should have a lower probability of object miss and finally the final term is telling you you have to find the class that is in this box is it a cat is the dog is it an elephant is whatever so this is an object detection loss function actually do you know why why you would have a square root here hmm what's the TV know except for Dex the reason we have the square root is because if you want to penalize more errors on small bounding boxes rather than big bounding boxes so if I give you an image of a human like that and they're cats like this you can have so this box the one inside is the ground truth is a very tight box this one same and the box that are predicted or the predictions so these are the predictions and the other ones are the ground truth what's interesting is that a two pixel error on these cats is much more important than a two pixel error on this human because the box is smaller so that's why you use a square root to penalize more the errors on small boxes than on big boxes okay and finally the final slide okay let's go over so just recalling what we have for next week you have two modules to complete for next Wednesday which are c1 m3 with the following quiz and the following programming assignments c1 m4 with one quiz and true programming assignments you're going to build your first deep neural network this is all going to be on the web it's already on the website and we'll publish the slides now you have ta project membership that is mandatory this week so ta project mentorships are mandatory this week to start the week before the project proposal the week before the project no after the proposal after the project milestone and before the final project submission okay and try ATS sections you're going to do some neural style transfer and art generation fill in the AWS form I don't know if it's been done yet we're going to try to give you some credits for your projects with GPUs ok thanks guys