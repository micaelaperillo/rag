hi everyone welcome to lecture number seven so up to now I believe can you hear me in the back is it easy okay so in the last set of module that you've seen you've learnt about convolutional neural networks and how they can be applied to imaging notably you've played with different types of layers including pooling max pooling average pooling and convolutional layers you've also seen some classification with the most classic algorithms all the way up to inception and resonance and then you jumped into advanced application like object detection with Yolo and the faster scan and faster our CNN series with an optional video and finally face recognition and your site transfer that we talked a little bit about in the past lectures so today we're going to build on top of everything you've seen in this set of modules to try to delve into the neural networks and interpret them because you you noticed after seeing the set of modules up to now that a lot of improvements of the neural networks are based on trial and error so we try something we do hyper parameter search sometimes the model improves sometimes it doesn't we use a validation set to find the right set of methods that would make our model improve it's not satisfactory from a scientific standpoint so people are also searching how can we find an effective way to improve our neural networks not only with trial and error but with theory that goes into the network and visualizations so today we will focus on that we first will see three methods saliency Maps occlusion sensitivity and class activation maps which are used to kind of understand what was the decision process of the network given this output how can we map back the output decision on the input space to see which part of the inputs were discriminative for this output and later on we will delve even more in details into the network by looking at intermediate layers what happens at an activation level at a layer level and at the net clever with another set of methods gradient ascent class model visualization data set search and deconvolution we will spend some times on the deconvolution because it's it's a cool it's a cool type of mathematical operation to know and it will give you more intuition on how the convolution works from a mathematical perspective if we have time we go over a fun application called deep dream which is super cool visuals for some of you who know it okay let's go menti code is on the board if you guys need to sign up so as usual we go over some contextual information and in small case studies so don't hesitate to participate so you've built an animal classifier for a pet shop and you gave it to them it's it's super good it's been trained on imagenet plus some other data and what what is a little worrying is that the pet shop is a little reluctant to use your network because they don't understand the decision process of the model so how can you quickly show that the model is actually looking at a specific animal let's say your cut if I give it an input that is a cat we've seen that together one time already remembers so I go quickly you have a network here's a dog given as an input to a CNN the CNN assuming the constraint is that there is one animal per image was trained with the softmax output layer and we get a probability distribution over all animals iguana dog car cats and in crap and what we want is to take the derivative of the score of dog and back propagated to the input to know which parts of the inputs were discriminative for this score of dog does that make sense everybody remembers this and so the interesting part is that this value is the same shape as X so it's the size of the input it's a matrix of numbers if the numbers are large in absolute value it means the pixels corresponding to these locations had an impact on the score of duck okay what do you think the score of dog is easy the output probability R now what what do I mean by s of yep score on the dog it's a score of the dog yeah but easy point 80 pipe that's what I need yes it's the the score that is pre soft max it's the score that comes before the soft max so as you reminder here's a a soft max layer and this is how it could be presented so you get us a vector that is a set of scores that are not necessarily probabilities they're just scores between minus infinity and plus infinity you give them to the soft max and the soft max what is going to do is that is going to output a vector where the sum of all the probabilities in this vector are going to sum up to one okay and so the issue is if instead of using the derivative of what we called Y hat last time we use the score of dog we will get a better representation here the reason is in order to maximize this number score of dog divided by the sum of the score of all animals or like maybe I should write exponential of score of dog divided by sum of exponential of the score of all animals one way is to minimize the scores of all the other animals rather than maximizing the score of dog so you see so maybe moving a certain pixel we minimize the score of fish and so this pixel will have a high influence on Y hats the general output of the network but it actually doesn't have an influence on the score of dog one layer before does it make sense so that's why we would use the scores pre soft max instead of using the scores post soft max that are the probabilities okay and what's fun is here you cannot see there's the sides are online if you want to if you want to look at it on your computers but you have some of the pixels that are roughly the same positions as the dog is on the input image that are stronger so we see some white pixels here and this can be used to segment the dog probably so you could use a simple trash holding to find where the dog was based on this pixel pixel there is the pixel score map doesn't work too well in practice so we have better methods to do segmentation but this can be done as well so this is what is called salience see maps and it's a common technique to quickly visualize what the network is looking at in practice we will use other methods so here's another contextual story now you've built the animal classifier they're still little scared but you want to prove that the model is actually looking at the input image at the right position you don't need to be quick but you have to be very precise ya know the saliency map is literally this thing here is the values of the derivative so you you you take the score of dog you back propagate the gradient all the way to the input it gives you a matrix that is exactly the same size as X and you use you use like a specific color scheme to see which pixels are the strongest thank you okay so here we have our CNN the dog is four propagated and you get a score of probability score for the dog now you want a method that is more precise than the previous one but not necessarily too fast and this one we've talked about it a little bit it's occlusion sensitivity so the idea here is to put a gray square on the dog here and we propagate this image with the gray square at this position through the CNN what we get is another probability distribution that is probably similar to the one we had before because the gray square doesn't seem to impact too much image it's at least from a human perspective we still see a dog right so the score of dog might be high 83% probably what we can say is that we can build a probability map corresponding to the class dog and ha and we write down on this map how confident is the network if the gray square is that testing location so for our first location it seems that the network is very confident so let's put a red square here now I'm going to move the gray square a little bit I'm shifting it just as we do for convolution and I'm going to send again this new image in the network it's going to give me a new probability distribution output and the score of dog might change so looking at this score of dog I'm going to say ok the network is still very confident that there is a dog here and I continue I shift it again here same networks still very confident that there is a dog now I shift the square vertically down and I see that partial that the the face of the dog is partially occluded probability of dog will probably go down because the network cannot see one eye of the dog is not confident that there is a dog anymore so probably the confidence of the network went down I'm going to put a square that is tending to be blue and I continue I shift it again and here we don't see the dog face anymore so probably the network might might classify this as a chair right because the chair is more obvious than the dog now and so the probability score of dog might go down so I'm going to put a Blue Square here and I'm going to continue here we don't see the tail of the dog it's still fine the network is pretty confident and so on and what I will look at now is this probability map which tells me roughly where the dog is so here we use the pretty big filter compared to the size of the image the smaller the sorry the pretty big gray square the smaller the gray square the more precise this probability map is going to be does that make sense so this is if you have time if you can you can take your time with the pet shot to explain them what's happening you would do that yeah we will see that in the next slide that's correct so let's see more examples here we have three classes and these these these images has been have been generated by much as I learned Rob Fergus this paper visualizing and understanding convolutional networks is one of the seminal paper that has led the research in in visualizing and interpreting neural networks so I'd advise you to take a look at it and we will refer to it a lot of time in this lecture so now we have three examples one is a pomeranian which is this type of cute dog a car wheel which is the true class of the second image and napkin hound which is this type of dog here on the last image so if you do the same thing as we did before that's what you would see so just to clarify here we see a blue color it means when the gray square was positioned here or centered at this location the network was less confident that the true class was Pomeranian and in fact if you look at the paper they explained that when the gray square was here the confidence of Pomeranian went down because the cumference because the confidence of tennis ball went up and in fact the Pomeranian dog has a tennis ball in the mouth another interesting thing to notice is on the last picture here you see that there is a red color on the top left of the image and this is you exactly at what as what you mentioned Adam is that when the square was on the face of the human the network was much more confident than the true class that the true class was the dog because we removed a lot of meaningful information for the network which was the face of the human and similarly if you put the square on the dog the true class that the network was outputting was human problem that makes sense ok so this is called occlusion sensitivity and it's the second method that you now have seen for interpreting word the network looks at on an input so let's move to class activation Maps so I know if you remember but two weeks ago Pranav when he discussed the techniques that he has you in healthcare he explained that you get a he get a chest x-ray and he manages to to tell the doctor where the network is looking at when predicting a certain disease based on this chest x-ray right remember that so this was done through class activation labs and that's what we're going to see now so one important thing to notice is that we discussed that classification networks seem to have a very good localization ability and we can see it with the two methods that we previously discussed same thing for those of you who have read the yellow paper that you've studied in this set of modules the yellow v2 algorithm has first been trained on classification because classification has a lot of data a lot more than object detection has been trained on classification built a very good localization ability and then has been fine-tuned and retrained on object detection data sets okay and so the core idea of class activation map is to show that CNN's have a very good localization ability even if they were trained only on image level labels so we have this Network there is a very classic Network used for classification we give it a kid and a dog this class activation map is coming from MIT MIT lab with Balaji at all in 2016 and you for propagate this image of a kid with a dog through the network which has some comic spool classic series of layers several of them and at the end you usually flatten the last output volume of the comp and run it through several fully connected layer which are going to play the role of a classifier and send it to a soft Max and get the probability output now what we're going to do is that we're going to prove that this CNN is generalizing to localization so we're going to convert this same network in another network and the part which is going to change is only the last part the downside of using flattened plus fully connected is that you lose all spatial information right you have a volume that has spatial information although it's been gone through some max pooling so it's been down sampled and you lost some part of the special localization flattening kills it you flatten it you run it through a fully connected layer and then it's over you it's it's super hard to find out where the activation was corresponds to on the input space so instead of using flattened possibly connected we're going to use global average pooling we're going to explain what it is a fully connected softmax layer and get the probability output and we're going to show that now this network can be trained very quickly because we just need to train one layer the fully connected here and can show where the network looks at the same as the previous network so let's talk about it more in detail assume this was the last complex and it outputs a volume a volume that is sized to simplify 4 by 4 by 6 so 6 filters were used in the last comp and so we have 6 feature Maps now that make sense I'm going to convert this using a global average pooling to just a vector of 6 values what is global average pooling is just taking these feature Maps each of them averaging them into one number so now instead of having a 4 by 4 by 6 volume I have a 1 by 1 by 6 volume but we can call it a vector now that makes sense so what's interesting is that this number actually holds the information of the whole feature map that came before in one number being averaged over it I'm going to put these in a vector and I'm going to call them activations as usual a 1 a 2 a 3 a 4 a 5 a 6 as I said I'm going to train a fully connected layer here with the softmax activation and the outputs are going to be the probabilities so what is interesting about that is that the feature Maps here as you know will contain some visual patterns so if I look at the first feature map I can plot it here so these are the values and of course this one is much more granular than 4 by 4 it's not a 4 by 4 it's much more but this you can say that this is the feature map and it seems that the activations have found something here there was a visual pattern in the input that activated the feature map and the filters which generated this feature map here in this location same for the second one there is probably two objects or two patterns that activated the filters that generated this feature map and so on so we have six of those and after I've trained my fully connected layers here my fully connected layer I look at the score of dog score of dog is 91% what I can do is to know this 91% how much did it come from these feature maps and how can I know it is because now I have a direct mapping using the weights I know that the weight number one here this edge you see it is how much this score was dependent on the orange feature map that make sense the second weight if you look at the green edge is the weights that has multiplied this feature map to give birth to the output of a dog so this weight is telling me how much this feature map the green one has influence on the output that make sense so now what I can do is to sum all of this a weighted sum of all these feature Maps and if I just do this weighted sum I will get another feature map something like that and you notice that this one seems to be highly influenced by the green one the green feature map yeah it means probably the weight here was higher it probably means that the second filter of the last comp was the one that was looking at the dog that make sense okay and then once I get this feature map this feature map is not the size of the input image right it's the size of the height and width of the output of the last comp so the only thing I'm going to do is like I'm going to up Sam back simply so that it fits the size of the input image and I'm going to overlay it on the input image to get my class activation the reason it's called class activation map is because this feature map is dependent on the class you're talking about if I was using let's say I was using car here if I was using car the weights would have been different right look at the edges that connect the first activation to the activation of the previous layer these weights are different so if I sum all of these feature Maps I'm going to get something else does that make sense so this is class activation mass and in fact there is a dog here and there's a human there and what you can notice is probably if I look at the class of human the weights number one might be very high because it seems that this visual pattern that activated the first feature map was the face of the kid okay so what is super cool is that you can get your network and just change the last few layers into global average pooling plus the softmax fully connected layer and you can do that and visualize very well it requires a small fine tuning yeah so it's a different vocabulary I would use failures see maps for the backpropagation up to the pixels and class activation maps related to one class it's not a back propagation at all it's just not sampling to the to the input space based on the feature maps of the last compilation mostly just examining the weights that are doing like a max operation not so much of a background obligation yes any other questions on class activation maps that it's not yeah that's a good question so taking the average does it kill the spatial information so let me let me write down a formula here this is the score that we're interested in let's say dog class see what you could say is that this score is a sum of k equal 1 to 6 fw k which is the the weight that that connects the output activation to the previous layer times what times a of the previous layer let's say we use a notation that is like k is the chase feature map and i j is the location and I sum that over the locations can you see in the back roughly so what I'm saying is that here I have my global average pooling that happened here and I can divide it by the certain number so divided by 16 4x4 okay I can switch the two sums so I can say that this thing is a sum over I J the locations times sum over k equals 1 to 6 of what W K times H a so the activations of the case feature map in position H I J and times the normalization 116 doesn't make sense does this make sense so I still have the location I still moved I still move the sum around and what I could do is to say that this thing is the score in location IJ of the class activation up is the class score for this location IJ and I'm summing it over all locations so just by flipping what the average pooling was doing over the locations I can say that by weighting using my weights all the activation in a specific location for all the feature maps I can get the score of this position in regards to the final output does that make sense so we were not losing the the spatial information the reason we're not losing it is because we know we know what the feature maps are right we know what they are and we know that they've been averaged exactly so we exactly can map it back because we assume that each filter that generated this feature Maps detects one one specific thing so like if if this is the feature map it means assuming the filter was detecting dog that we're going to see just just something here meaning that there is a dog here and if there was a dog on the lower part of the image we would also have strong activations in this part I'd say if you want to see more of the math behind it check the papers but this is the intuition behind it you can flip the summations using the global average pooling and show that you keep the spatial information the thing is you do the global average pooling but you don't lose the future maps because you know where they were from the output of the count right so you're not you're not deleting this information that make sense yeah the average yeah okay let's move on and watch a full video on how a class activation not work this video was from Kylie McDonald and it's it's life so it's very quick so you can see that the network is looking at the speedboat okay so now the three methods we've seen are methods that are roughly mapping back the output to the input space and helping of visualize which part of the inputs were the most discriminative to lead to this output and the decision of the network now we're going to try to delve more into details in the in the in the intermediate layers of the network and try to interpret how does the network see our world not necessarily related to a specific input but in general okay so the pet shop now trust your model because you've used origin sensitivity salient see map and class actuation maps to show that the model is looking at the right place but they got a little scared when you did that and they asked you to explain what the model thinks a dog is so you have this trained convolutional neural network and you have an output probability yep let me take one non image data that's that's a good question it's actually so the reason we're seeing images what most of the research has been focusing on images if you look at electric time series data so either speech or natural language the main way to visualize those is with the attention method are you familiar with that so in the next set of modules that you're going to start this week and you're going to study in the next two weeks you will see a visualization method called attention models which will tell you which part of a sentence was important let's say to output a number like assuming you're doing machine translation you know some languages they don't have a direct one-to-one mapping it means I might say I love cats but in another language maybe this same sentence would be attached I love or something it's fit and you want an attention model to seek to show you that the cat was referring to the second I think it's okay sorry guys so going back to the presentation now we're going to delve into inside the network and so the new thing is the pet shop is little scared and ask you to explain what the network think a dog is what's the representation of dog for the network so here we're going to use a method that we've already seen together called gradient ascent which is defining an objective that is technically the score of the dog - a regularization term what the regularization term is doing is it's saying that X should look natural it's not necessarily l2 regularization can be something else and we will discuss it in the next slide but don't think about it right now what we will do is we will compute the back propagation of this objective function all the way back to the input and perform gradient ascent to find the image that maximizes the score of the dog so it's an iterative process takes longer than the class activation map and we repeat the process forward propagate X compute the objective back propagate and update the pixels and so on you guys are familiar with that so let's see what what what we can visualize doing that so actually if you take an image net classification network and you perform this on the classes of goose or ostrich or Kitfox Husky Dalmatians you can see what the network is looking at or what the network think that almassian is so for the Dalmatian you can see some some black dots on a white background somehow but these are still quite hard to interpret it's not super easy to see and even worse here on the screen better on your computers but you can see a fox some here you can see orange color for the fox it means that pushing the pixels to an orange color would actually lead to a higher score of the kid fox in the output if you use a better regularization than l2 you might get better pictures so this is for flamingo this is for Pelican and this is for Hartley's so a few things that are interesting to see is that in order to maximize the score of flamingo what the network visualized is many flamingos it means that ten flamingos leads to a higher score of the class salmon go than one flamingo for the network talking about regularization what does l2 regularization say it says that for visualising we don't want to have extreme values of pixel it doesn't help much to have one pixel with an extreme value one pixel with a low value and so on so we're going to regularize all the pixels so that all the values are around each other and then we can rescale it between 0 and 20 255 if you want one thing to notice is that the gradient ascent process doesn't constrain the inputs to be between 0 and 255 you can go to plus infinity potentially while an image is stored with numbers between 0 and 255 so you might want to clip that as well this is another type of regularization one thing that led to beautiful pictures was what Jason your sinski and his team did is they for propagated an image computed the score computed the objective function back propagated updated the pixels and blurred them blurred the picture because what what is not useful for visualizing is if you have high frequency variation between pixels it doesn't help to visualize if you have many pixels close to each other that have many different values instead you want to have a smooth transition among pixels and this is another type of regularization called Gaussian blur Inc ok so this method actually makes a lot of sense in in in scientific terms you're you're maximizing an objective function that gives you what the network sees as flamingo which would maximize the score of flamingo so we call it also class model visualization yes a more realistic class model visualization correspond to more accurate so it's hard to map the accuracy of the model based on this visualization it's a good way to validate that the network is looking at the right thing yeah we're going to see more of this later I think the most interesting part is actually on this slide is we did it for the class score but we could have done it with any activation so let's say I stopped in the middle of the network and I define my objective function to be this activation I'm going to back propagate and find the input that will maximize this activation it will tell me what is this activation what does this activation fire for so that's even more interesting I think than looking at the input and then yep does that make sense that we could do it on any activation yep any questions on that okay so now we're going to do another trick which is dataset search it's actually one of the most useful I think not fast but very useful so the petrov loved the previous technique and asks if there are other alternatives to to show what what an activation in the middle of a network is thinking you take an image for propagated through the network get your output now what you're going to do is select a feature map let's say this one we're at this layer and the feature map is of size 5x5 by 256 it means that the complan had 256 filters right you're going to look at these feature maps and select probably yeah what you're going to do select one of the feature maps okay we select one out of 256 it feature map and we're going to run a lot of data for propagated to the network and look which data points have had the maximum activation of this feature map so let's say we do it with the first feature map we notice that these are the top five images that really fired this feature map like high activations on the fibula what it tells us is that's probably this feature map is detecting shirts could do the same thing let's say we take the second feature map and we look which data points have maximized the activations of this feature map out of a lot of data and we see that this is what we got the top five images probably means that the other feature map seems to be activated when seeing edges so the second one is much more likely to appear earlier in the network obviously than later so one thing that you may ask is this images sim crop like I don't think that this was an image in the dataset is probably a sub part of the image what do you think this crop corresponds to any idea how we cropped the image and why these are cropped like what why didn't I show you the full images how was I able to show you the cropped back anything that's correct so let's say we pick an activation an activation in the network this activation for a convolutional neural network often time doesn't see the entire input image right doesn't see it what it sees is a subspace of the inputs image that make sense so let's look at another slide here we have a picture of unis 64 by 64 by 3 it's our inputs we run it through a five layer confidence and now we get an encoding volume that is much smaller in height and width but bigger in depth if I tell you what this activation is seeing if you map it back you look at the stride and the filter size you've used you could say that this is the part that this interesting this-this-this activation is same it means the pixel that was up there had no influence on this activation and it makes sense when you think of it you're the easiest way to think about it is looking at the the top picks the top entry on the encoding volume top left entry you have the input image you put a filter here this filter gives you one number right this number this activation only depends on this part of the image but then if you add a convolution after it it will take more filters and so the deeper you go the more part of the image the activation will see so if you look at an activation in layer 10 it will seem much a much larger part of the input than an activation in layer 1 that make sense so that's why that's why probably the pictures that I showed here these ones are very small part crops small crops of the image which means the activation I was talking about here is probably earlier in the network it sees a much smaller part of the input the final image nice what a nice one pixel exact is going to respond to one daily part the image with the pipe yeah yeah so what you look at is which activation was maximum you look at this one and then you match this one back to crop it make sense ok so here's again up and same this one would correspond more in the center of the image this intuition make sense ok good so let's talk about deconvolution now it's gonna be the hardest part of the lecture but probably helping with with more intuition on the convolution you remember that that was the generative add virtual networks scheme and we said that giving a code to the generator the generator is able to output an image so there is something happening here that we didn't talk about is how can we start with a 100 dimensional vector and now to put a 64 by 64 by 3 image that seems weird we could use you might say a fully connected layer with a lot of neurons right to up sample in practice this is one method another one is to use a deconvolution network so convolutions will encode the information in a smaller volume in heightened with deeper in in depth while the deconvolution will do the reverse it will up sample the height and width of an image so that would be useful in this case another case where it would be usefully segmentation you remember our case studies for segmentation lifecell microscopic images of cells give it to a convolution Network it's gonna encode it so it's gonna lower the height and width the interesting thing about this encoding in the middle is that it holds a lot of meaningful information but what we want ultimately is to get a segmentation mask and the segmentation mask in height and width has to be the same size as the pixel image so we need volution Network - up sample it so the conversion are used in these cases today the case we're going to talk about is visualization remember the gradient ascent method we talked about we define an objective function by choosing an activation in the middle of the network and we want the objective to be equal to this activation to find the input image that maximizes its activation through an iterative process now we don't want to use an iterative process we want to use reconstruction of this activation directly in the input space by one backward path so let's say I select this feature map out of the max book 255 sorry 5x5 by 256 what I'm going to do is I'm going to identify the max activation of this feature map here it is is this one third column second row I'm going to set all the others to zero just this one I keep it because it seems that this one has detected something don't want to talk about the others I'm going to try to reconstruct in the input space what this activation has fired for so I'm going to compute the reverse mathematical operation of pooling relu and convolution I will pull I will unreal you let's say doesn't like this word doesn't exist so don't use it but unreal ooh and decomp and I will do it several times because this activation went through several of them so I will do it again and again until I see all this specific activation that I selected in the feature map fired because it showed the ears of the duck and as you see this image is cropped again it's not the entire image it's just the part that the activation has seen and if you look at where the activation is located on the feature map it makes sense that this is the part that corresponds to it so now the higher-level intuition is this we're going to delve into it and see what do we mean by and pull what do we mean by unreal oh and what do we mean by decock okay yes you're at the mall and whatever values they were at what we have just gone and I read the instruction of the whole image so the difference I mean if we don't zero out all the activations it says that this through construction would be Messier it would be more messy doesn't doesn't necessarily mean you will not get the full image because probably the other activations probably didn't even fire it means they didn't detected anything else it's just that it's gonna is going to add some noise to this reconstruction okay so let's talk about the convolution a little bit on the board so to start with the convolution and you guys can take notes if you want we're going to spend about 20 minutes on the board now to discuss the convolution okay to understand the deconvolution we first need to understand the convolution we've seen it from a computer science perspective but actually what we're going to do here is we're going to frame the convolution as a simple matrix vector mathematical operation I'm going to see that it's actually possible so let's start with a 1d come for the 1d convolution I will take an input X which is of size 12 X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 so 8 plus 2 padding which gives me the 12 that I mentioned so the input is a one-dimensional vector which has padding of 2 on both sides I will give it to a layer that will be a 1d comm and this layer will have only one filter and the filter size we'll be four we will also use a stride equal to two so my first question is what's the size of the output can you guys compute it on your on your notepad and and tell me what's the size of the output input size twelve filter of size four stride of two padding of to fight yeah I heard yeah so remember you use an X sorry n y equals n X minus F plus two T divided by stride and you will get five so what I'm gonna get is y1 y2 y3 y4 y5 so I'm going to focus on this specific convolution for now and I'm going to show now that we can define it as a as a mathematical operation between a matrix and a vector so the way to do it is I guess the easiest way is to write the system of equation that is underlying here what is y1 y1 is the filter applied to the four first values here does it make sense so if I define my filter as being Y W 1 W 2 W 3 and W 4 what I'm gonna get is that Y 1 equals W 1 times 0 plus W 2 times 0 plus W 3 times X 1 plus W 4 times X 2 this makes sense just a convolution element-wise operation and then sum all of it why to is going to be same thing but we just ride off to going to down so he's going to give me W 1 times X 1 plus W 2 times X 2 plus W 3 times X 3 plus W 4 times X 4 correct everybody's following know same thing we will do it for all the wise until Y 5 and we know that Y 5 is element wise operation between the filter and the 4 last number here summing them so it will give me W 1 times X 7 plus W 2 times X 8 plus 0 plus W 3 times 0 plus W 4 times 0 ok now what we're going to do is to try to write down Y as a matrix vector operation between W and X we need to find what this W matrix is and looking at the system of equation it seems that it's not impossible so let's try to do it I will write my Y vector here y 1 y 2 y 3 y 4 y 5 and I will write my matrix here and my vector X here so first question is what do you think will be the shape of this W matrix five by twelve correctly we know that this is five by one this is 12 by one so of course W is going to be 5 by 12 right so now let's try to fill it in 0 0 X 1 X 2 X 3 blah blah blah x800 can you guys see in the background oh yeah ok cool so I'm going to fill in this matrix regarding this system of equation I know that the y1 would be W 1 times 0 W 2 times 0 W 3 time X 1 w 4 times X so this vector is going to multiply the first row here so I just have to place my w's here W 1 will come here multiply 0 w 2 will come here W 3 would come here and W 4 would come here and all the rest would be filled in with zeros right I don't want any more multiplications how about the second row of this matrix I know that Y 2 has to be equal to this dot product with this rope and I know that it's going to give me W 1 X 1 plus W 2 X 2 plus W 3 X 3 X 1 is the third input on this vector third third entry so I would need to shift what I had in the previous row with the stride of 2 it will give me that does it make sense so if I use the dot product of this row with that I should get the second equation up there and so on and you understand what happens right this pattern we just shift we just ride off to on the side so I would get zeros here and I would get my W 1 W 2 W 3 w 4 and then zeros and all the way down here and all the way down here what I we get is w 4 W 3 w 2 W 1 and zeros so the only thing I want to mention here is that the convolution operation as you see can be framed as a simple matrix times a vector yes on the right side the top row should it be the left because that's going to fight for the top row wide the zeros are on the right side yes because I don't want Y hat y1 to be dependent on X 3/2 X 8 so I want this to be 0 multiplicate pliers okay so why is this important for the intuition behind the deconvolution in the existence of the deconvolution is because if we managed to write down y equal WX we probably can write down X equal W minus 1 Y if W is an invertible matrix and this is going to to be our deconvolution and in fact what's the what's the shape of this new matrix yes 12 by 5 we have 12 by one on one side five by one on the other it has to be 12 by 5 so it's flipped compared to W so one thing we're going to do here is we're going to make an assumption first assumption is that W is an invertible matrix and on top of that we're going to make a stronger assumption which is that W is an orthogonal matrix and without going into the details here same as when we proved Xavier initialization in sections we made some assumptions that are not always true this assumption is not going to be always true one one intuition that you can have is if I'm using a filter that is assume the filter is an edge detector so like plus 1 0 0 minus 1 in this case the matrix would be orthogonal why a matrix that is orthogonal means that if I take two of the columns here I dot product them together it should give me 0 same with the rows you can see it so what's interesting is that if the stride was for there will be no overlap between these two rows it would give me an orthogonal matrix here let's try these two but if I replace this W 1 by minus 1 0 0 plus 1 sorry plus 1 0 0 minus 1 and minus plus 1 0 0 minus 1 you can see that the dot product would be 0 the zeros will multiply the ones and the ones were multiplied the zeros give me a 0 dot prod so this is a case where it works practices doesn't always work the reason we're making this assumption is because we want to make a reconstruction right so we want to be able to this w- one this this is invert and the reconstruction is not going to be exact but at a first-order approximation we can assume that the reconstruction will still be useful to us even if this assumption is not always true in the case where w is orthogonal i know that the inverter of w is w transpose or another way to write it is that for orthogonal matrices w transpose time w is the identity matrix so what it tells me is that X is going to be W transpose time y times y so let's see what we get from that let me write down the main C code so let's say now we have our X and we want to regenerate our we will have our Y and we want to generate our X using this method so I would what I would write is to understand the 1dd comp we can use the following illustrations where we have X here which is 0 0 X 1 X 2 X 3 all the way down to X 8 okay and I will have my W matrix here W transpose and my Y vector y1 y2 y3 y4 and y5 here and so I know that this matrix will be the transpose of the one I have here right so I can just write down the transpose the transpose will be w1 w2 w3 w-4 okay I will shifted we destroyed of two and so on and this whole thing will be w transpose so the small issue here is that this in practice is not is going to be very similar to a convolution but because but it's going to be a tiny little different in terms of implementation another question I might ask is how can we do the same thing with the same pattern as we have here it means the stride is going from left to right instead of going from up to down I'm going to introduce that with the technique called sub-pixel convolution and for those of you who read papers in segmentation in visualization often time this is the type of convolution that is used for reconstruction so let's see how it works I just want to do the same operation but instead of doing it we just try going from up to down I want to do it from a strike going from left to right what one thing you want to you want to notice here is that the two lines that I wrote here are cropped and the reason is because we're using a padded input here we will just crop the two top lines and same for the two last lines they will be cropped look at that w1 we multiply y one and this one we multiply Y two and so on so this dot product will give me w1 times one but I don't want that to happen because I want to get to padded zero here so we just drop that in this matrix is actually going to be smaller than it seems and is going to generate my X 1 through X Y 8 and then I will pad the top values in the bottom values okay just the hack so let's look at the subpixel convolution I have my input and now we do something quite fun I would perform a sub-pixel operation on Y what does it mean I will insert zeros almost everywhere I would insert them and I will get 0 0 Y 1 0 Y 2 0 Y 3 0 Y 4 0 Y 5 and 0 0 even more zero here so this vector is just the vector Y with some zeros inserted around it and also in the middle between the elements of Y now why is that interesting it trans resting because I can now write down my convolution by flipping my weight so let me explain a little bit what happened here what we wanted is in order to be able to efficiently compute the deconvolution the same way as we've learnt to compute the convolution we wanted to have the weights scattered from left to right with the stride moving from left to right what we did is that we use a sub pixel version of Y by inserting zeros in the middle and we divided the stride by 2 so instead of having a stride of two as we had in our convolution we have a stride of one in our deconvolution so notice that I shift my weights from one at every step when I move from one row to another second thing is I flipped my weights I flipped my weight so instead of having W 1 W 2 W 3 w 4 now I have W for W 3 w 2 W 1 and what you could see is looking at that first look at this row the first row that is not cropped the result of the dot product of this row with this vector is going to be Y 1 times W 3 plus y 2 times W 1 yeah now let's look what happened here I look at my first row here the dot product of this first room with my Y here is going to be a sorry sorry we these two are cropped is what in same here so looking at my first non cropped row here as a dot product with this vector what I get is w 3 times y 1 plus W 2 sorry plus W 1 times y 2 so exactly the same thing as I got there so these two operations are exactly the same operations they're the same thing you get the same results to a different way of doing it one is using a weird operation with strides going from top to bottom and the second one is exactly a convolution these are convolution convolution plus flipped weights insertion of zeros for the subpixel version of Y and on top of that padding here and there so this was the hardest part okay does it give more intuition on the convolution here you know now how convolution can be framed as a mathematical operation between a matrix and a vector and you know also that under these assumptions the way we will deconvolve is just by flipping our weights dividing the stride by two and inserting zeros if we just do that we're deconvolve Inc four propagates in a convolution the following way you want to deconvolve just flip all the weights insert zeros sub-pixel and finally divide the stride and that's the deconvolution it's a super complex thing to understand but this is the intuition behind it now let's try to have an intuition of how it would work in two dimension let me write it down why do we use that because in terms of implementation this is the same as what we've been using here is very similar while this one is another implementation so you could do both the same is the same operation but in practice this one is easier to understand because it's exactly the same operation of the convolution with flipped weights insertion of zeros and divide it right that's why I wanted to show that when yes assumption doesn't hold yeah so oftentimes the assumption doesn't hold but what we want is to be able to sear a construction and if we use this method we will still see our construction practice if we had really W minus one the reconstruction would be much better but we don't so let me go over to to the the 2d example we're going to go a little over time because we have two hours technically for one hour and 50 minutes and and let me go over the 2d X and then we will answer this question on why we need to make this assumption so here is the interpretation of the 2d deconvolution let me write it down here the intuition behind the 2d become is I get my input which is 5 by 5 and this I call it x i4 propagate it's using a filter of size 2 by 2 in a conflate and astride of - this is my convolution what I get so if you do 5 minus 2 plus the padding which is 0 divided by 2 plus 1 oh I forgot the plus 1 here plus 1 and you floor it so so 5 minus 2 divided by 2 gives you 3 divided by 2 plus 1 no actually it will give you 3 by 3 yeah 3 by 3 a Y of 3 by 3 that's what you get and now this you call it Y what you're going to do here is you're going to deconvolve Y in order to deconvolve Y in order to deconvolve it you're going to use a stride of 1 and what we said is that we need to divide the stride by 2 right so we need astride of 1 and the filter will be the same two by two and you remember that what we've seen is that the feature is the same it's just that is going to be flipped so you will use a filter of Dubai to but flip and now what do we get we hope to get a five by five input which is going to be our reconstructed X five by five input and the way we're going to do it is this is the intuition behind it yeah okay up to my - thanks yeah five by five here that's what we hope to reconstruct the way we will do it is we will take the filter s is two by two we will put it here and we will multiply all the weights of this filter by y1 1 all the weights will be multiplied by y 1 1 so we get four values here which are going to be W 4 y 1 1 W 3 y 1 1 and so on now I will shift this with the stride of 1 and I will put my filter again here and I will multiply all the entries by Y 1 2 and so on and you see that this entry has an overlap so it will it will it will be updated at every step of the convolution it's not like what happened in the fourth pass so this is the intuition behind the two deconvolution 3d same thing you have a volume here so your filter is going to be a volume what you're going to do is you're going to put the volume here x 1 1 1 and so on and then if you have a second filter you would put it again on top of it and multiply by 1 1 1 all the weights of the filter and so on it's a little complicated but this is the intuition behind the convolution ok let's get back to the lecture I'm going to take one question here if you guys need clarification no worries you don't understand the convolution fully is the important part is that you get the intuition here and you understand how we use it so let me make a comment why do we need to make this assumption and do we need to make when we want to reconstruct like we're doing here in the visualization we need to make this assumption because we don't want to retrain waits for the D convolutional Network what we know is that the activation we selected here on the feature map is has gone through the entire pipeline of the confidence so to reconstruct we need to use the weights that we already have in the confidence we need to pass them to the deconvolution and reconstruct if we're doing the segmentation like we talked about for the lifecell we don't need to do this assumption we're just saying that this is a procedure that is the D convolution and we will train the weights of the deconvolution so there is no need to make this assumption it's just we have a technique that is dividing this right by one and inserting zeroes and then beam we will train the weights and we get an output that is an AB sampled version of the input that was given to it so there's two use case one where you use the weights and one where you don't in this case we don't want to retrain we want to use the weights so let's see let's see a version more visual of the up sampling so we do the subpixel image this is my image 4x4 i insert zeros and I pad it I get a nine by nine image I have my filter like that and this filter will convolve I will it would convolve over the input so I would place it on my input and at every step I would perform a convolution up I will get a value here the value is blue because as you can see the weights that affected the output were only the blue weights I would use a stride of one beam now the weights that affect my input are the green ones and so on and I would just come valve as I do usually and so on and now one step down I see that the weights that are impacting my input are the purple ones so I would put a purple here and so on so I just do the convolution like that and so so one thing that is interesting here is that the values that are blue in my out 6x6 output were generated only using the blue values of the filter the blue weights in the filter the ones that are green were only used you were only generated using the green values of my filter so actually this subsample sub-pixel convolution or deconvolution could have been done with for convolutions with the blue weights green weights purple white sand yellow weights and then just just replaced such that the adjustment would be the output just put the output of each of these comp and mix them to give out a 6x6 output only thing you need to know we have an input 4x4 and we get an output 6x6 that's what we wanted we wanted to of sample the image we can retrain the weights or use the transport version of them so let's see what happens now we understood what what the curve was doing so we're able to decomp what we need to do is also to ampoule and to unreal ooh fortunately it's easier than the decomp so we're not going to do board work anymore so let's see how uncool works if I give you this input to the pool link to a max pooling layer the output is obviously going to be this one 42 is the maximum of these four numbers assuming we're using a two-by-two filter with right of two vertically and horizontally 12 is the maximum of the green numbers six is the maximum of the red numbers and seven the orange ones now question I give you back the output and I tell you give me the input can you give me the input or no no what why you need you need you only keep the maximum so you you lost all the other numbers I don't know anymore the 0 1 and minus 1 that's where the red numbers here because they didn't pass through the maximum so max pool is not invertible from mathematical perspective what we can do is approximate its invert how can we do that spread it out that's a good point we could spread out the the 6 among the 4 values that would be an approximation a better way if we managed to catch some values is to catch something we call the switches we catch the values of the maximum using a matrix that is very easy to store of zeros and ones and we pass it to the unpooled and now we can approximate the inverse because we know where 6 was we know where 12 was we know where 40 2007 was but it's still not invertible because we lost all the other numbers think about max pool back propagation it's exactly the same thing these numbers 0 1 minus 1 they had no impact in the loss function at the end because they didn't pass through the for propagation so actually with the switches you can have the exact back propagation well you know that the other values are going to be zeros because they didn't affected the loss during the forward propagation but that make sense okay so this is max pooling and pooling and max pooling and we can use it with the switches you can approximately yeah why don't we just catch the whole origination quickly could catch the entire thing but in terms of back for back propagation in terms of efficiency we would just use the switches because it's enough for on pulling you're right we could catch everything but then it's cheating like you you kept it so you just give it back yep ok so now we know how I'm pulling works let's look at the relevant so what we need to do in fact is to pass the switches and the filters back to the end to Lindy count in order to reconstruct switches are the matrix of zeros and ones indicating where the maximum were and filters are the filters that I will transpose under this assumption on the board okay and so on and so on and I get my reconstruction I just need to explain the rail you now I give you this input to relu and I forward propagate it what do we get all the negative numbers are going to be equalized to 0 and the others are going to be kept now let's say I'm doing a back propagation through Lu what do I get if I give you that this is the gradients that are coming back and I'm asking you what are the gradients after the rally during the back propagation how does the Rayleigh behave in backdrop Zero's which ones are zeros the negatives are zeros do you agree the negatives in this yellow matrix are going to be zeros during the backdrop I guess sure think always about what was the influence of the input on the last function and you will find out what was the backpropagation look at this number this number here - - did this number have the fact that it was - - did it have any influence on the last function no it could have been -10 it could have been -20 it's not going to impact the last function so what do you think should be the number here zero even if the number that is coming back the gradient is 10 so what do you think should be the value backward output same idea is Mac spending what we need to do is to remember the switches remember which of these values had an impact on the loss we passed the switches all these values here that are kind of a y-you know this is a why all these ones had no impact on the last function so when you back from a gate their gradient should be set to zero it doesn't matter to update them it's not gonna make the loss go down so these are all zeros and the rest they just pass why do they pass with the same value because relu for positive numbers was one so this number one here that passed the rally during the for propagation it was not modified its gradient is going to be one that makes sense so this is really backward now in this reconstruction method we're not going to use rayleigh back part we're going to use something we call value D confident let's say the reason we're not the intuition between why we're not using value backward is because what we're interested in is to know which pixels of the input positively affected the activation that we're talking up so what we're going to do is that we're just going to do a rail you we're just going to do a rally backward another reason is when we reconstruct we want to have the minimum influence from the forward propagation because we don't really want our reconstruction to depend on the forward propagation we would like our reconstruction to be unbiased and just look at this activation reconstruct what happened so that's what you're going to use again this is a hack that has been found through trial and error and it's not going to be scientifically viable all the time okay so now we can do everything and we can reconstruct and find out what was this activation corresponds to it took time to understand it but it's super fast to do now just one path not iterative we could do it with every layer so let's say we do it with the first block of conv rail you max pool I go here I choose an activation I find the maximum activation I set all the others to zero I unpolluted I come and I find out the reconstruction this via activation was looking at edges like that so let's delve into the phone and see how we can visualize inside what's happening inside the network so all the visualization we're going to see now can be found in Matthews dealers and Rob fair uses paper visualizing understanding convolutional networks I'm going to explain what they correspond to but check check out their papers if you want to understand more into details so what happens here is that on the top left you have nine pictures these are the crop pictures of the data set that activated the first filter of the first layer maximum so we have a first filter on the first layer and we run all the data sets and we recorded what are the main pictures that activate this filter these were the main ones and we did the same thing for all the filters of the first layer and there are nine times nine of them there are a lot of them I think in the bottom here you have the filters which are the weights that were plotted just take the filter plot the weights this is doing this is important only for the first layer when you go deeper in your network the filter itself cannot be interpreted it's super hard to understand it here because the weights are directly multiplying the pixels the first layer weights can be interpretable and in fact you see that the let's look at the third one the third filter here on the first row the third filter has weights that are kind of diagonal like one of the diagonals and in fact if you look at the data that maximized these filters activation the feature map corresponding to this filter they're all like cropped images that correspond to diagonals that's what happens now the deeper we go the more fun we have so let's go results on a validation set of 50,000 images what's happened here is they took 50,000 images therefore propagated to the network they recorded which image is the maximum the one that maximized the activation of the feature map corresponding to the first filter of layer two second filter and so on for all the filters let's look at one of them we can see that okay we have a circle on this one it means that this the filter general which generated the feature map corresponding to this has been activated through probably a wheel or something like that so the image of the wheel was the one that maximizes the activation of this one and then we use the Dickens method to reconstruct it any questions on that yeah good question what if the activation function is not relevant in practice you would just use a backward to reconstruct if it's damaged you would use the same the same type of method and you will try to approximate the reconstruction okay let's go a little deeper so now same layer two four propagate all the images of the dataset find the nine images that are the maximum activate that lead to the maximum activation of the first filter these are plotted on top here what you can see is like for this filter that is the sixth row first filter features are more environment to small changes so this filter actually was activated too many different types of circles spirals wheels and so it's it's still activated although the circles were different size can go even deeper up third layer what's interesting is that the deeper you go the more complexity you see so at the beginning we're seeing only edges now we see much more complex figures you can see a face here in this in this entry it means that this filter activated for when it's exist when it has seen a data point that had this face then we were constructed it cropped it on the face the face is kind of red it means that the more red it was the more activation it led to and same top nine for layer tree so these are the nine images that actually led to the face these are the nine images that maximize the at the activation of the feature map corresponding to that filter and so on so here is a you dishonor normalization layers we can switch back and forth between showing the actual activations and showing images synthesized to produce high active easily he is giving his own image to the network right now but the time we get to the fifth convolutional layer the feeders being computed represent abstract concepts so these are the green that after example in Italy this neuron seems to respond to phases we can further investigate this neuron by showing a few different types of information first we can artificially create optimized images using new regularization techniques initially the one we thought the bus needs that simulation showing his neuron fires in response to a face and show this one is that they also that the images are training set to activate this neuron the most as well as pixels from those images most responsible for the high activations computed via the deconvolution DC that the convolution rings feature response to multiple faces in different locations and by looking at the decon we can see that it would respond more strongly if we had even darker eyes and rosy lips we can also confirm that it carries about the head and shoulders that ignores the arms and torso we can even see that it fires to some extent for cat faces using back prop or decon we can see that this unit depends most strongly on a couple units in the previous layer contour and not about a dozen or so in conservation try to track by where it's look at another marriage neural net so what is this unit doing from the top nine images we might conclude that it fires four different types of clothing but examining the synthetic images shows that it may be detecting not clothing per se but wrinkles in the live plot we can see that it's activated my shirt and smoothing out half of my shirt causes that hack with the activations to decrease finally here's another interesting neuron this one has learned to look for printed text in a variety of sizes colors and fonts this is pretty cool because we never asked the network to look for wrinkles or text or faces the only papers we provided were at the very last layer so the only reason the network learned features like text and faces in the middle was to support final decisions at that last layer for example the text detector may provide good evidence that a rectangle is in fact a book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase which was one of the categories we trained the net to recognize in this video we've shown some of the features of the deep list toolbox and a few of the things we've learned by using it you can download it yeah so they have a toolbox which is exactly what you need right here and you could test the toolbox on your model takes time to get get it to run but but if you want to visualize all the neurons it's very helpful okay so let's go quickly we'll spend about three minutes on the optional deep dream one cause it's fun and yeah feel free free to jump in and ask questions so the Google and the page the blog post is by Alexander morte Vince F the idea here is to generate art using this knowledge of visualization and how they do that is quite interesting then we take an input for propagated to the network and I took specs to declare that we called the Dreamliner then we'll take the activation and set the gradient to be equal to these activations the gradient at this layer and then back propagate the gradient uniqua so earlier what we do is that we define the new objective function that was equal to an activation and we try to maximize its objective function who they doing it even stronger then you take the activations and they the gradients to be equal to the activations and so the stronger the activation the stronger is going to become later on and so on and so on and so on so they're trying to see what the network is activating for and increase even this activation so for propagate the image set the gradient of the dreaming layer to be code to exaggeration but back propagate all the way back to the inputs and update the pixel of the image do that several time and every time the activations will change so you have to set again the new activations to be the the gradients of the green layer and back propagate and also makes it you would see things happening so it's hard to see here on the screen but you would have a pig appearing here you'd have like a tree somewhere there and some animals and a lot of animals are going to start appearing in this cloud it's interesting because it means let's say you see this cloud here if the network thought that this cloud looked a little bit like regard so one of the the the feature maps was which would be generated by the filter that the textile would activate itself a little bit because we set the gradient to be equal to the activation is going to increase the appearance of the dog in the image and so on and then you will see a dog appearing after a few iterations it's quite fun and if you zoom you see that type of thing so you see a big snail it's kind of a pig with a snail carapace camel bird dog dogfish I advise you to like look at this on the slides rather than on the screen but it's quite fine and same like if you give that type of image you would see that because the network thought there was like a tower a little bit you will increase the networks confidence in the fact that there is a tower by changing the image and the tower will come out and so on it's quite a cool yeah and if you dream in lower layers obviously you will see edges happening or patterns coming because the lower layers seem to detect an edge and then you will increase its confidence in its edge so it we between create an edge on the image these are fine deep dream on video [Music] what's funny [Music] [Applause] get some trippy on the side so one one inside that is fun about it is if the network and this is not only for D dream it's also its most default gradient assets let's say we have an output score of the dumbbell and we define our objective function to be the dumbbell score and we try to find image that maximizes the dumbbell we will see something like that the interesting is that the network thinks that the dumbbell is a hand with a dumbbell not only the number and you can see it here you see the hands and the reason is it has never seen a dumbbell alone so probably image that there is no picture of a dumbbell alone in a corner and leave all that samba but instead it's usually a human triangle with fire okay so just to summarize what we've learned today we are now able to answer all the following questions what part of the best way to go what is the role of a given neuron feature layer become whoa reconstruct search in the data set what are the top images and who gradient ascent check can we check what the network focus is on occasion sensitivity saliency map class activation maps how does the network see our world I would say gradient descent maybe deep drains of cool stuff and then what are the implications and use cases of these visualizations you can use segment C mapped assignments not very useful given the new methods we have but the convolution that we've seen together is widely used for segmentation and reconstruction also for generative a virtual networks to generate images and parts sometimes these visualization are also helpful to detect if some of the neurons in your network are dead so let's then you have a network and you use the tool box and you see that whatever the input image you give some feature maps or always dark it means that the feature that generated the feature map icon holding over the inputs probably never detected anything so it's not being even trained that's the type of like you can get okay thanks guys sorry we'll wipe over time