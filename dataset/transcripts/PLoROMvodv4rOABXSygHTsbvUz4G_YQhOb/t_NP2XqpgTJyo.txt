hi everyone and welcome to lecture nine for CS 2:30 today we're going to discuss an advanced topic that will be kind of the marriage between deep learning and another field of AI which is reinforcement learning and we will see a practical application and how deep learning methods can be plugged in another family of algorithm so it's interesting because deep learning methods and deep inner networks have been shown to be very good function approximate errs essentially that's what they are we're giving them data so that they can approximate a function there are a lot of different fields which require these function approximate errs and deep learning methods can be plugged in all these methods this is one of these examples so we'll first motivate the setting of reinforcement learning why do we need your enforcement learning why cannot what why can't we use deep learning methods to solve everything there is some set of methods that we cannot solve with deep learning and reinforcement learning that reinforcement learning applications are examples of that we will see an example to introduce an algorithm of real for smuggling algorithm called cue learning and we will add deep learning to this algorithm and make it deep cue learning as we've seen with generative add virtual networks and also deep neural networks most models are hard to train with how we had to come up with the initialization we drop out with batch norm and mirrors mirrors of methods to make this deep neural networks trained in gans we had to use methods as well in order to train guns and tricks and hacks so here we will see some of the tips and tricks to train deep cue learning which is a reinforcement learning algorithm and at the end we will have a guest speaker coming to talk about advanced topics which are mostly research which buying deep learning and reinforcement learning sounds good okay let's go so different force malaria is a very recent field I would say although both things are our enforcement ring has existed for a long time only recently it's been shown that using deep learning as a way to approximate the functions that play a big role in reinforcement learning algorithms has worked a lot so one example is alphago and you probably all have heard of it it's google deepmind's alphago has beaten world champions in a game called the game of Go which is a very very old strategy old game and the one on the right here or on your right human level controlled through deep reinforcement learning is also a deep mind a google deepmind paper that came out and hit the headlines on the front page of nature which is a one of the leading multi disciplinary peer-reviewed journals in the world and they've shown that with deep learning plugged in a reinforcement learning setting they can train an agent that beats human level in a variety of games and in fact these are Atari games so they've shown actually that their algorithm the same algorithm reproduced for a large number of games can beat humans on all of these games most of these games not all of these so these are two examples although they use different sub techniques of reinforcement learning they both include some deep learning aspect in it and today we will mostly talk about the human level control through deep reinforcement learning also called deep cue Network presented in this paper so let's start with with motivating reinforcement learning using the the alphago setting this is a board of goal and the picture comes from deep mind block so go you can think of it as a strategy game where you have a grid that is up to 19 by 19 and you have two players one player has white stones and one player has black stones and at every step in the game you can position a stone on the on the board on one of the grid cross the goal is to surround your opponent so to maximize your territory by surrounding your opponent and it's a very complex game for different reasons one reason is that you have to be you cannot be short-sighted in this game you have to have a long term strategy and other reason is that the board is so big it's much bigger than a chess board right chess board is 8 by 8 so let me ask you a question if you had to solve or build an agent that solves this game and beats humans or plays very well at least with deep learning methods that you've seen so far how would you do that someone wants to try so let's say you have a you have to collect the data set because in classic supervised learning we need a data set with X&Y what do you think would be your x and y yeah okay input is game board and output is probability of victory in that position so that's that's a good one I think input output what's the issue with that one so yeah it's super hard to represent what the probability of winning is from this boy even like nobody can tell even if I ask an expert human to come and tell us what's the probability of black winning in this or white winning in this setting they wouldn't be able to tell so this is a little more complicated any other ideas of data sets yep okay good point so we could have the grid like this one and then this is the input and the output would be the move the next action taken by probably a professional player so we would just watch professional players playing and we would record their moves and we would build a data set of what is a professional move and we hope that our network using this input-output will at some point learn how the professional players play and given an input state of the board we'll be able to decide of the next move what's the issue with that [Music] yes you need a whole lot of data why and you said it you said because we need basically to represent all types of positions of the board all states so if you were actually let's let's do that if we were to compute the number of possible states of this board what would it be so 19 by 19 word remember what we did with adversarial examples we did it for pixel's right now we're doing it for a board so what's the question first is yes you want to try yeah 3 to the power 9 10 times 90 or 93 yeah so why is it that is it yeah each spot and there are 19 times 19 spots can have 3 state basically no stone white stone or black stone so this is the all possible state this is about 10 to the 117 so it's super super big so we can probably not get even close to that by observing professional players first because we don't have enough Pro shelters and because we're humans and we don't have infinite life so the professional players cannot play forever they might get tired this way but so one issue is the state space is too big another one is that the ground truth probably would be wrong it's not because you're a professional player that you will play the best move every time right every player has their own strategy so the ground truth where we're having here is not necessarily true and our network might might not be able to beat these human players what we're looking into here is an algorithm that beats humans okay second one to many states in the game as you mentioned and third one we will likely not generalize the reason we will not generalize is because in classic supervised learning we're looking for patterns if I ask you to build an algorithm to detect cats versus dogs it will look for what the pattern of a cat is versus what the pattern of the dog is in and the convolutional filters we learn that in this case it's about a strategy it's not about a pattern so you have to understand the process of winning this game in order to make the next move you cannot generalize if you don't understand this process of long term strategy so we have to incorporate that and that's where RL comes into place RL is reinforcement learning a method that we could be described with one sentence as automatically learning to make good sequences of decision so it's about the long term it's not about the shorter and we would use it generally when we have delayed labels like in this game the label that you mentioned at the beginning was probability of victory this is a long term label we cannot get this label now but over time the closer we get to the end the better we have we are at seeing the victory or not and it's for to make sequences of decision so we make a move then the opponent makes a move then we make another move and all the decisions of these move are correlated with each other like you have to plan in advance when you're human you do that when you play chess when you play go so examples of RL applications can be robotics and it's still a research topic how deep RL can change robotics but thinking about having a robot walking from here and you want to send it there you want to send the robot there what you're teaching the robot is if you get there it's good right it's good you achieve the task but I cannot give you the probability of getting there at every point I can help you out by giving you a reward when you arrive there and let you trial and error so the robot will try and randomly initialize the robot we just fall down at the first at first gets a negative reward then repeats this time the robot knows that it shouldn't fall down it shouldn't go down you should probably go this way so true trial in there and reward on the long term the the robot is supposed to learn this pattern another one is games and that's the one we will see today games can be represented as as a set of reward for reinforcement learning algorithm so this is where you win this is where you lose let the algorithm play and figure out what winning means and what losing means until it learns okay the problem with using deep learning is that the algorithm will not learn because this reward is to long term so we're using reinforcement learning and finally advertisement so a lot of advertisements are real time bidding so you want to know given a budget when you want to invest this budget and this is a long term strategy planning as well that reinforcement learning can help with okay so this was the motivation of reinforcement learning we're going to jump to a concrete example that is a super vanilla example to understand cue learning so let's start with this game or environment so we call that an environment generally and it has several states in this case five states so we have these states and we can define rewards which are the following so let's see what is our goal in this game we define it as maximize the return or the reward on the long-term and what is the reward is the numbers that you have here that were defined by a human so this is where the human defines the reward now what's the game the game has five states state one is a trash can and has a reward of plus two state two is a starting state initial state and we assumed that we would start in the initial state with the plastic bottle in our hand the goal will be to throw this plastic bottle in a can if it's the trash can we get plus two if we get to state five we get to the recycle bin and we can get plus ten super important application state four has a chocolate so what happens is if you go to state four you get a reward of one because you can eat the chocolate and you can also through the the chocolate in the in the in the recycle bin hopefully that's the setting makes sense so these states are of three types one is the starting state initial which is brown the normal state which is not starting neither neither starting nor an ending state and it's gray and the blue states are terminal states so if we get to the terminal state we end up a game or an episode let's say that's the setting make sense okay and here two possible actions you have to move either you go on the left or you go on the right an additional rule will we'll add is that the garbage collector will come in three minutes and every step takes you one minute so you cannot spend more than three minutes in this game in other words you cannot stay at the chocolate and it chocolate forever you have to move at some point okay so one question I have is how do you define the long-term return because we said we want a long-term return we don't want we don't care about short-term returns what do you think is a good way to define a long-term return here yeah the sum of the terminal states the sum of how many points you have when you reach the terminal stage so let's say I'm in state 2 I have 0 reward right now if I reach the terminal state on the run on the on your left the plus 2 I get plus 2 reward and I finish the game if I go on the right instead and I reach the plus 10 you're saying that the long-term return can be all the sum of the rewards I got to get there so plus 11 so this is one way to define the long term return any other ideas we probably want to incorporate the time steps and reduce the reward as as time passes and in fact this would be called a discounted return versus what you said would be called a return here we use a discounted return in and it has several advantages some are mathematical because the return you described which is not discounted might not converge it might go up to plus infinity this discounted return will converge with an appropriate discount so intuitively also why is the discounted return intuitive is it's because time is always an important factor in our decision making people prefer cash now than cash in 10 years right or similarly you can consider that the robot has a limited life expectancy like it has a battery and loses battery every time it moves so you want to take into account this disk of if I can eat chocolates close I go for it because I know that the chocolate is too far I might not get there because I'm losing some battery some energy for example so this is the discounted return now if we take gamma equals 1 which means we have no discounts the best strategy to follow in this setting seems to be to go to the to the left to go to the right starting in the initial state - right and the reason is it's a simple computation on one side I get plus 2 on the other side I get plus 11 what if my discount was point 1 which one will be better yeah the left would be better directly to plus and the reason is because we compute in our mind we just do zero plus 0.1 times Mount one which gives us 0.1 plus 0.1 squared times 10 and it's less than two we know it okay so now we're going to assume that the discount is 0.9 and it's a very common discount to to to to to use in reinforcement learning and we use a discount a traitor so the general question here and it's the core of reinforcement learning in this case of cue learning is what do we want to learn and this is really really think of it as a human what would you like to learn what are the numbers you need to have in order to be able to make decisions really quickly assuming you had a lot more states than that in actions any ideas of what we want to learn what would help our decision-making optimal action at each state yeah that's exactly what we want to learn for given States tell me the action that I can take and for that I need to have a score for all the actions in every state in order to store these scores we need a matrix right so this is our matrix we will call it a cue table it's going to be of shape number of states times number of actions if I have these matrix of scores and the scores are correct I'm in state three I can look on the third row of this matrix and look what's the maximum value I have is it the first one or the second one if it's the first one I go to the left if it's the second one that is maximum I go to the right this is what we would like to have does that make sense this Q table so now let's try to build the Q table for this example if you have to build it you would first think of it as a tree oh and by the way every entry of this Q table tells you how good it is to take this action in that state state corresponding to the row action corresponding to the color so now how do we get there we can build a tree and that's that's similar to what we would do in our mind we start in s2 in s2 we have two options either we go to s1 we get to or we go to s3 and we get zero from s2 week from s1 we cannot go anywhere it's a terminal state but from s3 we can go to s2 and get zero by going back or we can go to s4 and get one that make sense from s4 same we can get zero by going back to s3 or we can go to s5 and yet plus 10 now here I just have my immediate reward for every state what I would like to compute is the discounted returned for all the states because ultimately what should lead my decision-making in a state is if I take this action I get two new States what's the maximum reward I can get from there in the future not just the reward I get in that state if I take the other action I get to another state what's the maximum reward I could get from that state not just the immediate reward that I get from going to that state so what we would do we can do it together let's say we want to compute the value of of the actions from s3 from s3 going right and left from s3 I can either go to s4 or s2 going to s4 I know that the immediate reward was 1 and I know that from s4 I can get +10 this is a maximum I can get so I can discount this 10 multiplied by 0.9 10 times 0.9 Jesus 9 plus 1 which was the immediate reward is just 9 Jesus means 10 so 10 is the score that we give to the action go right from state s3 now what if we do it from one step before s2 from s2 I know that I can go to s3 + 2 s 3 I get 0 reward so the immediate reward is 0 but I know that from s3 I can get 10 reward ultimately on the long-term I need to discount this reward from one step so I multiply this 10 by 0.9 and I get 0 plus 0.9 times 10 which gives me 9 so now instead two going right will give us a long-term reward of 9 make sense and you do the same thing you can copy back that going from s4 to s3 will give you 0 plus the maximum you can get from s3 which was 10 discounted by point 9 or you can do it from s2 from s2 I can go left and get +2 or I can go right and get 9 and the immediate reward would be 9 would be 0 and I will discount the 9 by 0.9 and get 8.1 so that's the process we would do to compute that and you see that it's an iterative algorithm you will just copy back all these values in my matrix and now if I'm in state 2 I can clearly say that the best action seems to go seems to say go to the left because the long-term discounted reward is 9 while the long-term discounted reward for going to the right is 2 and I'm done that's to learning I solved the problem I had I had a stay a problem statement I found a matrix that tells me in every state what action I should take I'm fine so why do we need deep learning it's a question we will try to answer so the best strategy to follow with point nine is still right right right and the way I see it is I just look at my matrix at every step and I follow always the maximum of my row so from state to nine is the maximum so I go right from state 310 is the maximum so I still go right and from state 410 is the maximum so I go right again so I take the maximum over all the actions in a specific state okay now one interesting thing to follow is that when you do this iterative algorithm at some point it should converge and ours converged to some values that represent the discounted rewards for every state and action there is an equation that this Q function follows and we know that the optimal Q function followed this equation the one we have here follows this equation this equation is called the bellman equation and it has two terms one is R and one is this count times the maximum of the Q scores over all the actions so how does that make sense given that you know state s you want to know the score of going of taking action a in this state the score should be the reward that you get by going there plus the discount times the maximum you can get in the future that's actually what we used in the iteration does this bellman equation make sense okay so remember this is going to be very important in to learning this bellman equation it's the equation that is satisfied by the optimal Q table or Q function and if you try out all these entries you will see that it follows this equation so when she didn't is not optimal it's not following this equation yet we would like you to follow this equation another point of vocabulary reinforcement learning is a policy policies denoted P sometimes or new and sorry pi PI of s is equal to arc max over the actions of the optimal Q not sure what it means it means it's exactly our decision process it's even that we're in state s we look at all the columns of this state s in our Q table we take the maximum and this is what PI of S is telling us it's telling us this is the action you should take so PI our policy is our decision-making okay it tells us what's the best strategy to follow in a given state any questions so far ok and so I have a question for you why is deep earning helpful yes that's very Z number of states is way too large to store a table like that like if you have a small number of states and number of actions then easy you can use a few table you can add every state look into the key table it's super quick and find out what you should do but ultimately this Q table will get bigger and bigger depending on the application right and the number of states for go is 10 to the power 170 approximately which means that this matrix should have a number of rows equal to 10 with 170 zeros after you know what I mean it's very big and number of actions is also going to be bigger and go you can place your action everywhere on the board that is available of course okay so many way to many states and actions so we would need to come up with maybe a function approximator that can give us the action based on the state instead of having to store this matrix that's where deep learning will come so just to recap this first 30 minutes in terms of vocabulary we learn what an environment is it's the it's the general game definition an agent is the thing we're trying to train the decision-maker a state an action reward Total Return a discount factor the cue table which is the matrix of entries representing how good it is to take action a in state s a policy which is our decision making function telling us what's the best strategy to apply in a state and Batman equation which is satisfied by the optimal cue table now we will tweak this cue table into a cue function and that's where we shift from cue learning to deep cue learning so find a cue function to replace the few table ok so this is the setting we have our problem statement we have our cue table we want to change it into a function approximator that will be our neural network does that make sense how deep learning comes into reinforcement learning here so now we take a state as input for word propagated in the deep network and get an output which is an action an action score for all the actions it makes sense to have an output layer that is the size of the number of actions because we don't want to we don't want to give an action as input and the state as input and get the score for this action taken in this state instead we can be much quicker you can just give the state as input get all the distribution of scores over the output and we just select the maximum of this vector which will tell us which action is best so if I wait for in states to let's say here we're in state two and before propagate state 2 we get two values which are the scores of going left and right from state two we can select the maximum of those and it will give us our action the question is how to train this network we know how to train it we've been learning it for nine weeks compute the loss back propagate can you guys think of some issues that that make this setting different from a classic supervised learning setting the reward changes dynamically so the reward doesn't change the reward is set you define it at the beginning it doesn't change that m'kay but I think what you meant is that the Q score is changed dynamically that's true the Q scores change dynamically but that's that's probably okay because our network changed that our network is now the Q score so when we update the parameters of the network it updates the Q scores what's what's another issue that we might have No Labels remember in supervised learning you need labels to train your network what are the labels here and don't say compute the Q table use them as labels it's not gonna work okay so that's the main issue that makes this problem very different from classic supervised learning so let's see how how deep burning can be tweaked a little and we want you to see these techniques because they're helpful when you read a variety of research papers we have our network given a state gives us two scores that represent actions for going left and right from these states the last function that will define is it a classification problem or a regression problem regression problem because the Q score doesn't have to be a probably B to the 0 & 1 it's just a score that you want to give and that should look that should mimic the long term discounted reward so in fact the last function we can use is is the L to last function y minus the Q score squared so let's say we do it for the Q going to the right the question is what is why what is the target for this queue and remember what I copied on the top of this slide is the bellman equation we know that the optimal queue should follow this equation we know it the problem is that this equation depends on its own Q you know like you have to on both sides of the equation it means if you set the label to be R plus gamma times max of Q stars then when you will back propagate you will also have a derivative here let me let me go into the details let's define the target value let's assume that going left is better than going right at this point in time so we initialize the network randomly we forward propagate state to in the network and the Q score for left is more than the Q score for right so that's the action we will take at this point is going left let's define our target Y as the reward you get when you go left immediate plus gamma times the maximum of all the Q values you get from the next step so let me spend a little more time on that because it's a little complicated I mean s I move to s next using a move on the left I get immediate reward R and I also get a new state s prime s next I can for propagate this state in the network and you understand what is the maximum I can get from this state take the maximum value and plug it in here so this is hopefully what's the optimal Q should follow it's a proxy to a good label it means we know that the bellman equation tells us the best q satisfies this equation when in fact this equation is not true yet because the true equation will have Q star here not q q star which is the optimal q what we hope is that if we use this proxy as our label and we learn the difference between where we are now and this proxy we can then update the proxy get closer to the optimality train again update the proxy get closer to optimality train again and so on our only hope is that this will converge so does it make sense how this is different from the burning the labels are moving they're not static labels we define a label to be a best guess of what would be the best few function we have then we compute the loss of where the Q function is right now compared to this we back propagate so that the Q function gets closer to our best guess then now that we have a better q function we can have a better guess so we make a better guess and we fix this guess and now we compute the difference between this Q function that we have and our best guess we back propagate up we get to our best guess we can update our best guess again and we hope that doing that iteratively will end with the convergence and a q function that will be very close to satisfy the bellman equation the optimal penguin equation does it make sense this is the most complicated part of Q learning yeah we generate the output of the network we get two Q function we compare it to the q the best q function that we think is the one that satisfies the bellman equation we don't but we guess it based on the Q we have so basically when you have Q you can compute this Batman equation and it will give you some values these values are probably closer to where you want to get to where from where you are now where your now is is further from this optimality and you want to reduce this gap by by like to close the gap you back propagate yes so the question is is there possibility for this to diverge so this is a broader discussion that would take a full lecture to prove so I put a paper here from for Francisco Melo which proves the convergence of this algorithm so it converges and in fact it converges because we're using a lot of tips and tricks that we will see later but if you want to see the math behind it and it's a it's a full lecture of proof I invite you to look at this simple proof for convergence of development equation ok ok so this is the case where a Left score is higher than right score and we have two terms in our targets immediate reward for taking action left and also discounted maximum future reward when you are in state it's as next ok the tricky part is that let's say we we compute that we can do it we have everything we have everything to compute our target we have R which is defined by the by the human at the beginning and we can also get this number because we know that if we take action left we can then get s next and we for propagate ethnics in the network we take the maximum output and it's this so we have everything in this in this equation the problem now is if I plug this and my Q score in my loss function and I ask you to back propagate back propagation is what W equals W minus alpha times the derivative of the last function with respect to W the parameters of the network which term will have a nonzero value obviously the second term Q of s go to the left will have a nonzero value because it depends on the parameters of the network W but Y will also have a nonzero value because you have Q here so how do you handle that you actually get a feedback loop in this back propagation that makes the network unstable what we do is that we consider this fixed we will consider this Q fixed the Q that is our target is going to be fixed for many iteration let's say a million or a hundred thousand iteration until we get close to there and our gradient is small then we will update it and we'll fix it so we actually have two networks in parallel one that is fixed and one that is not fixed okay and the second case is similar if the cue score to go on the right was more than the Q score to go on the left we would define our target as immediate reward of going to the right plus gamma times the maximum Q score we get if we're in the states that we in the next state and take the best action does this make sense is the most complicated part of Q learning this is the hard part to understand so immediate reward to go to the right and discounted maximum feature reward when you're in state s next going to draw it so this is hold fix for backdrop so no derivative if we do that then no problem Y is just a number we come back to our original supervised learning setting y is the number and we compute the loss and we back propagate no difference okay so compute DL over DW and update W using stochastic gradient descent method rmsprop Adam whatever you guys want so let's go over this this full dqn deep Q network implementation and this slide is a pseudocode to help you understand how this entire algorithm work we will actually plug in many methods in this in this pseudocode so please focus right now and if you understand this you understand the entire rest of the lecture we initialize our two network parameters just as we initialize the network in deep learning we loop over episode so let's define an episode to be one game like going from start to end to a terminal state this is one episode we can also define episodes sometimes to be many states like breakout which is the game with the paddle usually is 20 points the first player to get 20 points finishes the game so episode will be 20 points once you're looking over episode starts from an initial state s in our case it's only one initial state which is state two and loop over time steps for propagate S state two in the Q network execute action a which has the maximum Q score observe a immediate reward R and the next step s prime compute target Y and to compute Y we know that we need to take s prime for propagated in the network again and then compute the last function update the parameters will gradually set does this loop make sense it's very close to what we do in general the only difference would be this part like we compute target Y using a double for propagation so we for a propagation before propagate two times in each loop do you guys have any questions on on this pseudocode okay so we will now see a concrete application of a Dipsy network so this was the theoretical partner we're going to the practical part which is going be to be more fun so let's look at this game it's called breakout the goal when you play breakout is to destroy all the bricks without having the ball pass the line on the bottle so we have a paddle and our decisions can be idle stay stay where you are move the paddle to the right or move the paddle to the left right and this demo and you have the credits on the bottom of the slide shows that after training breakout using Q learning they get a super intelligent agents which figures out the trick to finish the game very quickly so actually even like good players you don't know this trick professional players no district but in breakout you can actually try to dig a tunnel to get on the other side of the bricks and then you will destroy all the bricks super quickly from top to bottom instead of bottom up what's super interesting is that the network figured out this on its own without human supervision and this is the kind of thing we want to remember if we were to use input the go board and output professional players we will not figure out that type of stuff most of the time so my question is what's the input of the Q network in this setting our goal is to destroy all the bricks so play break out what should be the input try something that position position of bricks position of the paddle function of the bricks what else ball position okay yeah I agree so this is what we would call a future representation it means when you're in an environment you can extract some features right and these are examples of features give me the position of the ball is one feature give me the position of the bricks another feature give me the position of the paddle another feature which are good features for this game but if you want to get the entire information you'd better do something else yeah the pixels you don't want any human supervision you don't want to put features you just okay take the pixels play the game you can control the paddle take the pixel so yeah this is a good input to the cue Network what's the output I said it earlier probably the output of the network will be three key values representing the action going left going right and staying idle in a specific state that is the input of the network so give it a pixel image we want to predict choose scores for the three possible actions now what's the issue with that you think that would work or not can someone think of something going wrong here looking at the inputs okay I'm gonna help you if I give ya you won't try oh yeah good point based on this image you cannot know if the ball is going up or down so actually it's super hard because the action you take highly depends on if the goal is going up or down right if the ball is going down and even if the ball is going down you don't even know which direct direction is going down so there's a problem here definitely there is not enough information to make a decision on the action to take and if it's hard for us it's going to be hard for the network so what's a hack - to prevent that it's to take successive frames so instead of one frame we can take four frames successive frames and here the same setting as we had before but we see that the ball is going up we seed which direction is going up and we know what action we should take because we know the slope of the ball and also also if it's going up or down that make sense okay so this is called a pre-processing given a state computer function Phi of s that gives you the history of this state which is the four sequence of four last frames what other pre-processing can we do and this is something I want you to be quick like we we learnt it together in deep learning input pre-processing remember the second lecture where the question was what resolution should we use remember you have a cat recon mission what's resolution would you want to use here same thing if we can reduce the size of the input let's do it if we don't need all that information let's do it for example do you think the colors are important very minor I don't think they're important so maybe we can grayscale everything that removes three chat that converse three channels into one channel which is amazing in terms of computation what else I think we can crop a lot of this like maybe there's a line here we don't need to make any decision we don't need this course maybe so actually there are some games where the score is important for a decision making an example is football like or soccer when you're when you're winning 1-0 you you'd better if you're playing against the strong team defend like get back and defend to keep this one zero so the score is actually important in the decision-making process and in fact their famous coach in football which have this technique called park the bus where you just put all your team in front of the goal once you have scored a goal so this is an example so here there is no park the bus but we can definitely get rid of the score which remove some pixels and reduces the number of computations and we can reduce to grayscale one important thing to be careful about when you reduce your grayscale is that grayscale is a dimensionality reduction technique it means you you lose information but you know if you have three channels and you reduce everything in one channel sometimes you would have different color pixels which will end up with the same grayscale value depending on the grade scale that we use and it's been seen that you lose some information sometimes so let's say the ball and some bricks have the same grayscale value then you would not differentiate them or let's say the paddle and the background have the same grayscale value then you would not differentiate them so you have to be careful of that type of stuff and there's other methods that do greyscale in not other ways like luminance so we have our Phi of s which is this which is this input to the key network and the dip to network architecture is going to be a convolutional neural network because we're working with images so we for propagate dots this is the architecture from min cavusoglu silver at all from the pine cone value can't really control uu to fully connected layers and you get your Q scores and we get back to our training loop so what do we need to change in our training loop here is we said that one frame is not enough so we pre process all the frames so initial States is converted to 5s the fault propagated state is 5s and so on so everywhere we had s or s Prime we convert to Phi of s or Phi of s prime which gives us the history now there are a lot more techniques that we can plug in here and we will see three more one is keeping track of the terminal state in this loop we should keep track of the terminal state because we said if we reach a terminal state we want to end the loop break the loop another reason is because the Y function so basically we have to add create a boolean to detect the terminal States before looping through the time steps and inside the loop we want to check if the new s Prime we're going to is a terminal state if it's a terminal state then I can stop this loop and go back play another episode so play another start at another starting state and continue my game now this Y target that we compute is different if we're in a terminal state or not because if we're a terminal state there is no reason to have a discounted long-term reward there's nothing behind that terminal state so if we're in terminal state we just set it to the immediate reward and we break if we're not in a terminal state then we would add this discounted future reward any questions on that yep another issue that we're seeing this and which makes this reinforcement learning setting super different from the classic supervised learning setting is that we only train on what we explore it means I'm starting in a state s I compute I forward propagate this Phi of s in my network I get my vector of Q values I select the best Q value the largest I get a new state because I can move now from state s to s prime so I have a transition from s take action a get s prime or Phi of s take action a get Phi of s prime now this is what I will use to train my network I can forward propagate Phi of s prime again the network and get my why targets compare my why to my queue and then back propagate the issue is I may never explore this state transition again maybe I will never get there anymore it's super different from what we do in supervised learning where you have a data set and your data set can be used many times with batch gradient descent or with any gradient descent algorithm one epoch you see all the data points so if you do to epochs you see every day two points two times if you do ten epochs you see every day to prostrate three times ten times so it means that every data point can be used several time to train your algorithm in classic deep learning that we've seen together in this case it doesn't seem possible because we only train when we explore and we might never get back there especially because the training will be influenced by where we go so maybe there are some places where we will never go because why we train and why we learn it will it will kind of direct our decision process and we will never train on some parts of the game so this is why we have other techniques to keep this training stable one is called experience replay so as I said here is what we are currently doing we have Phi of s for propagates get a from taking action a we observe an immediate reward R and a new state Phi of s Prime then from Phi of s Prime we can take a new action a prime observer a new reward R prime and the new state Phi of s prime prime and so on and each of these is called a state transition and can be used to Train this is one experience leads to one iteration of gradient descent a 1 e 2 e 3 experience one experience to experience tree and the training will be trained on experience one then trained on experience two then trained our experience tree what we're doing with experience replay is the following we will observe experience one because we start in a site we take an action we see another state and earn a reward and this is called experience one we will create a replay memory you can think of it as a data structure in computer science and you will place this experience one topo in the your play memory then from there we will experience experience - we will put experience - in the replay memory same with experience 3 put it in a replay memory and so on now during training what we will do is we will first train on experience 1 because it's the only experience we have so so far next step instead of training on e 2 we will train on a sample from a 1 in we - it means we will take one out of the replay memory and use this one for training but we will still continue to experiment something else and we will sample from there and at every step the replay memory will become bigger and bigger and while we train we will not necessarily train on the step we explore we will train on a sample which is the replay memory + the new state way we explore why is it good is because e 1 as you see can be useful many times in the training and maybe one was a critical state like it was a very important data point to learn or q function and so on and so on does the replay memory make sense so several advantages one is data efficiency we can use data many times don't have to use one day to appoint only one time another very important advantage of experience replay is that if you don't use experience replay you have a lot of correlation between the successive data points so let's say the ball is on the bottom right here and the ball is going to the top left for the next 10 data points the ball is always going to go to the top left and it means the action you can take is always the same it actually doesn't matter a lot because the ball is going up but most likely you want to followed where the ball is going so the action will be to go towards the ball for 10 actions in a row and then the ball will bounce on the wall and on the top and go back down here down to the bottom left the bottom right what will happen if your paddle is here is that for 10 steps in a row you will send your paddle on the right remember what we said when which when we asked the question if you had to train a cat vs. dog classifier with batches of images of cats batches of images of dog trained first on the cats then trains on the dogs then trains on the cats then trains on the dogs we will not converge because your network will be super biased towards predicting chat after seeing ten images of cat super bias bit with predicting dogs when it sees ten images of dog that's what's happening here so you want to deke or elate all these experiences you want to be able to take one experience take another one that has nothing to do with it and so on this is what experience pure play goes and the third one is that the third one is that you're basically trading computation and memory against exploration exploration is super costly the state space might be super big but you know you have enough computation probably you can have a lot of competition and you have memory space let's use an experience replay okay so let's address experience replay to our code here the transition resulting from this part is added to the experience to the replay memory D and will not necessarily be used in the iteration space so what's happening is before propagate Phi of s we get we observe a reward and an action and this action leads to a state Phi of s prime this is an experience instead of training on this experience I'm just going to take it put it in the replay memory add experience to replay memory and what I will train on is not this experience is a sample random mini batch of transition from the replay memory so you see we're exploring but we're not training on what we explore we're training on the replay memory but the replay memory is dynamic it changes and update using the sample transitions so the sample transition from the replay memory will be used to do the update that's the hack now another hack we want the last hack we want to talk about is exploration versus exploitation so as a human let's say you're commuting to Stanford every day and you know the road you're commuting yet you know it you always take the same road and your bias towards taking this road why because the first time you took it it went well and the more you take it the more you learn about it not that it's good to know the tricks of how to drive fast but but like you know the tricks you know that this this these slides is going to be green at that moment and so on so you you build a very good expertise in this road super expert but maybe there's another road that you don't want to try that is better you just don't try it because you're focused on that road you're doing exploitation you exploit what you already know exploration would be ok let's do it I'm gonna try another road today I might get late to the course but maybe I will have a good discovery and I will like this road and I will take it later on there's a trade-off between these two because the RL algorithm is going to figure out some strategies that are super good and we'll try to do local search in these to get better and better but you might have another minimum that is better than this one and you don't explore it using the algorithm which currently have there is no trade-off between exploitation exploration we are almost doing only exploitation so how to incentivize this exploration you guys have an idea so right now when we're in a state as we're for propagating the state process states in the network and we take the action that is the best action always so we exploiting we're exploiting what we already know we take the best action instead of taking this best action what can we do yep Monte Carlo sampling we point another one you wanted to try something else get out of her and there that's the ratio times e take the best action versus exploring another action okay take a hyper parameter that tells you when you can explore when you can exploit that what you mean yeah that's a good point so I think that's that's a solution you can take a hyper parameter that is a probability telling you with this probability Explorer otherwise with one - is this probability exploit that's what that's what we're going to do so let's look why exploration versus exploitation doesn't work we're in initial state 1 s 1 and we have three options either we go using action a1 2 s 2 and we get reward of 0 or we go to action use action to get to s 3 and get reward of 1 or use action 3 and go to s 4 and get a reward of 1,000 so this is obviously where we want to go we want to go to s 4 because it has the maximum reward and we don't need to do much computation in our head it's simple there is no discount it's direct just after initializing the Q networks you get the following Q values for propagates s1 induction network and get 0.5 for taking action 1.4 for taking action 2.3 for texting action 3 so this is obviously not good but our networking was randomly initialized what it's telling us is that 0.5 is the maximum so we should take action 1 so let's go take action 1 observe s2 you observe a reward of 0 our targets because it's a terminal state is only equal to the reward there is no additional term so we want our target to match our queue our target is 0 so Q should match zero so we train and we get the cue that should be zero that make sense now we do another round of iteration we look we're in s1 we get back to the beginning of the episode we see that our cue function tells us that action 2 is the best because point 4 is the maximum value it means go to s3 I go to s3 I observe reward of 1 what does it mean it's a terminal state so my target is 1 y equals 1 I want the cue to match my white so my Q should be 1 now I continue third step up q function says go to a 2 I go to a 2 nothing happens I already matched the reward for step go to a 2 you see what happens we will never go there we will never get there because we're not exploring so instead of doing that what we're saying is that 5% of the time take a random action to explore and 95% of the time follow your exploitation ok so that's where we add it we probably see Epsilon the hyper parameter take random action a otherwise do what we were doing before exploit does that make sense ok cool so now we plugged in all these tricks in our pseudocode and this is our new studio code so we have to initialize the replay memory which we didn't have to do earlier in blue you can find the replay memory added lines in orange you can find the added lines for checking the terminal state and in purple you can find the added line is related to Epsilon greedy exploration versus exploitation and finally in bold the pre-processing any questions on that so that that's that's we wanted to see a variant of how deep learning can be used in the setting that is not necessarily classic supervised learning setting and you see that the main advantage of deep learning in this case is it's a good function approximator the convolutional neural network can extract a lot of information from the pixels that we were not able to get with other networks okay so let's let's see what we have here we have our super battery but that's gonna dig a tunnel and it's going to destroy all the bricks super quickly it's good to see that after building it like so this is work from deep mines team and you can find this video on YouTube okay another thing I wanted to say quickly is what's the difference between weed and without human knowledge you will see a lot of people a lot of papers mentioning that this algorithm was trained with human learned knowledge or this algorithm was trained without any human in the loop why is human knowledge very important like think about it just playing one game as a human and teaching that the algorithm will help the algorithm a lot when the algorithm sees this game what it sees its pixels what we see when we see that game we see that there is a key here we know the key is usually a good thing so we have a lot of context right as a human we know I'm probably gonna go for the key I'm not gonna go for this this thing no same ladder what is the ladder we directly identify that the ladder is something we can go up and down we identified that this rope is probably something I can use to jump from one side to the other so as a human there is a lot more background information that we have even without knowing it without realizing it so there's a huge difference between algorithms trained with human-in-the-loop and without human in the loop this game is actually Montezuma revenge the dqn algorithm when the paper came out on underneath on nature in nature the second the second version of the paper they showed that they beat human on 49 games that are the same type of games I as break out but this one was the hardest one so they couldn't beat human on this one and the reason was because there's a lot of information and also the game has is very long so in order it's called Montezuma revenge and I think ranting pyramids is going to talk about it a little later but in order to get to win this game you have to go through a lot of different stages and it's super long so it's super hard for the algorithm to explore all the state space okay so that said I will show you a few more games that that the deepmind team has solved pong is one sequence is another one and space invaders that you might know which which is probably the most famous of the three Juno okay so that said I'm gonna hand in the microphone to we're lucky to have an oral expert so Rammstein terawatts is a fourth-year PhD students in RL working with professor Bernstein at Stanford and he will tell us a little bit about his experience and he will show us some advanced applications of deep learning and RL and how these plug in together thank you thanks Cal for that introduction oh yeah can everyone hear me now all right good cool okay first I have like eight nine minutes I have more okay okay first question after seeing that lecture so far look how many are you're thinking that RL is actually cool look honestly that's like oh that's a lot oh yeah that's a lot okay my hope is after showing you some other advanced topics ears then the percentage got even increase so let's let's see it's almost impossible to talk about like advancement RL like recently without mentioning alphago I think somewhere right now who wrote that on a table that it's almost 10 to the power 170 different configuration of the board and that's roughly more than I mean that's more than the estimated number of atoms in the universe so one traditional algorithm before the deep learning and stuff like that was like three searching RL which is basically go exhaustively search all the a possible action that you can take and they'll take the best one in that situation also good that's all almost impossible so what they do that's also a paper from deep mind is that they train anyone Ezra for that they kind of marriage the tree search we do a bit different and neural network that they have they have two kinds of networks one is called value network and value network is basically consuming this image image of a board and telling you what's the probability that if you can win in this situation so if the value is higher than the probability of winning is higher how does it help you he help you in the case that if you want to search for the action you don't have to go until the end of the game because the end of the game is a lot of steps and it's almost impossible to go to the end of the game in all the simulations so that just helps you to understand what's the value of each game like beforehand like after look for these simple 50s that if you're gonna win that game or if you're gonna lose that game there's another a network of the policy Network which helps you to take action but I think the most interesting thing of the Alpha goal is that it's trained from scratch so it's trance from nothing and if they have a tree called self play that there is two AI playing with each other the best one I replicate the best the best one I can keep it fixed and I have another one that is trying to cop beat the previous version of itself and after it complete the previous version of itself like you reliably many times then I replace this again for the previous part and then I just said so this is a training curve of like a self a self play of the alphago as you see and it takes a lot of compute so that's kind of crazy but finally they beat the human okay another type of algorithm but this is like the whole different class of algorithm called a policy gradients I've developed an algorithm called trust region policy yeah can I stop the this method residual during locomotion controllers procedure can you use this out please okay great so policy gated algorithm but I can do is that it stop this from here that is not episode work okay so here like in the diction that you have seen you you came and like compute a Q value of HS state and then what you have done is that you take the arc max of this with respect to action and then you choose the action that you want to choose right but what care at the end of the day is the action which is the mapping from a state to action which if we call it a policy right so what you can at the end of the day is actually the policy like what action should I take is not really Q value itself right so this clatters a class of methods that call the policy gradients is trying to directly optimize for the policy so rather than updating the Q function I compute the gradient of my policy I update my policy network again and again and again so let's see these videos so this is like this guy that is trying to reach the pink ball over there and sometimes that gets hit by the some external forces and if it's called the algorithm product EPO obviously policy gradient and try to reach to that ball so I think that you've heard of open a I like five like the but that is putting dota so this is like completely like PPO algorithm and they have like a lot of compute to showing that and I guess I have the numbers here there is like 180 years of play in one day this is how much code could be so that's why there is another even funnier video right again the same idea it's conjugate yet is that you put two asian in front of each other and they try to beat each other and if they beat each other they give everyone their the most interesting part is that for example in that game the purpose is just to pull the other one out right but they understand some emerging behave yeah which is it for us human makes sense but for them to learn out of nothing is kind of cool so there's like one risk here that when they're playing okay this guy's trying to kick the ball inside but one risk here is to overfit that's also cool again by technical point before move but I got 28 is that here but where is that not the next one okay here that - OH - agent playing with each other and we are just updating the person with the best other agent previously we are doing a self play is that you over fit to the actual agent that you're in front of you so the agent in front of you is powerful but you might over fit to this and if I put the agent that is not that powerful but is using a simple trick that the powerful agent that can never use this then you might just lose the game right so one trick here to make it more stable is that rather than playing against only one agent you alternate between different version of the agent itself so it all like learns all the skill together it doesn't over fit to the stuff so there's another thing conduct meta learning meta learning is a whole different algorithms again and the purpose is that a lot of tasks are like similar to each other right the core example of watching two left and working - right and like working in the front direction they're like same test essentially so the point is rather than training on a single test which is like go left or go right you train a distribution of tests that are similar to each other and then the idea is that for each a specific task I should learn bit like a very few gradient step so very few updates should be enough for me so if I learn okay play this videos like at the beginning this agent that has been trained with metal before it doesn't know how to move but just look at the number of gradient steps like after two or three guillotine steps it totally knows how to move that's that normally takes a lot of steps to train but that's only because of the meta learning approach that we've used here meta learning is also cool I mean the algorithm is from Brickley Chelsea Finn which is not also coming to Stanford is called model agnostic meta learning so all right another point this very interesting game Montezuma revenge that young talk how much time they have yeah so you've seen a exploration exploitation dilemma right so it's it's it's bad if you don't you're gonna fail many times so if you do the exploration scheme that you just saw that this is a map of the particular game and you can see call it things of that game if you like exploration land of it and I think has a twenty one or twenty something different that it's hard to reach so this recent paper anything from Google brain for mark dilemma and team is called unifying the count based methods for exploration exploration essentially very hard challenge mostly in the situation that the reward is a sparse for exactly in this game but the first reward that you get is when you reach the key right from top to here it's almost like two hundred steps and getting the number of actuals after two hundred steps exactly right but like a random exploration it's almost impossible so you're never gonna do that what a very interesting trick here is that you kind of skip account on how many times you visited a state and then if you visit a state that is that has like a few accounts then you give it a revert to the agent so we call it the intrinsic reward so it's kind of makes the so environment is also intensive ability it has the instant tips to just go etc because increase the counts of this statement has never seen before so this gets the nature tries to the experiment so it just goes down visit like different rooms like them so this game is interesting if you certain people that to solve the game is huge research online okay the highest of all of the game and it's just fun also to see the agent play [Music] any question well right there is also another interesting point that would be just fun to know about is called imitation learning imitation learning is the case that well I mean RL agent so sometimes you don't know the revoir like for example in Atari games the revolt is the key very well-defined right if I get the key I get the reward that just obvious but sometimes like defining the reward is hard for example when the car like the blue one wanna drive in a in some high rail what is the definition of the river right so we don't have a clear definition of that but on the other hand you have a person like you have human expert that can drive for us and then we see oh this is the right way of driving right so in this situation we have something called imitation learning that we try to mimic the behavior of an expert so not exactly copying this because if we copy this and then you show us it completely different states that we don't know what to do but from now we learn and this is like my example and there's a paper that called journal that the Soviet imitation learning which was like from a Stefano's group here at Stanford and that was also interesting well I think that's advanced topic if you have any questions I'm here here put yeah for next week so there's no assignments you guys not finished let's eat bye and you know about speak like smuggles now project project these partners get 2 turkeys as you know and there is going to be the project team mentorships in this Friday we have connection with reading research papers we go over the the object detection you know and there will be two papers from red mouse okay