okay let's get so welcome to lecture number four today we will go over two topics that are not discussed in the Coursera videos you've been learning c 2 m 1 and c 2 m 2 if I'm not mistaking so you've learnt about what an initialization is how to tune your own networks what tests validation and trainsets are today we're going to go a little further you should have the background to understand 80% of this lecture there's maybe 20% that I want you to look back after you've seen the batch norm videos for those of you who haven't seen them so we'll fit the lecture in two parts and I put back the attendance code at the at the very end of the lecture so don't worry one topic is attacking neural networks with adversarial examples the second one is generative adversarial networks and although these two topics have a common word which is adversarial there are two separate topics you will understand why it's called adversarial in both cases so let's get started with adversarial examples and in 2013 a Christian Zagat in his team have published a paper called intriguing properties of neural networks what they noticed is that neural net was neural networks have kind of a blind spots a spot for which several machine learning including the state-of-the-art ones that you will learn about vgg 1619 inception networks and residual networks are vulnerable to something called adverse early examples these adverse early examples you're going to learn what it is in three parts first by explaining how these examples in the context of images can attack a network in their blind spot and and make the network classify these images as something totally wrong how to defend against this type of examples and why are networks vulnerable to this type of examples this is a little bit more theoretical and we're going to go over it on the board the papers that are listed on the bottom or the two big papers that started this field of research so I would advise you to go and and read them because we have only one hour and a half to go over two big topics in in deep learning and we will not have the time to go into details of everything okay so let's set up the goal the goal is like is that given a pre trained network so a network trained on imagenet on a thousand classes millions of images find an input image that is not an iguana so doesn't look like the animal iguana but will be classified by the network as an iguana we will call this an adversarial example if we manage to find it okay yeah one question so 2848 89 let me write it down on the board can you guys see okay that's more so we have a network pre trained on Imogen it's a very good network what I want is to fool it by giving it an image that doesn't look like anyone a bird is classified as an iguana so if I give it a cat image to start with the network is obviously going to give me a vector of probabilities that has the maximum probability for cats because it's a good network and you can guess what's the output layer of this network is probably a soft max it's a classification network now what I want is to find an image x that is going to be classified as an iguana by the network okay does the the setting make sense to everyone okay now as usual this this might remind you of what we've seen together about neural style transfer remember the art generation thing where we wanted to generate an image based on the content of a first image and the style of another image and in that problem the main difference with classic supervised learning was that we fix the parameters of the network which was also pre trained and we back propagate the error of the loss all the way back to the input image to update the pixels so that it looks like the content of the contents image and the style of the style image the first thing we did is that we rephrase the problem we try to phrase what exactly we want so what would you say is a sentence that defines our last function let's say yes an image that provides minimum cost ok what's the cost you're talking about expected iguana and not expected iguana what do you mean exactly by that we're trying to Train it yeah okay so you want this image to minimize a certain loss function and the loss function would be the distance metric between the output you're looking for and the output you want okay yeah so I would say we want to find X the image that Y hat of X which is the result of the forward propagation of X in the network is equal to Y iguanas which is a one hard vector with the one at the position of iguanas does that make sense so now based on that we define our loss function which is can be an l2 loss can be an l1 loss can be a croissant repea in practice this one works better so you see that minimizing this loss function will lead our image X to be outputted as an iguana by the network that makes sense and then the process is very similar to neural side transfer where we will optimize the image iteratively so we will start with X we will forward propagate it compute the loss function that we just defined and remember we're not training the network right which take the derivative of the loss function all the way back to the inputs and update the input using a graduate descent algorithm until we get something that is classified as anyone yeah any question on that okay so you mentioned that it doesn't warranty that X is not going to look like something the only thing is guaranteeing is that this X will be classified as an iguana if we trained properly we will talk about that now another question in the back I thought yeah oh yeah it could be binary croissant it could be croissant repiy yeah so in this case not binary cross-entropy because we have a vector of of n classes but it could have been croissant from here okay so yeah that's true we are we Goren T that's the forged image X this one is going to look like an iguana who thinks it's going to look like an iguana a few who thinks it's not going to look like anyone okay majority of you so can someone tell me why it's not going to look like an iguana okay so you say the loss function is unconstrained is very unconstrained so we didn't put any constraint on what the image should look like that's true actually the answer to this question is it depends we don't know maybe it looks like an iguana maybe does it but in terms of probabilities it's high chance that it doesn't look like anyone so the reason is here let's say this is our space of input images an interesting thing is that even if as human on a daily basis we deal with images of the real world so like I mean if you look at the TV that is totally buggy you see pixels random pixels but in other contexts we usually see real word distribution images our network is deterministic it means it takes an image any input image that fits the the first layer would would be would produce an output right so this is the whole space of input images that the network can see this is the space of real images it's a lot smaller can someone tell me what's the size of the the space of possible input images for a network so infinite it's not infinite it's been a lot but okay yeah there is an idea here someone here is number impossible pixel permutations yeah that's true so more precisely you would start with how many pixel values are there there are 255 256 pixel values and then what's the size of an image let's say 64 by 64 by 3 and your results would give you 256 so you fix the first pixel 256 possible value then the second one can be anything else then the third one can be anything else and you end up with a very big number so this is a huge number and the space of real images is here now if we had to plot the space of of images classified as an iguana it would be something like that right and you see that there is a small overlap between the space of real images and the space of him is classified by as an iguana by the network and this is where we probably are not we're probably in the green part that is not overlapping with the red part because we didn't constrain our optimization problem does that make sense okay now we're going to constrain it a little bit more because in practice this type of attacks are not too dangerous because as a human we would see that the pictures look like garbage the dangerous attack is if the picture looks like a cat but the network sees it as an iguana and humans see it as a cat can someone think of of like malicious applications of that face recognitions you could show a face you could show your your picture of your face I'll push the network to think it's a face of someone else what else yeah breaking CAPTCHAs if you know what the output what output you want you can force the network to think that this CAPTCHA did this input CAPTCHA is the output it's looking for or in general I would say like social medias if someone is malicious and wants to put violent content online there is all these companies have algorithms to check for this violent content if people can use adverse you're examples that look still violent but are not detected as violent by the algorithms using this methodology they could still publish their violent pictures think about self-driving cars a stop sign that looks like a stop sign for everyone but when the self-driving car sees it it's not a stop sign so these are malicious applications of adversity examples and they're a lot more okay and in fact the picture we generated previously would look like that it's nothing special so now let's constrain our problem a little bit more we're going to say we want the picture to look like a cat but be classified as an iguana okay so now same we have our neural network if we give it a cat is going to predict that it's a cat what we want is she'll give it a cut but predict that it's only one okay III go quickly over that because it's very similar to what we did before so I just plot I just put back what we had on the previous slide okay exactly the same thing now the way we phrase our problem will be a little different instead of saying we want only y hat of x equals y y now we have another constraint what's the other constraint the picture X should be closer to the picture of the cat so we want X equal or very close to X cat and in terms of loss function what it does is that it adds another term which is going to decide how X should be close to X cat if we minimize this loss now we should have an image that looks like a cat because of the second term and that is predicted as an iguana because of the first term does that make sense so we're just building up our loss functions and I guess you guys are very familiar with this type of thought process now okay an same process we optimized until we hopefully get a cat now a question is what should be the initial image we start with we didn't talk about that in the previous example yeah white noise well yeah possibly white noise any other a cat yeah which cat I don't know probably the cat that we put in the last function right because it's the closest one to what we want to get so if we want to have a fast process we'd better start with exactly this cat which is the one we put in our last function here right if we put another cat is going to be a little longer because we have to change the pixel of the other cat to look like this cat that's what we told our last function if we start with white noise it will take even longer because we have to change the pixels all the way so that it looks real and then it looks like a cat that we defined here so yeah the best thing would be probably to start with the picture of the cat does that make sense and then move the pixels so that this term is also minimized yeah yeah this is this is empirical the fact that we use that type of loss function but in practice it could have been any distance between X and X cat and any distance between you I hurt my cat yeah and why you go on Oscar yes exactly it's a bunch of cats I'm not sure about the second method but just to repeat the point you mentioned is that here we had to choose a cat it means the X cat is actually an image of a cat so what if we don't know what the cat should look like we just want a random cat to come out and be classified as an iguana we're going to see a generative networks after which can be used to do that type of stuff but but for the second part of the question I'm not sure what the optimization process would look like okay let's move on so yeah it's probably a good idea to start with the cat image that we specified in the last function okay and so then we have an image of a cat that originally was classified as 92% cat and we modified a few pixels so you can see that this image looks a little blurry so by doing this modification the network will think it's an iguana okay and sometimes this modification can be very slight and we can even not be able to notice it sounds good now let's add something else to this to this to this draft we add a third set which is the space of images that look real to human so that's interesting because the space of images that look real to human is actually bigger the space than the space of real images an example is this one this is probably an image that looks real to human but it's not an image that we could seen in the daily life because of this slight pixel changes okay so these are the space of dangerous are there examples they look real to human but they're not actually real they might be used to fool model okay now let's see a video by cracking at all on real world example of adversity all examples so for those who cannot read they're taking a camera which which classify which has a classifier and the classifier classifies the first part as the library and the second image that is that the same as a prison so the second image has slight different pixels but it's hard to see for him same here so the the classifier on the phone classifies the first image as a washer with fifty-two percent accuracy confidence and the second one as a doormat so this is a small example of what can what can be done okay now let's go we've seen how to generate these adverse real examples it's an optimization process we will see what are the type of attacks that we can lead and what are defenses against these adverse all examples so we would usually split the attacks into two parts non targeted attacks and targeted attacks so non targeted attacks means that we just want output we just want to find an adversary example that is going to fool the model while targeted attack is we want to force this ad versatile example to be output to output a specific class that we chose these are two different type of attacks that that are widely discussed in in the research knowledge of the attacker is something very important for those of you who did some crypto you know that we talk about white box attacks black box attacks so one interesting thing is that a black box attack a white box attack is when you have access to a network so we have our image and pretend free train network we have fully access to to all the parameters and the gradients so it's probably an easier attack right we can we can back propagate all the way back to the image and update the image like with it box attack is when the model is probably encrypted or something like that so that we don't have access to its parameters activations and architecture so the question is how do we attack in blackbox attack if we cannot back propagates because we don't have access to the layers any ideas yeah numerical grade yeah good idea so you know you will trick the image a little bit and you will see how it changes the laws looking at this you can you can do have an estimate of the numerical gradient even if the model is a black box model this assumes that you can query the model right you can query it what if you cannot even query the model or you can query it one time only it's to send you add virtual example how would you do that so this becomes more complicated so there is a very complex property of this address your example is is that they're highly transferable it means I have a model here that is a nanny Moll classifier okay I don't have access to it I cannot even query it I still want to fool it what I'm going to do is that I'm going to build my own animal classifier forge an adversarial example on it it's highly likely that it's going to be an adverse example for the other one as well so this is called transferability and it's still a research topic okay we're trying to understand why this happens and also how to defend against that you know maybe your defense against that is - is - we're going to see it after I'm not going to say enough so does that make sense or no this transferability probably it's because - animal classifiers look at the same features in images right and maybe these pixels that are playing we're playing with or changing also the output of the other network let's go over some kind of defenses so one solution to defend against these adversely networks is to create a safe safety net a safety net is what is a net that like a firewall you will put it before your network every image that comes in will be classified as fake like forged or real by the network and you only take those which are real and not not adversarial does that make sense so you could you could you could say that okay but we can also build an adversarial Network that that fools this network right just we beat black box or white box we can just create an adversary at example for this network it's true but the issue is that now we have two constraints we have to fool the first one and the second one at the same time you know maybe if you fool the first one there is a chance that the second one is going to be fooled we don't know okay it just makes it more complex there is no good defense at this point - - to all type of adversarial examples this is an option that people are researching for so the paper is here if you want to check it out can you guys think of another solution trained on multiple loss functions through different networks so you're talking about an assembly maybe we can maybe we can create five networks to do our tasks and it's highly unlikely that the address on that example is going to fool the file networks the same way right any other ideas generates adversarial examples and trained on those okay so you will generate a cat image that is adversarial so some pixels have been changed to full a network you will label it as the human sees it so as a cat because you want the network to still see that as a cat and you will train on those the downside of that is that it's very costly we've seen that generating adversarial examples is super custom and also we don't know if we can generalize to other adversary examples maybe we're going to overfit to the ones we have so it's another optimization problem now another solution is to train on add virtual examples at the same time as we train on on normal examples so look at this loss function this loss function the loss new is a sum of two loss functions one is the classic loss function we would use so let's say croissant repiy in the case of classification and the second one is the same loss function but we give it the adversary genomics so what's the complexity of that at a very gradient descent step [Applause] for every iteration of our gradient descent we're going to have to iterate enough to forge an adversarial example at every step right because we have X what we want to do is forward propagate X to the network to compute the first term generate X adversarial with the optimization process and forward propagate it to calculate the second term and then back propagate over the weights of the network these super costly as well and it's very similar to what you said is just online just all the time ok so what is interesting is we're going to delve a little more there's another technique called logic pairing I just put it here we're not going to talk about it there's a paper here if you want to check it it's another way to do adversarial training but what I would like to talk about is more from a theoretical perspective why our neural network vulnerable to adversarial examples so let's let's do some some work on the board yeah the noise thing is also nice but you you so the thing is that it's just like in crypto every time you come up with a defense someone will come up with an attack and it's a race between humans you know so this is the same type of problem security problems are ok so let's go over something interesting that is more on the in on the intuition side of adverse early examples so let me let me write down something so one question we asked ourselves is why do adversity an example exists what's the reason and young good fellow and and and his team have came up with explaining with the the one of the seminal papers of adversity examples where they argue that although many people in the past have have attributed these existence of adversity examples to high nonlinear non-linearity zuv neural networks and overfitting so because we over it to a specific data set we actually don't understand what cats are we just understanding what what we've been trained on they argue that it's actually the linear parts of networks that is the cause of the existence of adversary examples so let's see why and the example I'm going to I'm going to look at is linear regression so together with similar gistic regression linear regression is basically the same thing without the sigmoid so before the sigmoid we have Y hat equals W X plus B so the for propagation of our network is going to be Y hat equals W X plus B okay and our first example is going to be a six dimensional input okay we have a neuron here but the neuron doesn't have any activation because we're in linear regression so here what happens is simply w8 plus B okay and then we get Y hats and we probably use an l1 or l2 loss because it's a regression problem to to train this network now let's look at the first example a first example where where X where we strained our network so network has been trained so network has been trained and converged to W equals 1/3 minus 1 2 to 3 this is w and you know like because we defined X to be a vector of size is a column vector W has to be a row vector of size so the network converts to this value of W and B equals zero so now we're going to look at this input we're giving a new input to the network and then the input is going to be 1 minus 1 to 0 3 minus 2 ok so I'm going to 4 propagate this to get Y hat equals W X plus B and this value is going to be 1 times 1 minus 3 minus 2 plus 0 plus 6 minus 6 if I didn't make a mistake up up 2 minus 3 okay and so we we basically get minus 4 ok so this is the the first the first example that was propagated now the question is how to change X into X star such that Y hat changes radically but X star is close to X so this is basically our problem but we're still examples can we find an example that is very close to X but radically radically changes the output of our network and we're trying to build intuition on adversarial networks so the interesting part is to is to identify how we should modify X and the intuition comes from the derivative if you take the derivative of Y hat with respect to X you know that the definition of this term is is like correlated to the impact on Y hat of small changes of X right how what's the impact of small changes of X to on the output and if you compute it what do you get W everybody agrees what's the shape of this thing shape of that is the same as shape of X so it should be W transpose remember derivative of a scalar with respect to a vector is the shape of the vector okay now it's interesting to to see this because if we compute X star to be let's say X plus a small perturbation like I will call it perturbation value yeah sorry and can you see the top one listen yes or no so what if X star equals x plus epsilon time w transpose you know and this epsilon I will call it value of the perturbation now if we for propagate X star it means we do y hat star equals W X star plus B but B is 0 at this point we're going to get W X plus epsilon W times W transpose and W times W transpose is a dot product right so this is the same as W squared so what is interesting it's interesting because the smart part was that this term is always going to be positive it means we removed a little bit X because we can make this change little by changing epsilon to a small value but it's going to push Y hat to a larger value for sure you know and if I had a minus here instead of a plus it would push Y hat to a smaller value and the interesting thing is now if we compute X star to be X plus epsilon times W transpose and we take epsilon to be a small value like let's say point two you can make the calculation what we get is is this so 1 minus 1/2 0 3 minus 2 plus 0.2 times 1 0.2 times 3 minus 0.2 plus 0.4 plus 0.4 and zero point six so if you look at that all the positive values have been pushed on the right degree and all the negative values sorry sorry no that's my way no that's sorry so let's finish the calculation and I'll give the inside after one point to a minus zero point four one point eight zero point four three point four and minus one point four so this is our X star that we hope to be adversarial okay let's compute y hat star to see what happens it's W X star plus B which is zero so what we get when we multiply W by X star is 1.2 1.2 minus one point two minus one point eight plus zero point eight plus six point eight and minus 4.2 which I believe is going to give us zero point five okay so we see that a very slight change in X star has pushed Y hat from minus 4 to point 5 and so a few things we want to notice here so insights on this on this small example the first one is that if W is large then X star is not similar to X right the larger the W the less X star is is likely to be like X and specifically if one entry of the a value is very large X I the pixel corresponding to this entry is going to be very different from X I star if W is large X star is going to be different than X so what we're going to do is that we're going to take sine sine of W instead of taking W what's the reason why we do that because the interesting part is the sine of of the W it means if we play correctly with the sign of W we will always push the X this term W X star in the positive side because every entry here this multiplication is going to give us a positive number right and the second insight is that as X grows in dimension the impact of plus Epsilon sign of W increases that makes sense so the impact of sign of W on white hats increases and so what's interesting to notice is that we can keep epsilon as small as possible it means X and X star will be very similar but as we grow in dimension we're going to get more term in this a lot more term and the change in Y hat is going to grow and grow and grow and grow and grow and so the one reason why adversity all examples exist for images is because the dimension is very high 64 by 64 by 3 so we can make epsilon very small and take the sign of W we will still get Y hat to be far from the original value that it had does it make sense do you guys have any question on that so epsilon doesn't grow with the dimension but its impact of this term increases with the dimension okay to a cooler I think who's watching - I know - included I think what's what into what but wait in between these gives you a map it to another can't eat this okay so you like you try to earn adversarially yeah I I don't know if that's has been done I don't think that has been done so you're talking about taking a little coder that takes the adverse an example to convert you to a normal image of the cat and then give the cat maybe yeah I don't know so it's a topic of research okay let's move on because we don't have too much time so just to conclude what we're going to count as a general way to generate adversely examples is this formula this is going to be a fast way to generate adversary example so this method is called the phase fast gradient sign method so basically what we're doing is that we can we're linearizing the cost function in in the proximity of the parameters and we're saying that what applied to linear networks here is going to also apply for this general formula for deeper networks so we're pushing the pixel images in one direction that is going to impact highly the output okay so that's the intuition behind it now you might say that okay we did this example on a linear network but neural networks are not linear they're highly nonlinear in fact if you look where the research has been going for the past few years we're trying to linearize all the behaviors of these neural networks with value for example or width of your initialization all that type of methods even a sigmoid when we train on sigmoid we do all we can to put sigmoid in the linear regime because we want fast training okay and one last thing that I'll mention for adversary' examples is if I have a network like this so fully connected with three-dimensional inputs up yeah and then one here and then the output what's interesting is computing the chain rule on this neuron will give you that derivative of the loss function with respect to let's say X is equal to the derivative of the loss function with respect to Z 1 1 here times derivative of Z 1 1 with respect to X let's see where we're going we're going there's actually a summation here but anyway just let me illustrate the point what we're what we're saying is that what we're what we try to do with neural networks is to have these gradients be high because if this gradient is not high we're not able to train the parameters of this neuron and we need this gradient to be high because if you want to do the same thing with the we W 1 1 which is the parameters related to this neuron you would need to go to this Traynham correct so we need this gradient to be high and if this gradient is high the gradient with respect to the input is also going to be high because you use the same gradient in the chain rule so networks that are that have high gradients and that are operating in the linear regime or even more vulnerable to adverse real examples because of this observation so any question on adversarial examples before we move on I think we don't have time and I would like to to go over the gans with you guys so let's move on to guns I'll stick around to answer questions on that part so the general question we're asking now is do neural networks understand the data because we've seen that some some data points look like there would be real but the neural networks don't understand it so more generally can we build generate these networks that can mimic the real world distribution of images let's say and this is what we will call generative address all at work we'll start by motivating it and then we look at something called the minimax game between two networks a generator and a discriminator that are going to help each other improve and finally we will see that gans are hard to train we'll see some tips to train them and finally go over some nice results and methods to evaluate ganz ok so the motivation behind generative iverson networks is to endow computers with an understanding of our world ok so by by that we mean that we want to collect a lot of data use it to train a model that can generate images that look like they're real even if they're not so a dog that has never existed can be generated by this network and finally the number of parameters of the model is smaller than the amount of data we already talked about that and this is the intuition behind why a generative network can exist is because there is too much data in the world any images count as the data for generative network and there are not enough parameters to mimic this data you know you have the network needs to understand the salient features of the data set because it doesn't have enough parameter to overfit everything so let's talk about probability distributions so these are samples from real images that have been taken and if you plot this real data distribution in a 2d map it would look like something like that I made it up but this is the image space similar to what we talked about in adverts or networks and this green shape is the space of real-world images now if you train a generator and generate some images that look like this and these images come from Stagg an from John this distribution if the generator is not good is not going to match the real world distribution so our goal here is to do something so that the red distribution matches the real-world distribution going to train the network so that it realizes what we want so this is our generator and it's what counts it what what we want to train ultimately we want to give it let's say a random number or a random latent code of 100 dimension scalar numbers and we want it to output an image but of course because it's not trained initially it's going to output a random image looks like something like that random pixels now this image doesn't look very good what we want is these images to look like generated images that are very similar to the real world so how are we going to help this generator train it's not like what we did in classic supervised learning because we don't have we don't really have inputs and labels you know there is no label we could maybe give it an image of a cat and ask it to output another cat but we want the network to be able to output things that don't exist things that we've never seen right so we want the network to understand what a cat is but not overfit to the cat we give it so the way we're going to do it is through a small game between this network called the generator G and another network called the discriminator D let's let's look at how it works we have a database of real images and we're going to start with this distribution on the bottom which is the real world data distribution is the distribution of the images in this database now our generator has this distribution initially it means the pixels that you see here probably follow a distribution that doesn't match the real world will define a discriminator D and the goal of the discriminator will be to detect if an image is real or not so we're going to give several images to discuss to measure some times we will give it generated images and sometimes we will give it real-world images what we want is that this discriminator is a binary classifier that outputs one if the image is real and zero if the image was generated okay so let's say we give it X coming from the generated image it's going to give us zero because we want the discriminator to detect that X was actually G of Z if the image came from our database of real images we want the discriminator to say one so it seems like the discriminator would be easy to train right it's just a binary classification we can define a loss function that is the binary cross-entropy and the good thing is we can have as many label as we want like it's it's unsupervised but a little bit supervised you know we have this database and we label it all as one it's just this image exists let's label them as one for this creator and everything that comes out of the generator let's label it as zero for the discriminator so basically data is not costly at all in this point the way we will train is that we will back propagate the gradient to the discriminator to train the discriminator using a binary croissant roughly but what we ultimately want is to train the generator that's what we want at the end we're not going to use the discriminator we just want to generate images so we're going to direct the gradient to go back to the generator and why does this gradient go back to the generator the reason is that X is G of Z it means we can back propagate the gradient all the way back to the input of the discriminator but this input depends on the input of the generator if the image was generated so we can also back propagate and direct the gradient to the generator does it make sense there is a direct relation between Z and the last function in the case where the image was generated if the image was real then the generator couldn't get degraded because X doesn't depend on Z or on the features and parameters of the generator ok so we would run an algorithm such as ad simultaneously on two many matches one for the true data and from forms generated data does this scheme make sense to everyone yeah one question so there's many method of doing your question is about mixing them in batches usually we would use we would use one mini batch for the real data and one mini batch for the fake data but if in practice you can try other things so there are many methods that are being tried to train Gans properly we're going to delve a little more into the details of that when we will see the loss functions so we hope that the probability distributions will match at the end and if it matches we're going to just take the generator and generate images normally it should be able to generate images that look real that look like they came from this distribution okay sounds good so now let's talk more about the training procedure and try to figure out what the loss function should be in this case what should be the cost of the discriminator assuming assuming we give too many batches one for real data so real images and one for generated data that come from G yes the same basic the same basic loss function we use from binary class for binary classifiers it's true we're going to tweak it a tiny bit but it's the same idea so this is what it can look like we're going to call it JD cost function of the discriminator it has two terms what does the first term say what does the second term say and you can recognize the binary croissant trophy here the only difference is that we have a able that is why real and a label that is why generated in practice why real and why generated are always going to be set to values we know that Y generated is zero and we know that Y real is one so we can just remove these two terms because they're both equal to 1 the first term is telling us these should correctly label real data as one - croissant repeater the first term of a binary cross-entropy the second term is going to tell us this should correctly label generated data 0 so the difference with classic croissant roba we've seen is that this summation is the summation over the real mini batch and the summation on the 2nd cross entropy is the summation and generated mini batch that make sense so we both want D to correctly identify the real data and also correctly identified fake data that's why we have two terms now what about the generator what do you think should be the cost function of the generator yes if I can put it either that's from the generator I want to run the first half because I don't have any Wi-Fi and inputs coming into generator yeah exactly yes but in your batch you will have had like a certain number of real example of certain dimmer of generating examples the generated examples have no impact on the first cross entre P and same for the real examples on the second course on true any other questions okay so coming back to the cross to the to the cost of the generator what should it be this is a tiny bit complicated let's move let's move on because we don't have too much time the cost of the generator basically should say that G should try to swing it the goal is to forge it to generate real samples and in order to generate real samples we want to fool D if J managed to fool D and D is very good it means G is very good right the problem is that it's a game because if D is bad and G fools D it doesn't mean that G is good because G because D is bad it doesn't detect very well the real versus fake examples we want D to go up to be very good and G to go up at the same time until the equilibrium is reached at a certain point where D will always output one half like random probabilities because it cannot distinguish the samples coming from G versus the real samples so this cost function is basically saying for generated images we want to classify them as one okay so you know like if you're using so how to implement that if you're using a different framework you've been building a graph right and at the end of your graph you've been building your cost functions D that is very close to a binary cross-entropy what you're going to just do is to define a node that is going to be minus the cost function of D it's going every time you're going to call the function J of G is going to run the graph that you define for JFD and run a an opposition operation an opposite of operation same way propagate gradients back the same way we're not going to propagate the same way we're going to turn into a minus sign for the grade for the generator so you know you you back propagate on the on the on D and when you back propagate on G you would flip you would flip the sign that's all we do the same thing with the sign fleet terms of implementation is just another operation okay now let's look at something interesting is that this Lord logarithm let's look at the graph of the logarithm so I'm going to plot against the abscess axe G sorry D of G of Z so what does this mean this axis is the output of D when given a generated example G of Z is going to be between 0 and 1 because it's a probability D is a binary classifier with a sigmoid our output probably if we plot logarithm of X so like this type of thing this would be log of the of G of Z does it make sense is the logarithm function if I plot minus that minus that so let me let me plot minus logarithm of G of G Ozzy or or let me let me do something else let me plot logarithm of minus D of G of Z this is it do you guys agree now what I'm going to do is that I'm going to plot another function that is this one that is logarithm of one minus D of G of Z okay so the question is right now what we're doing is that we're saying the cost function of the generator is logarithm of one minus D of G of Z so it looks like this right it looks like this one what's the issue with this one what do you think is the issue with this cost function looking at it like that sorry can you say louder it goes to negative infinity in one that's what you mean yeah yeah and so the consequence of that is that the gradient here is going to be very large the closer we go to one but the closer we are to zero the lower is the gradient and is the reverse phenomenon for this logarithm the gradient is very high and very high I mean in absolute value a very high when we're close to zero but it's very low when we go close to one okay so which loss function you think would be better a loss function that looks like this one or a loss function that looks like this one to train our generator the broader question is where are we early in the training are we close to here or always close to death what does it mean to be close they're two to one you're fooling the network it means that the kinks that generated samples or real you're here this place is the contrary he thinks that generated samples are fake it means correctly finds out that they're fake early on we're generally here because the discriminator is better than the generator generator output garbage at the beginning and it's very easy for the discriminator to figure out that it's fake because this garbage looks very different from real-world data so early on we're here so which function is the best one to to to to be our cost yeah so probably this one is better so we have to use a mathematical trick to change this into that right and the mathematical trick is pretty standard right now we're minimizing something that is in log of 1 minus X we can say that doing so is the same as maximizing something that is in log of X near a simple flip min max flip and we can also say that it's the same as minimizing something in minus log of X does it make sense so we're going to use this mathematical trick to convert our function that is a saturating cost we would say into an on-stage rating class that is going to look more like this let's see what it looks like so to sum up our cost function currently looks like that it's a saturating cost because early on the gradients are small we cannot train G we're going to do a flip that I just talked about on the board and convert this into another function that is a non saturating cost ok what you yeah so the reason it's the blue one is like that is because I added a minus sign here so I'm flipping this okay and it's the same thing it's just the sign of the gradient that is going to be different like that the gradient is high at the beginning and low at the end that makes sense so we're going to do the use this flip and so we have a new training processor now where J of D didn't change but J of G changed we have a minus sign here and instead of the log of 1 minus D of G of Z we have the log of G of G of Z does that make sense to everyone cool and actually so this is a fun thing if you should check this paper which is really cool Oregon's created equal it's a large study of many many different guns it shows what people have tried and you can see that people have tried all types of laws to make guns trainable so it looks it looks complicated here but actually the mm gun is the first one we saw together is the minimax lost function the second one is the non saturating one that we just see so you see between the first two the only difference is that on the generator we get the log of 1 minus D of X hat becoming law of minus log of D of X I okay now another trick to train guns is to use the fact that a non saturating to use the fact that D is usually easier to train than G but as the improved G can improve if D doesn't improve G cannot improve so you can see the the performance of D has an upper bound to what G can achieve because of that we will usually train D more time than we will train G so we will basically train for nomination K times D 1 time G K times D 1 times e and so on so that the discriminator becomes better then the generator can catch up better then can catch up and so on that make sense there's also methods to use like different learning rates for D ng to take this into account to train faster the discriminator okay because we don't have too much time I'm going to skip the bathroom Wiggins we're going to sit probably next week together after you guys have seen the bathroom videos okay great school so just to sum up some some tips to Train ganz is to modify the cost function we've seen one modification there are many more keeping D up to date with respect to G so updating D more than you update g using virtual match norm which is a derivate of batch norm so it's a different type of action or is used here and something called one-sided lai label smoothing that i'm not going to talk about it today because we don't have time so let's see some nice result now and that's the funnest part so some of you have worked with word embeddings and you you might know that we're done weddings are vectors that can encode the meaning of the word and you can compute operation sometimes on this on these words so if you take if you take king - quinn it should be equal to mine - woman operations like that that happen in the space of encoding so here's the same you can use a generator to generate faces and the paper is listed on the bottom here so you give a code that is a random code and it will give you an image of a face you can give it a second code it's going to give you a second image that is different from the first one because the code was different you can give it a third one it's going to give you a third interface the fun part is if you take code 1 - code 2 plus code 3 so basically image of a man with glasses - image of a man plus image of the women will give you an image of a woman with glasses so this is interesting because it means that linear operation in the latent space of codes have impact directly on the image space okay let's look at something even better so you can use guns for image generation of course these are very nice samples you see that sometimes guns have problem with with oh no I don't think that's the dog but but but these are stag and plus vs. is a very impressive gun that has generated that has been state of the art for a long time okay so let's see something fun something called image to image translation so actually the the project winners last quarter in spring was a project dealing with exactly that generating satellite images based on the map image so given a map image generate the satellite image using a gun so you see that instead of giving a latent code that was 100 dimensional you could give a very detailed code the code can be this image right and you have to find a way to constrain your network in a certain width in a certain way to push it to output exactly the satellite image that corresponds it to this map image there are many other results that are found converting zebras to horses to zebras and zebras to horses and apples to oranges and oranges to Apple so let's do a case study together let's say our goal is to convert horses to zebras on images and vice versa can you tell me what data we need let's go quickly so that we have some time yeah horses and zebras do you need her images you know like you need to have the same image of your horse as a zebra yeah so the problem is okay we could have labels images you know like a horse and it's zebra doppelganger in the same position and we could train a network to take one and out with the other unfortunately we don't not every horse has a doppelganger that is a zebra so we cannot do that so instead we're going to do unpaired unpaired generative address or networks it means we have a database of horses and a database of zebras but these are different horses and different zebras they're not one-to-one there's no one to one mapping between them there's no mapping at all what architecture do you want to use nice not really ok so let's see about the architecture and the cost so I'm gonna go over it quickly because it's a it's a very fun gun with its called cycle gun so the way we're going to work it out is we have a horse called capital H we want to generate the zebra version of this horse right so we give it to a generator that we call g1 you can call it h to Z like horse to zebra it should give us this horse H as a zebra right and in fact if we're training again we need a discriminator so we will add a discriminator that is going to be a binary classifier to tell us if this image outputted by generator 1 is real or not so this discriminator is going to take in some images of zebras probably or yes zebras or horses and it's going to also take the generated images I'm going to see which one is fake which one is real on the other hand we're going to do and the vice-versa is very important we need to enforce the fact that this horse G 1 of H should be the same horse as H in order to do that we're going to create another generator which is going to take the generated image and generate back the input image and this is where we will be able to enforce the constraints that G 2 of G 1 of H should be equal to H do you see why this loop is super important because if we don't have this loop we don't have the constraints on the fact that the horse should be the the zebra should be the horse as a zebra the same horse as H so we'll do that and we have a second discriminator to decide if this image is real this is one step H to Z another state might be z2h where we start with the zebra give it to generator to generate the horse version of the zebra discriminate generate back the zebra version of the zebra and this commit does that make sense so this is the general pattern used in cycle Gans and what I'd like to go over is what loss should we minimize in order to unforce the fact that we want the horse to be converted to a zebra that is the same as the horse and someone gives me the terms that we need someone wants to give it a try go for two minutes yes you want to make sure that the picture in the end that is a zebra that you sure talk with matches the secret that you started with or the horse - shirt off with matches of course that you had immediately okay same time you also need to have discriminator - identifying that the image is a real zebra or real horse yeah because you don't want it to just sort of input in the sampled image and then output back to you the same okay so okay that's great so you're saying we need the classic cost functions that we've seen previously plus another one that is the matching between H and G - of G 1 of H and Z and G 1 of g12c correct so we'll have all these terms one term to Train d1 which is the classic term we've seen differentiate real images from generated images g1 as well same we were using the non saturating cost on generate images same for D - same for G - these are classics the one we need to add to all of this is the cycle cost which is the distance between this term g2 of G 1 of H and H and the same thing for zebras does that make sense so you have the intuition to build that type of losses we just sum everything and gives us the cost function we're looking for the same cost function for d1 and d2 yeah so the you could but it's not going to work that well I think so I think there is a there's a tiny mistake here is that the zi here the small zi should be small H I and the small H ion top should be a small C eye because the discriminator one is going to receive generated samples that look like zebras because it came out of g1 so you want the real database that you give it to to be zebras as well to force to force the generator want to output things that look like zebras and vice-versa for the second one okay and this is my favorite so you can convert the ROM into a face and back to a ramen is the most fun application I found is from Naruto me at all antic we attack oh so it's Japanese research lab were working hard to do face to ramen yeah and actually in two in two to three weeks you will learn object detection you know to detect faces and if you learn that maybe you can start a project to like detect a face and then replace it by a ramen because also funny funny work by Naruto me okay oh this is a super cool application as well so let's look at that okay so we have so this model is a conditional gun that was conditioned on learning learning edges and generating cuts based on the edges so I'm gonna I'm gonna try to draw a cat sorry I cannot see again I'm not a good driver it's the cat okay he's going down with the model I hope it's gonna work okay I don't think it works but it's supposed to work so you can generate cats baits on on edges and you can do it for different things you can do it for a shoe so all these model have been trained for that okay yes go for it straighten your feet you have to train it specifically for the domain so like these models are different swing of the 13 train okay looking for my presentation I missed it the presentation disappeared okay another application is super resolution you can give a lower resolution image and generate the super resolution version of it using guns and this is pretty cool because you can get a high resolution image downsample it and use this as the minimax game you know like you have the high resolution version of the lower very lower resolution image other applications can be privacy-preserving so some people have been working on you know in medical in the medical space privacy is a huge issue you cannot share data set among hospitals among medical teams is common so people have been looking at generating a data set that looks like a medical data set if you train a model on this data set is going to give you the same type of parameters than the other one but this data set is anonymized so they can share the anonymized data with each other and train their models and that without being able to access the information of the patient and who is manufacturing is important as well so Gans can generate very specific objects that can replace bones for humans personalized - - to the human body so same for dental if you lose the teeth the the technician can take a picture and decide what the the crown should look like the gun can generate it another topic is how to evaluate guns you know you might say we can just look at the images and see if they look real and it will give us an idea if the gun is working well in practice it's hard because maybe the images you're looking at or overfitting images from the real samples you gave to the to the to the discriminator so how do you check that it's very complicated so human annotation is a big one where you would you would build a software push it on the cloud and people around the world are going to select which images look generated which images look not generated to see if a human can can can compare your gun to real-world data and how your gun performs so it would look like that a web app indicates which image is fake which image is real you can you can do different experiments like you can show very quickly an image for a fraction of a second and ask them was it real or not or you can give them unlimited time different experiments can be led there's another one that is more scalable because the human annotation is very painful you know every time you train again you want to do that to verify if the gun is working well takes a lot of time so instead of using humans why don't we use a very good network that is good at classification in fact in fact the inception network is a tremendous network that does classification we're going to give our image samples to this inception network and see what the network thinks of this image does it think that it's a dog or not does it look like a dog for the network or not and we can scale it and make it very quick and there is a inception score that that we can talk next week about when we will have time it measures the quality of the samples and also it measures the diversity of the sample I'll go over it next week there's another distance that is very popular that has been growing ly popular recently called the fresh inception distance and I advise you to check some of this paper to more interested in it for for your projects so just to end for next Wednesday we'll have C 2 & 3 and also the whole C 3 modules you'll have 3 quizzes be careful these two quiz C 3 M 1 and C 3 M 2 or longer than the normal quizzes there are like wild case studies so take your time and go over it and you'll have 1 programming assignments make sure you understand the batch norm videos so that we can go over the virtual batch norm hopefully next week together and hands-on section this Friday you will receive your project proposal as soon as possible and meet with your project TAS to go over the proposal and to make decisions regarding the next steps for your projects I'll stick around in case you have any questions ok thanks guys