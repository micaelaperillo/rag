thanks for being here five of 2:30 today we have the chance to to host a guest speaker Pranav reporter who is a PhD student in computer science advised by Professor Android and professor Percy Liang so Pranab is is working on AI and high impact projects specifically related to healthcare and natural language processing and today he is going to to present an overview of AI for healthcare and he's going to dig into some projects he has led through case studies so don't hesitate to interact I think we have a lot to learn from Pranav and he's really an industry expert for AI for healthcare and I let you the mic run off thanks for being here thanks Karen thanks for inviting me can you hear me at the back is the mic on alright fantastic well really glad to be here so I want to cover three things today the first is give you a sort of broad overview of what AI applications in healthcare look like the second is bring you three case studies from the lab that I'm in as demonstrations of AI and healthcare research and then finally some ways that you can get involved if you're interested in applying AI to high impact problems in healthcare or if you're from a healthcare background as well let's start with the first so one way we can decompose the kinds of things I can do in healthcare is by trying to formulate levels of questions that we can ask from data at the lowest level are what are descriptive questions here we're really trying to get at what happened then there are diagnostic questions where we're asking why did it happen if a patient had chest pains I took their x-ray what is that chest x-ray show if they have palpitations what is their ECG show then their predictive problems sure I care about asking about the future what's going to happen in the next six months and then at the highest level our prescriptive problems here I'm really trying to ask okay I know this is the patient this is the symptoms they're coming in with this is how their trajectory will look like in terms of in terms of things that may happen that their risk off what should I do and this is the real action point and that's I would say the the goldmine but to get there requires a lot of data and a lot of steps and we'll talk a little bit more about that so in CS 2:30 you're all well aware of the paradigm shift of deep learning and if we look at machine learning in healthcare literature we see that has a very similar pattern is that we had this feature extraction engineer who was responsible for getting from the input to a set of features that a classifier can understand and the deep learning paradigm is to combine feature extraction and the classification into one step by automatically extracting features which is cool here's what I think will be the next paradigm shift for AI in healthcare but also more generally is we still have a deep learning engineer up here that's you that's me that are designing the networks that are making decisions like a convolutional neural network is the best architecture for this problem the specific type of architecture there's an RN and CN n and whatever NN you can throw on there but what if we could just replace out the ml engineer as well and I find this quite funny because everyone you know in AI for healthcare question that I get asked a lot is are we going to replace doctors with all these AI solutions and nobody actually realizes that we might replace machine learning engineers faster than we might replace doctors of this earth this to be the case a lot of research is developing algorithms that can automatically learn architecture some of what you might go through in this class great so that's the general overview now I want to talk about three case studies in the lab of AI being applied to different problems and because healthcare is so broad I thought I'd focus in on one narrow vertical and let us go deep on that and that's medical imaging so I've chosen three problems and one of them's a 1b problem the second is a 2d problem as and the third is a-- is it 3d problem so I thought we could we can walk through all the different kinds of data here so this is some work that was done early last year in the lab where we showed that we were able to detect arrhythmias at the level of cardiologists so arrhythmias are an important problem affect millions of people this has especially come to light recently with devices like the Apple watch which now have a ECG monitoring and the thing about this is that sometimes you might have symptoms and know that you have arrhythmias but other times you may not have symptoms and still have arrhythmias that can be addressed with with if if you were to do an ECG and the ECGs test is basically showing the heart's electrical activity over time the electrodes are attached the skin-safe tests and it takes over a few minutes and this is what it looks like when you're hooked up to all the different electrodes so this test is often done for a few minutes in the hospital and the finding is basically that in a few minutes you can't really capture a person's of normal heart rhythms so let's send them home for 24 to 48 hours with a holter monitor and let's see what we can find there are more recent devices such as the Zeo patch which let let patients be monitored for up to two weeks and it's it's quite convenient you can use it in the shower or while you're sleeping so you really can capture a lot of what what's happening in the hearts ECG activity but if we look at the amount of data that's generated in two weeks it's one point six million heartbeats that's a lot and there are very few doctors who'd be willing to go through two weeks of ECG reading for each of their patients and this really motivates why we need automated interpretation here but automated detection comes with a challenges one of them is you know you have in the hospital several electrodes and in more recent devices we have just one and the way one can think of several electrodes is sort of the electrical activity of the heart is 3d and each one of the electrodes is giving a different 2d perspective into the 3d perspective but now that we have only one lead we only have one of these perspectives available and the second one is that the differences between the heart rhythms are very subtle this is what a cardiac cycle looks like and when we're looking at arrhythmias or normal heart rhythms one's going to look at the sub structures within the cycle and then this the structure between cycles as well and the differences are quite subtle so when we started working on this problem oh maybe I should share this story so we started working on this problem and then it was me my my collaborator on e and and professor Inge and one of the things that he that he mentioned we should do he said let's just go out and read ECG books and let's do the exercises and if you're in med school they're these books where where you can where you can learn about ECG interpretation and then there are several exercises that you can do to test yourselves so I went to the med school library you know they have those hand crank shouts at the bottom see if to move them and then grab my book and then we went for two weeks and did learn sir did go through two books and learn ECG interpretation and it was pretty challenging and if we looked at previous literature to this I think they were sort of drawing upon some domain knowledge er in that here we're looking at waves how can we extract specific features from waves that doctors are also looking at so there was a lot of feature engineering going on and if you're familiar with wavelet transforms they were this sort of they were the most common approach with a lot of sort of like different mother wavelets etc etc pre-processing bandpass filters so everything you can imagine doing what signals was done and then you fed it into your SVM and you called it a day now with deep learning we can change things up a bit so on the Left we have an ECG signal and on the right is just three heart rhythms we're gonna call them a B and C and we're gonna learn the mapping to go straight from the input to the output and here's how we're gonna break it out we're gonna say that every label labels the same amount of the signal so if we had four labels and the ECG would be split into these four sort of this rhythm is labeling this part and then we're going to use a deep neural network so we've built a 1d convolutional neural network which runs over the time dimension of the input because remember we're getting one scalar over over time and then this architecture is 34 layers deep so I thought I'd talk a little bit about the architecture have you seen resonance before okay so should I go into this okay cool here's my 1-minute spiel of ResNet then is that your going deeper in terms of the number of layers that your having in a network you should be able to represent a larger set of functions but when we look at the training error for these very deep networks what we find is that it's worse than a smaller network now this is not the validation error this is the training error that means even with the ability to represent a more complex function we aren't able to represent the training data so the motivating idea of residual networks is to say hey let's add shortcuts within network so as to minimize the distance from the error signal to each of my layers this is just math to say the same thing so further work on ResNet showed that ok we have the shortcut connection how should we make information flow through it the best and the finding was basically that anything you you add to the shortcut to the highway think of these as stop signs or or or signals on a highway and it's basically saying the fastest way on the highways to not have anything but addition on it and then there were a few advancements on top of that like adding dropout and increasing the number of filters in the convolutional neural network that we also added to this network okay so that's the convolutional neural network let's talk a little bit about data so one thing that was cool about this project was that we got to partner up with a with a startup that manufactures these hardware patches and we got data off of patients who were wearing these patches for up to two weeks and this was from around 30,000 patients and this is 600 times bigger than the largest data set that that was out there before and for each of these ECG signals what happened is that each of them is annotated by a clinical ECG expert who says here's where rhythm a starts and here's where ends so let's mark the whole ECG that way obviously very time-intensive but a good data source and then we had a test set as well and here we use here we use a committee of cardiologists so they'd get together sit in a room and decide ok we disagree on the specific point let's try to let's try to discuss which one of us is right or what this rhythm actually is so they arrive at a ground truth after discussion and then we can of course test cardiologists as well and the way we do this is we have them do it individually so this is not the same set that did the ground truth there's a different set of cardiologists coming in one at a time you tell me what's going on here and we're going to test you so when we compared the performance of our algorithm to cardiologists we found that we were able to surpass them on the f1 metrics so this is precision and recall and when we looked at where the mistakes were made we can see that sort of the the biggest mistake was between distinguishing two rhythms which look very very similar but actually don't have a difference in in in treatment here's another case where the model is not making a mistake which the experts are making and turns out this is a costly mistake this is saying a benign heart rhythm or what experts thought was a benign heart rhythm was actually a pretty serious one so that's that's one beauty of automation is that we're able to catch these catch these misdiagnosis here are three hard blocks which are clinically irrelevant to catch on which the model outperforms the experts and on atrial fibrillation which is probably the most common serious arrhythmia the same pulse one of the things that's neat about this application a lot of applications in health care is what automation with deep learning machine learning enables is for us to be able to continuously monitor patients and this is not something we've been able to do before so a lot of even science of understanding how patients risk factors what they are or how they change hasn't been done before and this is an exciting opportunity to be able to advance science as well and the Apple watch has recently released their their ECG monitoring and it'll be exciting to see what new things we can find out about at the health of our hearts from from these inventions okay so that was our first question yeah so repeat the question how was it to to sort of deal with data privacy and sort of keep patients information private so in in this case we did not have we had completely de-identified data so it was just some was ECG signal without any extra information about their their clinical records or anything like that so it's it's very it's very you have like it like signed off that's credible Authority you're like total hospitals I'm getting that to be oh sure and I think we can we can take this question offline as well but one of the beauties of working at Stanford is that there's a lot of Industry research collaborations and we have great infrastructure to be able to work with that so which brings me on to my second case study sorry yeah go for it that's a good question so just to repeat the question how did we define a gold standard when we have experts setting the gold standard so here's how we did it so one one way to come up with the gold standard is to say okay if we looked at what a consensus would say what would they say and so we got three cardiologists in a room to set the gold standard and then to compare the performance of experts these were individuals who were separate from those groups of cardiologists who sat in another room and said what they thought of the of the ECG signals that way there's there's some disagreement where the gold standard is set by the committee great so here we looked at how we can detect pneumonia off of chest x-rays so pneumonia is an infection that affects millions in the u.s. it's big global burden is actually in in kids so that's where it's really useful to be able to detect that automatically and well so to detect pneumonia there's a chest x-ray exam and chest x-rays are the most common imaging procedure with two billion chest x-rays done per year and the way of normalities are detected in chest x-rays is they present as areas of increased density so where things should appear dark they appear brighter or vice-versa and here's what characteristically pneumonia looks like where it's like a fluffy cloud but this is an oversimplification of course because pneumonia is when the alveoli fill up with pus the alveoli can fill up with a lot of other things as well which lead to very different interpretations and diagnosis for the patients and treatment for the patient so it's quite confusing which is why radiologists trained for years to be able to do this the setup is we'll take an input image of someone's chest x-ray and output the binary label 0 1 which indicates the presence or the absence of pneumonia and here we use a 2d convolutional neural network which is pre pre trained on imagenet ok so we looked at short good connections earlier and dense Nets had this idea to take short cut connections to the extreme it says what happens if we connect every layer to every other layer instead of just connecting sort of one instead of having just one short cut which is what ResNet had and dense net beat the previous state of the art and has generally lower error and fewer parameters on the image net challenge so that's what we used for the data set when we started working on this project which was around October of last year there was this large data set that was released by the NIH hundred thousand chest x-rays and this was the largest public data set at the time and here each x-ray is annotated with up to 14 different pathologies and the way this annotation works is there's an NLP system which reads a report and then outputs for each of several pathologies whether there is a mention whether there is a negation like not pneumonia for instance and then annotates accordingly and then for a test set we had four radiologists here at Stanford independently annotate and tell us what they thought was going on in those x-rays so one of the questions that comes up often in medical imaging is we have we have a model we have several experts but we don't really have a ground truth and we don't have a ground truth for several reasons sometimes one of them is just that it's difficult to tell whether someone had pneumonia or not without additional information like their clinical record or even once you gave them and to be antibiotics did they get treated so really one way to evaluate whether a model is better than a radiologists or as well as doing as well as the radiologist is by saying do they agree with other experts similarly so that's what we use here does the idea we say okay let's have one of the radiologists be the the the prediction model we're evaluating and let's set another radiologist to be ground truth and now we're going to compute the f1 score once change the ground truth do it the second time change it again third and then also use the model as the ground truth and do it again and we can use a very symmetric evaluation scheme but this time having the model be evaluated against each of the four experts so we do that and then we get a score for both of them well for all of the experts and for the model and we showed in our work that we were able to do better than the average radiologist at this task two ways to extend this in the future is to be able to look at patient history as well and look at lateral radiographs and be able to improve upon this diagnosis at the time at which we released our work on all 14 pathologies we were able to outperform the previous state-of-the-art okay so model interpretation model interpretations yes this question that you had a future work so almost like okay so if you have pneumonia you present into the stalker without like fever to talk to here with syrup and coffee too much actually although that's not included at the model so thank you my question is that if you're going to a dataset you're trying to determine does this person have pneumonia or that like that's one thing but you don't sound not that you just don't have that data but you're not looking at other differences let's take this out that's not cancer resemblance to have like job other longer because he'll call grass justice it's all those are images that you're not really looking at so let's say and that's been a tough situation so the obvious situation doesn't really give you much to it right but the tough situation is you get a patient that has a fever it's coffee violently cancer or pneumonia or black lung disease then how do you how do you get your operative working that condition and also if you're not including all those other cases then it's not just that what's the use of it but like you'd only say yeah and this alone so I'm trying to keep it at this technical to the technical class what's and is there a neural network architecture that you would use to be able to solve a number one it's a multi task learning isn't it like sure sure okay so let me try to boil those sort of sets of questions down so one is patients are coming in we're not getting access to their clinical histories so how are we able to make this determination at all so one thing is that when we're training the algorithm we're training the algorithm on on pathologies extracted from radiology reports and these radiology reports are written with understanding of full clinical history and understanding of sort of what the patient presented with in terms of symptoms as well so we're training the model on on these radiology reports which had access to more information and the second is that the utility of this is not as much in being able to compare a patient's x-rays day to day as much as here is a new patient with a set of symptoms and can we identify things from their chest x-rays which brings us to model interpretation so if you were a end-user for model oh I so when I was back in undergrad and I was in the lab we were working on autonomous cars and I thought about this a lot how many of you have been in an autonomous car how many if you would trust being an autonomous car yeah I thought about this as well would I trust being an autonomous car and I thought it'd be pretty sweet if the algorithm that was that was in the car would tell me whatever decision it was going to make in advance I know that's not possible at high speeds so that you know just in case I disagreed with a particular decision I could say no abort and and half the model sort of you know remake its decision and I think the same holds true in healthcare as well the one advantage that happens in healthcare is rather than having to make decisions within seconds like in the case of autonomous car there is often a larger time frame like minutes or hours that we have and and here it's it's useful to be able to inform the clinician that's treating the patient to say hey here's what my model thought and why so here's the technique we use for that class activation maps which you may cover in another lecture so I'll just I'll just leave it at saying that there are ways of being able to look at what parts of the image are most evident of a particular pathology to generate these these heat maps so here's a heat map that's generated for pneumonia so this x-ray has pneumonia and I can and and and the algorithm in red is able to highlight the areas where it thought was most problematic for that here's one in which it's able to do a collapsed right lung here's one in which Abel is able to find a small cancer and here the goal is to be able to improve healthcare delivery where in the developed world one of the things that it's useful for is to be able to prioritize the workflow make sure the radiologists are getting to the patients most in need of care before once who's x-rays look more normal but the second part which I'm quite excited about is to increase the access of medical imaging expertise globally where right now the World Health Organization estimates that about two-thirds of the world's population does not have access to Diagnostics and so we thought hey wouldn't it be cool if we just made an app that was able to allow users to upload images of their off x-rays and be able to give its diagnosis so this is still in the works so I'll show you what we've got running locally and so here I'm presented with a screen that asked me to upload an x-ray and so I have I have several x-rays here and I'm gonna pick the one that says cardiomegaly so cardiomegaly refers to the enlargement of the heart so I uploaded it now it's running the models running in the backend and within a couple of seconds its outputted its diagnosis on the right so you'll see the 14 pathologies that the model is trained on being listed and then next to them a bar and at the top of this list is cardiomegaly which is what this patient has the hardest sort of out and if I hover on cardiomegaly I can see that the probability is displayed on there and that we talked about interpretation how do I believe that this model is actually looking at the heart rather than looking at something else and so if I click on it I get the class activation map for this which shows that indeed it is focused on the heart to be able to and and is looking at the right thing so I guess you can say the algorithms hearts in the right place cool but I thought so this is an image that I got from the the data set that we were using NIH but it's pretty cool if an algorithm is able to generalize to populations beyond and so I thought what we do is we could just look up look up an image of cardiomegaly and download it and just see if our model is able to this one looks pretty large so does this I don't want an annotated one all right that's good so we can do that save it desktop and now we can upload it here and it's already we done a thing and on the top is cardiomegaly once again so it's able to generalize to and there it's the highlight so it's able to generalize to populations beyond just the ones it was trained on so I'm very excited by that and what I got even more excited by is we're thinking of deploying this out in out in different parts of the world and when we got an image that showed how x-rays are read in this hospital that we were working with in Africa this is what we saw and so the idea that one could snap a picture and upload it seems and get a diagnosis seems very powerful so the third case study I want to take you through is being able to look at M R so we've talked about 1d a 1d setup where we had an ECG signal we've talked about a 2d setup with an x-ray how many of you thinking of working on a 3d problem for your project whew that's good cool so here we looked at niyama so mrs of the knee is the standard of care to evaluate knee disorders and more mr examinations are performed on the knee than any other part of the body and the question that we sought out to answer was can we identify knee of normalities two of the most common ones include an ACL tear and a meniscal tear at the level of radiologists now with the 3d problem one thing that we have that we don't have in a 2d setting is the ability to look to look at the same same thing from different angles and so when radio I'll just do this diagnosis they look at three views the sagittal the coronal and the axial which are four to three ways of looking through the 3d structure of the knee and in an mr you get different types of series based on the magnetic fields and so there are three different series that are that are used and what we're gonna do is output for a particular knee amar examination the probability that it's abnormal the probability of an ACL tear and the probability of a meniscal tear important thing to recognize here is this is not a multi-class problem in that I could have both types of tears it's a multi-label problem so we're going to train a convolutional neural network for every view pathology pair so that's nine convolutional networks and then combine them together using a logistic regression so here's what each convolutional neural network looks like I have a bunch of slices within a view I'm gonna pass each of them to a feature extractor I'm gonna get an output probability so we had a thousand four hundred knee mr exams from the Stanford Medical Center and we tested on 120 of them where the majority vote of three subspecialty radiologists established the ground truth and we found that we did pretty well on on the three tasks and had the model be able to pick up the different abnormalities pretty well and one can extend these these methods of interpretive interpretability to to 3d 3d inputs as well so that's what we did here okay so I I saw this I saw this cartoon a few a few weeks ago and I thought it was pretty funny which is a lot of machine learning engineers think that they don't need to externally validate which is find out how my model works on works on data that's not my where my original data set came from so there's a there's a difference in in distribution but it's really quite exciting when a model does generalize to two data sets that it's not seen before and so we got this data set that's that's public from a hospital in Croatia and here's how it was different so it was a different it was a different kind of series two different magnetic properties is a different scanner and it was a different institution in a different country and we asked okay what happens when we run this model off the shelf that was trained on Stanford data but tested on that kind of data and we found that it did relatively well without any training at all but then when we trained on it we found that we were able to outperform the previous lis best reported result on the data set so there's still some work to be done in being able to generalize sort of my network here that was trained on my data to be able to work on datasets from different institutions different countries as well but we're making some steps along that way remains a very open problem for taking yeah so we did the best we could in terms of processing so we had so one of the pre-processing steps that's important is being able to get the mean of the of the input data to be as close to the mean of the input data that you train up so that was one pre-processing step we tried when we were trying to minimize that to say out of the box how would this work if we had never seen this data before how would it work on that population so one big topic in across a lot of applied fields is asking question okay we're talking about models working automatically autonomously how would these models work in when working together with experts in different fields and here we ask that questions about radiologists and about imaging models would it be possible to be able to boost the performance if the model and the radiologists work together and so that's really the set up a radiologists wit model is that better than the radiologists by themselves and here's how we set it up we said let's have experts read the same case twice separated by a certain set of weeks and then see how they would perform on the same set of cases and what we found that we were able to increase the performance generally with a significant significant increase in specificity for ACL tears that means if someone if a patient came in without a without an ACL tear I'd be able to find it better so in the future yes question the opinion of the radiologists art is that that intended thing that one kind of bias anything import actually looks at the Commission's out yeah so that's a good question and I and I think how so the sort of automation bias captures a lot of this and that once we have sort of models working with experts together can we expect that the experts will sort of take it less seriously that's that's a big concern and start relying on what the model says and says I won't even look at this exam I'm just going to trust what the model says blindly that's absolutely possible in a very open area of research some of the ways that people have tried to address it is to say you know what I'm gonna do from time to time I'm going to pass in an exam to the radiologist for which I'm going to flip the answer and I'll know the right one and if they get that wrong I'll alert them that you're relying too much on the model stop but there are a lot of more sophisticated ways to go about addressing automated bias and as far as I know it's a very open field of research especially as we're getting into deep learning assistants and one utility of this is to say basically that the set of patients don't need follow-up let's not send them for unnecessary surgery great so I shared three case studies from lab the final thing I want to do is to talk a little bit about how you can get involved if you're interested in applications of AI to healthcare so the first is the ability for you to just get your hands dirty with datasets and and be able to try out your own model so we have from our lab released the Morra dataset which is a large data set of bone x-rays and the task is to be able to tell if it's if the x-rays are normal or not and they come from different parts of the of the upper body and that's that's what the dataset x-rays look like and this is a pretty interesting setup because you have more than one view more than one angle for the same body part for the same study for the same patient and the goal is to be able to combine this well into convolutional neural network and and be able to output the probability of abnormality and one of the interesting things here for transfer learning as well is do you want to train the models differently per body part or do you want to train them train the same model for body parts or combine certain models it's a lot of design decisions there and this is what train some train models look like this is a model baseline that we released that's able to identify a fracture here and a piece of hardware on the right and you can download the data set of our website so if you google Maura data set or go on our website Stanford ml group github I oh you should be able to find it the second way to get involved is through the AF for healthcare bootcamp which is a two quarter long program that our lab runs which provides students coming out of classes like 2:30 an opportunity to get involved in research and here's students receive training from PhD students in the lab and medical school faculty to work on structured projects over two quarters and if you have a background in sir AI which you do then you're encouraged to apply and we're working on a wide set of problems across radiology EHR Public Health and pathology right now this is what the lab looks like we have a lot of fun and the applications for the bootcamp starting in the winter are now open so the early applications deadline is remember 23rd and you can go on this link and and and apply so that's my time thank you so much for having me and thanks for having me can yes I'll take a couple questions you asked a question about privacy concerns in terms of other ethics concerns what about compensation for the medical experts figures potentially putting out of business with a rule like the one day you're you're developing or you know and just in general because their their knowledge is being used to train these models it's not free yeah so the question was we're having these automated AI models trained with the knowledge of medical experts what are ways in which we're thinking of compensating these medical experts right now or in the future when we have possibly automated models I think a lot of people are thinking about these problems and working on them right now there are a variety of approaches that people are thinking about in terms of economic incentives and there's a lot of fear about sort of well AI actually work with or augment experts in whatever field they're working on I don't have a great silver bullet for this but I know there's there's a lot of work going on in there when you're eating through MRIs we show looking at four or five category of issues like one of them is most likely it's possible that a human looking at it could point out something that was not being looked at by the AI model at that time yeah so how do you yeah that's a great question so that just to repeat the question it's we have we're looking at M our exams and we're saying from these three pathologies were able to output the probabilities what happens if there's another pathology that we haven't looked at so I have a couple of answers for that the first is that one of the one of the categories here was simply to tell whether it was normal or abnormal so the idea here is that the abnormality class will capture a lot of different pathologies there at least the ones seen at Stanford but it's often the case that we're building for one particular pathology and then there's obviously a a burden on the the model and the model developers to be able to convey hey look our algorithm model only does this and you really need to watch out for everything else that the model doesn't cover maybe that's the unless there's one more question no all right that's the last question we'll take then thank you once again so now you've got you've got the the perspective is the microphone working yeah now you've got the perspective of may I researcher working in healthcare now you are going to be the AI research researcher working in healthcare we're going to go over a case study that is targeted at skin disease so you know in order to detect skin disease sometimes you take pictures microscopic pictures of cells on your skin and then analyze those pictures so that's what we're going to talk about today so let me talk about the problem statement your deep learning engineer and you've been chosen by a group of healthcare practitioners to determine which parts of a microscopic image corresponds to a cell okay so here's how it looks like on the the black and white it's not a black and white image it's a color image which looks black and white the input image is the one that is closer to me and the yellow one is the ground truth that has been labeled by a doctor let's say so what you're trying to do is to segment those cells on this image and we talk about segmentation yet or a little bit segmentation is is about producing value a class for each of the pixels on our image so in this case each pixel would correspond to either no cell or cell zero or one and once we output a matrix of zeros and ones telling us which pixels correspond it to a cell we should get hopefully a mask like the yellow mask that I overlapped with the input image does that make sense yeah color image the other one you don't have the boundaries for yourself yeah we'll talk about the boundary later but right now assume it's a binary segmentation so 0 & 1 no cell and cell okay so it's going to be very interactive and I think we're going to use Monte for several question and group you guys into groups of three so here are other examples of images that were segmented with a mask now doctors have collected 100,000 images coming from microscopes but the images come from three different microscopes there is a type a Type B n type C microscope and the data is split in between these three as 50% for type a 25% for type B 25% for type C the first question I'll have for you is given that the doctors want to be able to use your algorithm on images from the microscope of type C this microscope is the latest one it's the one that is going to be used widely in the field and they want your your network to work on this one how would you split your data set into trained dev and test set as a question and please group in teams of two or three and discuss it for a minute on how you would split this data set you can start going on men's Ian and write down your answers as well okay so take a 30 seconds to input your your insights on on mentee you can do one per team and we'll start going over some of the answers here okay the f-test sleaze split see train on a plus B 20k in train 2.5 in Devon test training 80 on a all the 5k see deaf 10k see test 10k see 95 5 where tests and them is from population we care about I think these are good answers I think there is no perfect answer to that but two things to take into consideration you have a lot of data so you probably want to split it into 95 5 closer to that than 260 2020 and most importantly you want to have see images in the test dev and test set to have the same distribution among these two that's what you've seen in the third course and we would prefer to have actually see images in the train set you want your algorithm to have seen see images so I would say you're very good answer is is this one 1955 where the five and five are exclusively from see and you also have see images in the 90% of train images any other insights on that what your grease yeah how do we attack that like microscopes Nvidia doesn't have my hidden you know features that mess up any yeah so there is much more thing we didn't talk about here one is how do we know what's the distribution of microscope a images and microscope images versus microscope see do they look like each other if they do all good if they don't how can we how can we make sure the model doesn't get bad hints from these two distributions another thing is data date augmentation we could augment this dataset as well and try to get as much as C distribution images as possible we're going to talk about that okay speed has to roughly be 95 5 not 60 20/20 distribution of dev and test sets has to be the same contain images from CN there also there should also be see images in the training set now talking about date augmentation do you think you can augment this data and if yes give only 3ds things method you would use if no xsplit explicate explain why you cannot you want to take 30 seconds to talk about it with your neighbors okay okay guys let's go over some of the answers so rotation zoom blur I think looking at the images that we have from the cells this might work very well rotation zoom blur translation combination of those stretch symmetry like probably a lot of those work a one follow-up question that I'll have is can you can someone give an example of a task where the augmentation might hurt the model rather than helping it if you want to overfeed on the test set can you be more precise like then you don't want to generalize oh you don't want your model to generalize too much okay yeah that did there's some cases where you don't want them what model to generalize too much especially no to an encoder but any any other ideas you're doing like face detection move on the face be like upside down or like either side I see so if you do face detection you probably don't want the face to be upside down although we never know depending on the use but it's it's not gonna help much if the camera is always like that and it's filming humans that are not upside now and yeah but I don't think it's gonna hurt the model it's probably going to not help the model I guess yeah if you really stretch the image so there's our algorithms like maybe you know flow net it's an algorithm that that's used for long videos to detect the speed of the car let's say if you stretch the images probably you cannot detect the speed of the car anymore any other examples [Music] character recognition I think it's a good example so let's say you you're trying to detect pcs and you do symmetric flip and you get that you know like your your labeling is be everything that was D and as d everything that was B for nine and six it's the same story so these data augmentations are actually hurting the model because you don't real able when you data when you match your data right okay okay so yeah many augmentation methods are possible cropping adding random noise changing contrast I think the atomic chain is super important I remember a story of of a company that was working on a self-driving cars and and also virtual assistants in cars you know like this type of interaction you have with someone in your car a virtual assistant and they noticed that the speech recognition system was actually not working well when the car was going backwards like no idea why like why is this doesn't seem related to the speech recognition system of the car and they test it out and they looked and they figured out that people were putting their hands in the passenger seat looking back and talking to the virtual assistant and because the microphone was in the front the voice was very different when you were talking to to the back of the car rather than the front of the car and so they used date augmentation in order to augment their current data they didn't have de town that type of of people talking to the back of the car so by augmenting smartly you can change the voices so that they look like they were used by someone who was talking to the back of the car and then solve the problem okay small question we can do it quickly what is the mathematical relation between NX and NY so remember we have an RGB image and we can we can flatten it into a vector of size n X and the output is a mask of size and Y what's the relationship between NX and NY someone wants to go for it yeah three boys their equality who thinks they record good things they're not equal and why because you have RGB on this side and why would be 3 and X I saw n X will be 3 and Y because you have RGB images and for each RGB pixel you would have 1 output 0 or 1 ok that was a question on one of the midterms was a complicated question what's the last activation of your network sigmoid you want probably an output 0 & 1 and if you had several classes so later on we will see we can also segment per DC's then you would have the softmax what loss function should we use I'm going to give it to you to go quickly because we don't have too much time you're going to use a binary cross-entropy loss over all the outputs the entries of of the outputs of your network doesn't make sense so always think the thinking through the last function is interesting ok so you have a first try and you've coded your own neural network that you've yelled f2 you've named model n1 m1 and you've trained it for a thousand eight bucks it doesn't end up performing well so it looks like that you give it the input image through the model and get an output that is expected to be the following one but it's not so one of your friends tells you about transfer learning and they they tell you about another labelled data set of 1 million microscope images that have been labeled for skin disease classification which are very similar to those you want to work with from microscope C so a model m2 has already been trained by another research lab on this new data set on a 10 class disease classification and so here is an example of input output of the model you have an input image that probably looks very similar to the ones you're working on the network has a certain number of layers and softmax classification at the end that gives you the probability distribution over the disease that seems to correspond to this image so they're not doing segmentation anymore right they're doing classification okay so the question here is going to be you want to perform transfer learning from M 2 to M 1 what are the hyper parameters that you will have to tune it's more difficult than it looks like so think about it discuss with your neighbors for a minute try to figure out what are the hyper parameters involved in this transfer learning process okay take 15 more seconds to wrap it up okay let's see what you guys have learning rate it is a hyper parameter I know if it's specific to the transfer learning weights of the last layers so I don't think that's a high parameter weights are parameters new cost function for additional output layers I think that's the hyper that the choice of the loss you might count it as a high parameter I don't think it's specifically related to transfer learning you will have to train with the loss you've used on your model m1 number of new layers yeah weights of the new not a hyper parameter okay last one or Twitter layers of M - so do we train what do we fine-tune it's a lot about layers actually size of added layers not sure okay let's go over it together because it seems that there's a lot of different answers here you try to write it down here so let's say we have we have the model M - is it big enough for the back we have the model M - and so we give it an input image okay input and the model M 2 gives us a probability distribution softmax so we have a soft max here you will agree that we probably don't need the softmax layer we don't want it we want to do such segmentation so one thing we have to choose is how much of this pre-trained network because it's a pre-trained network how much of this network do we keep let's say we keep these layers because they probably know the inherent salient features of the data set like the edges of the cells that were very interested in so we take it so we have it here and you agree that here we have a first high parameter that is L the number of layers from m2 that we take now what other hyperparameters do we have to choose this is L we probably have to add a certain number of layers here in order to produce our segmentation so there's probably another hyper parameter which is l0 how many layers do I stack on top of this one and remember these layers are pre-trained but these ones are randomly initialized that make sense so too hyper parameters anyone see the third one the third one comes when you decide to train this new network you have the input image give it to the network get the output segmentation mask segmentation mask let's say sag mask and what you have to decide is how many of these layers will I freeze how many of the pre train layers I freeze probably if you have a small data set you'd prefer keeping the features that are here freezing them and focusing on retraining the last few layers so there is another high parameter which is how much of this will I freeze LF what does it mean to freeze it means during training I don't train these layers I assume that they've been seeing a lot of data already they understand very well the edges and less complex features of the data I'm going to use my new my small data set to Train the last layers so three hyper parameters l l0 and LF that makes sense okay so this is for transfer learning so it looks more complicated than the question the question was more complicated and it looked like okay let's move where am i okay let's go over another question okay so this we did it now it's interesting because here we have an input image and in the middle we have the output that the doctor would like but on the right you have the output of your algorithm so you see that there is a difference between what they want and what we're producing and it goes back to someone mentioned it earlier there is a problem here how do you think you can correct the model and/or the data set to satisfy the doctor's request so the issue with with this image is that they want to be able to separate the cells among them and they cannot do it based on your algorithm it's still little hard there is there's something to add so can someone come up with the answer or do you want to explain actually you mention one of the answers so that we we can finish this lecture now it looks like you could have like three cells on the bottom left blurring it together it and so if you ask for adding boundaries it makes the cells more well-defined good answer so one way is when you label your datasets originally you label it with zeros and ones for every pixel now instead you will label with three classes zero one or boundary likes let's say 0 1 2 4 boundary or even the best method I would say is that for each pixel for each input pixel the output will be the corresponding okay this one is not good the corresponding label like this is a sell picture he off sell P of boundary and P of no sell what you will do is that instead of having a sigmoid activation you will use a soft max actuation okay and the soft max will be for a pixel one other way to do that if it still didn't work doesn't work even if you label the boundaries what is another way to do that Yury label your datasets by taking into account the boundaries the model still doesn't perform what I think it's all about the waiting of the last function it's likely that the number of pixels that our boundaries are going to be fewer than the number of pixels that ourselves or no cells so the network will be biased towards predicting cell or no cell instead what you can do is when you compute your loss function your loss function should have three terms one binary cross entropy let's say for no cell 1 4 cell and one 4 boundary okay and this is going to be summed over I equals 1 to N I the whole output pixel values what you can do is to atribute a coefficient to each of those alpha beta or 1 and by tweaking its coefficient if you put a very high a very low number here in the it means you're telling your model to focus on the boundary you're telling the model model that if you miss the boundary it's a huge penalty we want you to train by figuring out all the boundaries that's another trick that you could use one question on that yeah good question what do I mean by really really willing your dataset this this last try this section you've been labeling bounding boxes you know for the yellow algorithm so the same tools are available for segmentation where you have an image and you would draw the different lines in practice if the more if the tool that you were using the line used will just count as a cell everything including the language with everything inside what you draw plus the boundary would count as cell and the rest has no cell it's just a line of code to make it different the line you drew will count as boundary everything inside will count as cell and everything outside would count as no cell so it's the way you use your labeling tool that's all I think it's not learnable parameters it's more hyperparameters to tune you know the same way you tune lambda for your regularization you would tune alpha and beta so when you make it so this is not an attention mechanism because it's just a training trick I would say you cannot know how much attention we tell you for each image how much the model is looking at this board versus that part this is not going to tell you that it's just a training trick what's the advantage to doing it this way suppose like object detection like the tank thing so the question is what's the advantage of the segmentation rather than detection yeah so detection means you want to output a bounding box if you output a bounding box what you could do is output the bounding box crop it out and then analyze the cell and try to find the contour of the cell but if you want to separate the cells if you want to be very precise segmentation is going to work well if you want to be very fast bounding boxes would work better and you guys too the way segmentation is not working as fast as the yo logarithm works for object detection yeah I would say that but it's more much more precise okay so modify the datasets in order to label the boundaries on top of that you can change the last function to give more weights to boundaries or penalize false positives okay we have one more slide I think so let's go over it so now the doctors they give you a new data sets that contain images similar to the previous ones the difference is that each mean image now is labeled with zero and one zero meaning there are no cancer cells on that image and one means there is at least a cancer cell on this image so we're not doing segmentation anymore it's a binary classification image cancer or no cancer okay so you easily build the state-of-the-art model because you're you're a very strong person in classification and you achieve 99% accuracy the doctors are super happy and they ask you to explain the networks prediction so given an image classified as one how can you figure out based on which cell the model predicts one soprana I've talked a little bit about that there are other methods that you should be able to figure out right now even if you don't know class activation Maps so to sum it up we have an image input image put it in your new network that is a binary classifier and the network says one you want to figure out why the network size one based on which pixels what do you do huh visualize ways visualize the weight what do you visualize in the words so I think visualizing the weights is not related to the input the weights are not gonna change based on the input so here you want to know why this inputs led to one so it's not about the weight good idea so you know after you get the one here this is why hats basically it's not exactly one let's say it's point seven probability what you can remember is that this number derivative of Y hat with respect to X is what it's a matrix of shape same as X you know two matrix and each entry of the matrix is telling you how much moving this pixel influences Y hat you agree so the top left number here is telling you how much X 1 is impacting Y hat is it or not maybe it's not if you have a card detector and the cat is here you can change this this pixel it's never gonna change anything so the value here is going to be very small closer to zero let's assume the cancer cell is here you will see high number in this part of the matrix because this this these are the pixel that if we move them it will change Y hat does it make sense it's a quick way to interpret your network it doesn't it's not too too good like you're not gonna have tremendous results but you should see these pixels have higher derivative values than the others okay that's one way and then we will see in two weeks how to interpret neural networks visualizing the weights included and all the other methods okay so gradient with respect to your model detects cancer cells from the test set images with 99% accuracy while a doctor would on average perform 97% on the same task is this possible or not who thinks it's possible to have a network that achieves more accuracy on the test set than the doctor okay can someone can someone say why you have an explanation okay the network probably looks at complex things that doctor didn't say they didn't see that's what you're saying possibly I think there's a more rigorous explanation so here we're talking about Bayes they're human level performance and all that stuff that's when you should see it so one thing is that there are many concepts that you will see in course tree that are actually implemented in the industry but it's it's not because you know them that you're going to understand that it's time to use them and that's what we want you to get to like now when I ask you this question you have to talk think about pacer human level accuracy and so on so the question that you should ask here is what was the data set labeled what were the labels coming from if the data set was labeled by individual doctors I think that looks weird like if it was labeled by individual doctors I think it's very weird that the model performs better on the test set then what doctors have labeled because simply because the labels are wrong three percent of the time on average the labels are wrong so you're you're teaching wrong things to your model three percent of the time so it's surprising that it gets better could happen but surprising but if every single image of the dates that has been labeled by a group of doctors as pronoun I've talked about it then the average accuracy of this group of doctor is probably higher than one doctor maybe it's 99 percent in which case it makes sense that the model can beat one doctor this is make sense so you have a sir you're trying to approximate with we'd like the best error you can achieve so we're grouping grouping a cluster of doctors probably better than one doctor this is your human level performance and then you should be able to beat one doctor it's like okay so you want to build a pipeline that goes from image taken by the front of your car to a steering direction for autonomous driving what you could do is that you could send this image to a car detector that detects all the cars a pedestrian detector that detects all the pedestrians and then you can give it to a pass cleaner let's say that cleanse the path and outputs the steering direction let's say so it's not n to it and two n would be I have an input image and I give it I want this output so a few other disadvantages of this is is a something can go wrong anywhere in the model you know how do you know which part of the model went wrong can you tell me which part I'll give you an image the same direction is wrong why good idea looking at the different components so what you can do is look what happens here and there look what's happening here and there you think based on this image the car detector worked well or not you can check it out do you think the pedestrian detector worked well not you can check it out if there is something wrong here it's probably one of these two items it doesn't mean this one is good it just means that these two items are wrong how do you check that this one is good you can label ground truth images and give them here as input to this one and figure out if it's figuring out the steering direction or not if it is it seems that the path planner is working what if it is not it means there's a problem here now what if every single component seemed to work properly like let's say these to work properly but there is still a problem it might be because what you selected as a human was wrong the path standard cannot detect cannot get the steering Direction correct based on only the pedestrians and the car detection and the cars probably need the stop signs and stuff like that this way you know and so because you made hand engineering choices here your model might go wrong that's another thing and another advantage of this type of pipeline is that data is probably easier to find out at M for every algorithm rather than the for whole the whole end-to-end pipeline if you want to collect data for the entire pipeline you would need to take a car put a camera in the front like like build a kind of steering wheel angle detector that will measure your ceiling wheel at every step while you're driving so you need to drive everywhere basically with a car that has this feature it's pretty hard you need a lot of data a lot of roads while this one you can collect data of images anywhere and label it's a label the pedestrians on it you can detect cars by the same process okay so these choices also depend on what data can you access easily or what data is harder to acquire any questions on that you're going to learn about convolution on your networks now we're gonna get fun with a lot of imaging you have a quiz into programming assignment for the first module second module same midterm next Friday not this one everything up to C for m2 will be included in the meter so up to the videos you're watching this week includes TA sections and next one and every in-class lecture including next Wednesday's in this Friday you have a TA section any questions on that okay see you next week guys