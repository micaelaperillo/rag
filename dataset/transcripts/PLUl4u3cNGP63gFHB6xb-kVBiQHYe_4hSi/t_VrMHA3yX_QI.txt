the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT open courseware at ocw.mit.edu [Music] well what we're going to do today is climb a pretty big mountain because we're going to go from a neuron net with two parameters to discussing the kind of neural Nets in which people end up dealing with 60 60 million parameters so it's going to be a pretty big jump but along the way our couple things I wanted to uh underscore from our previous discussion last time I I tried to develop some intuition for the kinds of formulas that you use to actually do the calculations in a small neural net about how the weights are going to change and the main thing I tried to emphasize is that when you have a a neural net like this one everything is sort of divided uh in each column you can't have the Performance Based on this output affect some weight change back here without going through this finite number of output variables the y1s by the way there's no Y2 and y4 this is there's no Y2 and Y3 the dealing with this is a really a notational nightmare and I spent a little time a lot of time yesterday trying to clean it up a little bit but basically what I'm trying to say has nothing to do with the notation I've used but rather with the fact that there's a limited number of ways in which that can influence this even though the number of paths through this network can be growing exponential so those equations underneath are equations that are derive from trying to figure out how the output performance depends on some of these weights back here and what I've calculated is I've calculated the dependence of the performance on W1 going that way and I and I've also calculated the dependence on performance of performance on W1 going W1 going that way so that's one of the equations I've got down there and another one deals with W3 and it involves going both this way and this way and all I've done in in both cases in all four cases is just take the partial derivative of performance with respect to those weights and use the chain rule to expand it and when I do that this is the stuff I get and that's just a whole bunch of partial derivatives but if you look at it and let it sing a little bit to you what you see is that there's a lot of redundancy in the computation so for example this guy here partial of performance with spec W1 depends on both Paths of course but look at the first elements here these guys right here and look at the first elements in the expression for calculating the partial derivative of performance respective W3 these guys they're the same and not only that if you look inside these expressions and look at this particular piece here you see that that is an expression that was needed in order to calculate one of the downstream weights changes one of the downstream weights but it happens to be the same thing as you see over here here and likewise this piece is the same is the same thing you see over here so each time you move further and further back from the outputs toward the inputs you're reusing a lot of computation that you've already done so I've trying to find find a way to sloganizer what's done is done and cannot be no no no that's not quite right is it it's um what's computed is computed and need not be recomputed okay so that's what's going on here and that's why this is a calculation that's linear in the depth the depth of the neural net not exponential there's another thing I wanted to point out in connection with uh these neural Nets and that has to do with what happens when we look at a single neuron and note that what we've got is we've got a bunch of Weights which are multiplied times a bunch of inputs like so and then those are all summed up in a summing box before they enter some kind of nonlinearity in our case a sigmoid function but if I ask you to write down the expression for the value we've got there what is it well it's just the sum of the W's times the X's what's that that's a DOT product remember a few lectures ago I said that some of us believe that the dot product is a fundamental calculation that takes place in our heads so this is why we think so if neural Nets are doing anything like this then there's a DOT product between some weights and some input values now it's a funny kind of dot product because in the models that we've been using these input variables are all or none or zero or one but that's okay I have an in good authority that there are neurons in our head for which the values that are produced are not exactly all or none but rather have a kind of proportionality to them so you get a real dot product type of operation out of that so that's by way of a couple of asides that I wanted to underscore before we get into the um Center of today's discussion which will be to uh talk about these so-called deep Nets now let's see what's a deep net do well from last time you know that a deep net does that sort of thing and it's interesting to look at some of the uh some of the offerings here by the way how how good was this uh performance in 2012 well it turned out that the fraction of the time that the system had the right answer in its top five choices was about 15% and the fraction of the time that it got exactly the right answer as its top pick was about 37% uh error 15% error if you count it as an error if it's what am I saying you got it right if you got it in the top five and the error rate on that calculation about 15% if you say you only get it right if you got it if it was your Top Choice then the error rate was about 37% so pretty good uh especially since some of these things are highly ambiguous even to us and what kind of a system did that well it wasn't um it wasn't uh one that looked exactly like that although that is the essence of it the system actually looked like that there's quite a lot of stuff in there and what I'm going to talk about is not exactly this system but I'm going to talk about the stuff of which such such systems are made because there's nothing particularly special about this it just happens to be a particular assembly of components that tend to reappear when everyone when anyone does this sort of neural net stuff so let me explain that uh This Way first thing we I need to talk about is the concept of um well I don't like the term it's called convolution I don't like the term because you know in in the second best course at The Institute signals and systems you learn about impulse responses and convolution integrals and stuff like that and this hints at that but it's not the same thing because there's no there's no memory involved in in what's going on as these signals are processed but they call it convolutional don't that's anyway so here you are you've got some kind of image and even with lots of uh computing power uh and gpus and all that sort of stuff we're not talking about images with you know 4,000 4 million pixels we're talking about images that might be 256 on a side as I say we're not talking about images that are a thousand by a thousand or 4,000 by 4,000 or anything like that they tend to be a uh kind of compressed into a 256 by 256 image and now what we do is we um run over this with a neuron that is looking only at a 10x10 square like so and that produces an output and next we run over that uh again having shifted this Neuron a little bit like so and then the next thing we do is we shift it again so we get that output right there so each of those deployments of a neuron produces an output and that output is associated with a particular place in the image this is a process uh that is called convolution as a term of Art this guy this convolution operation results in a bunch of points over here and the next thing that we do with those points is we look in local neighborhoods and see what the maximum value is and then we take that maximum value and we construct yet another mapping of the image over here using that maximum value then we slide that all over like so and we produce another value and then we slide that value then we slide that over one more time with a different color and now we got get another value so this process is called pooling and because we're taking the maximum this particular kind of pooling is called Max pooling so now let's see what's next um this is taking a particular neuron and running it across the image but maybe there are lots of we call that a kernel uh again uh sucking some terminology out of signals and systems but now what we're going to do is we're going to say we're going we we can use a whole bunch of kernels so the thing that I produced with one kernel can now be repeated many times like so in fact a typical number is 100 times so now what we've got is we've got a 256x 256 image we've gone over it with a 10x10 kernel we've taken the maximum value of values that are in the vicinity of each other and then we've repeated that 96 100 times so now we can take that and we can feed all those results into some kind of neural net and then we can do perhaps a fully connected job on the final layers of this and then the in the ultimate output we get some sort of indication of How likely it is that the thing that's being seen is say a might so that's roughly how these things work so what have we talked about so far we've talked about pooling and we've talked about convolution and now we can talk about some of the good stuff but before I I get into that this is this is what we can do now and and and you can compare this with what was done in the old days it was done in the old days before massive amounts of computing became available uh is a kind of neural net activity that's a little easier to see you might in the old days only have enough computing power to deal with a small grid of picture elements or so-called pixels and then each of these might be a value that is fed as an input into some kind of neuron and so you might have a column of neurons that are looking at these pixels in in in your in your in your image and then there might be a small number of columns that follow from that and finally something that say this neuron is looking for things that are a number one that is to say something that looks like a number one in the image so this stuff up here is a con is what you can do when you have a massive amount of computation rela to the kind of thing you used to see in the old days so what's different well what's different is instead of a few hundred parameters we've got a lot more instead of 10 digits we have a thousand classes instead of a few hundred samples we have maybe a thousand examples of each class so that makes a million samples and we got 60 million parameters to play with and the surprising thing is that the net result is we've got a function approximator that astonishes everybody and no one quite knows why it works except that when you throw an immense amount of computation uh into this kind of arrangement it's possible to get a performance that no one expected would be possible so that's sort of the bottom line but now uh there are a couple of ideas beyond that that I think are especially interesting and I I want to talk about those first idea that's especially interesting is the idea of Auto coding and here's how the idea of Auto coding works I'm going to run out of board space so I think I'll do it right here you have some input values they go into a layer of neurons the input layer then there is a so-called hidden layer that's much smaller so maybe there are in an example there'll be 10 neurons here and just a couple here and then these expand to an output layer like so now we can take the output layer Z1 through ZN and compare it with the with desired values D1 through DN you following me so far now the trick is to say well what are the desired values let's let the desired values be the input values so what we're going to do is we're going to train this net up so the output's the same as the input what's good of that but we're going to force it down through this neck down piece of network so if this network is going to succeed in taking all the possibilities here and cramming them into this smaller inner layer this so-called hidden layer such that they it can reproduce the input at the output it must be doing some kind of generalization of the kinds of things it sees on its input and that's a very clever idea and it's seen in various forms in a large fraction of the papers that appear on deep neural Nets but now I want to talk about an example so I can show you a demonstration okay so um we don't have gpus and we don't have three days to do this so I'm going to make it uh make up a very simple example that's reminiscent of what goes on here but but uh involves hardly any computation what I'm going to imagine is we're trying to recognize animals from how tall they are uh from the Shadows that they cast so we're going to recognize three animals a cheetah um a zebra and and a giraffe and they they will each cast a shadow on a Blackboard like me no uh no vampire involved here and what we're going to do is we're going to use the the shadow as a input to a neural net all right so let's see how that would work so there is um our Network and if I just clicked into one of these test samples that's the U height of the shadow that a cheetah cast on a wall and uh there are 10 input neurons corresponding to each level of the Shadow uh they're rammed through three inner layer neurons and from that it spreads out and becomes the outer layer values and we're going to compare those outer layer values to the desired values but the desired values are the same as the input values so this column is a column of input values on the far right we have our column of desired values and we haven't trained this dret yet all we've got is random values in there so if we run the test samples through we get that and that yeah cheetahs are short zebras are medium height and giraffes are tall but our output is just pretty much 0 five for all of them for all of those Shadow Heights all right no training so far so let's run this thing we're just using simple back prop just like just like on our world's simplest neur net and it's interesting to see that what happens is we you see all those values changing now I need to mention that when you when you see a green connection that means it's a positive weight and and the green and the density of the green indicates how how positive it is and the red ones are negative weights and the intensity of the red indicates how red it is so here you can see that we we still have from from our random inputs a variety of red and green values we haven't really done much training so everything uh correctly looks pretty much random so let's run this thing and after only a thousand iterations uh going through these uh examples and trying to make the output the same as the input we reached a point where the airor rate has dropped in fact it's dropped so much it's interesting to relook at the test cases so here's a test case where we have a cheetah and now the output value is in fact very close to the desired value in all in all the output in all the output neurons so if we look at another one once again there's a correspondence in the right two columns and if we look at the final one yeah there's a correspondence in the right two columns now when you back up from this and say well what's going on here it turns out that you're not training this thing to classify animals you're training it to understand the nature of the things that it sees in the environment because all it sees is the height of a shadow it doesn't know anything about the classifications you're going to try to get out of that all it sees is that there are there's a kind of consistency in the kind of data that it sees on the input values right now you might say okay oh that's cool because what must be happening is that that hidden layer because everything is forced through that narrow pipe must be doing some kind of generalization so it ought to be the case that if we click on each of those neurons we ought to see it specialized to a particular height because that's what sorts of stuff that's the sort of stuff that's presented on the input well let's go see what is what in fact is the maximum stimulation to be seen on the neurons in that hidden layer so if I when I click on these guys what we're going to see is the input values that maximally stimulate that neuron and by the way I have no idea how this is going to turn out because this is all you know the initialization is all random well that's good that one looks like it's generalized the notion of short ug that doesn't look like medium in fact um the maximum stimulation doesn't involve any stimulation from that lower neuron here look at this one that doesn't look like tall so we got one that looks like short and two look just look completely random so in fact maybe we better back off the idea that what's going on in that hidden layer is generalization and say um that what's going on in there is maybe the encoding of the generalization it doesn't look like a encoding we can see but there is an encoding there is a generalization that's let me start that over we don't see the generalization in the stimulating values what we have instead is we have some kind of encoded generalization and because we've got this stuff encoded it's what makes these neuron Nets so extraordinarily difficult to understand we don't just don't understand what they're doing we don't understand why they can recognize a cheetah we don't understand why it can recognize a school bus in some cases but not in others because we don't really understand and what these neurons are responding to well that's not quite true uh there's been a a lot of work recently on trying to to sort that out but it's still a a lot of me mystery in this world but in any event that's the that's the auto coding idea it comes in various guyses sometimes people talk about bolts machines and things of that sort but it's basically all the same sort of idea and so you can do this layer by layer once you've trained the input layer then you can use that layer to train the next layer and then that can train the next layer after that and it's only at the very very end that you say to yourself well now I've accumulated a lot of knowledge about the environment and what can be seen in the environment maybe it's time to get around to using some some samples of particular classes and train on train on classes so that's the story on autoc coding now the next thing to talk about is that final layer so let's see what the final ER might look like let's see it uh it uh might look like this there's a summer there's a minus one up here now let's see there's a minus one up here back there's a minus one up there there's a multiplier here and there's a threshold value there now likewise there's some other input values here let me call This One X and it gets multiplied by some weight and then that goes into the summer as well and that in turn goes into a summing into a sigmoid that looks like so and finally you get an output which will call Z so it's clear that if you uh just write out the value of Z as it depends on those inputs Now using the formula that we uh worked with last time then what you see is that Z is equal to 1 over 1 + e to the minus W * x minus t uh plus T I guess right so that's a sigmoid function that um depends on the value of that weight and on the value of that threshold so let's look at how that those values might change things so here we have an ordinary sigo and what happens if we shift it with a with the threshold value if we change that threshold value then it's going to shift the place where that sigmoid comes down so a change in t could cause this thing to shift over that way and if we change the value of w that could change how steep this guy is so we might think that the performance since it depends on W and T should be adjusted in such a way as to make the classification do the right thing but what what's the right thing well that depends on the samples that we've seen suppose for example that this is our Sigma function and we see some uh examples of a class some positive examples of a class that have values that lie at that point and that point at that point and we have some values that correspond to situations where the class is not one of the things that are associated with this neuron and in that case what we see is examples that are over in this vicinity here so the probability that we would see this particular guy in this world is associated with the value on the sigma white curve so you can think of this as the probability of that positive example and this is the probability of that positive example and this is the probability of that positive example what's the probability of this negative example well it's one minus the value on that curve and this one's one minus the value on that curve so we could go through the calculations and what we would determine is that to maximize the probability of seeing this data this this particular stuff in a set of experiments to maximize that probability we would have to adjust T and W so as to get this curve doing the optimal thing and it's nothing mysterious about it it's just more partial derivatives and that sort of thing but the bottom line is that the probability of seeing this data is dependent on the shape of this curve and shape of this curve is dependent on those parameters and if we want to maximize the probability that we've seen this data then we have to adjust those parameters accordingly let's have a let's have a look at a demonstration okay so there's an ordinary sigid curve here are a couple positive examples here's a negative example let's put in some more positive examples over here and now let's run a a good old gradient Ascent algorithm on that and this is what happens you see how the probability as we adjust the shape of the curve the probability of seeing those examples of the class goes up and the probability of seeing the non-example goes down so what if we put some more uh examples in uh if we put a negative example there not much is going to happen What would happen if we put a positive example right there then we're going to start seeing some dramatic shifts in the shape of the curve so that's probably a noise point but we can put some more negative examples in there and see how that adjusts the Curve all right so that's what we're doing we're viewing this output value as something that's related to the probability of seeing a class and we're adjusting the parameters on that output layer so as to maximize the probability of the sample data that we've got in hand right now there's one more thing you can see what we've got here is we've got the basic idea of back propagation which has layers and layers of ADD um let me be flattering and call them ideas layered on top so here's the next here's the next idea that's layered on top so um we've got an output value here and it's a function after all and it's a it's it's got a value and we're going to have a uh if we have a th classes we're going to have a th output neurons and each is going to be producing some kind kind of value and we can think of that value as a probability but what it really but I don't want to write a probability yet I just want to say that what we've got for this output neuron is a function of class one and then there'll another be another output neuron which is a function of class two and these values will be presumably higher this will be higher if we are in fact looking at class one and this one down here will be in fact higher if we're looking at class in so what we like to do is we' like to uh not just pick one of these uh outputs and say well you're the you're the you've got the highest value so you win what we want to do instead is we want to give some we want to associate some kind of probability with each of the classes because after all we want to do things like find the most probable five so what we do is we say all right so the actual probability of class one is equal to the output of that sigmoid function divided by the sum over all functions so that takes all of that entire output vector and converts each output value into probability so when we use that sigo function we did it with a view toward thinking about that as a probability in fact we we assumed it was a probability when we uh made this argument but in the end there there's there's such an there's an output for each of those classes and so what we get is in the end not exactly a probability until we divide by a normalizing factor so this by the way is called this by the way is called uh not on my list of things but it soon will be since we're not talking about taking the maximum and identify ing the and using that to classify the picture what we're going to do is we're going to say uh we're going to use what's called softmax so we're going to give a a range of classifications and we're going to associate a probability with each and that's what that's what you saw in in in in all of those samples you saw yes this is a container ship but maybe it's also this or that or third or fourth or fifth thing so that is a pretty good summary of the kinds of things that are involved but now we've got one more step because what we can do now is we can take this output layer idea the softmax idea and we can put it together with the auto coding idea so we've trained this uh middle layer up and now we're going to detach it from the output layer but retain those weights that connect the input to the hidden layer and when we do that what we're going to see is something that looks like this and now we've got a trained first layer but an untrained output layer we're going to freeze the input layer and train the output layer using that that that that so-called that sigmoid curve let's see what happens when we do that oh by the way let's let's run our test samples through you can see it's not doing anything the output is half for each of the categories even though we we got a train middle layer so we have to train the outer layer let's see how long it takes whoa that was pretty fast now there's a extraordinarily good match between the outputs and the desired outputs so that's a combination of the auto coding idea and the softmax idea they get one more idea that's worthy mention and that's the idea of dropout the the plague of any neural net is that it gets stuck in some kind of local maximum so it was discovered that these things train better if on every iteration you you um flip a coin for each neuron and if the coin ends up tails you assume it's just died and has no influence on the output it's called dropping out those neurons and then on the Next Generation you drop out a different set so what this seems to do is it seems to get this thing from prevent this thing from going into a frozen local maximum state so that's deep Nets uh they should be called by the way wide Nets because they tend to be enormously wide but rarely more than 10 layers 10 layers 10 columns uh deep now let's see where' it go from here um maybe what we should do is talk about the awesome uh curiosity in the current state-ofthe-art and that is that all of this sophistication with uh output layers that are probabilities and uh training using autoc coding or bolman machines um it doesn't seem to help much relative to plain old back propagation so back propagation with a convolutional net seems to be do just about as good as anything and while while we're on the subject of a a ordinary deep net I'd like to examine a situation here where we have a deep net well it's not really it's not very it's a classroom deep net and we'll put five layers in there and its job is still to do the same thing it's a classify an animal as a cheetah a zebra or a giraffe based on the height of the Shadow it casts and as before if it's green that means positive if it's red that means negative and right at the moment we have no training so if we run our test samples through the output is always a half no matter what the animal is all right so what we're going to do is just going to use ordinary back propop on this same thing as in that uh sample that's underneath the Blackboard only now we've got a lot more parameters we've got five columns and each one of them has nine or 10 neurons in it so let's let's let this one run now look at that stuff on right it's all turned red at first I thought this was a bug in my program but you know it makes absolute sense if you don't know what the actual animal is going to be and there are a whole bunch of possibilities you better just say no for everybody it's like when a biologist says we don't know that's the most probable answer well but eventually after about 160,000 iterations it seems to have got it let's run the test samples through yeah it's doing great let's do it again just to see if this is a fluke all red on the right side finally you start seeing some changes go in the in the in the final layers there and if you look at the air rate uh down at the bottom you see that it kind of kind of falls off a cliff so nothing happens for a real long time and then it falls off the cliff now what would happen uh if uh this uh neural net were not quite so wide good question but before we get to that question what I'm going to do is I'm going to do a a funny kind of variation on the theme of Dropout what I'm going to do is I'm going to kill off one neuron in each column and then see what if if I can retrain the network to do the right thing so I'm going to reassign those to some other purpose so now there's one fewer neuron in the network if we rerun that we see that it trains itself up very fast so we seem to be still close enough to a solution we can do without uh one uh you know one of the neurons in each column let's do it again yeah it goes up a little bit but quickly Falls falls down to a solution try again quickly falls down to a solution oh my God how much is this am I going to do each time I knock something out and retrain it finds a solution very fast whoa I got all the way down to two neurons in each column and it still has a solution it's interesting don't you think uh but let's uh repeat the experiment but this time we're going to do it a little differently we're going to take our five layers and we're going to before we do any training I'm going to knock out all but two neurons in each column now I know that with two ands in each column I've got it a solution I just showed it I just showed one let's run it with uh this way it looks like increasingly bad news what's happened is that this sucker has got itself into a local maximum so now you can see why there's been a breakthrough in in this neural net learning stuff and it's because when you widen the net you turn local Maxima into saddle points so now it's got a way of crawling its way through this vast space without getting stuck on a local maximum as suggested by this all right so those are some uh I think interesting uh things to look at um by way of uh these demonstrations but now I'd like to go back to my slide set and show you some examples that will address the question of whether these things are seeing like we see so you can try these examples online there are a variety of websites that allow you to put in your own picture and there uh is a cottage industry of producing uh papers in journals uh that fool neuron Nets so in this case a very small number of pixels have been changed you don't see the difference but it's enough to take it take this particular neural net from a high confidence that it's looking at a school bus to to to thinking that it's not a not a school bus those are some things that think are a school bus so to appears to be the case that what is triggering this school bus result is that it's seeing enough local evidence that this is not one of the other 999 classes and and enough positive evidence from the from these these local looks to conclude that it's a school bus so do you see any of those things I don't here you can say okay well look at that base one yeah that looks like it's got a little bit of baseball texture in it so maybe what it's doing is looking at texture uh these are some uh examples from a a recent and very famous uh paper by Google uh which uses essentially the same ideas to put captions on uh pictures so the thing that this by the way is what has stimulated all this enormous concern about artificial intelligence because naive viewer looks at that picture and says oh my God this thing knows what it's like to play or be young or move or what a frisbee is and of course it knows none of that it just knows how to label this picture and to the credit of the people wrote this paper they show examples that don't do so well so yeah it's a cat but it's not lying's a little girl but she's not blowing bubbles what about this one so we've been doing our own work in my laboratory on uh on some of this and and and and the way the following set of pictures was produced was this you you take an image and you separate into a bunch of slices representing each representing a particular frequency band and then you go into one of those uh frequency bands and you knock out a rectangle from the picture and then you reassemble the thing and if you hadn't knocked that piece out when you reassemble it it would look exactly like it did when you started so what we're doing is we knock out as much as we can and still retain the neural Net's impression that it's the thing that it started out thinking it was so what do you think this is It's identified by a neural net as a railroad car because this is the image that it started with how about this one that's easy right that's a guitar we weren't able to mutilate that one very much and still retain the guitar of it how about this one what's that what a lamp any other ideas Ken what do you think it is see he's an expert on the subject who was identified as a barbell what's that what cello you didn't see the little girl or the instructor how about this one what it's a grasshopper what's this wow you're good it's actually not a two-headed wolf so two wolves are close together it's a bird right good for you it's a rabbit about that Russian wolf found if you've been to Venice you recognize this so bottom line is that these things are an engineering Marvel and do great things but they don't see like we see