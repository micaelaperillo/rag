I was in Washington for most of the week prospecting for gold another byproduct of that was that um I uh forgot to arrange a substitute for Bob Berwick for the Thursday uh recitations I shall probably go to hell for this but in any event we have many explanations none of them good but today we'll try to get back on track and you'll learn something fun in particular you'll learn how uh a graduate student of mine uh Mark fingon uh together with a summer Europe student Brett fudin one of you managed to pull off a tour force and recognize in these two descriptions uh the pattern that we humans commonly call Revenge it was discovered the system didn't have a name for it of course they just knew that there was a pattern there and sat waiting for us to give a name to it that's where we're that's where we're going to end up but it'll be a bit of a journey before we get there because we got to go through all that stuff on the outline and in particular uh we want to start off uh by a little tiny bit of review because some of the stuff we did last time went by pretty fast in particular you may remember they we had this wonderful joint probability table which tells us all we want to know all we want to know we can decide what the probability of uh the police being called is given the this and the that and all that sort of stuff by clicking the appropriate boxes the trouble is Gee there are only three variables of that there and when there are lots of variables it gets pretty hard to make up those numbers or to even collect them so we're driven to an alternative and we got to that alternative just uh at the end of the show uh a week ago uh and we uh got to the point where we were defining these inference Nets sometimes called Bas Nets and the one we worked with looked like this there's a burglar a raccoon the possibility of a dog barking the police being called and a trash can being overturned so more variables than that those that only has three this has got five but we're able to do some magic with this because we us humans when we Define when we draw this graph we're making an assertion about how things depend or don't depend on one another in particular there's something to write down and and and and memorize to the point where it rolls off your tongue and that is that any variable in this graph is said by me to be independent of any other non- descendant given its parents independent of any non descendant given its parents so that means that the the probability of the dog barking given its parents doesn't depend on tea the trash can being overturned because the intuition is all of the causality is flowing through the parents and can't get to this variable D without going through the parents so that is an asserted property of the Nets that we draw and we tend to draw them in a way that reflects causality so it tends to make sense so somehow this thing this thing is going to be we're going to use this thing instead of that thing but wait uh we may need that thing in order to do all the computations we want to perform so we need to be able to show that we can get to that thing by doing calculations on this thing so what to do well we're going to use the chain Rule and you remember that the chain rule Came To Us by way of the basic U axioms of probability uh plus a definition plus a little colored shock so we got to the point last time where we sort of believe this it's it's a really magical thing it says that the probability of all this stuff happening together is given as the product of a bunch of conditional probabilities and the conditional probabilities in this product are arranged such that this first guy depends on everybody else the second guy doesn't depend on the first guy but depends on everything else so that list of dependencies get smaller and smaller as you go down here until depends only on one thing there's no there's no conditional at all so that's going to come to our rescue in and because it enables us to go from calculations in here to that whole table but first I have to show you a little bit a little bit more slowly how that comes to be what I'm going to do uh before I think about probability is I'm going to make a linear list of all of these variables and the way I'm going to make it is I'm going to chew away it from the those variables from the bottom and taking advantage of a very important property of these nets and that is there are no Loops you can't follow the arrows in any way so that you get back to yourself so there's always going to be a bottom so what I'm going to do is I'm going to say well the there are two bottoms here there's C and T so I have a choice I'm going to choose C so I'm going to pretend that that I'm going to I'm going to take that off and uh pretend that it's not there anymore then I'm going to take this guy that's now now a bottom because there's nothing below it I've already taken C out so we'll take that out next and now I've got this guy this guy and this guy this guy no longer has anything below it so I can list it next now over here I've got raccoon and trash can but trash can Trash Can is at the bottom so I've got to take it next CU I'm I'm I'm working from the bottom up I want to ensure that there are no descendants before me in this list so finally I get to Raccoon so the way I've constructed this list like so ensures that this list arranges the elements so that for any particular element none of its descendants appear to its left all right right and now that's the magical order that I want to use for which I want to use the chain rule so now I can write I can pick C to be my variable n and I can say that the chain rule says that the joint probability of all these variables P of C D B T and R the probability of any particular combination of those things is equal to the probability of C given everybody else next in line is D given everybody else next in line is t uh next in line is B given everybody else oops and next in line is T given everybody else and finally just R so this combination of things has a probability that's given by this chain rule expression ah but first of all none of those expressions condition any of the variables on anything other than non-descendants right that's just because of the way I arrange variables and I can always do that because there are no Loops I can always chew away at the bottom that ensures that whenever I write a variable it's going to be conditioned on stuff other than its descendants so all of these variables in any of these Pro in any of these conditional probabilities are non-descendants but wait when I drew this diagram I asserted that no variable depends on any non- descendant given its parents so if I know the parents of a variable I know that the variable is independent of all other non-descendants all right now I can start scratching stuff out well let's see I know that c from my diagram has only one parent D so given its parent it's independent of all other non-descendants so I can scratch them out D has two parents B and R but given that I can scratch out any other non-dependent non descendent B is conditional on T and R ah but B has no parents so it actually is independent of those two guys the trash can yeah that's dependent on R and R over here the final thing in the chain that's just a probability so now I have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables voila so anything I can do with a table I can do in principle with this little Network okay but now the question is I've got a some probabilities I'm going to have to figure out here so let's me let me draw a slightly different version of it so up here we got the op prior probability of B well that's just probability of B down here with the dog I got a bigger table because I got a probabilities that depend on the values of his parents the probability of dog barking depends on the probabil depends on the condition of the parents nothing else so let's see I've got to have a column for B I've got to have a column for the raccoon the burglar and the raccoon and there are a bunch of possibilities for those guys and once I get those then I'll be able to calculate the probability of dog barking so there are two of these variables so there are four combinations there's TT there's a TR R RT and well what am I doing wake up T false false true and false false so what I really want to do is I want to calculate all of these probabilities that give the probability of the dog condition on the condition of the of the burglar and the raccoon similarly I want to calculate the the probability of B happening doesn't depend on anything else so I don't know what to do well I what I'm going to actually do is I'm going to do the same thing I had to do up there I'm going to keep track of uh I'm going to try a bunch of I'm going to get myself together a bunch of data Maybe I do a bunch of experiments maybe somebody hands it to me but I'm going to use that data to construct a bunch of tallies which is going to which are going to end up giving me the probabilities for all of those things so I don't know let's see how should we start find step one find colored chalk step two I'm going to extend these tables a little bit so I can keep track of the tallies so this is going to be all the ones that end up in a particular row and these are going to be the ones for which dog is true simil I'm going to extend this guy up here in order to keep track of some tallies this are going to be the ones for which B is true and this one will be all so that's my that's my setup and now suppose that my first experiment comes roaring in and it's all T's so I have t t t that's my first experimental result My First Data item so let's see uh the arrangement here is burglar re ton dog so burglar has a true and there's one tally count in there likewise the TT that's the burglar and raccoon that brings me down to this first row so that gives me one tally in there and dog is true so that gives me a tick mark in that one all right you with me so far now let's suppose the next thing happens to be all false well uh burglar is false but there is uh one experiment uh everybody's false so we come down here to false false and that's the row we're going to work on we get a tally in there do we put one in here no because that's false dog is false that's that's what sta element says so that's cool uh maybe one more let's suppose we have um ttf well in that case we have a tick mark here and a tick mark here because the burglar element is true then we have TT that brings us to the first row again so we get a tick mark there but dog is false so no tick mark there so that's how it works I suppose You' like to see a demonstration right always like to see demonstration so here's what here's what it actually looks like so on the left you see the network as we've constructed it with a bunch of probabilities there and what I'm going to do now is I'm going to start simulating away so as to accumulate tick marks tally marks and see what kinds of probabilities that they indicate for the table I happen to be using a process for which the model on the left is a is a correct reflection so there's one simulation oh so the dog barking let's see the burglar is false the raccoon is true I get one tick mark so the probability there is one of course I'm not going to just leave go with one I'm going to put a whole bunch of stuff in there so I'll just run a bunch more simulations note that I don't even have an entry at all yet for TF here that's because I haven't run enough data so let's Let Me Clear it and instead of doing it uh one at a time let me run a 100 simulations see it's still not too good because it says this TT probability is is is true this this is because I'm feeding a data right and I'm keeping track of what the data elements tell me about how frequently a particular combination appears yes T so when you're like doing one simulation is that a setting of the variables when I'm doing one simulation I'm just keeping track of of of that combination in each of these tables because it's going to tell me something about the probabilities that I want reflected in those tables so it's pretty easy to see when I go up here to burglar you know if I have a lot of data elements they're all going to tell me something about the burglar as well as the other variables so if I just look at that burglar thing the fraction of time that it turns out true over all the data elements is going to be its probability so now when I go down to the Joint tables I can still get these probability numbers but now they're condition on particular condition of its parents so that's how I that's how I get these probabilities so I didn't do too well here uh because that TT combination gave me an excessively high probability so maybe 100 simulations isn't enough let's run 10,000 so with that much data running through the probabilities I get let's see I've got 893 here instead of 0.9 807 instead of 08 607 instead of 6 and that one's dead on at 0.01 so if I run enough of these simulations I get a pretty good idea what the probabilities ought to be given that I've got a correct model okay so that takes care of that one and of course I didn't draw the other things in here but by extension you can see how those would work oh but you know what I think I will put a little uh probability of raccoon table in here because the next thing I want to do is I want to go the other way this is recording tallies from some process process so I can develop a model but once I've got these probabilities of course then I can start to simulate what the model would do right how would I do that well do I or do I want do I want to use the same table I think just to keep things sanitary what I'll do is I'll go over here and and do it again here's B it's got a probability of B here's R here's a table probability of r that comes down into a joint table for dog and it's got four elements depending on the burglar condition and the raccoon condition we get a probability of dog and now imagine these have all been filled in so what do I want to do if I want to simulate this system generating some combination of values for all the variables well I do the opposite of what I did when I was working around with this chain Ru showing that I could go from the table to those probabilities now I've got the probabilities I'm going to go the other direction instead of chewing away from the bottom I'm going to chew away from the top because when I go into the top and chew away everything I need to know to do a coin flip is there so in particular when I go up here up in here I've got the probability of a burglar now so I'm going to use that probability to flip a coin and say it produces a t so that takes care of this guy and I can now scratch it off since it's no longer in consideration it's no longer a top variable so now I go over to Raccoon I do the same thing I take this probability I do a flip and say it produces an F whatever its probability is I flip a bias coin and that's what I happen to get but now having dealt with these two guys that uncovers this dog thing and now I got enough information because I've done everything above to make a calculation for whether the dog is going to be barking or not but wait I have to know that I've got a T and A T and A T and an f and an f and a t and an f and an F because I have to select the right row so I know that b is T and I know that R is f so that takes me into the table into the second row so now I get this probability I flip I flip that coin and I get some result say t voila I can do that with all the other two variables and I've got myself an experimental trial that is produced in accordance with the probabilities in the table okay of course of course yeah in fact how did I get those numbers actually what I did is I Ed the model on the left to generate the samples that were used to compute the probabilities on the right so you've seen that you've seen a demonstration of this already now of of course I don't know all of this sort of depends on having everything right uh I've written a thing to write it one more time burglar raccoon dog call the police trash can but somebody else might say oh you got it all wrong this is the way it really looks like the right the dog doesn't care about the raccoon at all so that's a correct model now when I do a simulation I could fill in the tables in either model right I'm sure You' like to see a demonstration so let me show you a demonstration of that so there are the two tables and I can run 10,000 simulation on those guys too and look the guy on the left is pretty good reflection of the probabilities in a model I use to stimul to to to to produce the data but the guy on the right doesn't know any better it just fills in its own tables too so what to do I say this one's the right model and you say that one's the right model who's right well maybe we'll never know and uh the guy on the left will get rich in the stock market and the guy on the right will go broke that'll be nice if we could figure out who's right so would you like to see how to figure out who's right yeah so would I what we're going to do is we're going to look at naive Basi and inference and that's our next chore so here here's how it works uh we know from the definition of conditional probability we know that the probability of a given B is equal to the probability of A and B divided by the probability of B right equal to by definition so that means that the probability of a given B times the probability of B I'm just multiplying it out is equal to that joint probability oh but by symmetry there's no harm in saying I can go I can turn that around and say that the probability of B given a times the probability of B is also equal to that joint Pro that joint probability right I've just expanded it a different and symmetric way if I got to write a B on b b a on a thank you who who was complaining good work that would have been a major league disaster but now having written that I can forget about the middle CU all I'm really interested in is how I've turned the probabilities around in that conditional why would I care about doing that that by the way we're now talking about the work of the Reverend Bay because we can rewrite this yet again as the probability of a given B uh is um equal to the probability of B given a times the probability of a divided by the probability of B that's just Elementary algebra but now I'm going to do something magical I'm going to say I've got uh a classification problem I want to know which disease you have that's classification problem maybe you've got the swine flu uh maybe you've got uh indigestion who knows but I get all these symptoms I get all these pieces of evidence you've got a fever you're throwing oh well let's not go into too much detail there but what I'm going to do is I'm going to say well let's suppose that a is equal to a class that I'm interested in the disease you've got and B is equal to the evidence the symptoms I observe voila I may have a pretty hard time figuring out what the probability of the class is given the evidence but figuring out the probability evidence given to class might not be so hard let me get another board in play and show you what I mean by plugging uh class and evidence into Bay's rule what I get is the probability of some class given the evidence is equal to the probability of the evidence given the class times the probability of the class divided by probability of the evidence now you got to let that snc to you a little bit suppose I I've got several classes that I'm trying to decide between I'm trying to select the best out of that that batch of classes well I've got the evidence and if I know the probability of the evidence given each of those classes and if I know the op priority the initial probability of the class then I'm done because I got the two elements in the numerator why am I done because the denominator is the same for all the classes it's just the probability of the evidence and then I could just sum everything up I know it adds to one anyway so that's cool but sometimes this evidence actually there's not more than one piece of evidence let's say that there's some class some I and we're trying to figure out if that's the correct class so we got C subi there and C subi there and suppose that that evidence is actually a bunch of pieces of evidence so it could be E1 e and oops a premature right bracket all that evidence given the class I times the probability of the class I over some denominator that we don't care about because it's going to be the same for everybody so I'll just write that as D now what if these pieces of evidence are all independent given the class so if you have the swine flu the probability you have a fever is independent of the probability you're going to throw it say then can we write this another way an easier way sure because when things are independent The Joint probability is equal to the product Pro is equal to the product of the individual probabilities so that is to say it's easier to see it if you write it down and if you just say it this probability here from these two elements here is equal to the probability of E1 conditioned on C subi times the probability of E2 condition on C subi all the way down to the probability of e n condition on C subi divided by some denominator we don't care about see what I'm trying to do is I'm going to go over I'm going to go through this for all the CI and see which one's the biggest this is the probability of right here oh yeah she F yeah yeah thanks can't write and think at the same time thanks okay so I've just figur out which one of these is biggest and I've identified the class now you say to me well I would like to see an example so I don't know does anyone have any spare change migle a quarter this is not because of our infantes low raises here at MIT I just need it for a demonstration and we need two coins don't forget to get these back I'm tend to be now suppose these two coins are not exactly the same uh one of these coins is a legitimate highly prized American Quarter the other one is a fake and with this one the probability of heads let us say is 08 instead of 0 five so I mix these all up and I pick one and I start flipping it and I get a head and then I flip it again and I get a tail which coin did I pick well we're going to use this stuff to figure it out here's what happens before I forget thank you very much so what we've done is we've uh selected these things from uh my hands and I can't draw hands so I'll draw a little cup here and there two coins here and we're going to pick one and one has a probability of head of uh equal to 0.8 and this one has a probability of a head of uh 0.5 so here's the draw I picked one each has a probability of 0.5 this one is the one with the 0.8 is the probability of head and this one is one with a probability of 0.5 is ahead okay so now suppose the first flip as it was is T well that's a piece of evidence that's here probability evidence given to class well in the case of having drawn this biased coin the probability of of of coming up with a with a with a tail ah let's say a head just to make my numbers a little easier probability of coming out there with a head is equal to8 given that it's up here in this Choice the probability given that you have a fair coin is 0.5 so now if we take the the the next coin and take it to be a tail then the probability of this guy given that evidence is2 and the probability of this guy given that evidence it's a fair coin so it doesn't care still 0. five so now what's the probability of this class given this evidence it's the product of 0.5 time8 * .2 and what's the probability of this guy it's .5 * .5 * .5 divided by a denominator which is the same in both cases so let's forget about the early 05 here because it's the same in both cases and we just multiply those numbers together that gives us8 * 2 what's that 1.16 and this guy .5 * .5 that's 0.25 so it looks awful lot like with this combination that I've picked the coin is is fair one more flip so let's flip it again and suppose we come up with a head that puts a 0.8 in here and a 0.5 in here and when you multiply those out that's one 25 and this is 0.128 so it's about equal so you see how that works all right so we're using the coin flips as evidence to figure out which class is involved okay so I don't know you you probably like to see a demonstration of this too right but you say to me gosh uh just two kinds of coins that's not very interesting let's try five kinds of coins so what I'm going to show you is uh a bunch of is how the probabilities for all these coins or five of them colorcoded how they how the probabilities vary with a series of flips let's suppose I've got a head The Gray Line by the way is the number of heads or the fraction of heads so that's going to be one because I'm just doing hits you see that black line Rising shooting up like a rocket that's the probability that the that's the pro that's the that's the coin which it only shows heads probability of head is one and I'm flipping a whole bunch of heads here is that cool now what happens if I suddenly put in a tail by the way you'll note down here on the extreme left the probability of the the initial probability of the P equal Z coin was 0.1 as soon as I flipped a head that went to zero and will never get off zero right that makes sense because if the probability you get a head is one you should never see a tail if you ever do that isn't your coin what happens now if I interrupt this series of of heads and produce a tail what that the black one will go to zero what else happens by the way the blue one is the one with the highest probability of being ahead well look boom that blue one shot up not going up slowly it shot up and now CU now the preponderance of evidence with all those heads is that I've flipped the coin with a bias of 75 towards heads so let's clear this pick any probability you want 0.255 and so on I don't know let's pick 0.25 since we've been at the upper end so orange is 0.25 and sure enough the probability that I've selected the 0.5 coin is going up and up and up and up after a little original irregularity the law of large numbers is setting in and the probability I've got that 0.25 coin in play is pretty close to one all right so that's cool now you say to me uh that's awfully nice but stop awfully nice but um not very not very real worldish so let me give you another problem um it's well known that you are with high probability of belonging of of the same political persuasion as your parents so if I wanted to figure out which party a parent belongs to I could look at the party that their children belong to right so it's just like flipping coins the the particular coin I I have chosen corresponds to the parent the individual flips correspond to the political party that the child belongs to so let's get up a little bit by the way I wrote all this stuff over the weekend so who knows if any of it'll work but let's see um party parent party classifier there it is Democrats and Republicans and now the prior for being a republican I've given here is 05 I but I don't know this is a little bit Democratic state so so um let's adjust that down a little bit that might you know somewhere in there might be about right but let's just for the sake of a classroom illustration go down here so now the meter is showing the prior probability because that's the only thing in the in the formula so far I've got no evidence so now let's suppose the child number one is a republican back to neutral so I've got a low probability that the parent PRI a priority probability that the parent is a Republican and a uh child who's a republican oh I notice that 0 2 and 08 the pro the conditional is 08 and the prior is point2 that's why it comes out to balance each other right so now if we get another Republican in there it goes way up if I have a democratic child it goes back down if I have an equal balance between children then it goes way back down because of that prior probability being low so if I make that high and even though the children are balanced I'm still going to be have a high probability of being a republican now let's see if I take that slider there the conditional probability and drive it to the left here let me make that equal again and let's make that uh one thing I don't know what am I doing now if I make the probability less than 0.5 what's that mean that means you're s at your parents and you want to belong to a different party okay all right so now what's next oh gosh what's next this is what's next what's next to somewhere yeah this is what's next this here we got two models remember I wanted I said we wanted to decide between them can we use that Basi and hack to do that too sure because we can we got these two models we've got the probabilities in them so now I can take my data and calculate the probability of the left model given the data and the probability of the right model given the data multiply that times the op priority probabilities which we all assume are equal then I can do a model selection deal much in defiance of what I was hinting at before so let's try that whoa there are my two models yes there they are we've already trained them up with and they've got their probabilities now what we're going to do is we're going to use the original model to do the Sim to simulate the data so what we're going to do is we're going to simulate draws simulate events simulate combinations of all the variables using a model that looks like the one on the left that is the one on the left except for the slight differences in probabilities okay then we're going to do this basian thing and see where the meter goes so run one data point oops went the wrong way makes me nervous I just finish this at 9:15 maybe there's a bug oops two data points swings to the left three swings back to the right of course that's not much data so let's put some more data in yeah boom there it goes let's try that again that was cool so let's run Thousand simulations and One data point wobbles around a little bit and goes flat over to the left because that is the model that reflects the one that the data is generated from so now we got basian classification except now the classification has gone one step more and it becomes structure Discovery we've got two choices of structure and we can use this basian thing to decide which of the two structures is best that cool well it's only cool if you could do what so if you had two choices you can select between them and pick the best one but there gosh for this number of variables there are a whole lot of different networks that satisfy the no looping criteria and don't have very many parents there awful lot of them in fact if you restrict this network to two parents they probably thousands and thousands of possible structures so do I try them all probably not that's too much work when you get 30 variables or something like that so what do you do we know what to do right we're almost veterans at 6034 we have to search so what we do is we take the loose and we modify it and then we modify it again and we keep modifying it until uh we drop dead or we get something that we're happy with so let's see what happens if we change this problem a little bit and do structure Discovery we're starting out with nothing linked and we're going to just start running this guy so what's going to happen is that the good guy will prevail and the bad guy will be a copy of the good guy perturbed in some way so it's a random search you'll notice that score that's too small for you to read all these things are too small to read let me make it a little bigger too small to read but that number on the on the on the right there is not the product of the probabilities actually it's the sum of the logarithms of the [Music] probabilities they go together right and the reason you use this instead of the probabilities is because these numbers get so small that with a 32-bit machine you eventually lose so you use the log of the probabilities rather than the um the product of the probabilities you use a sum of the logs instead of the product of the probabilities and eventually you hope that this thing converges on on the correct interpretation but you know what this thing is so flat as a space and so large uh and so telephone pole like that you have to that it's full of local Maxima so with this program is doing is every once in a while I think with probability one and 10 I forgot what parameters I use every once in a while it'll do a total radical rearrangement of the structures in other words it's a random restart it keeps track of the best guy so far and every once in a while does a totally random restart in its effort to search this space so that's how you go from probably sck inference to structure Discovery now when is this stuff useful well I hinted that medical diagnosis right that's a situation where you got some symptoms and you want to know what the disease is so as soon as you use the keyword diagnosis you've got a problem for which this stuff is a candidate so what other kinds of diagnosis problems are there well you might be lying to me so I can put a lie detector on you and each of those variables that are measured by the lie detector are in dependent indication of whether you're telling the truth or not so it's this kind of basian is this kind of basian Discovery thing basian naive basian classification what other kinds of problems speak to the issue of diagnosis well we like to know how well you know the material so we can use quizzes as pieces of evidence uh thank God we don't use exactly a naive Basi and classifier because then we wouldn't be able to do that combination we have to use a slightly more complex what you could think of was a slightly more complex basian net to do that particular kind of diagnosis you might have a spacecraft with all kinds or an airplane or other piece of equipment with all sorts of symptoms you're trying to figure out what to do next what the cause is so using the evidence to go backward to the cause so maybe you got some program that doesn't work happens to me a lot so I use the evidence from the symptoms of the misbehavior to figure out where what the most probable cause is but now uh to conclude the day last time last time there were no there weren't any powerful ideas but if you consider if you take the the combination of this of the last lecture and this lecture uh to be a um candidate for Goldstar ideas these are the ones I'd like to leave you with we got here is uh this basian stuff all these probabilistic calculations are the right thing to do they're the right way to work when you don't know anything which would make it sound like they're not very useful because you think you always but in fact there a lot of situations where you either can't know everything don't have time to know everything or don't want to take the effort to know everything so in medical diagnosis all you got is these symptoms you can't go in there and figure out in a more precise way exactly what's wrong so you use the symptoms to determine what the cause is and in all those other kinds of cases that I mentioned but now what what other kinds of structure Discovery are there well the kind of structure discovery that I hinted at in the beginning will be the subject uh that we'll begin with uh during our next and sadly final conversation here in 10250 on Wednesday it will feature not only a discussion of how this stuff can be used to discover patterns and stories but we'll also talk about um what's on the final uh what kind of thing you could do next that sort thing uh to uh to finish off the subject and that's the end of the story for today