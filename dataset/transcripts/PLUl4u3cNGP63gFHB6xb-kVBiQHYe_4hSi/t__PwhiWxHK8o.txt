so where are we we started off with the simple methods for learning stuff then we talked a little about uh learning approaches to learning that were vaguely inspired by the fact that our heads are stuffed with neurons and that we seem to evolve from primates uh then uh we talked about looking at the problem and address the issue of phonology and how it's possible to learn Concepts uh but now we're coming uh full circle back to the beginning and thinking about how to divide up a space with decision boundaries but whereas you do it with a neural net or a nearest neighbors or a ID tree those are very simple ideas that work very often today we're going to talk about a very sophisticated idea that still has a simple implementation so this needs to be in the tool bag of every civilized person this is about support Vector machines uh an idea that was developed well I want to talk to you today about how ideas develop actually because you look at stuff like this in a book and you think well uh Vladimir VAP I just uh figured this out one Saturday afternoon when it was the weather was too bad to go outside that's not how it happens it happens very differently I want to talk to you a little bit about that nice thing about about things that were great things that were done by people are still alive is you can ask them how they did it you can't do that with forier you can't say to forier how did you do it did you dream it up on a Saturday afternoon but you can call vapnik on the phone and ask him questions that's the stuff I'm going to talk about toward the end of the hour well it's all about decision boundaries and now we have several techniques that uh we can use to uh draw some decision boundaries and here's the same problem and if we uh Drew decision boundaries in here uh we might get something that would look like maybe this if we were doing a nearest neighbor approach and if we're doing ID trees would just draw in a line like that and if we're doing neural Nets well you know you can put in a lot of straight lines wherever you like with the neural net depending on how it's trained up or if you just simply go in there and design it so you could you could do that if you wanted and you would think that uh after people have been working on this sort of stuff for 50 or 75 years that there wouldn't be any tricks in the bag left and that's when everybody got surprised because around um the early 90s uh Vladimir vapnik introduced the ideas I'm about to talk to you about so what vapnik says uh is H something like this here you have a space and you have some negative examples and you have some positive examples how do you divide the positive examples from the negative examples and what he wants to what he says we want to do is we want to we want to draw a straight line but which straight line is the question well we want to draw a straight line well would this be a good straight line one it went up like that probably not so hot how about one is just right here well that's might separate them but seems awfully close to the negative examples so maybe what we ought to do is we ought to draw our straight line in here sort of like this and that line is drawn with a view toward putting in the widest street that separates the positive samples from the negative samples that's why I call it the widest Street approach so vn's way of putting in the decision boundary is to put in a straight line but in contrast with the way ID tree puts in a straight line it tries to put the line in and such a way as the separation between the positive and negative examples that that street is as wide as possible all right so you might think to do that in the europ project and then let it go with that you know what's the big deal so what we got to do is we're going to kind of go through why it's a big deal so first of all we we kind of like to think about uh how you would make a decision rule that would use that decision boundary so what I'm going to ask you to imagine is that we've got a vector of any length you like constrained to be perpendicular to the median or if you like perpendicular to the gutters it's perpendicular to the to the median line of the street right it's drawn in such a way that that's true we don't know anything about its length yet then we also have some unknown uh say right here and we have a vector that points to it like so so now what we're really interested in is whether or not that unknown is on the right side of the street or on the left side of the street so what we want to do is we want to project that Vector U Down on the one that's perpendicular to the street because then we'll have the distance in this direction or a number that's proportional to the distance in this direction and a and the further out we go the closer we'll get to being on the right side of the street where the right side of the street is not the correct side but actually the right side of the street so what we can do is we can say let's take W and Dot it with u and measure whether or not that number is equal to or greater than some constant C so remember that the dot product is taking the projection onto W and the bigger that projection is the further out along this line the projection will lie and eventually be so big that that projection crosses the median line of the street and we'll say it must be a positive sample or we could say without loss of generality that the dotproduct plus some constant B is equal to or greater than zero if that's true then it's a positive sample so that's our decision Rule and this is the first in several elements that we're going to have to line up to understand what this idea called support Vector machines so that's the decision Rule and trouble is we don't know what constant we to use and we don't know which W to use either we know that W has to be perpendicular to the median line of the street but there are lots of W's that are perpendicular to the median line of the street because it can be of any length so we don't have any con enough constraint here to fix a particular b or a particular W are you with me so far yeah all right so what we're going to do next and this by the way we get just by saying that Cal minus B what we're going to do next is we're going to uh lay on some additional constraints with a view toward putting enough constraint on the situation that we can actually calculate calculate a b and a w so what we're going to say is this that this if we if we look at this quantity that we're checking out to be greater than or less than zero to make our decision then what we're going to do is we're going to say that if we take that Vector W and we take the dotproduct of that with Su X plus some positive sample now this is not an unknown this is a positive sample if we take those the dotproduct of those two vectors and we add B just like in our decision rule we're going to want that to be equal to or greater than one so in other words you can be an unknown anywhere in this Street and be just a little bit greater or just a little bit less than zero but if you're a positive sample we're going to insist that this decision function gives a value of one or greater likewise if W dotted with some negative sample uh is provided to us then we're going to say that that has to be equal to or less than minus one all right so if you're a minus sample like one of these two guys or any minus sample that may lie down here this function that gives us the decision rule must return minus one or less so there's a separation of distance here of minus one to plus one for all of the samples so that's cool but we're not quite done because I don't know this is carrying around two equations like this this it's kind of a pain so what we're going to do is we're going to uh introduce um another variable to make things make life a little easier and like many things we do when we develop this kind of stuff uh introducing this variable is not something that God says has to be done it's just a Min what is it it's a we we introduced this additional stuff to do what to make the mathematics more convenient it's a mathematical convenience so what we're going to do is we're going to introduce a variable y sub I such that y sub I is equal to + one for plus samples and - one for negative samples all right so for each sample we're going to have a value for this new quantity we've introduced Y and the value of y is going to be determined by whether it's a positive sample or a negative sample it's a positive sample it's going to be+ one for this situation up here and it's going to be minus for this situation down here so what we're going to do with this first equation is we're going to multiply it by Y subi and that is now X of I plus b is equal to or greater than one and then you know what we're going to do we're going to multiply the the left side of this equation by we're going to multiply the left side of this equation by y subi as well so the the second equation becomes y sub I time x of I + B and now what do what does that do over here we we multiply this guy times minus1 so it used to be the case that that was less than minus one so if we multiply it by minus one then it has to be greater than + one oh oops the two equations are the same because of the introduce this this little mathematical convenience so now we can say that uh y sub I * x sub I + B well what we're going to do right oh did I leave out a w I'm sorry thank you yeah I wouldn't have got very far with that so that's doed with W do it with W thank you br those are all vectors I'll pretty soon forget to put the little Vector marks on there but you know what I mean so that's W plus b and now let me bring that that one over to the left side and that's equal to or greater than zero all right now with Brett's correction I think everything's okay but we're going to take one more step and and and and we're going to say that y subi * x sub I * w plus B minus one you know it's always got to be equal to or greater than zero but what I'm going to say is that for for X of I and a gutter so they always got to be greater than zero but we're going to say we're going to add the additional constraint that it's going to be exactly zero for all of the samples that end up in the in the in the gutters here of the street so the value of that expression is going to be exactly zero for that sample exactly sorry zero for this sample and this sample not zero for that sample it's going to be greater than one all right so that's step number two and this is Step number one okay so now we've just got some Expressions that talk about some constraints now what are we trying to do here I forgot what do we oh I remember now we're trying to get we're trying to figure out how to arrange for the line to be such that the street separating the pluses from the minuses as wide as possible so maybe we better figure out how we can express the distance between the two gutters let's just uh repeat our drawing we got some minuses here got pluses out here and we got gutters that are going down here and now we've got a vector here to a minus and we've got a vector here to a plus so that's um we'll call that X+ and this x minus so what's the what's the width of the street I don't know yet but what we can do is we can take the difference of those two vectors and that will be a vector that looks like this right so that's x + - xus so now if I only had a unit normal that's normal to the median line of the street if it's a unit normal then I could just take the dotproduct of that unit normal and this difference vector and that would be the width of the street right so in other words if I had a unit Vector in that direction then I could just dot the two together and that would be the width of the street so let me write that down before I forget so the width is equal to x + - xus okay that's the difference vector and now I got a multiply by a unit Vector but wait a minute I I said that that W is a normal right W is a normal so what I can do is I can multiply this times W and then we'll divide by the magnitude of w and that will make it a unit Vector so that that that dot product not not a product that dot product is in fact a scaler and it's the width of the street doesn't do us much good because doesn't look like we can get much out of it oh but I don't know let's see what can we get out of it oh gee you know we got this equation over here this this equation that constrains the samples that lie in the gutter so if we have a positive sample for example then this is+ one and we have this equation so it says that X+ time W is equal to oh 1 minus B see I'm just taking this part here this this Vector here and I'm dotting it with X+ so that's this piece right here Y is one for this kind of sample so I'll just take the one and the B back over to the other side and I've got one minus B okay well we can do the same trick with x minus if we've got a negative sample then y sub I is negative that gives us our negative W * dotta with X of I and now we take this stuff back over to the right side and we get one minus b 1 plus b all right so that all licensed us to rewrite this thing as 2 over the magnitude of w how did I get there well I decided I was going to enforce this constraint I noted that the width of the street has got to be this difference Vector times the unit Vector then I Ed the constraint to plug back some values here and it discovered to my delight and amazement that the width of the street is two over the magnitude of w yes Brett yeah let's see what if I got a A minus here then that makes that minus and then the B is minus and I take the B over the other side it becomes plus yeah no no sorry this this expression here is oneus 1 plus b okay trust me it works I haven't got my legs all tangled up like last Friday quite not yet anyway it's possible there's going to be a lot of algebra here eventually so uh this quantity here this is Miracle number three this this this quantity here is the width of the stet and what we're trying to do is we're trying to maximize that right so we want to maximize two over the magnitude of w if we're to get the widest Street under the constraints that we've decided we're going to work with all right so that means that it's okay to maximize 1 over W instead we just drop the constant and that means that it's okay to minimize the magnitude of w right and that means that it's okay to minimize 12 * the magnitude of W Squared right Brett why did I do that why did I multiply by half and square it it's mathematically convenient thank you so this is a this is point number three in the development so where do we go we we we decided that was going to be our decision rule we're going to see which side of the line we're on we decided to constrain the situation so that the value of the decision rule is plus one in the gutters for the positive samples and minus one in the gutters for the negative samples and then we discovered that maximizing the width of the street led us to an expression like that which we wish to maximize how we how we should we take a break should we get coffee too bad we can't do that in this kind of situation but we would if we could and I'm sure when VAP got to this point he went out for coffee so now we back up and we say well let's let these Expressions start developing into a song like not like that that's vapid speaking of vapnik what song is it going to sing we've got an expression here that would like to find the minimum of the extremum of and we've got some constraints here that we would like to honor what are we going to do let me put what we're going to do to you in the form of a puzzle is it got something to do with LeAndre has it got something to do with um llas or does it have something to do with lrange she says lrange actually all three were said to be on for's doctoral defense committee must have been quite an exam but we want to talk about LR because we're going to we got a situation here this 1801 1802 1802 we learned in 1802 that if we're going to find the extremum of a function with constraints then we're going to have to use lrange multipliers that will give us a new expression which we can maximize or minimize without thinking about the constraints anymore that's how L Grange multipliers work so this brings us to Miracle number four piece of the developmental piece number four and it sort of works like this we're going to say that L the thing we're going to try to maximize in order to maximize the width of the street is equal to 12 * the magnitude of that Vector W squared minus and now we got to have a summation over all the constraints and each of those constraints is going to have a multiplier Alpha sub I and then we write down the constraint and when we write down the constraint it is up there but I've got to be hyper careful here because otherwise I'll get lost in the algebra so the constraint is y sub I times Vector W dotted with the vector x sub I plus b and now I've got a closing parenthesis a minus1 and that's the end of my constraint like so I sure hope I've got that right because I'll be in deep trouble if that's wrong does everybody anybody see any bugs in that that looks right doesn't it we got the we got we got the original thing we're trying to work with now we got lrange multipliers all multiplied uh with respect to that constraint up there where each constraint is constrainted to be zero well there's a little bit of mathematical slight of hand here because in the end the ones that are going to be zero the lrange multipliers here the ones that are going to be non zero are going to be the ones connected with vectors that lie on the gutter the rest are going to be zero but in any event we can pretend that this is what we're doing and then we got to find a Max I don't care whether it's a maximum or minimum I've lost track but what we're going to do is we're going to try to find an extreme of that so what do we do what does 1801 teach us about finding maximum well we got to find the derivatives and set them to zero and then after we've done a little bit of that manipulation we're going to see a wonderful song start to emerge so let's see if we can do it let's take the partial of L the Lan with respect to the vector W oh my God how do you differentiate with respect to a vector turns out that it has a form that looks EX exactly like differentiating with respect to a scalar and the way you prove that to yourself is you just expand everything in terms of all the vectors components you differentiate those with respect to what you're differentiating with respect to and everything turns out the same so what you get when you differentiate this with respect to the vector W is two comes down and we have just magnitude of w is it the magnitude of w yeah like so is the magnitude of well any event we got to it's not the magnitude W it's just W like so no magnitude involved then we got a w over here then we got to we got so we got to differentiate this part of it with respect to W as well but that part's a lot easier because all we have there is a w it's not there's no magnitude it's not raised by any power so what's W multiplied by well it's mul by X and Y sub I and Alpha sub I all right so that's means that that this expression this derivative of the lent with respect to W is going to be equal to W minus the sum of alpha sub I uh y sub i x sub I and that's got to be set to zero and that implies that w is equal to the sum of some Alpha I some scalers times this minus one or plus one variable times x of I over I and now the math is beginning to sing because it tells us that the vector W is a linear sum of of the samples some sub all the samples or some of the samples it didn't have to be that way it could have been raised to a power there could have been a logarithm all sorts of horrible things could have happened when we did this but when we did this we discovered that W is going to be equal to a linear sum of these vectors here some of the vectors in the sample set and I say some because for some Alpha will be zero all right so this is something that we want to take note of as something important now of course we got to differentiate L with respect to anything else that might vary so we got to differentiate L with respect to B as well so what's that going to be equal to well there's no B in here so that makes no contribution uh this part here doesn't have a b in it so that makes no contribution there's no B over here so that makes no contribution so we get Alpha I * y sub I time B that has a contribution so that's going to be the sum of alpha I time y sub I then we differentiate we're differentiating with respect to B so that disappears there's a minus sign here and that's equal to zero or that implies that the sum of the alpha I * y sub I is equal to zero H that looks like that might be helpful somewhere now it's time for more coffee by the way these coffee periods take months you stare at it you work on something else uh you got to worry about your finals and you think about it some more and eventually you come back from coffee and do the next thing what is the next thing well we still got this expression that we're trying to find uh the um minimum for and you know you say to yourself this is really a job for the numeric analyst those guys know about this sort of stuff because of that little power in there that square this is a so-called quadratic uh optimization problem so at this point you'd be inclined to hand this problem over to a numerical analyst they'll come back in a few weeks with an algorithm you implement the algorithm and maybe things work maybe they don't converge but any case you don't worry about it but we're not going to do that because we want to do a little bit more math because we're interested in stuff like this we're interested in the fact that the decision Vector is a linear sum of the samples so we're going to work a little harder on this stuff and in particular now that we've got an expression for w this one right here we're going to plug it back in there and we're going to plug it back in here and see what happens to that thing we're trying to find the extremum [Music] of vis a break kind of relax taking a deep breath this isn't actually this is the easiest part this is just sub just doing a little bit of the algebra so the thing where we're trying to to uh maximize or minimize is equal to2 and now we got to have the the the magnitude of those the magnitude we got to have this Vector here uh in there in there twice right because we're multiplying it to together so let's see um we've got from that expression up there one of those w will just be the sum of the alpha I * y sub I time the vector x sub I and then we got the other one too so that's just going to be the sum of alpha now I want to keep things I'm going to actually eventually squish those two sums together into a double summation so I have to keep the indexes straight so I'm just going to write that is alpha subj y subj x subj so those are my two vectors and I'm going to take the dotproduct of those that's the first piece right boy this is uh this is not this is hard so minus and now the next term looks like Alpha i y sub i x sub I * W so we got a whole bunch of of these we got a sum of alpha I * y sub I * x sub I and then that gets multiplied times W so we'll put this like this have sum of alpha j y subj x subj and there like that and then that's a dotproduct of that well that's that wasn't as bad as I thought now I've got to deal with the next term the alpha I * y sub I * B so that's is that a that's a minus sum of alpha I * y sub I * B and then to finish it off we have plus the sum of alpha sub I that's that minus one up there minus one in front of the summation that's just the sum of the alphas are you with me so far just a little algebra it looks good I think I'm I think I haven't muffed it yet let's see Alpha I * y sub I * b b is a constant so I'll pull that out there and then I just got the sum of Al Alpha sub I * y subi oh that's good that's zero and now so so for every one of these terms we dott it with this whole expression so that's just like taking this thing here and and dotting those two things together right oh but that's just the same thing we've got here right so now what we can do is we can say we can rewrite this lran as we got we got uh that sum of alpha I that's the positive element and then we got we got one of these and half of these so that's minus 12 and now I'll just convert that whole works into a double sum over both I and J of alpha I uh times Alpha J * y sub I * y subj * x sub I dotted with X subj we sure went through a lot of trouble to get there but now we've got it and we know that what we're trying to do is we're trying to find a maximum of that expression and that's the one we're going to hand off to the numerical analyst so if we're going to hand this off to the numerical analyst anyway why did they go to all this trouble good question W do you have any idea why I went to all this trouble because I wanted to find out the dependence of this expression as Wana is telling me as I I'm I'm translating as I go she's telling me in Romanian I want to find what this maximization depends on with respect to the the these these these vectors the X the sample vectors and what I've discovered is that the optimization depends only on the dot product of pairs of samples and that's something we want to keep in mind that's why I put it in Royal purple now up here so so let's see let's let's what do we call that one up there that's two I guess this uh we'll call this piece here three this piece here is four and now there's one more piece because I want to take that that W and not only stick it back into that lran I want to stick it back into the decision rule so now my decision rule with this expression for w is going to be W plugged into that thing so the decision rule is going to look like the sum of alpha I * y sub I time x subi dotted with the unknown Vector like so and we're going to subtract I guess add B and we're going to say if that's greater than or equal to zero then plus so you know you see see why the math math is beginning to sing to us now because now we discover that the decision rule also depends only of the on the dotproduct of those sample vectors and the unknown so the total there's a total dependence of all of the math on the dot products all right and now I hear a whisper someone is saying I don't believe the mathematicians can do it I don't think those numerical analysts can find the optimization I want to be sure of it give me ocular proof so i' like to run a demonstration of it okay there's our sample problem the one I started the hour out with now if if the optimization algorithm doesn't get stuck in a local maximum or something it should find a nice straight line separating those two guys to finding the widest Street between the minuses and the pluses so in just a couple of steps you can see down there at Step 11 it's decided that it's done as much as it can on the optimization and it's got three alphas and you can see that the two negative the two negative uh samples uh both figure into the solution the weights on the the L gr multipliers are given by those little yellow bars so the two negatives participate in the solution as as one of the positives but the other positive doesn't so it has a zero weight so everything worked out well now I said as long as it doesn't get stuck in a local maximum guess what those mathematical friends of ours can tell us and Pro to us that this thing is a convex space that means it can never get stuck in a local maximum so in contrast with things like neural Nets where where where you have a plague of local Maxima this guy never gets stuck in a local Maxima let's try some other examples here's two vertical points no surprises there right uh you say well maybe it can't deal with diagonal points sure it can uh how about um this thing here yeah it only needed two of the points since any any two of those pluses any two a plus and a minus will Define the street let's try this guy oh what do you think will happen here well we're screwed right because it's linearly unseparable bad news so in situations where it's linearly unseparable the math the the mechanism struggles and eventually will just slow down and you truncate it because it's not making any progress and you see the red the red dots there are ones that had got wrong so you say well too bad for our side doesn't look like it's all that good anyway but then a powerful idea comes to the rescue when switch to another perspective so if we don't like the space that we're in because it it gives us samples that are not linearly acceptable linear linearly separable then we can say uh shoot here's our space here are two points here are two other points we can't separate them but if we could somehow get them into another space maybe we can separate them because they look like this in the other space and they're easy to separate so what we need then is a transformation that will take us from the space we're in into a space where things are more convenient so we're going to call that transformation Fe of the vector X that's the transformation and now here's the reason for all magic I said that the maximization only depends on dot products so all I need to do the maximization is the is the transformation of one vector dotted with a transformation of another Vector like so that's what I need to maximize or to find the maximum one then in order to recognize where did it go underneath the chalkboard oh yes here it is to recognize all I need is dot products too so for that one I need Fe of X dotted with f of U and just to make this a little bit more consistent a notational I'll call that XJ and this x of I and that's X of I those are the quantities I need in order to do it so that means that if I have a function let's call it K of X of I and X of J that's equal to f of x sub I dotted with f of x subj then I'm done this is what I need I don't actually need this all I need is that function K which happens to be called a kernel function which provides me with the dotproduct of those two vectors in another space I don't have to know the transformation into the other space and that's the reason that this stuff is a miracle so what are some of those what are some of the kernels that are popular one is the linear kernel that says that you dotted with with V + 1 to the N is such a is such a kernel because it's got u in it and v in it the two dotted the two vectors and this is what the dot product is in the other space so that's one choice another choice is a kernel that looks like this e to the minus let's take the dotproduct of those two the difference of those two guys let's take the magnitude of that and divide it by some Sigma that's a second kind of Kernel that we can use so let's go back and see if we can solve this problem by transforming it into another space where we have another perspective so let's that's the that's that's it that's a that's that's another kernel and so sure we again and that's the answer when transformed back into the original space we can also try doing that with the so-called radial basis colel that's the one with the exponential in it we can learn on that one boom no problem so we've got a general method that's convex and guaranteed to produce a global solution we've got a mechanism that easily and allows us to transform this into another space so it works like a charm of course it doesn't remove all possible problems look at that exponential thing here if we choose a sigma that is small enough then those sigmas are essentially shrunk right around the sample points and we can get overfitting so it doesn't immunize us against overfitting but it does immunize us against local Maxima and doesn't provide us with a general mechanism for uh doing a a transformation into another space with a better perspective now the history lesson all this stuff feels fairly new it feels like it's younger than you are here's the history of it uh bnck immigrated from the Soviet Union to the United States in about 1991 nobody ever heard of this stuff before he immigrated he actually had done this work on the basic support Factor idea in his PhD thesis at Moscow University in the early 60s but it wasn't possible for him to do anything with it because he didn't have any computers that he could try anything out with so he spent the next 25 years at some oncology Institute in the Soviet Union doing applications somebody from Bell Labs discovers him invites him over to the United States where he subsequently decides to immigrate in 1992 or thereabouts vapnik submits three papers to the to nips the neural information processing systems Journal all of them were rejected he's still sore about it but it's motivating so around 1992 199 three Bell Labs was interested in handwritten character recognition and in neural Nets vapnik thinks that neural Nets well would be a good word to use uh I can think of the vernacular but he thinks that they're not very good so he bets a colleague a good dinner that support Vector machines will eventually do better at handwriting recognition than neural Nets it's a dinner BET right it's not that big a deal but as Napoleon said it's amazing what a soldier will do for a bit of ribbon so VAP Mick's colleague who's working on this uh on this on this problem of handwritten recognition decides to try a support Vector machine with a kernel in which n equals 2 just slightly nonlinear works like a charm has was this the first time anybody tried a kernel fabik actually had the idea in his thesis but never thought of was very important and as soon as it was shown to work in the early '90s on the problem of handwriting recognition vapnik resuscitated the idea of the konel began to develop it and became an essential part of the whole approach of using support Vector machines so the main point about this is that it was 30 years in between the concept and anybody ever hearing about it it was 30 years between vapnik understanding of Colonels and his appreciation of their importance and that's the way things often go great ideas followed by long periods of nothing happening followed by um an epiphanous moment when the original idea is seen to have great power with just a a little bit of a Twist and then the world never looks back and vapnik who nobody ever heard of until the early 90s becomes famous for something that everybody knows about today who does machine learning