good afternoon cs19 how are you guys doing today yay fantastic we have an exciting class we get to talk about some of the probability behind ethics and machine learning and boy is this an important topic we've learned some pretty interesting machine learning in CS 109 and we are currently living the experience of watching the world contend with smarter and smarter algorith and it brings up two important things I'm glad you guys know machine learning cuz maybe you'll do something really helpful for the world with it and also it just feels like for us to understand what's going on in this world we should know the machine learning that is impacting our society and we should be able to see it uh look at it with a a sophisticated lens if possible and that's what we're going to do today okay oh and this is like such a cute thing to start out with I've been thinking about like what is my own personal basis of ethics and before we jump into this lecture I just want to tell you where I think I am right now um where does this idea of human goodness come from is a really interesting ethical question before we jump into like our cool lecture I want to tell you that I'm kind of gravitating towards this uh mous philosophy of ethics and the melanous philosophy of Ethics says that there is goodness in all of us there is a sprout of goodness goodness is not something that is given to us it's not something that comes later in life it is in our hearts from the beginning um and some people feed those Sprouts so he is this agricultural metaphor of the goodness is a sprout within us and if you water that Sprout it will grow and germinate into the fully benevolent person that we are all capable of being and he often argues at this point by using the uh allegory of the baby in the well if any of us saw a baby walking towards a well if we thought that baby was going to fall into the well you would have this not reflexive you know conscious but very predictable reaction to go help that baby and in fact all of us can just feel uncomfortable imagining a baby falling into well and he says this is his example of saying there is something hardwired about humans that has some goodness there is that sprout in us some people don't exhibit uh you know a mature version of that Sprout but it's everywhere and I just love this cuz you guys kind of know me by now I'm like I'm kind of an optimistic person I do see this goodness in everybody so I do like this ethical construct but my own personal ethical construct is a little bit less important than what we're going to talk about today the framing for today is very simple AI is impacting Our Lives all over the place you know it lives on our smartphones you know there security cameras social media all these different facets of life we're starting to see algorithms play a role in human existence and therefore we must think about Ai and how it could be fair there's a few ways of thinking about this one is with uh great power comes great responsibility another way of thinking about this there's a whole field and set of careers in people who are thoughtful about what fairness means in this new phase so you know I said at the beginning of class no matter what your interests are there is something you can combine with probability you know if you're into dance you can do dance and probability uh and and certainly if you're into philosophy there's a lot of room for careers in philosophy and probability now we do live in a time with real work should be done and some times the affordances of machine learning can be helpful you know we can all imagine Better Health Care you know I think of my daughter who has coar implants and like the the AI in those C implants will make her life better um you know we can make smart grids to be more intelligent about how we can balance uh electricity in this coming electrical age um and you know maybe we can increase access to high quality education there's all this real work that could be done with machine learning but also it is worth talking about some of the examples of shortcomings where we can look at deployments of machine learning think that's not good um and some some versions that uh show biases sometimes uh you know computer science can hit Beyond biases and in fact we'll talk about this example later um and there's a or there just case studies we can point to where like there could have been a better way and we're going to see if we can come with a nice construct for that can I give you guys learning goals couple things let's imagine where would we like to be when we want out of today's class I would like you to understand the limits of this idea of fairness through unawareness that's something I would like everybody to know I'd like you guys to know these two ways of measuring fairness and the know there Subway of relax calibration I want all of you guys to know that in fact those are things that this is something you will need for your problem set now this isn't something that I'll test you on but I also want you guys to walk out with some sense of techniques and principles to mitigate fairness issues so that's our goal for today does that sound good okay let's do it there is this other medical uh which is like how to become a responsible scientist and not show up in the news uh like this researcher okay um a couple Concepts that are going to come up in today's class what is a protected demographic what's the difference between distributive harm versus quality of service harm and then what is fairness what what's the philosophy of procedural versus distributive fairness and then these different definitions of fairness so you're going to see a whole bunch of very technical interesting things that come from the world of philosophy that apply to the world of computer science and then I did want to put up this warning this is my own take by the way this is a lecture made uh by Katie creel and myself um a lot of the research done on fairness comes from the US especially fairness in machine learning um and I do think of that as a kind of funny metabias like I we'll talk about fairness but be mostly centered within our own community and you can imagine every Community has accesses of discrimination that are worth talking about so even though our examples do come from the US and I would like to highlight that the framing of what you all learn does go beyond um the sorts of accesses that you will see in the US okay A little bit of review shall we ah machine learning we've been learning about it for a while and particularly we've been talking about blackbox models um where you have a model which we can take in some inputs make a prediction and that model is governed by parameters we call it blackbox model because we imagine using it like a blackbox where you just put your inputs in it and outputs come out and you know particularly we've been talking about classification algorithms where your inputs could be some set of features for an individual and the output could be some sort of discret prediction like a zero or a one for a class label we've talked about two major algorithms uh we learned about naive base and then we learned about logistic aggression uh I do want to tell you something that we didn't go too deep into but is very very interesting if you took logistic regression some people notice that if the input to the sigmoid is positive it predicts a one and if the input to the sigmoid is a negative it predicts a zero and so you can think about this set of points like what are all the inputs where the input to the sigmoid is a zero so it's perfectly between the positive and negative and for those of you guys who have seen this before you might recognize that this is the description of a straight line so what this is telling us is that logistic regression imagine you only had two features because then I can visualize it you only had features X1 and X2 and based on that you're either going to predict a one or a zero the idea is that logistic regression is only able to draw a straight line between the zeros and ones and if it can find a nice straight line it can do a perfect job on this task but if your points aren't linearly separable it can't do a perfect job does that make sense very cool way of understanding the limitation of logistic regression it does pretty well but we can think about the cases where it will break of course if you have more than two features it's really hard to think about this because you end up having like a straight line in higher dimensional space but it still has this straight line uh sort of property interestingly turns out naive Bay is also creating a linear separator in that there's no interaction between the X the different features that are allowed in terms of making its prediction so both logistic regression and naive Bays have this similar limitation and then in last Wednesday's class we took it to the next level we're like well what if we want to have more powerful predictors and we did we said hey logistic agression you kind of look like a cartoon model of a neuron what if we just stacked logistic regressions on top of logistic regressions would that be crazy and at first the world thought yes but then the world got powerful computers and said not crazy gamechanging and that is the backbone of all modern AI a bunch of logistic regressions put together called neural networks it turns out these neural networks can make predictions that are much more sophisticated than just straight lines we saw them in class we talked about how they make their predictions we even talked about how they use chain roll in order to gain intelligence so you can use maximum likelihood estimation not on just one logistic regression but on a whole neural network of many logistic aggressions what a time to be alive uh and this was an interesting thing that we pointed out in last Wednesday's class not only can they learn something more complicated than a straight line classifier when they train neural networks on things like recognizing faces the craziest thing happens where the first layers start to become Edge detectors the middle layers start to become part detectors and final layers start to look like ghost fa faces and the crazy thing about that is even though these were just getting their weights from mle applied with chain rule they learned something that is so similar to how our human brains work if you looked at our human brains the V1 cortex is an edge detector and if you looked at what particular neurons are looking for it looks a lot like that later in our brain you find part detectors and if you looked at what sort of inputs would most stimulate neurons later on in your V2 cortex you would see stuff like that uh and later on again you get these ghost faces insane something crazy is going on but it turns out these neural networks while amazing are not panas and not every example of neural networks applied to human problems has been successful we're going to talk about places where this stuff is broken and in order to do that it is worth noting that not not every broken neural network is equally bad uh and so let's bring in our first idea from philosophy thank you there are different ways of saying whoa that model was really janky here's one we think it is bad if a neural network that does something wrong introduces a quality of service harm and a quality of service harm is one where we say this neural network works better from certain demographics than others so for example maybe you have generative art or facial recognition and it doesn't work for everybody equally that's considered to be bad and harmful but not nearly as harmful as if you had machine learning that does something called a Distributive harm we think that there's particular opportunities resources or information that are really essential like for example getting a job or getting a loan or getting into a school these are really important decisions so if neural networks start to make mistakes over here we think of that as even more harmful uh and then finally just to like cap off this conversation there are existential harms I'm not sure when we're ready to talk about this but like do do we need to start thinking about when AI becomes intelligent I'm not sure I feel like I would just be speculating as much as anyone else but we do think of degrees of harm um all the way from quality of service harm to existential harm let me give you two case studies that give us an idea of AI producing uh quality of service harms and distributive Harms you guys ready so my first case study for you is a case study in quality of service harm uh and it comes down to these cameras that were actually released uh and were a were trying to do autofocus but the autofocus did not work if you were not white and particularly um would make some awful decisions uh for different demographics how did this happen we considered this to be a quality service harm one way of telling the story is to go a little bit back into the world of understanding neural networks so I told you the story on Wednesday of a neural network that was trained to look at images and tell you what's in those images the naral network uh we got really good at predicting you know whether or not an image was one of a thousand or more than a thousand or 22,000 categories but can I show you some places where that neural network makes mistakes and these are like a little bit more neutral uh than what we just saw these really really smart neural networks that can perform almost as well as humans make some pretty hilarious mistakes so for example it's pretty sure this is a sea sea lion it's pretty sure that this is a manhole cover what could possibly lead to this very intelligent neural network making those sorts of mistakes problems with the training data oh yeah totally but can you tell me a little bit more than just problems cuz you're right but it is worth getting a little bit more deeper into that what do you think what sort of problems like why the manhole cover for that particular image lack of coverage for that specific feature um and because it has so few data points to choose from it can pick and choose and get noise um yeah so it's probably not seen enough dragon flies that's certainly the case uh and but there's something specific about this picture uh yes I think angle that it's taken at um and even for the photoes are doing that thing yes yes yes okay so angles definitely is part of it an idea not to just read off the slide but I mean it seems like texture comes into it the sea lion has like a smooth surface and the manhole cover is sort of like graded as well yeah it turns out they get really obsessed over things that humans can see past like for example they're really into textur so this just happened to have a texture that looks like a manhole cover and this happened to have a texture from the water over here that looked like a sea line now there is a good argument that it's just a data problem uh so training data can really be a source of biases within neural network so what you train your neural network on really does matter there's lots of ways of telling the story one story is what we just saw you know it it got confused because it saw too many uh textures associated with man holism so when it saw a particular texture all always associated with manholes here's another way though of getting a little bit deeper into this data problem imagine you have a classifier and it could be linear it could be more complicated but let's imagine linear for now imagine you now have a population that has a majority group and a minority group in the majority group here's how you would classify class Zero from class one but for the minority group let's say it works pretty differently when you put put all this data together into your data set imagine you're learning a linear classifier is it going to being paying attention to the signal that comes from the majority group or the minority group when it's trying to figure out some classifier to best split the circles from the pluses talk about what the person next to you for a minute and then we will talk about what will actually happen together okay go for it I'll take a Time chance to draw over here you it's interesting to think about okay I so I'm sure a lot of you guys came to something like this it's using maximum likelihood estimation to train and so the likelihood Ood is going to treat each point equally every point will have exactly one vote uh and so because of that because there's so many more points voting for uh a algorithm that separates like this than there are coming from the minority demographic mle really wants to just ignore that Minority demographic and just focus on what is the overall trend and that really comes from the fact that Emily has whether or not you're in the log version it has that sum over data points or if you're in the um the non-log version you have the product over data points where it treat every single data point the same okay so at this point lack of data uh can be a huge problem and in fact going back to that camera we really do think that is one of the biggest problems was just one group looked like a minority to the algorithm and then the algorithm just hadn't been trained on enough data and was making bad mistakes it was trained on a faces in the wild data SP data set which was maybe unsurprisingly 84% white and 78% male and because of this it did very well poorly when it was out of distribution and actually this is something that we've gotten much better at they started this process of pointing out those mistakes they took a whole bunch of models that were in the wild that had been published and were actually being used and showed how they were making bad decisions uh based on demographics and then they produced data sets that you can use that are much more Bal across different contexts that people might be coming from and once people started training on these data sets those particular examples that we saw um became less prevalent so anyways cool little story that took place here at Stanford now there are other levels of harm so that was an example of quality of service harm but things get a little bit more serious when they affect people's lives and that makes sense like if you're making a decision about whether or not somebody goes to school or prison that's a different level of harm if you get it wrong and here's a case study of a Distributive harm in AI so there is this case of St George hospital it's a medical school and they had a whole bunch of applicants they have 2,500 applicants uh they would interview approximately 625 so 3/4 people get rejected between this stage and this stage and then eventually they offer spots to 425 so 70% of interviewees are uh accepted in 197 9 one of the Deans had this great idea they said hey we've got a lot of historical data of people who have applied and gotten past this stage I could train an Al a classification algorithm to decide who is going to get to the stage and we can just have a computer do this whole first pass so here is the timeline of what happened 1982 he argues that he's built this classifier and then he argues is he's getting 90 to 95% classification accuracy so when he makes a prediction 95% of the time it agrees with what a human would have said on the panel that sounded like high enough accuracy so um in 1982 they started using it then later on there is internal review which asked why are applicants being weighted by factors like name and place of birth when they actually looked into the algorithm it would be putting weits on things that people thought weren't that important and it led to a review of this algorithm um and then eventually this went to uh a ethics hearing because um it found out that the name and place of birth were being used to dock points from female and non-caucasian applicants so I guess this story tells us of hey when you make mistakes it is much worse when it's affecting something like well people end up in school now that was just to give you a case study of these two different harms but along the way we have picked up some insights we have this term in AI garbage in garbage out which means if you have bad data coming in you will make bad predictions if we go back to the St George's one one of the reasons that the algorithm was making bias predictions is because before the 1980s the humans were also making bias predictions and the algorithm learned to recreate the garbage that it was trained upon um similarly we can say you know improper use of sensitive features that algorithm really shouldn't have been looking at things like people's names that's a red flag that says that should not matter for whether or not you're ready for medical school um and then there's this idea that it's very believable that somebody who trained this algorithm didn't intend to be evil like they maybe thought they were doing something pretty reasonable but just having not thought it through they ended up creating quite a lot of harm so at this point we've just uh pointed out a couple times of far Farm can we get a little bit more formal about fairness and the answer is philosophers are so excited to tell us yes yes we can we've been thinking about fairness for a long time we can actually get into more specifics about how we can call an algorithm fair or not fair to start this conversation it's worth pointing out that there's two big fields of thought for what we call fairness one field of thought says fairness should come from the procedure you use they call this procedural fairness so what is the process of your classification and what do you do to ensure that your algorithm doesn't rely on unfair features that's procedural fairness and there is another school of thought that says well what really matters is the outcome of the decisions you make and so there's these two big philosophical camps let's be fair in our procedure or let's be fair in our outcome let's jump into a few few different definitions of fairness that speak to these different philosophies the first one speaks to procedural fairness and a lot of people historically subscribe to this being a good idea the motivating idea for this first definition of fairness fairness through unawareness is the best way to stop discrimination is to stop discriminating based on race weights that's a chief justice Roberts that's a little bit obscure so let's unpack it it's saying that if you're making a decision if you know nothing about somebody's sensitive features then you'll make a good decision that's the idea of unawareness be unaware of the demographics and you'll be fair with respect to demographics how to do it gets a little bit more interesting you know the first idea um is to exclude sensitive features so if you're making a prediction it is procedurally unfair if you're looking at any of these protected demographics and the second idea is we also have to be thoughtful about proxies let's jump into that a little bit it is worth also noting what do I mean when I say protected groups in the US there are legally protected groups in a lot of different important um uh in a a lot of different important cases the idea of a protected group often means that regardless of people's race color national origin religion age or sex sexual orientation physical or mental disability um or reprisal uh people should be getting Fair decisions made for them uh and reprisal I think is like you know if somehow somebody had been harmed in the past they shouldn't be discriminated in making in the decisions that are made against them in the future this is the definition that comes from the Equal Employment Opportunity Act but this same definition lives in a lot of places uh in legal code for the US like for example housing and Loans so this seems pretty reasonable but can we jump into how fairness through unawareness can be surprisingly difficult let's take some people who you know live across the street Facebook they do ads and at some point they were making people would be publishing ads on Facebook for housing housing is important because it's legally protected with respect to those protected demographics now I've never made an ad in Facebook but here's how I understand it works when you go to make an ad at some point you had an option of saying give me lookalikes and what lookalikes mean would be like you would create your ad and be like here's five people I want my ad to go to people who look like these five people um and they did this for all their ads and at some point in 2018 some people said that is not a good way to do advertisements for housing because advertisements for housing are not supposed to be biased based off of these protected demographics and Facebook had to change their algorithm and so Al Facebook made this change they said we're going to do fairness through unawareness we can do lookalikes but our algorithms are not going to look at any of the protector Grog graphic we're not going to look at the age that the look like we're not going to look at the gender the look like we're going look at the relationship status we're not going to look at the religious views the schools the political views any of this we're not going to look at any of that you can still give look likes but we will be unaware of their demographics and therefore it will be fair okay well anyway we'll jump into this but turns out long story is no and I'm going to give you guys a second to try and think about why now this is a funny uh plot it comes from a paper which basically said hey Facebook your new lookalike thing still is um able to pull People based on demographics so what they did is can you try to flip this X and Y AIS cuz the x is what the researchers were controlling they would make lookalikes with different fractions of men and they did this in the old version of Facebook and in the new version of Facebook that didn't look at demographics so you know in this set of look likes there' be no men at all and in the top set look like there would be 100% men and then this is the yellow one is when they were looking uh when they were looking at demographics and you would actually get zero men you know and men that looked proportional to the lookalike uh distribution but when Facebook didn't look at demographics look how closely it was able to produce the same fraction of men as in the lookalike set even though the algorithm was not looking at any def demographics at all how is this possible algorithm doesn't look at whether or not somebody is a male but it can figure out a population with the exact same proportion of men as in the lookalike population a little bit of mystery but a mystery that you guys could figure out if you talked with the person next to you for about a minute okay so think about that how could Facebook still be pulling out the right prop or the lookalike percentage of men e okay how does this happen they said don't look at gender they said don't look at age I'm not looking at your age I'm not looking at anyone have a idea yes iures Associated users do you want to give me one that you think maybe you could hypothesize might be Associated maybe like shaving products oh yeah like do you like shaving products yes uh you know everybody shaves um maybe like your movie selection I'm not saying that you know we don't all like the same movies but maybe if you're like all Jurassic parks and you know Star Wars then maybe that can just influence but if you put enough of these features together like you know maybe this doesn't tell you for sure and maybe this doesn't tell you for sure but there's this interesting result which is as we add more and more features like you tell me more and more about a person even if you don't tell me their demographics I can predict their demographics better and better and better like as you give me more of these things and more of them leak some information about demographics you put it all together and by the time you're Facebook and you've told me everything about this person like their friends their likes um you know their history on The Social Network then you have a really good way of guessing their demographics without being told and we call those proxies yes why would we add all that redund data in the first place so like what what does your movie interest have to do with wanting to rent a house good question it should not I agree with you but uh the look likes algorithm did that because people would do ads for things that were not houses like maybe you want to give ads for I don't know you're selling like the coolest pet app and you want lots of people to buy it and you think somehow like people's likes are going to uh be really important in predicting who's going to buy or p app I'm not trying to sell the pet app it's staying free forever just for 109 U but does that make sense is because it was for the ads platform and they thought these things would be important for ad Sellers Facebook their customer are the ad buyers yes don't they have our pictures they have our pictures I believe the look alikes yeah might have even be using the pictures possibly I don't know but if they did that would be a really strong signal maybe you wouldn't need you know like 80 features to predict to somebody's mail good good question I owe you Mander but come forward after class okay I have a question yes so the previous slide so how much teachers how much information can I tell Without Really compromising when you ah okay that's a deep question so the question was how much information could I give you without compromising your your proxy it depends on the feature itself and if we had a set of features we could talk about you know the probability that somebody could um predict this demographic you know what I would probably do to answer that question a simple way I would take the features and i' take the data I have and then I'd predict I'd build a deep learning or logistic regression model that would try and predict the demographics uh and I would see how accurate it got and that's how I would test but depends on the features and if you have the the right proxies it's just not that many such a good question I love it okay so at this point in our story we would like to get formal about fairness and the first natural thing was well we'll be fair by just not looking at the demographics but it turns out that's really hard fairness through unawareness is really difficult to do which maybe invites us to start thinking about this other philosophy of maybe we can be fair but by looking at outcomes so we can try fairness if not through unawareness then fairness through awareness let's be aware of demographics and make sure that our algorithms are fair that way now we've talked about this in section before so this is a little bit of a review but it's review worth uh restating because you're going to need it for your problem sets if you have a classification algorithm one of the things you could do is you can build this joint distribution table this is a joint distribution between three random variables which you can calculate based off your training data so one random variable is for any point in the uh training data set you can calculate D which is going to be your protected demographic now there could be more than two groups but just to keep it clear or simple we can say we have two different demographics d0 and D1 but because you have a training algorithm there's other variables you have access to so G AKA y hat could be your guess and G is the guess of your model so if you put this trading data through the model that you're evaluating if G is zero it's going to guess that the label is zero and if G is one your algorithm's guessing that the label is one and then there is a separate thing the guess of the algorithm is not always correct so because this is training day you also have access to the truth AKA Y and the truth could say you know in fact this out or the true output of this algorithm was zero or in fact the true output of this algorithm is one you could take all your training data and you could have a a joint table because you could think about these random variables together from these random variables there are many definitions of fairness through awareness we're going to just talk about a couple of them that are the most commonly used the first one is hey maybe I'll think about your algorithm as being fair if it satisfies parity and if you read the algorithm definition of parity it gives a problemistic definition it says the probability that you guess one for somebody from one demographic is the same as the probability that you guess one for somebody of another demographic so regardless of people's background your algorithm has the same probability of predicting a one and maybe we think that this is a good sign that the algorithm is making Fair decisions but it's not the only good sign a different good sign is that you know I want condition on Dem Graphics a different thing to be true the thing that I want to be true is that g equals T now that's different from saying g equals 1 this is saying that g equals T what's the difference yeah the parody uh thing on the board I believe differs from what's on this slide am I missing one here better yeah yes sorry so is it saying that the algorithm predicts correctness regardless of demographic or regardless of whether demographic is included yeah so this is saying regardless of the true demographic of the person the algorithm is just as likely to make the right decision so not like whether we include demographic or not no matter what it could be included it could be not included but the outcome is that correctness is not conditionally different based on um demographic you could say that it is conditionally independent of demographic yes question I was asked yes it's independent yeah so you know knowing demographic won't change your belief in whether or not the algorithm get it right and this first one parody says knowing the demographic won't change your belief that it's going to predict a one those are subtly different things yeah is there a situation where we wouldn't want it to be the same like would it in some cases actually make sense and be beneficial to actually match the true yeah there are cases where you might say uh um you know for this one particularly you say that you know for different demographics maybe there is a different underlying rate so an algorithm that's always predicting one independently of demographic then maybe it's inaccurate because there's a different base rate but some people say that even if there's a different base rate that might still be something worth aiming for because we'd like our society to move towards this um but then this is a little bit more saying you know there could be a different base rate so you could be predicting g equals 1 in different rates but you're correct the same amount of time and they're different philosophically and there's certainly been algorithms where I've seen people debate very hard about what they consider to be fairness because it does turn out that not all these fairness can be satisfied together you sometimes have to make a decision of what fairness you think you're going to be going for and that really depends on the case um I do want to not a little bit more that there is a legal standard for what what allow what level of independ we're looking for I know things are Independence is binary you're either independent or not but people do allow these things to be slightly different in some legal uh situations so for example the US has this legal standard of disperate impact which says an algorithm is acting um in a biased way with respect to calibration and that's not saying that these two things have to be exactly equal it allows the ratio to be you know either this ratio has to be greater than equal to one minus some Epsilon sorry that should be 0.2 um and regardless of which thing goes on the numerator and we think that if this numerator is close enough to being one then we think that this follows under uh calibration according to Legal standards now there is this historical story which is not that historical it's not that old back in the day there was this algorithm that people were using California and Florida to predict whether or not somebody would commit a crime again and this is very important because it was used in courts in order to decide if people would get lenient sentences or stronger sentences uh and I think also to decide whether or not people would be given bail so this algorithm which has the name Compass was being used in in these states making predictions but regardless of whether or not you measured calibration or parity it was doing a bad job and that the the Gap was very large between uh black inmates and white inmates so at this point I do think fairness through awareness has a lot of strengths fairness through unawareness is very difficult fairness through awareness has this idea that like okay we're going to pay attention to demographics and try and be fair but it does have weaknesses that are also worth pointing out the first one is you have to make a stance about what your definition of fairness is and there's more than these two there are like 17 definitions there's people in CS and philosophy who come up with new definitions all the time um and so you have to choose what fairness means to you there is this more subtle harm though that can come from fairness through awareness that is worth pointing out um and this was uh came out in a paper by Cynthia dor and Omar reingold who are professors here in the Cs Department um of this self-fulfilling prophecy challenge that can come through fairness through awareness the idea is let's say a classifier is made to be really good at one of these definitions of fairness if it turns out that pursuing that fairness means that the algorithm is a lot worse at choosing strong candidates in different groups then you might see a quality of service disparity later on which might lead people to look at differences in the groups and assume that there is a stronger difference than are between the groups which can change people's mindsets to think that some group is stronger than the other which they call allocation dispar uh disparity uh and so there is an argument for there's a lot more Nuance to this uh though a lot of though that's something worth keeping in mind this is still the state-ofthe-art that a lot of people go towards when we think about fairness okay so where are we in cs19 yeah great question yeah I was just wondering so I mean in all of these I understand that not consider demographic information like Race For example would be really important especially if you're looking at like crimes but if you're looking at something that's not like humans discriminating against humans but like diseases discriminating based on genetics or something like that how do you draw a line between not including race because you don't necessarily have enough data but still that being a major factor that contributes to different disease outcomes for people that's a good question so okay I like let's think about the disease thing often fairness comes down to like decisions made by humans so let's imagine in the world of disease what's a decision that humans could be making like whether to treat or not I guess yeah okay let's say whether to treat or not so um in this world whether to treat or not is a decision that you're making is a decision that we could think through the fairness construct um okay okay so now that I'm in this world what was the question sorry so I mean I guess like for example a doctor is trying to use this to decide should I treat this patient or not and maybe it's shown that for this subass of patients this race of patients this treatment doesn't work as well for whatever reason that's what data shows that might actually be true but if we're not ignoring that then we're potentially putting those people at harm yeah so that's a good argument I think for you should not be using parody in that case uh you should not be saying treat at the same rate for the two different groups I think that's still an argument where you might want to say you're calibrated which is I'm not going to treat at the same rates because we know that there might be different effects uh yet I want the accuracy of my decision to be the same so if I decide to treat and treat was the right decision you know that's plus one for accuracy and if I decide to treat but that was the wrong decision that's a zero for accuracy and we want that rate to be the same right so in calibration you're still like if you're training an algor you can still use demographic data to train it yeah potentially gu calibration doesn't speak about the procedure you use it just speaks about the outcomes though is it possible to try and do both of these at the same time absolutely and that I feel like that's maybe a good practice like don't try and use demographics in your prediction and then make sure that your outcomes are somehow Fair even if demographics have been shown to impact ah okay um no maybe that is a case where you say like using demographics could be important so maybe yeah maybe in the particular case you brought up that might be a good example for we want fairness through awareness and we particularly want calibration and not parity yes if a big model is going to infer demographics through proxies why not just include it at the beginning of the the model if we're so sure based on these like common considerations that it will be reconstructed so I mean going back to Facebook they're legally not allowed to so like you're not supposed to be looking at demographics when making the decision U because the law was built around procedural fairness so even okay so your question is hey these algorithms are going to build proxies should we just throw in demographics and I'd say that the arms race goes in the other direction we shouldn't include the demographics and we should be mindful about proxies and we should think of ways that we can deal with the proxies because it turns out there are things we could do want to learn more about those okay let's jump into it okay so at this point in class we've said hey machine learning is an interesting tool but it's possible to create harms we've talked about two specific types of harms we could talk about then we said okay we'd like to be fair and so that whatever harm if we're going to possibly have harms though it'd be great if we had zero harms uh we should be fair and we've talked about three versions of fairness fairness through unawareness and then we've got parity and calibration now at this point though I think it's nice to leave our story on the note of what could we possibly do about it so here are some Pro tips so one of the pro tips is being balanced in your training data that comes from our first set of stories you know if you're going to be training something that does facial recognition try and represent all people uh in your training data there's this other thing that's growing in the world of AI which is being very transparent about where your blackboxes come from so if you make a blackbox algorithm there's now this thing that you can attached to it called a model card and it tells everything like you know who are you making this model what is your intended use what are the factors that go into it how did you evaluate your model what was the training data that you allowed on it uh and what was the quantitative analysis like what are all the uh measures you've done maybe about Fair parody uh calibration so this is becoming uh standard and then here's a funny little story that actually starts in cs1 that's kind of interesting one of the things you could do is you could ask can we train the bias out so back to your question if you include demographics of course it can make decisions based on demographics if you exclude demographics it might make decisions based on proxies but can we build an algorithm that tries to remove out those proxies uh and that was a question that two seniors at Stanford uh former CS 109 students were interested in they're like can we remove these proxies so they looked at Compass they actually got there is a public data set now on compass on decisions it has made and in compass it's a the inputs look like all sorts of data about an inmate it has their zip code their past crimes their age lots of inputs like that and then the prediction is recidivism are they going to commit a crime again now first of all when I talk about these inputs a whole bunch of red flags should go off be like why do you care about their zip coid why is that going to make you make a good decision and just to you guys this was actually used in the state of California and Florida now compass from this data we can show was very biased whether or not you measure bias in calibration or in parity you want these gaps to be zero because the gaps are the difference between the size of your equations so the bigger the gaps the bigger problem you have and they had pretty big gaps so these two students said okay what if we tried to be creative and came up with a way to remove the bias that was in this Compass algorithm and so they built a two-step neural network the first neural network was the prediction and this was recreating something like Compass you would take in the inputs and then you would predict out whether or not somebody would recidivate but then they had a second neural network that would take the output of the first neural network and then try and predict demographic out and they said if you train this so that you do really well on model one so likelihood is really high on model one but then they said make likelihood really low on the second one so you're this combined model should be really good at making its prediction but very bad at pulling out demographics and the theory was if you did this then you could remove some of the proxies that exists within the data it worked very well so they ended up with a model that was just as accurate but they were able to remove calibration gaps and parody gaps but my favorite thing about this paper was they did all these cool things and then they're big claim in the paper from the beginning to the end was yes we can do this but you should not be using blackbox algorithms to make reation predictions and you should absolutely not be using these features like zip code shouldn't matter um and it should be something that we can be much more thoughtful yes judges have been biased but biased algorithms are not the direction we want to go towards and particularly they picked out this idea of using a blackbox algorithm because it's something that defendants couldn't argue against if the blackbox algorithm just said like yep you're committing a crime again no I'm just joking you haven't committed any crimes but I'm as an example um and you can't argue again you're like the neural Network's wrong you're like prove it you're like oh look at the weights the weights are wrong and uh it becomes really unfair to the person to argue against a blackbox algorithm which kind of brings us back to an idea we saw earlier in class neural networks are great but people can't really argue against them black boxes might be very accurate making predictions but you can't understand or figure out how it's making its decisions you know there's some things we can do some nice little tricks we can do to try to understand what the neural network is doing but the long story short is understanding a complicated Black Box can be as hard as understanding how a human makes a decision that's less true for beian networks you know basian networks this thing we learned about earlier where you have variables and you have conditional probabilities between the variables if somebody had an algorithm that was making prediction using a evasion Network it's a lot easier for somebody to say hey this is why your algorithm is wrong in my case because even though I have this variable this conditional probability is wrong for me um okay so that's a nice little aside so one of the things you could do is maybe you could try and train out bias but you should always be careful about what task you're solving actually maybe I should have put that in here how come I didn't put that in there it's like one of the most important things like choose the task you're working on think about what the world really needs and can you make something that will make the world a better place like for example like you think about the person who made the gar detector based on people's photos is that actually what the world needs you imagine like what if instead of doing all that intellectual work we put that intellectual work into making better smart grid so that we can better balance Powers maybe that's the most important thing uh but anyways uh here's a fun little story though um one of the interesting things is you will go on from here and some of you guys will go work in tech companies and it's interesting to think about what responsibilities we can have and even if you're an Academia OR tech companies what uh responsibility can we have if we're uh in those roles here's a nice little case study that's quite interesting this is from a while ago it's in polaroid so back in the day do you guys know Polaroid cameras like from the shake it shake it shake no anyways okay you take a photo it would print it you shake the photo you actually doesn't matter if you shake it over time it develops and you get to see it in like 10 seconds later anyways Polaroid was a popular American company um originally they only had one flash uh and the idea was that it didn't really work with the single flash that they had on people who are not white so it was really built to work for people who are white but then Polaroid turns out uh was also selling it to the South African government and they made a feature for the South African government to make the flash work um so that it could capture uh better uh black skin tones better um and it was used by the aparti government to build these pass books that were like a central part of discrimination uh in aparti South Africa uh and the interesting story about this is this would have kept on going except for an engineer who worked at Polaroid who brought this delight and said hey this is wrong we should talk about this and that's how change happened it came from inside the company um okay we're going to talk about a little bit more but I thought this would be a great time for a good pedagogical pause so we've talked about fairness we've talked about harms we talked about what you can do about it uh and then I'll I'll just tell you a little about my thoughts on uh ethics on my own okay so take two minutes and I'll come back and we'll continue our conversation thank you guys so much uh for following along so far e e e okay let's bring it on back for the home stretch of this lecture okay post pedagogical pause let's go back to learning goals so learning goals you know the first thing we talked about in terms of formal definitions of fa fairness was fairness through unawareness and I hope you guys understand why fairness through unawareness is a little more more complicated that Facebook example really drove that home because there could be lots of proxies if you wanted to make a decision just not including demographics is often not enough the second learning goal know the two two ways that we talk about in CS 109 of measuring fairness so you know hopefully this is making a little bit more sense um and you know even with this uh as stct version of the relax calibration uh and then we just recently talked about some techniques that we have for mitigating fairness issues now when I was thinking about this lecture I came to this decision that I don't think a lot of computer scientists want to be evil I just I just don't I've seen a lot of people I feel like a lot of people have maybe it's this menes in me I believe there's the Sprout of goodness in everybody uh though I do think about the blind spot more and more and let me tell you a story that you might have heard of and you can let me know uh if you haven't heard of it if you haven't then certainly I think a story we should all know my idea is that even though we've got this Sprout of goodness in us well intention people can still break things at scale especially while moving fast and the story that really struck me and actually when this developed I think it was kind of a phase change for me as a computer scientist I was like oh we must be so much more thoughtful like I don't think anyone meant to do harm here but real harm was done let's take a pause and think about what we're doing and there's a story of free basic just quickly who here knows the story of the harm from free basic a couple okay this is going to be a good story to hear so the story starts with this product that Facebook wanted to give it was called free Basics and the idea was we're going to give free internet to people around the world and that's a good start be like yeah getting people access to Internet feels like a good thing now maybe because Facebook wanted to get something out of it or maybe for other reasons they made fa the intermit be delivered through Facebook like we'll give you free internet but it comes through Facebook so like Facebook is really the internet that you get this was deployed in a few countries uh deployed in Sri Lanka it was deployed um I think in Kenya no actually not in Kenya it but it was deployed in mmar and when it's deployed in Myanmar you had a country where a lot of people didn't have access to Internet and all a sudden almost everyone hacks us to internet but it was internet mediate through Facebook so like everybody had a smartphone now that could open up Facebook and you can get access to it Myanmar had a military hun and the military hun started a misinformation campaign against a particular Muslim minority in the country called aringa a lot of misinformation now we know a lot about misinformation at the time it was a little bit nor novel but you can imagine all these people who didn't have much access to the internet before now have it and now are faced with a very sophisticated misinformation campaign but to add fuel to the fire Facebook only had two moderators for the entire country of Myanmar who could speak Burmese so they had just Unleashed free Basics lots of people were using it a huge misinformation campaign starts and there's just not enough people paying attention to realize it it led to a genocide you know we'll talk about causality in a second but the genocide that ensued afterwards was really awful millions of people uh displaced uh and the UN whenever there's a genocide they have a mission to try and figure out what happened and the UN concludes that this Facebook free Basics was a critical component in leading up to this genocide um and so I think they say the role of soci social media is significant Facebook has been a useful instrument for those seeking to spread hate in a context where for most users Facebook is the internet although improved in recent months the response of Facebook has been slow and ineffective and you know I think this is a when this happened it was such a drastic harm like we'd thought about I I'd seen examples of small harms medium harms and then all of a sudden this was like that's a big harm and I don't think anyone on Facebook was like yes genocide we're going to do it right that's not what people were thinking I think it was just a blind spot right they're like we're going to give Internet it's going to be great it's going to go through Facebook that'll be fantastic um you know why moderate that much and they just didn't see this big misinformation problem and what it could possibly lead to and so you know you can imagine imagine this move fast and break things mentality though seemed really exciting at the beginning really came into conflict with reality when they started to see these big blind spots and I think that's also especially true at scale you know Facebook was still just used in universities probably wouldn't be facing consequences like genocide and suddenly at scale when you make have blind spots it can certainly be worse Facebook has been asked what's the answer to this by lots of people you know we've seen misinformation not just in Myanmar uh and there answer interestingly is that the only way we can prevent hate is better machine learning and I'm not you know we can all have opinions on that but it's quite interesting to think that um you know scaled computers can be the problem and then the claim could be that scales computers could be the solution though I think some people some intelligent people could argue that maybe this is just a way to uh get around not actually addressing the issue but the point of this wasn't really just to dump on Facebook the yeah I do think though if I could drive home a point it's that I don't think anyone that I've met in the field of computer science really wants to do harm but we should be thoughtful about our blind spots you know maybe it's um whether or not we're predicting somebody gets admitted to a school or whether or not we're trying to give input internet to other people uh we should be humble in that if we're building something that will affect a lot of people's lives we should think about what are all the things the unknown unknowns that we might be facing uh and then I do want this is like a little bit of a personal thing but I would like to highlight one blind spot that I'm trying to contend with like I think many of us are trying to contend with this is a computer science blind spot that I I worth bringing up you guys know this story hopefully you know the story because it's becoming a little bit more popular bitcoin's using a lot of energy um and you know if you think about it in terms of hashes per second it's a lot if you think in terms of carbon dioxide in the environment it's a lot too and despite the fact that we're seeing more evidence that this is a problem it's really hard for us to look at this blind spot straight on and you one piece of evidence I'd Point towards that is that we teach a lot of blockchain but you know I don't think Bitcoin and the effect on the environment is something that even shows up in an ethics class on Stanford and I will update the slide once somebody tells me that has changed uh and it's not too hard to see why increased CO2 could be a problem this is like CO2 in the atmosphere and you can see it's become quite large since humans have started contributing it's not too hard to know the physics of why CO2 will make our world warmer you know we know the fact that it can have a cool video if you're curious um it's not too hard to know that the impacts would be really harsh if we have a large change in temperature you know we've seen the world or we have ways of understanding what the world was like uh when it was quite a lot colder and there was a lot less or the the change in temperature was a lot smaller and you can imagine when we get to the point where the change in temperature is quite deviant from what it was in the historical average that it could be something really scary and one of the reasons it's not too hard to imagine is that we're already starting to see impacts like in California fires and droughts but other places you know like hurricane EDI impacted over 3 million people they weren't very used to getting huge hurricanes in mosambique one of the reasons I think that this is such a blind spot is I think a lot of people in the world are not versed in the language that you're now versed in I think the challenge of climate change is filled with uncertainties you know you have to think about future amounts of uh CO2 you have to think about what this will do to the environment the climate sensitivity and then on top of that you have to think about how that will impact people and these could be future people in places far away and even though there's a lot of uncertainties one of the take away from CS 109 is that's not the end of the story you can reason under uncertainty you can make good decisions based on uncertain probis distributions that you might have access to but I am also going to acknowledge that maybe this is less of a blind spot um and do you guys ever feel like it's just hard to do something about it like I can tell you climate change is a problem but then like what are we going to do about it and you felt like that there is a thing that they're doing for Bitcoin specifically or on E they like s steak yes that's very interesting so point is that they have actually started to address this like for ethereum they switched to proof of stake from proof of work but Bitcoin has not Bitcoin has and actually there's something really interesting to that I was thinking why can't Bitcoin switch to proof of state so there's this idea of something that's a little less Democratic but will not have this huge environmental consequence you could do this for any blockchainbased coin and Bitcoin doesn't change why doesn't Bitcoin change hard to change it's completely decentralized it's like you know there's no one in control of Bitcoin there's no one I feel like if any one person was in control of Bitcoin you'd be like yo look at all the CO2 and they're like yeah that sucks let's change that immediately but the great idea of decentralization seems to have hit this problem of like there's an obvious thing that we should fix but decentralized we can't make that change um anyways it's something that I reflect on a lot as I get deeper into thinking about decentralized education but that's a different story um you know there is this philosophy that I sometimes find myself in which is like with regards to climate change like can't we just wait and see what happens uh and man I I this is I'm in such an uncomfortable place I've convinced myself that I can't say that I've benefited too much I produced more CO2 than the average person in the world um it's really unfair for me to say like oh I benefit from all this and then I'm just going to wait and see what happens especially because with I I'm not a wealthy person am a professor um but with the meager amounts of wealth I've accumulated which is more than most people in the world I probably won't be affected by the results as much so I've benefited from burning CO2 and I probably won't be affected by the worst changes in climate change because I could move if I needed to um you know California might not be hit as hard as places that maybe didn't burn as much CO2 so isn't it totally unfair of Chris to say I'm just going to wait and see what happens and and there's ways of telling the story you know like you can imagine this is a very extreme example um but you know there's a lot of people after World War II who ask the question you know what was the problem with bureaucrats in Hitler's Empire and there's a lot of people who are like you know what I'm just going to wait and see what happens that was a common idea of like let's just see how this thing develops so instead of ending on that note I'm trying to think for myself what can I do and maybe this is interesting for you guys thinking about what can we do I've decided on an individual level it's really unfair because I could cut all my CO2 emissions and that just won't change things there's big companies there's big nation states that are producing so much CO2 that my change on individual level is hard and then boy do I wish I could affect the nation state scale actually I don't really want to but it would be cool if I could be like hey just this one thing why don't we just like figure out this whole CO2 let's make a lot of clean energy it'll be fantastic if I could I would but um you know I don't live in that world but I do think for a lot of us there is a sweet spot in the middle of community and so that brings me back to the Bitcoin thingy like maybe this as computer scientists who are at Stanford therefore have a lot of clout you know maybe we can start thinking about ways to talk about uh this blind spot that might exist around uh the proof of work that is behind the blockchain in Bitcoin uh and then this is a very small thing as computer scientists if we had a perfectly clean grid every time you run an algorithm you don't have to worry about it you can go to sleep happy and be like it's going to be doing a bunch of hashes but I know that hash is powered by the wind uh and so I think all of us could sleep a lot better if we could get uh a clean Grid in California and in our other locals and then certainly I don't like to think about AI as a pan I think we should dress our problems that are rooted uh in in things that are outside of AI sometimes with human institutions and yet there's probably really cool problems to work on in the world of AI like I've seen people who are going to predict you know the electrical spikes that happen when people plug in electrical vehicles and if you figure this problem out then you could really reduce the impact you could predict buildings that are using a lot more CO2 and they probably not heated well so you could retrofit them so there's a lot of cool things that we could do and if any of you guys do that you rock and you should tell me about it okay now to end this lecture can I just give you guys a small thing to do because ethics is a complicated thing I can come and give you formal definitions I can give you some case studies but I really think that the work belongs in our hearts um so I'm not going to give you any more homework than I already have but if I could give you something to do after this lecture it'd be to give yourself some space maybe I should just put a period here man you guys work hard I really respect how hard you guys work at Stanford you know like you're trying to do all your classes you're trying to maintain all your friendships and be there for each other and actually just giving oneself space in general is great like put away our phones and like connect with yourself is always a good thing uh and maybe give yourself some space and and then reflect on what's your own sense of what is right cuz one of the things I've learned by working with wonderful students like yourselves is you know you guys have in you and what I really believe in your sense of we right and if you're thoughtful about it not only will you guys do wonderful things uh I think we you'll do wonderful and fair things uh and you know both what is right and also how can you craft a life well-lived with respect to what you think is right uh okay I appreciate you guys so much thank you so much come back next week we have just two more lectures and then we're done oh fin cheers CS