good afternoon CS one how are you guys doing today okay fantastic I'm glad to hear it uh uh we have ourselves an exciting class we're going to be talking about deep learning there is no word of the day um and that kind of reflects the point that we're at a very interesting part of the class where we have covered a lot of the core material we're going to be talking about uh fairness and ethics on Friday uh and that is something that you'll need for your problem sets uh but the positioning that I encourage for you for today is a little bit more of a positioning of Wonder than like a positioning of I must memorize every single detail here because this is a story that's unfolding in real time uh and it's also one that I think taking a step back and getting the big picture is what's really critical here now having said that we have an exciting class where we are going to go into the details of what is the mystery behind deep learning we've heard so much about it and today in class uh you get to learn how that works now deep learning is a tool but the effects of the tool you've probably heard about and you've started to see uh in your life and in uh news around you so a bunch of results recently have been pretty impressive uh as you might have heard computers were getting better and better at board games but there's this one board game that computers can never play because it was thought to be so complicated you needed human creativity and in order to play it and it's this game of go and go is complicated because you know it has a board that's 17x 17 every every Square can have a white dot a black dot or no dots and this just becomes so exponentially large that traditional compute power isn't so helpful for being able to solve it uh but then a team called um or a team in the United Kingdom came up with an algorithm that was able to beat the world champion at this game and this was perhaps the last standing board game uh computers had got really good at making art it started out a little bit uh modestly computers doing these things called Deep dreaming uh it then transformed into computers could recreate the styles of specific artw workk like this picture might look like it's by rembrand but it's not actually it's been created by a computer uh and if you guys have been paying attention recently realize that you can now open a thing called stable diffusion and you can say write a text and the computer will generate artwork based off the text that you've used uh very controversial uh but certainly also very powerful uh and there are other narratives that are perhaps more inspiring like for example algorithms that can look at this photo of someone's skin and at the end decide that this is in fact something that looks cancerous so if you're far from a hospital if you took a photo of this skin you put it through the algorithm it says this is cancerous maybe you know I need to go to hospital right now uh and that seems like quite an inspiring ing use and the crazy thing about all of these examples is that they're using the exact same technology which is what we're going to be learning today uh and just as a little bit of a warm-up I wanted to start today by training a little critter everybody this is going to be our little critter for class uh the Little Critter is trying to learn how to eat red dots and not to eat uh green dots and this L Critter is going to have a small little deep learning algorithm at first Our Little Critter is going to act pretty randomly but I'm just going to let this guy train throughout class and by the end of class we'll be able to talk a little bit more about how this little critter is working and gaining intelligence so this is going to be exciting certainly this is a journey we should go on and in order to go on this journey let's make sure we bring everyone along which means starting with a little bit of review in CS 109 we've been learning about machine learning particularly we've been learning about classification tasks classification tasks are ones where you're given a training data set and based on that training data set you're going to build a machine that can take inputs and learn to predict outputs and the outputs are always labels uh in our problem sets that are either zero or one and the fact that the outputs are discret is what makes something a classification task the idea is you're going to use that training data set and you're going to build a little black box that little black box is going to be governed by these special numbers that we call parameters and the metaphor we've been using in cs19 for parameters are sliders so once you build this little black box you should be able to put in a new set of inputs so maybe different regions of someone's Hearts you know like you'll give me all of these inputs and the Machinery will then be able to predict an output that output we call uh Y and sometimes we think of the probability of it as y hat an important thing is if you ever want to deploy one of these models you want to be able to make sure that it works beforehand so one thing that's worth noting and is relevant to your homework is instead of just thinking about all your data set as one piece we often take your data set and split it and we'll take some set of our data set and call it training that's where you'll learn your parameters and then once you've built your blackbox based off the parameters learned from this part of the data set we reserve some of your data set for making sure that the algorithm's working so we'll have a little test data set we'll evaluate how well the algorithm works and if it seems to be doing well enough and it seems to be acting fairly then we might consider deploying it okay questions feel free to just jump in I have enough mandarins for uh a whole bunch of questions today now there's a whole bunch of ways you could approach this problem we learned about naive Bays first but then on Monday's class we learned about logistic regression that's really where we're going to be picking off today logistic regression is one version of that blackbox algorithm and if you were to look behind the scenes for the blastbox algorithm it would look something like this you take your inputs every input gets weighted we sum up those weighted inputs we then have to squash the sum we interpret the squash as a probability that the output takes on the value one and then based on that probability we make our classification prediction this is really what it looks this is the mental model you should have but mathematically it's doing this you know this is the weighted sum this is the squashing function the sigmoid function not to be confused with the sigmoid from aian uh and then at the end of the day gives you the probability that y equals 1 it's not a true model of the world like nothing about the world works like this it is a model that is useful because it ends up making good predictions in general but this whole Machinery is a little bit imagined uh and so even though it's wrong and it's not capturing the way the world really works it ends up with a very useful machine okay and that is our review now it's time to take a journey from the mathematics that we have for logistic regression and get all the way to how could we use this for skin cancer detection I really like this idea by the way of thinking about logistic regression as being a Harry Potter Sorting Hat you know you're going to take some input uh and then you're going to make your prediction it's like the Harry Potter Sorting Hat looks at the input and says Ah this is a one or ah this is a zero and the beautiful thing about CS 109 is uh while you might have read the Harry Potter books and figured that the Harry Potter starting hat was interesting CS 109 you can learn how it actually could work uh so how Harry Potter's Sorting Hat could work is just logistic regression perhaps but you imagine that might not be powerful enough um okay just to be clear inputs in the case of your problem set sometimes it'll be things like movies that a person liked uh and so the inputs you know each input index would correspond to a particular movie when you have a particular user they'll either like or dislike those movies uh then you wait them sum them and it will end up producing your prediction the intelligence comes from the weights good weights and you make good predictions uh random weights and you don't make good predictions so that was a theory and that theory though led us to this wonderful set of mathematics and this wonderful set of mathematics says okay where are we going to get smart weights from we need smart weights where do they come from well first let's write down the logistic regression model based off that logistic regression model we can say for any one data point based on the current values of thetas How likely is that data point and this looked really complicated but recall this was just the log likelihood of a bruli and because basically we interpreted the output this thing as the P parameter of a bruli this is just a bruli where instead of the P parameter you put in the output of your logistic regression this log likelihood it's a scoring function it says hey how good are your current parameters great parameters will get a really high value on this score awful parameters get a very low value on the score a scoring function is helpful but it doesn't tell you how to get smart parameters in order to do that you need a derivative so derive your scoring function with respect to every single parameter and once you do that you can do hill climbing now we have a particular equation or um algorithm for hill climbing that we use uh in C s19 which is gradient Ascent uh and in gradient Ascent you're going to have a system for learning all your parameters we talked about this on Monday but it you start with random settings to parameters and then over time you improve each parameter based off of calculating its gradient and then taking a very small step in this direction of the gradient for each of your J parameters okay and the idea of this gradient Ascent is if you know the derivative of your scoring function with respect to each parameter and if you keep going uphill as you keep training you'll get higher and higher scores and higher and higher scores we think lead to smarter and smarter models okay before I move on to artificial neurons let me just stop here and give you guys a second to talk about this with the person next to you summarize what you've learned and see if you can come up with a good question uh take a minute and then we're going to jump into deep learning because this is our launching spot okay go for it have a good little conversation with the person next to you e good so as I said you know high level perspective but this would be a great time for for clarification questions if you found something confusing certainly other people in the class found it confusing too so what's confusing about this what's interesting what's Curious yes can you just elaborate on why like different why such a good question okay so at this point class we have two totally different algorithms for making predictions for classification tasks we have logistic regression and naive Bay uh and let me tell you something very interesting they're quite similar you know they have a lot in common uh they have the same level complexity have similar number of parameters so for similar problems they will both perform pretty similarly the question was when would you choose one or the other I'd say if for example the ones you have in your your problem sets it would be acceptable to go with either so that's my first answer is like for lots of similar problems either one will be just fine they're different but they're similar in their effectiveness that's my first answer it's kind of lame the second answer is you could try both if you had a problem you could run logistic aggression you could run naive Bay and see which one is doing better on the testing data set and you say like that's the one I want to go with uh and then there is another example which or answer which is all of our examples in CS 109 have binary inputs and binary outputs and if you were to change that you say like your inputs would become real valued or your outputs become something like multiclass uh it turns out inputs becoming real valued logistic regression handles that much more naturally and outputs being multiclass it turns out naive Bas handles that much more naturally so three separate answers but maybe the most satisfying is just try both of them whichever one works best that's your model I to clarify that I'm not sure if this this but I think I understand how there's different uses of it I'm wondering like how the actual process I know that for example the B point of uh Na B was that we're going to like see which probability is just bigger we're going to choose that one yes regression what exactly is different about like the sigo yeah it's like the the sigmoid doesn't even exist in naive Bas right like KN Bas sees no sigmoid because it it's not trying to directly model the probability of Y given X naive B is trying to you know use beian calulations to get that now of course that's almost impossible so we have to make the naive assumption so there's like it's like I'm going to approach this just using pure basian thinking I get to some point where it's impossible I make a big assumption uh and that big assumption leads to a classification algorithm logistic regression has a different path it says I'm not even going to try and use Bay I'm not going to try and use core ideas of probability instead I'm going to build a probability machine and this probability machine has nothing to do with Bay nothing to do with how probabilities actually work it's just going to be like a little machine that you stuff in x's and you get probabilities out and it's going to use this sigmoid mechanism so they're pretty different paths uh but they lead to they both lead to the same functionality of I will make a prediction yes I don't fly understand is why we take log with respect to theta's plural and not just Theta because when we did the algorithm and the problems that last week where we tried to find the optimization we just took one yeah good question so you didn't mean logs you meant derivatives I believe yes okay so the great question was why do you have to derivative with respect to every single Theta that's actually if you want to do optimization over parameters you need every partial derivative of all the movable pieces that's Hill criming needs all of those partial derivatives it's not enough to just say if I were to change Theta 0 how would my score change I have to think about every sing single Theta and if I were to change them how would my score change uh and that's the necessary component for getting to hill climbing AKA gradient Ascent so gradient Ascent wants to know for every single parameter if I were to change it how would the score change so we need lots more partial derivatives earlier in class we'd only have single parameters we did some assignments in when we looked at uh maximum likelihood estimation where you'd have a model with only one Theta or only one parameter in that case you only need one partial J such a good question yes um if the data is convex is there a functional difference between gradient desent and like setting all the partial derivatives equal to zero uh okay good question if it's convex would you get different answers if you did gradient sent versus if you uh set the derivative equal Zer first of all I'd encourage you to try setting this equal to zero and solving for it it turns out to be very very very difficult because of this sigmoid uh having said that if you were able to come up with an answer they would be the same now there's a good reason we do gradient Ascent cuz when we get to deep learning setting taking a derivative setting it to zero as a method of optimization will not work we are going to need this powerful tool of gradient Ascent okay oh such good questions okay one more um so for logistic regression like we mentioned last class um it gets its intelligence from Maximum like estimation which is basically just optimizing parameters that we use yes or that um we need to at some point or another pick a step size uh to perform creting aent just wondering is there some way of methodically picking a good step size is there some sort of convention for us to use the sort of like strikes a good balance between running time and atmosphere or is it just purely an arbitrary Choice oh my God I love I love it so there's this big mystery over the step size and the step size we use this little symbol for step size and here we can have like super small and really large if you have really large step sizes imagine you're trying to get to the top of this hill and you have huge step sizes you know there is this problem that like if you're here and your step size is huge and you go there and if your next step size is huge you could like end up bouncing back and forth over the top of the mountain you can never reach the top because your step sizes are to Big imagine somebody climbing Mountain instead of taking small steps they're just leaping and they just keep leaping over the top so if you have really large uh step sizes you kind of hit this problem that you won't actually converge just like this marker is not converging so over here you have like doesn't converge so when you choose a step size you might start with a large number and then you might run it and it might just never end up getting to a point where gradient starts zero um if you have a super tiny step size you're going to avoid that problem entirely like if you're trying to get to the top of the mountain take a really small step size you'll get there but as you said yeah if we had like run time like this is going to be really long and like as you get to smaller step sizes you'll be able to converge quicker we used to talk a lot about like the the craft of deep learning and a lot of The Craft of deep Lear where things like how do you choose good step sizes you know you could do something like start with a pretty mediumsized one uh see if you're in this uh territory um and if you are then just make it a little bit smaller now people have gotten more intelligent there are things like adaptive gradients where the computer is control of the step size uh and it's choosing at each step at each point what step size to do that you'll learn about in further courses so there's more sophisticated techniques and and there's the lay of the land good question and there's another one that's what I was kind of going to ask about like that there seems to be parameters that are't just like in within Theta like step size or like the bias like like Theta zero like like are those things all also like can you do like a likelihood function on those okay so the you bring up a good point so there seems to be other parameters in particularly the step sizes a good example that theta0 actually is a traditional parameter we think of it as a parameter and we're going to have a der and we're going to be doing our likelihood on um you know it goes into the likelihood function whereas step size doesn't show up in the likelihood function and so this sort of parameter gets a special name it's not just a parameter it's a hyper parameter like it's super excitable um and that's just to be like there's some parameters that we're not going to try and learn instead we're going to set them ourselves uh step size being one of them and as I said in cs19 we just give you some step sizes they work pretty well you can play around with them uh and in further classes you can learn really cool ways of making those intelligent or making decision Intelligent Decisions about those okay yes actually if um your view C is zero does that mean we're like disregarding that point yes it's not important yeah exactly and your thetas can then become positive or they can become negative and if it's positive saying like you know this one is going to be shifting my probability positively or you know to be more likely and if it's negative saying this data Point's making me think the probability is smaller so there you go you can have zero you can have positive you can negative for thetas okay and then you can put it through hill climbing and you get to a point fantastic there's such a simple idea behind deep learning it's amazing how close you are the simple idea about deep learning is hey you know that logistic regression that we just spent a lot of time talking about that logistic regression is like a cartoon model of how a human neuron works now if you haven't studied Neuroscience that's totally fine I'm not going to give you a really detailed idea of how a neuron works but I'm going to give you a cartoon model so in a cartoon model your brain is filled with these things called neurons did you know that and neurons have inputs often from other neurons but they could be from things like uh the retina which is getting light input uh so it has all these inputs that come into any neuron cell and for those inputs some of those inputs will be super important like if this neuron gets an input from this particular input it's definitely going to fire so the neuron gets all these inputs if the charge builds up high enough the neuron then fires and then any other neuron it's connected to will then get its input so one of the ideas somebody had is hey this logistic regression looks a lot like a cartoon model of what's going on in our brains but imagine you tried to learn to do a task and you only were given one human neuron it seems unlikely that one human neuron could become very smart uh instead if you want to do a task you might want to think about having a whole brain and a brain you can think of it as being a network of neurons where the this neuron would then connect to other neurons and this whole network of neurons would then be able to Output predictions so the simple idea is what if we just took a bunch of these logistic regressions think about every single logistic regression as like one tiny little Lego piece and we can start stacking these Lego pieces on top of each other and that's a simple idea so what you learned on Monday was that core unit and once you start putting those units together you get what we call a neural network a AKA deep learning the term deep comes from that you have depth in your layers of logistic regressions and that's the core idea behind the revolution AI we could just walk out and be like now you know deep learning but of course that's unsatisfying can I give you guys a little bit more details um okay anyways this is the idea that leads to alphao self-driving Cars computer making art and at its core deep learning is just many logistic regressions pieced on top of each other can I give you guys an example to show you this a little bit in more detail I'm going to use this running example of computer vision computer vision is the hard task of looking at an image and deciding what's in it so you guys are very smart humans uh what's in this image yeah it's a handdrawn zero but if you had to predict a zero or one you would say zero and how about this image ah such smart humans we are we have huge neural networks that are helping us with this prediction actually it is very misleading because you have billions of neurons helping you make that decision so your brain actually starts with a very complicated representation when you see this it's hitting the back of your eyeball a thing called the retina and the retina is you know seeing amounts of light that hit different parts another way of thinking about this is a very similar analogy is what the computer sees the computer doesn't see a picture of a zero instead it sees a whole bunch of pixels which either could be black or white and that whole bunch of pixels to it just looks like a list or a grid of zeros and ones same thing with your brain when you first see this image it's a whole bunch of light respon is on the back of an eye it doesn't have meaning computer vision is the particular type of neural network that could take an image and then make a prediction and you know just to be clear the reason that you find this easy is you have hundreds of millions of neurons in fact visual neurons make up about 30% of your brain so when we look at this problem you're going to think of all these tasks is quite easy but that's because you have such an impressive neural network and actually a fun thing is the first layer of your neural network actually happens in the back of your head which is not that important right now imagine you had to do this task and I just gave you a single logistic regression unit it would be really difficult the X's the features in this case will correspond to each different pixel in the image if you took a particular image that would turn turn on all the X vectors and then you could have a logistic regression that could wait each of those Summit squash it and then make a prediction but this would be a little bit similar to asking a single neuron to learn how to see you have 30% of your brain to see it seems unreasonable that a single logistic regression could possibly find good thetas you're like can't we just use gradient ascent and it'll just get better and better and better and the answer is no this model of logistic regression is just too far it's too puny to tackle such a large task there's not enough parameters so even though you can have a great optimization algorithm we can write a likelihood function we can try and optim it there's no set of parameters that will start to do a good task or a good job at this task anything confusing about that interesting curious yes so can you can you kind of go over again like what differentiates this from like thing like being able to tell you're like a movie or not like why is this not doable you know one way so the question is why is this not doable but the movie thing was doable it's a good question um part of it is that the task is hard part of it is think about the number of inputs like how many pixels do you have here and the number of inputs is really really really really really large uh and the combination of those inputs that leads to your prediction is very complicated so you have a huge dimension for x and what makes for a y the true methodology for deciding if this is a z one is very complicated you know it's not just is there a white here because you know the white could be over here when you draw your zero whereas for if this was a particular movie it's much easier because it doesn't change so much like if somebody likes this movie then it has a very consistent impact on whether or not they like your target movie so two answers one is much higher Dimension the second is much more complicated interaction between the inputs to making the output Compu right yes it could be fixed with more Compu power but the other part can't be yes fantastic question yes do we understand exactly why it becomes really powerful to stack these up or did we just kind of do it and realize it yeah there's some people who would say they have some guesses but you're not completely wrong in that we did it it seemed kind of like a cartoon model brain and then it worked and then we started analyzing it uh but now that we've analyzed it there are some cool theories that say like oh you could learn nonlinear functions uh there's actually no limit to the functions you could learn uh and people are still trying to tell more and more Nuance stories for why this works but your Insight was correct people basically just tried this wait can I tell you a hilarious thing somebody proposed this and they tried it and it didn't work because their computers were too puny and then no one researched it for a really long time and then a couple people were like hey computers are really good should we try that old idea and then it worked and they got Turing Awards really uh all of them uh but anyway so yeah it turns out the functionality was really what drove the research I'm going to start drawing a logistic regression like this if you guys don't mind like this is a logistic regression but now I'm going to kind of ignore all the middle parts the squashing or the summing and the squashing I'm just going to have an arrow that says all of these inputs get summed and squashed and they lead to this output does that sound reasonable okay now you know logistic regression maybe it could do a good job sometimes but it's not always going to do a good good job and so the very simple humble idea that somebody proposed and it didn't work cuz their computers were too puny and then it was a huge deal later is what if we had stacks of neural networks and to be clear here I'm going to say you have your inputs and you have your outputs but I'm going to introduce a new thing called a hidden layer and every single thing in this hidden layer is going to be a logistic regression connected to the inputs and then once I have this hidden layer I'll build a logistic regression that takes the hidden layer as its input and then predicts the output I am going to just to give you an idea of this as I said every single Circle in this hidden layer is a logistic regression it's going to take every input weight it in its own way with its own parameters sum it squash it and then it will turn on or off so this circle represents one logistic regression but so does that Circle this circle is also logistic regression it has the same inputs but it's got its own weights so it may turn on or off in a different way every single one of these circles is now a logistic regression so you have a huge amount more parameters and then finally if we did this we'd have a whole bunch of zeros and ones and that's not a prediction so we're going to take all of the outputs of these logistic regressions and call them inputs to a final logistic regression which is going to predict uh r0 or one cool or what what a simple idea if we can understand this that is you know this now counts as deep learning because we have depth of our neurons you can imagine the depth is that at some point you have a logistic regressions whose inputs were the outputs of other logistic regressions okay there are so many parameters here you know if you imagine this final logistic regression we've got a waiting for every single output but how about over here there's a whole bunch of parameters in fact I'd encourage you to think about how many parameters there could be but for now I'm just going to say we have way more parameters than we used to have okay uh and just to be clear you know what we just described to you is a neural network many ltic uh regressions stack to each other uh and deep learning is the process of maximizing likelihood for a neural network uh and you know at this point you can imagine as it's just like logistic regression but now your input to Output matching is just going to be lots of logistic regressions LOL okay it's a whole slide just for that one awful joke can I show you a demonstration I feel like it's cool to talk about it and show it in slide but like wouldn't be great to see a real live neural network let me show you one that does this task of predicting zeros and ones oh hey our Critter is still learning this whole time that we've been talking it's been practicing here is a logistic regression or sorry a deep Learning Network because it's not just one logistic regression at the bottom here you're going to have inputs and at the top you're going to have predictions I'm going to handdraw a zero uh here we go and I'm going to be not great about it and our Network's job is to take all those pixel values and predict if this was a zero or one and if you look at it at the very bottom of this neural network we have a bunch of cells that represent the initial pixel values of what I drew does that make sense then every single Cube here is going to be its own logistic regression we'll look into those in a little bit but if you go to the very top at the very top we have logistic regressions that represent that have numbers above them this is a logistic regression that's associated with a zero 1 2 3 4 5 6 7 8 9 and do you notice that when I drew that zero the logistic regression associated with a zero is the one that turned on so overall it's taking pixel values and recognizing it's a zero if we drew something different like a seven you know it'll take pixel values down here just what I drew and then turn on this one cell that represents a seven uh or if I drew let's say a one uh it could take all these pixel values and then turn on that one cell that represents a one that's the input and that's the output but clearly it doesn't just have one logistic regression anymore we have a whole neural network so if you look at this final cell up here it is a logistic regression and its inputs are not coming from the pixels its inputs are coming from all of these other logistic regressions which are turned on in different ways uh and if you looked at any one of them you can see that any one of these is a logistic regression whose inputs come from this previous layer we call these things layers uh of logistic regressions and if you looked at a different one like this one turned on but it had the same inputs as the other one we looked at it's just probably weighted those inputs differently so every single Cube here is a logistic regression uh and as you get lower and lower um you know there logistic regressions pulling from different parts of the image okay that's it that is deep learning and at this point because you understand logistic regression you could just start stacking these things on top of each other and you would have your own neural network now of course the difficulty will always be where did you get those thetas from you imagine if somebody made this neural network and every parameter was random it wouldn't be able to take pixels and predict what's in the picture all of its intelligence will still be coming from the parameters so the mystery still will be I can make a neural network but how could I learn those parameters so is there any to where you start your parameters or you truly just pick random you truly pick Rand there's one thing that's very different from what you do in your problem sets or logistic regression in logistic regression we start all our parameters at zero but for a neural network it turns out to be important that you start all of your parameters randomly and that allows the different neurons to learn different things um but no really we start random there's no like smart starting point it's just start it at random and then Hill Climb of course we'll get into the details yeah okay question so intuitively like what are these like cubes in each layer every single Cube is its own logistic regression it takes inputs it weights those inputs it squashes those inputs and then it comes up with its own value which is the squashing of all those inputs weighted inputs so every single Cube it is one logistic regression it will have its own parameters and then we'll do the summing squashing uh based on those parameters is that answer yeah also like what are the like combination of those cubes then for each of these like rectangles oh like over here like these layers yeah like each layer we have like a bunch of different like rectangles right combination those I mean I call these things layers when you get deeper in here we have these kind of grid light ones that has it's a particular way of doing uh images where we kind of have a few neur logistic regressions that as you notice have smaller number of inputs like they just take four inputs that's a nice little trick for images not worth focusing on too much we'll talk about that later um but these three layers are you know they're not fully connected it's not that every single cube is taking every single input As an X as a nice little design trick yeah so I know you said that the intelligence comes from like the uh weights but then also like you have to decide beforehand like how deep are you going to go and within each layer how many um going to be so how do you decide that like is some intelligence in that too it's a lot like this I would call those hyper parameter so this is a little bit more like the the art and craft of deep learning is you construct how many layers you construct how many neurons per each layer uh maybe there's some like neat little thing you'll do at the beginning so that these neurons have fewer inputs uh all those things are artistic choices I would say of course there was one person who was like can I have the computer build a neural network that could make those choices for me and people have definitely done that and yet still I still see a lot of people just being like we'll make a bunch neural network it'll have like you know 40 layers uh and each layer will have 100 neurons so artistic choices I'm just ask a question so if it's artistic U so you just have to like you know uh practice your own way and see your accuracy go have I yeah exactly so if you you could do something like if you didn't know you could make two neural networks train both of them and then see which one starts to get smarter and then that's how you can make your artist IC Choice sounds a little more like optimization then um yeah okay so at this point I hope you're understanding this big picture that deep learning is a bunch of logistics pointed together You' visualized it hopefully you understand it but I haven't told you where we're going to set all those thetas I've now exploded our number of thetas without talking about how we're going to set them and that seems impossible which leaves us with the fin mystery for today how do we train and you guys are getting that close to understanding deep learning it is such a simple idea Logistics on top of each other but you need to be able to answer this question as well and the start of this journey is going to be maximum likelihood estimation we're going to say everything in our neural network is a parameter and we want to choose the parameters that maximize the likelihood of some training data set there we go okay first a couple learning goals uh I do want you to walk away from today's class understanding chain rule a little bit better and understand how chain rule is the heart and soul of the gaining of intelligence for neural network the Deep learning I do want to De demystify deep learning like you guys might have heard about deep learning you might have heard people talk about it like it's some sort of sentient being but now I want you to understand no it's it's something you could uh tangibly understand uh and then finally you know one of my goals is you walk out today so ready to get the big picture of the logistic aggression that you have to do for your problem set okay and now it is time for some math worth learning we are going to go through the math that is how deep learning gets us intelligence and this might become something you could expect should become common knowledge like as deep learning becomes more and more pervasive it impacts Our Lives more and more whether or not you build it or you just want to understand this tool that exists in society we should know this this math okay A little bit of new notation I am going to write it on the board over here so that we can keep track of it over time I got X that was our inputs of our little black box I've still got y hat that's our output of our black box but now I've got a hidden layer H uh and I am going to talk about each of these things as XIs that's nothing too new I'm going to ref refer to each of these neurons in the hidden layer as H I think I call it J right yeah so we're going to call each of these ones HJ and HJ is going to have a whole bunch of parameters associated with it and the parameters associated with HJ I'm going to be calling the thetas and in superscript I'm going to say it's for HJ and that's going to tell me that I'm going to have a bunch more thetas oh sorry this is thetas for the hidden layer and particularly for this logistic regression it's going to have a Theta for every single one of the inputs so this HJ will have a Theta for x0 X1 X2 X3 does that make sense so HJ is going to have a Theta for Theta 0 it'll have a Theta for Theta 1 you'll have a Theta for Theta 2 a Theta for Theta you know for X3 and so because of that we're going to think of oh it looks like they're hanging out together um we're going to imagine there's going to be a whole set of parameters for this hidden layer and for this hidden layer it will have two different subscripts it'll say I have a parameter for in this hidden layer every combination of some input and some hidden neuron wow that's a whole bunch of param every single one of these hidden neurons is a logistic regression so HJ is going to be a logistic regression you guys ready for it that means it's going to be the squashing function of the weighted sum of two things one of those things is X and the other thing is going to be Theta uh is going to be the Theta H's for J but you know this sum this or this transpose is the same as saying okay we're going to sum over all the inputs in x i we're going to take every XI and we're going to weight it it's going to be a weight so it's a Theta it's coming from this hidden layer so it's going to have this superscript H and particularly this is the one that's going to the J hidden neuron so it's going to have a j subscript there and we're going to Loop over all the inputs uh and there's going to be a differenta for every single input here so this is my mathematical way of saying every single Circle here in this hidden layer is going to be a logistic regression it's going to be a logistic regression which has its own weights and there'll be one weight for every input hidden layer combination if I gave you x's and I gave you all the thetas you could use this formula to decide if HJ was turned you know close to one or not when you actually evaluate this this will become a number okay that was complicated let's take some questions yes so how manyas would there be in I guess that example it's such a good question I'm going to ask you in just a second but I'm going to let you guys start thinking about it you know like imagine there's uh 20 or you know 40 things here and 10 things here and one thing here you could start asking yourself how many thetas are there but I will ask you in just a second so think about it for a moment I do want to introduce just a little bit more notation and the last piece of notation I'd like to introduce is that all of this just describes the logistic regressions that go from X to H there's one final logistic regression that takes all of these outputs as inputs and then predicts oi and in that final logistic regression you know y hat is the output of a logistic regression so again there's a sum this sum is going to go over and it's going to pull out each of the H's and it's going to weight them and it'll be another weight and to distinguish the weights in this part of the neural network from the weights in this part in the neural network we're going to consider these weights to be Theta with Y in the superscript and that's just so that we can tell apart these weights from these weights so these are the weights in the H layer and these are the weights in the Y layer okay and then you know here it is on the screen for people who have trouble reading on the board but I'm going to keep this on the board the whole class through yes does that every H value on every single value and Isa value that you choose what you said was 100% correct that's awesome so every single H gets the exact same set of inputs and the only thing that's different is that they will have different thetas different parameters okay rock and rolling what I don't understand is you stud that the little circle is the is this the regression thing so how is it having thetas if everything's contained within the circle this little circle it's going to be a computation you'll start with the x's and how do you compute the value of this little circle the way you compute the value of this little circle is you use this formula so you take all your x's and you're going to be waiting them by these thetas These thetas are going to live in the memory of your computer uh and these thetas are going to tell you how you should be waiting the X's for this particular H so you weit each of your X's get that number throw it through our sigmoid function and that will be HJ if you chose a different H it would use the same X's but it would have its own thetas again this would live somewhere in your python memory um it would pull out those thetas uh it would do the sum the squash and it would get its own value okay fantastic so let's do a forward path if we take a picture of a one we don't have to guess the values of X we're told what they are each value of x corresponds to a pixel value like this will be the pixel value in the bottom right corner and that will be the pixel value in the top left corner then once we have X's we could calculate every single hidden neuron we could save every single hidden neuron calculate yourself based on the x's in order to do that they're going to use this formula which is also over here and this formula is going to take those x's and weight them every single thing in the hidden layer has its own set of Weights so they will come up with different answers even though they're all taking the same X's since each one's weighting its own weights they'll get different answers and then you'll activate the hidden layer we call this the forward pass this is the prediction pass so we took an image uh we activate the hidden layer and then finally we're going to activate this layer this layer is going to say okay I'm going to take each of these as my inputs I'm going to weight each of those inputs using the weights that are in this part of my Python program and then I'm going to get that Su squash it and I'm going to call that uh my prediction okay so that's forward pass if we wanted to get to the point of learning these thetas it's now time to score our forward pass so we did our forward pass how good a job did we do and the scoring function we want should be derivable with respect to the different thetas so when we say how good a job do we do we don't want to just give us 100% if we got it right or a 0% if we got it wrong instead we'd like to have something that we could learn from so again the scoring function we'd like to use at this point is going to be maximum likelihood estimation we're going to say hey you're predicting something binary and if you're predicting something binary you could imagine that there's some beri parameter p you come up like this very final thing is going to be interpreted as a probability so we call this y hat is like actually a probability and we're interpreting this very final thing as the probability that the label should be one we say whether or not the label should be one it's a beri and what came out of your logistic regression was the parameter for that beri and we can say How likely is a true label of one if your probability was A8 so if your probability is a08 you can say the likelihood here is going to be 8 to the^ of 1 time2 to the^ of 0 so 8 if you guess if the true label is a one and your final probability is of 08 we can score that and we score in the exact same way that we score logistic regression which I appreciate is complicated right we've got this whole like brli thing we have this continuous likelihood function that over there is the log of this continuous likelihood function so this scoring function is not very good for computers we want the log version of it and we end up with this crazy equation but the highle picture is the same you know you could make a prediction end up with a probability and I can score that probability using the mle mindset questions Curiosities concerns yeah like what's actually in layer age is it is it another set of datas for each progressions or is it actually a zero both there will be thetas and those thetas will lead to actual values they won't be zero and one because they're coming out of a sigmoid they'll be something between zero and one uh but they'll be smooshed so be close to zeros or ones and so they're both true there are thetas they'll have numbers there will be activations like if you put an input you can activate the hidden layer itself so there's both a Theta you can think of those as living between the layers and then there's the layer itself which will end up getting on taking on values yeah the probability sorry what was the I couldn't hear you score the probability ah we score the probability using mle so we're going to say How likely does one data point look like based on this prediction the data point will have a true label what's the true label here so you end up with a probability out here let's say it's a08 how do you score the 08 if the true label is one so y hat is going to be your 08 uh and the true label is going to be y and so you could put that into this little function here it's 8 put that through a log times it by one because the true label was one uh this is going to be Times by zero so we're going to forget that it'll just be 1 * log of8 will be your score so you have these two things this y hat which is the probability that you just calculated and if you had a true label because this is training data then you can put those two things together into a score it looks really confusing I feel like this equation is incredibly confusing for what it really is it's just taking a probability and scoring it based on the true label uh and it's just using this derivable version of the brui probability Mass function insane but very clever good job whoever figured this out okay yeah this is a sand check so I know you just said that layer H is basically a list of like outputs of the sing W but wouldn't we need to pass on like a list of binary inputs when we do the second round of logistic regression ah what a good question do you remember earlier somebody asked me hey when would you use logistic regression when would you use naive Bays I threw out a little bit of an interesting comment I said hey if your inputs weren't binary if your inputs were something like real valued that's totally fine for logistic regression it doesn't care but totally breaks naive base it turns out logistic regression doesn't rely on the fact that X's are zeros or ones logistic regression is totally fine if those are like 7 it's totally fine if those are like you know 20 it doesn't even change the probability analysis because the mle is all just based off of that final output anyways so there's no probability analysis here it's just a black box and that black box is totally fine with real valued inputs good question yes does that impact the answer though that you would get if you're putting in nonbinary things like I'm thinking in terms of maybe our movie example instead of doing they like it and they don't like it they 70% like it you know what logistic regression would just learn different weights okay and it would probably do better if it had richer inputs such a good question I love this yes right back to the L like yeah like if your true value is a number from like 0 to 10 in this case let's say Z or one we're still into the predicting zeros or ones because if it wasn't a 01 then we couldn't use the beri probably Mass function we'd have to use a different one I'll talk to about that at the end of today's class but actually yeah we are just predicting handdrawn zeros or handdrawn ones okay fantastic this is a crazy complicated slide it gets a little bit easier from here it looks scary at some point but it's just a ruse so if you can understand this you're in a good plot place okay ah okay we put it all together and we start to write neural networks in much more complic or compact ways we've got our X we've got our H and we've got our y between the X and the H we have some thetas between the H and the Y we've got some more thetas and then I have this question that somebody asked me earlier and I was so cheeky and I said you figure it out no I'm just joking so let's say our X is size 40 our H is size 20 in this case my first question for you is how many parameters are there in this part of the neural network why I'm after I ask you that I will ask you how many parameters are there in this part of the network why don't you talk about with the person next to you come up with a guess there's no uh reason that you should be able to answer this at this point but if you can fantastic uh and if not we'll talk about it okay let's do this who thinks it's a yeah who thinks this B A bunch of folks who thinks it's C no how about d a couple folks okay fun fact they're actually all a little bit wrong no no they're very close I would say if I had to choose between these I'd probably choose B because this this final layer is actually just one logistic regression and it's inputs are the hidden layers so if there's 20 elements in the hidden layers it'll have 20 inputs and each of those inputs need to be weighted exactly once so you'll need 20 parameters the reason I say it's slightly wrong is because I think the true answer should be 21 and the reason is we're not going to get too much into this in this lecture but you know he had that bias term when we did logistic regression we're still going to keep using those in our neural networks so there'll be one extra term for a bias but that's not important right now you know really the number of things in this Y is not too large how about this one how many parameters are there in this chunk of the neural network so this is the part that connects the X's to the H's think about it think about it it's time to vote okay imagine there's a plus one to all of these who wants to say 800 bunch of folks who wants to say 20 that would be nice who wants to say 820 that' be nice too who wants to say 16,000 just all the parameters are there will again uh in this case there's actually you know a little bit more than 800 if you forget the bias term there would be exactly 800 because every single one of these hidden neurons will have a parameter for every single input so each of these are going to have 40 parameters and there's 20 of them so there's 20 * 40 parameters uh in this chunk there'll be a few more once you have bias terms and 40 * 20 is 800 oh my God my I got it wrong it says how many parameters in total ha 800 + 20 is 820 man reading comprehension Chris tisk tisk okay so there's something like 820 parameters if you have 20 uh hidden neurons and 40 things in your inputs all you need in order to train is a partial derivative of that likelihood function for every single one of those 820 parameters so let's go do 820 partial derivers who's ready yeah yeah let's try and do something Brave let's talk about how we can get the partial derivatives for all these 820 parameters that all need setting and we only have to do three things we only have to write the likelihood assumption or you know get this likelihood equation and then once we have that we just have to derve it with respect to all of the different components and why do you need this cuz if you want a good neural network you need good thetas if you want good thetas you want to search for ones that maximize likelihood you can maximize likelihood using optimation optimization techniques such as gradient Ascent but for gradient Ascent you need to have the partial derivatives of the scoring function you have with respect to every single movable part in your model and all of our movable parts are the thetas so that is the story and it basically says uh when you guys actually I just want to have this met thing that understanding mle applied to deep learning is only hard because there's these four compounded oh sorry steps to get there so oh before we jump into this I think this is funny you know like this is a picture of a huge deep learning model and we've got scooby. do saying hey gang let's see what deep learning really is because at the end of the episodes they always reveal and in this case oh my God it's just convex optimization it's just a whole bunch of derivatives happening that can make your Theta smarter okay let's jump into it this is exactly the same as logistic regression our model is going to eventually output a probability which we're going to assume is the probability that the class label is one the probability class label is zero is going to be one minus that that's our assumption is that this model is creating those probabilities BAS off that we can get a super sweet scoring function if you had a single data point the way you could score this is you can use your beri probability Mass function so why hat is the probability that came out of your neural network you know if you want to get really detailed into it it's coming from this equation but it is the probability that we're assuming is the probability that y equals 1 and because of that that's the parameter of your bruli your data point has a True Value y it'll be the probability raised to the 1 or Z which which is the true value Y and this is the continuous version so great why is a beri we can use that as our likelihood function if we have many data points in our training example we're just going to do the product of those many times so you get a whole bunch of the outputs of my I the output of my logistic reg or my deep learning for the I training example raised to its label now this is going to be numerically unstable so instead of maximizing the likelihood we're going to maximize the log likelihood gets the same answer answer but it's going to be much easier for our computer friends so instead of doing this likelihood we're going to take the log of it and we take the log you get the exact same equation from mle of logistic regression like this is exactly character for character the same if you have end data points it has this inner sum for each of those data points amazing so this is the same as logistic aggression this is the same as logistic regression uh finally though all we have to do is derive this with respect to every single one of our thetas all the thetas here all the thetas here so you know we had 800 here and 20 there we're going to do that derivative now our goal is the derivative with respect to all of these output parameters and the derivative with respect to all these hidden parameters if you can calculate all those partial derivatives as a function you are done here is a bad approach let's say you want to calculate the derivative you're like I've got this scoring function and I want to get the derivative with respect to say a Theta here a bad approach was be you could say oh y hat is actually this this is the equation for how y Hat's calculated so I can just substitute that into my equation and so instead of having this log likelihood here you know I could uh put that into my equation then I could keep chaining these as I go that's would be correct but will lead to a math bug instead what I'd like to show you guys is derivatives without tear how can we derivatives without tear cuz this is going to be important because deep networks often don't just have one layer they have multiple layers so we need to know how to do this calculus without making us want to pull out our hairs so no tears calculus and it comes down to the chain Rule and Mr blanchon was uh my high school math teacher and he taught me the chain Rule and I told him I don't think it was going to be useful and he was right it was in fact useful and now governs most of artificial intelligence chain rule says if you have a function you can decompose using some intermediate step I find this a little bit abstract but I find this really easy to understand you want the Dera of log likelihood with respect to a Theta there's an intermediate result which is the probability so get the derivative of log likelihood with respect to that probability and then you can multiply it by the derivative of that probability with respect to the thing that went towards calculating the probability you can decompose each step in your calculation of log likelihood and get the derivative piece by piece also don't forget sigmoid we love it because it's got this beautiful derivative and uh another thing not to forget you know the derivative of a sum is the sum of the derivative so often from now on I'll just be thinking about a log likelihood that doesn't have the sum because you can do the derivative of this log likelihood without the sum then you can just put a sum outside and it will be the same okay in Monday's class when we did um logistic regression we had this practice thing we said what is the derivative of this expression sigmoid of theta transpose x uh where we know that sigmoid has this beautiful slope we used the chain rule we said okay take that input call it Z and then we'll get the derivative of sigmoid of Z with respect to Z and multiply that by the derivative of Z with respect to each Theta IJ uh and when we did all that and plugged and chug we ended up with this nice little equation you can review Monday um just a little bit of reminder of what we have done so let's do it this Stanford we can do something Brave derivative goals let's start out with getting this derivative I want the derivative of this scoring function for one data point with respect to a parameter here in the output layer first we have to decompose it we want to use the chain rule to say okay there's a Theta here and there's an output here but there's this intermediate computation which is the computation of the probability and let's split this up we say you know the like log likelihood comes from this probability and this probability comes from this Theta and we can do derivatives in two steps using the chain rule we can find the derivative with respect to Y hat multiply that by the derivative of y hat with respect to this particular parameter and that's fantastic what if we wanted to then get the D der ative with respect to one of these inner thetas and this might be the most important slide of today's because this is to show you how chain rule can make it easy to do calculations derivatives when you go deeper and deeper so now we have a scoring function over here and it is impacted by a parameter over here how can we figure out if you were to change a parameter here how much the scoring function would change aka the derivative again you could try and write the scoring function in an equation that has this parameter and then do the straight but that would not work very well instead you should use the chain Rule and to do the chain rule you should recognize that this leads to these calculations these calculations leads to this calculation this calculation leads to your log likelihood and the chain rule can allow you to do the derivatives of each of those steps on their own so the derivative with respect to this thing will become the derivative of log likelihood with respect to your output probability the derivative of your output probability with respect to your hidden activation and the derivative of your hidden activation with respect to to each of these you can just chain each of these layers together and you don't have to do one composed derivative you can do each derivative on its own decomposition what a good time so if we want to get that derivative of log likelihood with respect to the output parameter we'd first need this the derivative of log likelihood with respect to P or Y hat rather the probability that comes out of our deep learning algorithm we know log likelihood is just going to be this beri probability Mass function logged uh and when we do the derivative of this with respect to Y hat it's really not that bad you know you have this term what's this derivative with respect to Y hat well that looks like a constant and derivative of log of your variable is just going to be one of your variable something similar happens on this right hand side we are having a good time you could simplify this if you wanted to and you end up with a really nice expression for that first term okay not so bad that first term check done how about the this derivative the derivative of the output of your neural network with respect to one of the Hidden parameters ye don't forget the output is defined in terms of this logistic regression uh you can think of this as sigmoid of Z where Z is the inner part that goes into the zmo uh and just like we did before you can say okay this derivative of y hat with respect to that parameter uh is going to be this derivative over there uh [Music] oh oh right right right this is the sigmoid so the sigmoid derivative is just the sigmoid itself time 1us the sigmoid times the derivative um of Y hat with respect to H of the XI and you get this but this is the same as what we had for uh maximum likelihood estimation of logistic regression you know this is a very similar formula you have the output of your logistic regression Time 1 minus the output of your logistic regression Times by what used to be XI but is now hi okay not so bad and not too scary and this really comes from the fact that sigmoid has a nice slope you know this is the derivative of a sigmoid and so it's just going to be sigmoid Time 1 minus a sigmoid times the derivative of uh the inside with respect to the parameter you cared about and we get with something not so bad at this point we figured out the Der of log likelihood with respect to Y hat we' figured out the Dera of Y hat with respect to this and we just multiply those two things together and you would get the derivative of live likelihood with respect to one these output parameters and we're almost done we've now figured out the derivat one of these output parameters now let's talk about the deriv one of these input parameters you don't need to memorize this but I want to give you the flavor for how chain rule is allowing us to get bigger and bigger networks without the math becoming insane the way we can get this derivative is just chain rule you know get the derivative of this calculation with respect to times it by the derivative of this calculation times it by the derivative of this calculation so you have each of these parts on their own this derivative we've already done we already know the derivative of log likelihood with respect to Y hat we haven't done this derivative what's the derivative of y hat with respect to one of the H's okay you'd have to derive this with respect to one the hes luckily this is the derivative of a sigmoid and a sigmoid has this nice derivative where it's just going to be the sigmoid * 1us sigmoid time the derivative of the inside with respect to what you cared about what's the derivative of this inside with respect to HJ well there's only one thing that's multiplied to HJ and that's Theta J so this derivative not so bad hey is it over almost we've got this derivative we've got the Der of Y hat with respect to HJ and now we just need the derivative of HJ with respect to one of the parameters what's HJ here you go we need the derivative of this with respect to one of those thetas now this is a little bit nice because you're doing the derivative of sigmoid what's the derivative of the sigmoid it's just going to be the sigmoid itself Time 1 minus the sigmoid times the derivative of the inner part with respect to what we cared about in this case a Theta i j now there's only one Theta i j or there's only one number that's multiplied with Theta i j so that's when k equals I and when k equals i x also equals I there's only one number that's multiplied by Theta I so when we take the derivative of this with respect to Theta i j we just get an x i and we're done and you just multiply each of these values together your computer can calculate this term and multiply with this term and multiply with this term and that will be the number that is your derivative and that's all wait a second you now know how to build a neural network and and you now know that training is just requires you to get these partial derivatives and chain Ru will allow you get partial derivatives no matter how big your network ends up you guys could both build a neural network and you know if you really needed to you could sit down and you could Implement how we could do the training the training would use use gradient Ascent you'd have 820 parameters and you now have equations that you could use to calculate the derivatives of each of those 820 parameters at any one point and that's it you guys got it now of course I said the positioning for today's lecture should just be high level I want you guys to see the details so that you can appreciate the big picture this isn't Magic chain rule really is the answer to where all this deep learning intelligence is coming from what a powerful tool and allows us to do intelligence with bigger and bigger neural networks uh and so maybe we should just have a moment of silence to appreciate those simple facts demystified deep learning okay fantastic okay um so just to be clear what would you do if we had multiple hidden layers you would just do more chain roll like if you want to have the derivative with respect to these thetas then you can have the derivative of loss with respect to Y respect of Y with respect to H2 the Der of H2 with respect to H1 and D of H1 with respect to the thetas uh you could just keep adding chain rules uh to go deeper and deeper in your NE networks okay now it's time to do a little bit of fun uh exploration you know these neural networks there's this great little demo site that I've got here and it shows neural networks trying to learn more complicated functions than logistic regression could learn someone asked what's the theory behind neural networks and people would come up with these things where you have like every Point has an X1 X2 coordinate and then a label either red for zero or green for one and a neural network can learn a circle so if all of your ones are in the circle of this grid and all your X's are in the outside a neural network could learn the circle you know spatially a neural network could learn a spiral people got deeper and deeper into asking what can a neural network learn and now we feel that a neural network could learn any function of any complexity like there's no function that's not complicated enough that if I got infinite neural networks hey come on learn sometimes they get stuck but you know you can imagine my step size is a little small here oh yeah I got over the plateau keep going yes yes yes yes train neural network train do your chain rule okay now my neural network is able to make good predictions but um you know people got deeper deeper into this and as I said we now think that if you put enough neurons together there is no function that you couldn't learn if you were able to set all the parameters perfectly there's a couple things that I want to add as extra ideas one of the extra ideas is we've been talking about binary predictions but obviously that hand drawing digit one wanted us to make not just binary predictions but they want to make us predictions that were more multinomial so like your predicted 0 1 2 3 4 5 6 7 8 9 we aren't going to push you in that direction yet but if you wanted to instead of using a sigmoid you use a version of sigmoid called softmax and then your loss function would be the probability Mass function of a multinomial not the probably Mass function of a beri we saw in the hand drawing digit that not all layers were fully connected you know at the end we had these fully connected ones but ear on we had these boxes and those boxes were actually sharing weights and sharing weights is a great idea that people called convolution um and it turns out if you want to do images having convolution the first couple layers just makes for some nice wellt trable uh neural networks here's a crazy thing people when they were first starting to understand neural networks somebody made this crazy neural network where you would put in a face and would predict whose face it was and they started to put in lots of and lots of faces into this and when they did this the craziest thing happened they looked at what was learned on the first layer and they looked at what was learned in the middle layer and they looked at what was learned in the later layers and they saw something incredible when they looked at what each neuron in the first layer was doing they learned to do these things that look like Edge detectors and when they looked at what was happening in the middle layer these neurons would be most activated when they saw things that look like parts of faces like one neuron would be looking for nose one neuron would be looking for an eye and when they looked at later neurons they'd be looking for like ghost faces like each one of these neurons would be most activated by a face that looked like this picture you know what's insane about that if you people look at human brains when they look at human brains the first part of your visual cortex is called the V1 cortex and guess what it looks like it looks just like that the first part of our brain does Edge detection if you look deeper into our brain the next chunk of brain does things like finding parts and then eventually we get to higher level Concepts like who are we seeing in this so a neural network with the mechanism I just told you when it was trained on data without being told how a human brain worked replicated some of these Key Properties insane and at that point people are like whoa Turing awards for all of you guys um obviously neural networks have what many more than 820 neurons in fact you know there's this Google brain Network which does visual recognition and this is certainly outdated but a few years ago had a trillion uh neurons and when you looked at those trillion neurons they'd be you know 22 layers deep and somebody architected each of these layers uh painstakingly but at the very very very very very very end is still producing a probability of different class labels they trained it on a whole bunch of things they also trained it on like all frames from YouTube videos so got super into cats which is just hilarious um anyways if you looked at the different neurons that this trillion parameter Network learned they had semantic meanings like you could look at the best stimulus and You' be like okay here is a neuron and this is really looking for things that are diagonal but here is a neuron it's looking for these circular grids but later on you'd have like there's a duck neuron there's the flower neuron uh there's the knife neuron and stuff like that there's this really hard test it says I'm going to give you an image from 22,000 categories and you have to figure out which category is in it was created by our own f f Lee and these categories are pretty specific you could get a picture of a Stingray or you get a picture of a manta ray and you'd have to be able to tell the difference between those two if you were to random guess you would do awful at this task you know there's 22,000 classes before neural networks people spent a lot of time trying to get really good at this and they got to about 1.5% accuracy and people got papers off of 1.5% accuracy and at some point people started using neural networks for this and they started to get closer to like 44% accuracy you know humans can get close to 93% accuracy and the most uh up up toate neural networks can outperform humans on this task and we kind of know this um how many parameters is too many it turns out if you have enough data and you have enough computer power the scary thing is it doesn't seem like there is a limit here we in this really weird world where if you throw more compute power that can do more gradient descent and you have more parameters you have more data there doesn't seem to be a ceiling the models just seem to get smarter and smarter and that's why there's such a weird arms race going on between like Google and gpt3 and all these folks to make the best neural network um I'm going to talk about this more on Friday um and yeah I'll bring this up too I do want to lightly note that not everything's classification and I would like to leave on this note of this little critter we had training from the beginning of class do you guys want to go check in on our Critter ah there's our Critter here's our Critter and this Critter is now gotten pretty smart like the critor has now figured out okay I want to go and I want to re eat all the red dots without eating all the green dots how does this Critter work you can look at this Critter's neural network what's its inputs it has all these lines and it knows a distance and a color of what it sees based on those inputs it then has a whole neural network but then it has a very final output and that very final output doesn't look like it's predicting or one or zero does it it's predicting an action to take one of the actions is is move left one of the actions is move right one of the actions is move really far left and one of the actions move really far right and one of the actions is move forward uh and so at the very end it's just predicting which of these actions it should take its loss function you know the thing that was optimizing was based off of looking at its experience and saying hm was I getting good food to eat or not what a time to be alive you know this is just the start we can talk a little bit more about how far this has gone but you guys have understood that full picture like this simple idea of putting logistic regressions on top of each other that's what's behind algorithms that can make decisions that's what's Behind These algorithms that can draw pictures that's what's behind you know the self-driving cars it all comes to logistic agressions put together trains my chain rule what a crazy world we live in have a fantastic day come back on Friday we'll talk about ethics and fairness uh before our very final week of school have a great day CS 109 oh if you ask a question come get a mandarin front