let's start our class with a tiny bit of review we're in this terribly exciting part of Cs 109 where we are transer transferring from Pure probability Theory into the basics and core principles of machine learning and particularly what the rest of Cs 109 is going to look like is we're going to build up the foundations of classic machine learning algorithms and then talk about how those are the basis of the thing called Deep learning which we might have learned about but really all the Theory comes from these three different mechanisms for estimating parameters in fact this one single task perhaps is the heart and soul of what you call machine learning and what is modern artificial intelligence and it's such a straightforward task it's amazing to think about the impact it's had and the straightforward task is you have some parameters and you need to figure out what they are to put that that in other words in CS 109 we've often had either random variables or probabilistic models that's just you know more random variables being random together and when we had those random variables a lot of times those random variables would be defined by numbers and those numbers we're going to now be calling parameters before this section of Cs 109 we would always give you those numbers we would tell you you have a normal and here's the mean and here's its variance but now we're going to start talking about calling those things parameters and talking about the problem of estimating those parameters Based on data and parameters could be from single random variables but just to be clear it gets really exciting when you get into problemistic models and problemistic models still have parameters those are these magical numbers that showed up in our model and parameter estimation the task of choosing those numbers based off of data uh is the same problem whether it's one random variable or multiple and the reason we care so much the reason it is such a big deal is because most of artificial intelligence looks like a three-step process the first step is find a real world problem that you think is worth solving and then model the problem and when you model the problem you might end up with a bean Network or you may end up with a single random variable but we always think about this model as having parameters and we're going to call those parameters Theta so the first step of solving a real world problem is to model it and then the second step is almost always okay you've got a model and it's got some Theta based on some historical data can I choose what those values of the parameters are we call this training um and you know you could have different algorithms for estimating those parameters uh and the result will be a model with numbers you'll have a model with numbers for each of those important parameters so if I could rephrase machine learnings it's basically parameter estimation uh for probabilistic models there's several algorithms we can use one of the most popular ones is the one that we learned on Monday the one that we learned on Monday started with a really nice simple intuition if I give you data for example I give you all this data points I tell you that my model for where these data points come from is a gaussian and your job is to choose parameters the idea is for different settings of parameters those data points will look more or less likely and the simple idea of the algorithm we talked about on Monday is what if we just chose the parameters that make the data look as likely as possible this has a name it's called maximum likelihood estimation and the sorts of problems that you'll have to solve will often look like this I'll be like here's a data set I assume that each of these numbers is coming from you know a particular model that model has a parameter in in this case is Alpha and could you estimate Alpha the the answer for mle will be a little equation that you could use you can say take each of these data points which we call exis sum up the log of them n divided by that number that's going to be my estimate for the parameter but how do we get there well we use the mle intuition and then Emily intuition starts with let's write down How likely is are all my data points based on a given state of the parameter so if the parameter was a different Alpha you would get a different value for How likely all the data points look and because we assume the data points are IID this likelihood is a big old product now we would like to just choose the alpha that makes this as large as possible but we have this cool intuition that the alpha that makes this large is same as the alpha that makes the log of this expression large and it turns out the log of this expression which could be written like this or Rewritten like this is much easier to work with choosing the alpha that maximizes this log of the expression is way easier than choosing the alpha that maximizes this original thing and so almost always we're going to be choosing an alpha that maximizes the log likelihood because logs are monotonic whichever Alpha makes this the largest will be the same Alpha that makes this the largest yay thank you logs so now all you have to do is choose whichever Alpha makes this equation as big as possible there's a few different ways of doing it but all the different methods that I know about for choosing an alpha to make this equation as large as possible are going to require you to take the derivative of this equation with respect to Alpha this derivative will be the key to choosing the biggest Alpha possible can I pause here for a second because maybe people had questions that was a lot of content that we covered on Monday questions thoughts concerns yes what's the difference between likelihood and probability ah likelihood and probability are sound very synonymous but likelihoods a particular term which is the probability of the data I saw so in this case you know this is the probability of my whole data set and so likelihood is a probability if you use likelihood synonymously with probability in all CS 109 no one would blink an eye but if you want to be really really really pedantic which I don't want to be technically it is the probability of a data set does that answer the question and to find like the likelihood of a single event we use the um the PF yeah so the likelihood of the entire data set is going to be the product cuz they're they're independent of the likelihood of each data point each data point x sub I that's my notation for one data point it's likelihood is just the probability density function if it was discret you'd use the probability Mass function if it was a model you would put the joint probability here good questions very good questions okay we're getting our mle review done rocking and rolling so what's the story so far we have this beautiful Philosophy for choosing parameters now it uses this cool bit of math and the hardest thing will be choosing an alpha to make this as large as possible we know that we'll need a derivative but so far in this review I haven't really talked about how to use the derivative I want to start by just mentioning this hidden mysterious problem we have to solve which is if you want to choose an alpha that makes this as large as possible that's not the easiest thing to do one thing we know how to do is we can take the derivative and in Monday's class we use a particular hack to find the argmax and that particular hack was we know that the maximum will be at a place where the derivative is equal to zero and for some equations it turns out if you just put in zero you can solve for Alpha and then you get the right answer it's not a very general method not many times in my life as a mathematician do I use this method because it doesn't really apply nicely when you have models uh it doesn't always it's not always guaranteed to work because you know the derivative of zero could also be a minimum there could be several Maxima there's lots of reasons that what we used on Monday is a bit of a hack so Story So Far machine learning is all based on parameter estimation one of the great theories for parameter estimation is maximum likelihood estimation we can get this log likelihood function and we want to choose the parameters that make that as large as possible that's going to require us to do some optimization we have a pretty crummy method for optimization right now so that's the end of review and where i' really like to continue this conversation is can I just give you a better method for optimization now certainly you can take entire class at Stanford where you learn about optimization all these different ways for choosing AR solving ARG Maxes but if there's one algorithm that every single person in Stanford CS should know it is this method for optimization it's not Lon King not everyone has to watch Lon King though you know fine movie what you need to know is this other method for optimization AKA another method for doing argmax and that's called gradient descent or gradient asent it goes like this let's say you're trying to choose the Theta which is the argmax of some function and let's say particularly this is the function you're trying to find the argmax of just as a bit of riew before I jump into the algorithm is this the argmax he yell it come on is this the argmax is this it is this it is this it is this it is that it yeah hey how easy would argmax be if somebody just gave you a visualization of the entire likelihood function like if somebody visualized as you change Theta all the different values of lik Hood choosing argmax would be so simple you just kind of look at the curve and you'd be done but that's really really really computationally expensive you have to try you have to calculate likelihood for every possible input of theta too much work people don't do that if you have more than one Theta becomes exponentially bad so if you can't see this blue curve is there another way of getting the Theta that takes you to the top of the mountain and I still give you the fact that you know the derivative for any Alpha you can calculate the likelihood and you can calculate the derivative of the likelihood it' be too much effort to calculate the likelihood for every Alpha that will take too much work but can we use a more clever algorithm to get to the top of the mountain and a lot of you guys might have seen this before so let me just remind those who have seen it and teach those who have not seen it before the very simple idea is Imagine you start with any Theta you choose an initial value maybe you choose zero maybe you choose 50 doesn't really matter which one you choose because that will not be your final guess for the argmax you can only see what's in Red so you choose a Theta you look at the likelihood of your Theta and you can know the derivative at that point you can't see the whole blue curve but let's say if you look at the derivative you know that the derivative is like this that means if you increase Theta you think likelihood will go up CU it's a positive derivative so if the derivative is positive do you think we should takea to the right or to the left is that from your perspective larger value of theta if we increase the value of theta because the slope is positive the slope's telling us that uphill is this way increased Theta should lead to increased likelihood so even though we can't see the Blue Line we know that going this way should make likelihood go up so the very simple algorithm is start anywhere figure out which direction is uphill and take a step in that direction so maybe we take one step in this direction now we can see what's in the red we're here we can know that we've improved and we can also know the derivative the derivative is not as steep but it's still positive it's still telling us that uphill is that direction so imagine you're a climber and you're somewhere on a mountain you can't see the whole Mountain what a weird Mountain to be climbing maybe it's very foggy but you do know that uphill is this direction and if you want to get to the top of that Hill you go in that direction so very simply if uphills in this direction let's go that way boom boom boom boom boom and if we repeat this process enough eventually we'll get to the top and at this point you are still the Red Dot but you're that final Red Dot you can't tell that you're at the top of the mountain but what you can do is you can look at your gradient and you can figure out that my gradient is zero so my slope is zero that means no direction is going to take me up any anymore and therefore I think I've made it to the top of a mountain how simple is that algorithm some people call it hill climbing some people call this gradient Ascent now of course we don't care about the likelihood we cared about the Theta that gave that likelihood and so we would just run this algorithm and return back Theta this is the number one most important algorithm that you need to know in optimization there's people who do slightly fancier versions of this adaptive gradients is a thing you know people have improved upon this but this is the core Insight that people use when they're doing something like training in neural network they're going to try and choose parameters that argmax something and they use hill climbing questions ideas yeah um what's the size of the step you set constant how annoying is that like what happens if you over shoot oh my God that would be a disaster okay the question is what's the size of the step and I'm telling you that you have to choose that number before you run the algorithm so let's run the algorithm I've got an imagined curve and let's say you choose a really big step size so you start here gradient this way so you take a huge step and you step over here and now you take the gradient here and it's negative and you take a huge step and you end up over here and you end up with a positive gradient ideally you'll start narrowing in on the top there is a possibility though if you keep taping huge steps it's like you keep leaping over the top of the mountain and you never converge to the top so small step sides guarantee you converge but it'll take more longer to get there is it eventually disc into a sort of binary search then you can think of it as having some binary search capacities yes but step size does matter so your original question was really good you know if your step size is too big you're going to have to choose a number which I'll talk about in second and that's an important choice yes is this sort of similar to what we did in the last lecture where you were just like putting in numbers and like seeing if it was going up and going down I mean yeah a little bit so the question is is this similar to what I did at the beginning of last lecture so just to remind people at the beginning of last lecture I did this thing right where I was trying in different parameters and I was seeing what was the largest it's a little bit smarter when I was doing this I wasn't looking at gradients I was just looking at likelihoods and this time I'm also going to be looking at gradients and that's going to guide the computer and actually that's what's going to allow the computer to really do this both automatically and a lot more efficiently so does that make sense what a cool question yes another question um do we assume that the graph that we look at always looks like that in SS of having like one single pump like hump that could have have like two humps or like two P at some point with okay so imagine the mysterious foggy log likelihood function remember we're trying to choose log likelihood of our Theta you want the Theta that maximizes it which one of these is the argmax this one no this one yeah but what if you did hill climbing and you started here you go uphill uphill uphill uphill uphill and then you get here and be like I'm at the top of the mountain woohoo and you would return not the best parameters ever which is a little bit sad not really the end of the world but a little bit sad okay if you were feeling super hacky how would you solve this oh you have an idea you taking like 20 AI classes what's your idea do it again yeah do it again there's a this is the really hacky thing that people actually do is they'll choose random starting positions and be like oh I started here and I got to the top here I started here and I ended up at this spot again but then like one time I start here and I end up over here and another time I start here I end up over here and you know I ran it four times and out of those I this was the top of the mountain that I found super hacky but people actually do it people are crazy and then they they present themselves like very elegant machine learning theorists now I do want to note that not every problemistic model only has one parameter a gaussian for example has two parameters if you had a whole beijan network you could have hundreds of parameters this method works even if you have more than one parameter so if you have more than one parameter your log likelihood function becomes you know a three-dimensional graph where you know High values on this third dimension means High log likelihoods and we'd still like to get to the top the parameters that have the highest log likely and it'll be a combination of parameters that lead to high log likelihoods and you know you could have different starting points and you can still use hill climbing and hill climbing you just need a derivative with respect to each parameter and if you have a derivative with respect to each parameter you you can know which way is uphill and it works with more than two parameters does it work with more than two oh yeah it works with three four and five but our puny little human visual system can only visualize two parameters optimizing at the same time but the same algorithm works if you have hundreds of parameters and you're trying to jointly optimize all of them what's confusing about this questions okay so this whole algorithm works the only thing you need to do to be able to use this is you need to know the derivative of thing you want to optimize with respect to each parameter that's the thing that you must do that's your work and if you do that work then you can just use uh optimization I have really good news almost every single optimization that you do in CS 109 will be functions that have a very beautiful property which is called that they're convex and convex has this nice property that if you find an Optima it will be the global Optima so we don't have to do any of these like really complicated hacks uh if you're curious to go deeper about convex let's talk about that offline now if I had to give you that algorithm written in mathematical notation here's what it would look like when you're choosing a new you will continually update your guess for the best parameter when you're choosing your next guess you're going to take your old guess and then you're going to figure out the derivative of likelihood at your current Point multiply that derivative by some constant and then add that to your guess so if your derivative is positive constant is always positive if your derivative is positive then Theta will increase in value if your derivative negative then your Theta will decrease in value and we will repeat this many many times and isn't this such profound life philosophy we do not need to just jump to the top of a mountain if every day you wake up and figure out which way is uphill and take a small step uphill you will eventually make it to the top top of your mountain is that such a nice way to live okay other nice things gradient Ascent now if I to give you that exact same thing in code you could have many parameters give them all initial random starting points and then many times calculate the gradient and then run that update equation where each parameter you add to it a constant step size times the gradient for that parameter the gradient of log likelihood for that parameter that means that you will have to do some work every step you'll have to calculate a gradient um and you know if we go back to that example from last class of fitting the Paro this was the gradient we calculated we had a log likelihood function it was based on a parameter Alpha and we figured out the derivative of the log likelihood with respect to Alpha was this thing if you want to actually Implement that you would have to turn this math equation into code and if you could turn this math equation into into code then this would give you the best choice of alpha start Alpha at some random start then many times calculate the gradient of alpha based on the data and then change Alpha based off of the gradient you just calculated now of course you can't just type latch into a computer but it wouldn't be too hard to turn this into Python and just to show you what that would look like it would look like this many times when you're calculating the gradient you know start it out as this value and then you're going to have to calculate this term so start it out as n / Alpha and then for every XI in your data you get its log and you subtract it off of that when you're done with this whole expression you'll have the gradient for Alpha based on the current Alpha you had and your data points and then you update questions comments concerns wow optimization changes the world yes will we ever be expected to find the global maximum or if there's more than one will the local be enough yeah CS 109 if you find the local Maxima it will be the global Maxima but I do want to be complete in my Exposition because someday in the world you may end up in a local Maxima but you know what is perfection always the goal we want maybe not other life philosophy we can talk about that offline okay but I do want to leave this up for a second because this is what it looks like to put into code and a lot of times you know when we Implement our machine learning algorithms later in CS 109 you'll write code that looks a lot like this I want to do Max on my parameters so I'll use gradient Ascent okay has anyone oh yeah question um for gradient Alpha why can we just divide n by Alpha um because that was my this was this term so n will be a positive number hopefully Alpha didn't start out at zero um and an interesting thing Alpha will change every time through this equation and when I use Alpha it will be just my current estimate of alpha okay but basically this expression is the derivative yeah this expression is calculating the derivative and then storing it into a variable crazy a derivative can be started as a variable yeah it will be just a number at the end of the day the derivative will be a value that whole crazy expression becomes Whatever Gets stored in that bucket wow exciting has anyone ever heard of an algorithm called gradient descent it turns out every python package every computational package you ever use doesn't give you gradient Ascent but I just told you it's the most important thing ever it's because everybody uses gradient descent every package gives you an algorithm not to find the highest point but to find the lowest point it's because you want to minimize regret or minimize loss if we want to use a gradient descent algorithm so a gr and decent algorithm you start somewhere and it will give you the lowest point I want to maximize likelihood I don't want to minimize anything hackers unite if you found a gradient descent algorithm how could you use gradient descent to choose a parameter that maximizes a log likelihood oh it's on the slides for those who are fantastic at reading yes yes exactly you know just this is an aside in CS 109 you can always Implement gradient scent but if you ever find a package that does gradient descent you'll notice that they will be minimizing not the log likelihood but the negative log likelihood so normally we want to maximize the positive log likelihood and if somebody gives me gradient descent an algorithm for gradient descent it'll just minimize the negative log likelihood again that's not so important because in cs19 you'll always be writing your algorithms from scratch but if you ever find a log a library that that just does it for you uh you just need to know that descent will be a negative fantastic okay I want to take a moment this is early for pedagogical pause but this is critical we just talked about some of the most important things and if you can solidify this knowledge then the rest of lecture will be a lot easier to follow so I want you guys to take a minute I want you to think about what we've learned I want you to see if you can think about some question that you care about feel free to talk about this with the person next to you before we jump into another way of doing parameter estimation so what do we talk about and what's confusing go for it take a minute and a half e it's got to be on the top but I think they close let's talk about after okay let's bring it back together I started writing the plot here just cuz I want to make sure no one loses the plot why are we doing this all of machine learning is based off of parameter estimation so far we have one great idea for parameter estimation which is that we can have a function uh a log likelihood function which for any state of the parameters we can say How likely the data looks and then we want to choose the parameters that make this log likelihood as large as possible and so we can choose that parameter that that maximizes log likelihood using gradient sent that's the plot so far but questions probably came up there's probably subp parts of this plot that are confusing and if it's confusing to you it's confusing to a lot of people I love the questions in this class yes what's the benefit of using a gradient asent over the first method we discussed just like doing so it's a good question the why would you use gradient Ascent instead of the the first method the first method was you take your derivative set equals z and then just solve it's just that the first method doesn't Sol work for General problems for example this method we just talked about this whole plot line is going to work for deep learning and deep learning you'll have a huge neural network with millions of parameters you want to choose the parameters that maximize log likelihood and when you have millions of parameters you can't sit down and set all the derivers equal to zero and solve for them and so you're going to need a more General algorithm so gradient descent is the most General bread and butter algorithm the setting equal to zero Works in some cases but just not in all what a cool question and I bet a lot of people were wondering about that I was used a lot last lecture was the idea of an unbiased estimator we have used that at all this lecture but I wonder if you can talk a little bit more about like what that means exactly and so you know before we did mle a different way for doing parameter estimation was using the things that taught us about a couple weeks ago where you could estimate the sample mean and there is an equation for that right you can estimate the sample mean by taking all your data points summing them up and then dividing by n and you could estimate the sample variance and you could do you know that uh cool equation that we had before where you take all of your data points subtract off uh your your guess of the sample mean uh squared / 1 - nus one these can be used for parameter estimation not in general like the PTO you couldn't use this for the prto but you could use do this for very very specific cases like if you're estimating the mean and variance of a gaussian you could use this if you were estimating the P of a bruli you could use this because it turns out this works out well so for a very small class of problems you could estimate parameters just by figuring out these statistics and then knowing how to calculate your parameters based off the statistics does that make sense it's not General though it doesn't work for Paro distributions it doesn't work for beijan networks it doesn't work for deep learning so while that was a nice idea we needed something general so mle is general and gradient Ascent they're General they work for any model any parameters you might care about okay fantastic now this could be the end of our plot line we could just be like great we're done with machine learning or we know the core theoretical basis of it and this is one of the core iCal bases but I want you to know why it doesn't always work and particularly in Monday's class we talked about one reason it doesn't work and to put that into words it's that mle likes to overfit the data it's seen it's choosing the parameters that makes the data it's seen look as likely as possible it doesn't think about data it might not have seen so for example if you're fitting a uniform and it got this data it will choose parameters that make makes that data look as likely as possible without imagining that there will be data outside so mle if it had to fit a uniform to this sets the minimum to be the smallest value in this list and the maximum parameter to be the largest value but if you've only seen seven points How likely is it that one of the points we saw was exactly the Min and How likely is it that one of the points we saw is exactly the max it's overfitting to the small amount of data it's given those parameters that comes up with are not very general in fact these values I chose 1 2 3 4 5 6 7 I can tell you that MLD did a very bad job of using those parameters because I generated those seven numbers from a uniform whose true Min was zero and whose true max was one and you just happen to see 0.15 as your smallest value because if you only see seven data points you're not going to get something too close to the true Min you're not going to see something too close to the two Max so you'll hear a lot of people talking machine learning about overfitting it's when the parameters you choose describe your data set too well and they don't describe things that you'll see in the future very well so ml is great but it does have this little problem now for a little bit of foreshadowing I am going to need a volunteer it's got to be somebody who hasn't volunteered before you come on up and we are going to play a game as you make your way up I will explain the game though please ask questions if you have any I have two envelopes one is labeled a and one's label B before we play this game what's your name nice to meet you I am Chris hey everybody this is class classes did I say that right everybody this is a class A little round of applause for our wonderful volunteer okay I have two envelopes I'm going to allow you to have one I'll take a yeah good choice here is the game and it's a wild one also if the lights bothers you can stand over here one of these envelopes contains X and the other envelope contains $2 X so you got an envelope and you can look inside okay so now here is the question do you want to switch I've given you information I told you one envelope has X and one envelope has $2 X before you make your decision do you want to see some math sure do you want to know the expected amount of money in the other envelope and and everyone else in class pay attention because we're going to help make an optimal decision so maybe you want to choose this based on the expected amount of money in this envelope so let's say that why is the amount of money in the envelope if you think about the expected amount of money in the other envelope there's a 50% chance that this has twice as much money and a 50% chance it has half as much money so there's you know a 50% chance you've got y over two and a 50% chance of 2 y you do lot of total expectation and you get that the expected amount of money in here is 5 over4 y isn't that wild now what do you think about switching it now I want everyone to think we're going to get to pull the audience we're going to say do you want to do you guys think you recommend I switch or I don't switch so I want you guys to think expectation says in expectation there's more money in the other envelope but you might have other opinions expectations is not the end of the world it's not the only thing we can do in probability okay do you want to give them some time to think or should we make them make their decision I'll give them time to yeah yeah that sounds good okay you guys think lot on the line it's why right I guess that means that it could either be $80 or $20 you can wait opinions however you want okay this envelope could have $80 or it could have $20 and since $80 is so much larger in expectation switching always seems like a good strategy but now wants to get your advice who who says switch who says doesn't switch first just make your noise say switch or no switch no switch oh wow do you want to see hands or was that clear enough feel like said no switch Yeah that seemed like but expectation says you should switch but you can choose no switch that's a fair Choice I'll choose okay do you want to know what's in the other one there's in fact 20 you made the right okay everybody good advice thank you very much take a seat yeah yeah yeah but I want the envelope hard to find envelopes now that is interesting everybody says don't switch and yet expectations said that there is more likely to be money in the other envelope and I'm going to say that there's a bug in our thinking which is similar to the bug in mle if you got to play this game a whole bunch of times you could figure out um a lot of interesting things and particularly one of the things that you could figure out is when we made this calculation I assumed it was equally likely that there was $20 and just as likely that there was $80 in the other envelope if you could play this game many many many many times you could actually get better at estimating those you could be like Chris never puts $80 he don't have that kind of money uh and you could become more and more intelligent about this and in fact though even without playing this many times times I can tell you that this is really misleading this idea that it's just as likely to have half as much money as double as much money and to drive that point home let's say you opened the first envelope this is back when I was younger and didn't have $40 to my name and it could have had $10 in it if you claim that $20 and $5 are equally likely if you were curse on that assumption you could say $40 must then also be equally likely and $80 must be equally likely and then $160 must be equally likely it turns out there's infinite powers of Two And if you have infinite values that are equally likely you no longer have a valid probability distribution and you're claiming that it's equally likely that I have $20 in my envelope as it is that I have like two you know like to the 50th Dollar in my envelope and we know that that's just not true I'm a professor so that was a brok broen assumption and the broken assumption you know it could have been solved in a few different ways one way you could have solved it is you could have played this game a whole bunch of times if you played this game hundreds of times or if you watched all the past videos of Cs 109 you could have realize that there's a pretty tight distribution over the amounts of money I'll put in envelopes but the problem is I only let you play this game once and there are so many contexts in the world in your algorithms in problems that you'll want to solve where you don't get to have a ton of data you don't get to play a game many times before you have to make a decision under uncertainty and one of the ways to solve this is before you walk into the game you have a prior belief you use all your knowledge of Chris and what sort of things I might put in envelopes and you encode that in a way that's not through experimentation and bean prior beliefs is a really good way to talk about that you can have sub objective probabilities you can have beliefs before you see any data um now of course by extension if you have a prior belief all posterior is have some subjectivity in them um but it's really helpful when you have to answer questions when you have limit to no data and it can also be very very helpful when you would like to not overfit the data that you've seen so far so your subjective probability if you had a belief about envelopes before you might think 20 is a pretty likely number for Chris to put it and like 10's possible but I think that the probabilities of these other numbers are somehow smaller and particular like it's very unlikely that I put $1 in because if I put $1 in then you'd kind of be able to weigh the other one be like there's no coins in here uh and similarly it's really unlikely I put $5 in because then you know $2.50 would also be really easy to way you can have a really strong prior belief about what's in this envelope and my claim is you guys all knew the right answer and the reason you knew the right answer is because in your brains you were holding on to this subjective belief over How likely different numbers were you in yourselves felt that it was very unlikely that I put $80 in there and this is called this is a paradox and the solution to the pars the reason humans don't find this hard is because we have strong priors okay so oh we've already played the game but in my first version I had you standing up here through that whole Exposition now if I could summarize the major takeaways from this probabilities really do have a belief element to them and some level probabilities representing what we don't know about the world will help us make better decisions so if you can incorporate prior beliefs maybe when you have small amounts of data you will make better decisions you will overfit your data less and you'll maybe end up with better parameters we've actually seen this play out before and I want to recall a problem from earlier in class and I want to show you the equivalent version as parameter estimation earlier in class I gave you a problem that says a medicine is tried on 20 patient it works for 14 and it doesn't work for six what is your new belief that the drug works that was a question I asked you earlier but I could have phrased this as a parameter estimation question I said I could say have 20 IID samples from beri estimate p here's your data 16 1es and or sorry 14 1es and 6 zeros now if you set up this problem we can now just throw in mle and mle if you worked out the mle estimate we talked about what it is for bruli is just the average so you'd sum up all these values and divide by 20 and that would give you 14 divid by 20 mle would just give you 07 but we saw this particular problem earlier in class we actually had a different way of solving it that didn't require us to use this particular plot line in this case we had a much more elegant way of talking about the probability uh parameter P we had this elegant conversation about a thing called a beta you said I can have a prior belief in what my parameter is and then I can update my belief in what p is after I saw data and this was so elegant for two reasons the first reason was El an is like the envelope example it allows for you to incorporate what you know before you start doing any experiments maybe you know something about these drugs and you can incorporate that information so it's stronger for that reason it was also stronger for the other reason which is instead of just giving you about a single number it gave you a whole distribution around that number so you can express not just that I think 71 is the most likely value of P but I can also talk about my confidence so for a few reasons what we did earlier was more elegant than what we're doing now now reason we haven't seen this in place of mle is because that worked for the P of a beri could we take that same beautiful idea and make it work for other parameters CU if we could then maybe we could do a little bit better than mle and we could learn about our final estimator so today the plot line is going to diverge and and we're going to be learning about a different way that we could do our estimations and it's going to be a beian policy that we're going to talk about called maximum a posteriori and really what it's going to allow us to do is going to allow us to incorporate prior for parameter estimation it will be a different method for choosing our parameters I would like to introduce it by contrasting it explicitly with mle mle says hey you want parameters pal I'll give you parameters how I'm going to give you parameters is I will choose the parameters that maximize the likelihood of your data which parameters make the data look as likely as possible the beian approach looks very similar but if you're very attentive you'll notice it answers a slightly different question there is another way of estimating parameters it says yo pal you want parameters I'll get you parameters but I'm not mle I'm going to give you more elegant parameters instead of choosing the parameters that make the data most likely I'm going to choose the parameters that are the most likely given the values of the data I saw I saw like 20 heads go like what yeah you know the these are slightly different philosophies right this is saying what makes the data looks likely and this is saying what's the most likely parameter they're slightly different they're the conditional in the reverse and when you put the conditional in the reverse then we're going to have to solve this using a different mechanism and that different mechanism is going to invoke base theorem and you know this is the unknowable as the parameter and the data is the observable so this is the more natural direction to express in terms of equations we have likelihood functions whereas this is something that we don't exactly know how we would have to solve for it just to be clear these two different philosophies are going to lead to two different algorithms because we never did this algorithm directly instead we did the log likelihood we would argmax over the log likelihood and similarly we're going to do some manipulations to make this easier to work with but today we're going to learn about our final estimator and the final estimator starts by taking a different philosophical stance and the idea of the different philosophical stance is it will allow us to incorpor at priors it allow us to come up with maybe better estimates especially when we have small amounts of data okay so a little bit of shorthand because I'm going to have to write equations like this over and over again and I want to be clear that when I use shorthand I don't lose people so we're thinking about our parameters now as being random variables just like the beta and we want to choose the assignment to those random variables that are the most likely given the assign ments to the IID samples we have but I'm going to start using Theta as Shand for the event of the random variables for theta equals Theta and XI as Shand for the event the I data point takes on the value uh that we saw in the database also one small thing did you notice how I used to have the I data point using a subscript and now I've moved to a superscript I'm just trying to gently get us ready for when we get to machine learning algorithms on Friday and Beyond because when we get to machine learning algorithms our data points will have multiple values per data point so I'm just going to start putting my eyes as superscripts but this is still saying XI using this shorthand notation I'm still saying choose the thetas that are the most likely given the data points that we've observed okay let's jump in but give people a moment let that sink in it's like okay it's mle but flipping the conditional that seems fine will it actually be different H we'll find out okay um now just to be clear if you estimating the most likely value of a parameter given data for a particular model of a bruli so I'm telling you all your data points come from a beri if you wanted to choose you know the values of the most likely value of theta given my beri data so that could be like heads equals 16 and Tails or heads equals 14 and taals equals 6 we already know how to think about this probability this would actually be a beta distribution so if you said you know what is the distribution of the likelihoods of the parameters given the data you have observed we know this will come out to a beta distribution in the case of a bruli and then if we put an ARG Max here it's saying which value of theta has the highest value and this is actually technically a probability density in the probability density of theta when we did the beta distribution these were you know X's because X was our random variable for p but now I'm using thetas because Thea is my more General random variable for p so you know if you say the distribution of your your parameter given data you saw it is a b or sorry it is a beta and if you say ARG Max it will just return the mode uh which is whichever input to the beta has the highest probability okay so before we get into any heavy math I just want to tell you that if you use this new philosophy this map philosophy and if you used it to estimate the parameter for a bruli or a binomial it would be really really straightforward because we've already done the math for a beta because we've already done the math for beta what this would look like is you're going to have to choose a prior belief in your parameter you'll express that as a beta recall that a beta with AB means that you saw a minus one imaginary successes and B minus one imaginary failures before you started looking at any data then you run an experiment which is you start to see some data points you'll see particularly n data points of successes and M data points of failures the posterior of your belief in the probability is going to be a new beta and we derived this earlier where the parameters of new beta is a which came from our prior plus n that came from the data we observed so imaginary successes minus one plus true successes observed and the second parameter is imaginary failures minus one plus the true number of failures that we observed so for estimating just the P of um beri you could end up with this posterior belief over the parameter in this case the p and your posterior belief would be a beta once again maximum a posterior then throws an argmax around this so we saw this in class maximum a posterior this other way of choosing parameters will choose a single number instead of giving you back a full beta it will choose the number which is whichever value makes the beta as large as possible that is the mode and it's always equal to um the first parameter minus one divid by the sum of the two parameters minus 2 it's always that for a beta distribution so back to our plot line and I'm not going to write this in green because that's hard to see we now have this core need you need parameter estimation for machine learning mle is great but it generally overfits and it doesn't allow you to have priors so we're going to have map to eliminate P or to to not overfit and while this is a good idea and we can see how this would be a great idea if we're estim P we have one huge issue mle works for more than just SMP parameter estimation is supposed to be something general for any machine learning model we should be able to estimate parameters and while map maximum a posterior that really fancy term for just saying what's the most likely parameters given my data while that seems like a good idea so far we've only seen it for estimating a single parameter for a single type of model it doesn't feel that General I do have good news people are like this seems like a good idea let's get to work crew and a whole bunch I imagine everybody had a meeting and then everybody went and figured out what's the way to do maximum a posterior for a whole bunch of classic random variables and they did and they thrived and they had a good time somebody figured out how did you do the maximum a posterior for a p well we're going to use a beta prior that's going to lead to a beta posterior and then we can just do the mode of the beta fantastic that also works for binomial then somebody's job was to do pan hey you want to estimate the lambdaa great you have to give me a prior there's a particular distribution called a gamma and if you express your prior as a gamma then your posterior will also be a gamma and then we can choose the uh the value that maximizes your belief in Lambda by choosing whatever maximizes the posterior gamma same thing with the exponential Lama um and people did something for like the normal mean oh it turns out your prior should also be a normal and people did the same thing for a normal of uh the variance of the variance parameter in a normal uh and each these somebody worked out the math for how you could do maximum a posterior I'm just going to show you a few of these I'm going to show you what it looks like for this Lambda uh in a Pon I'm going to show you what it looks like for multinomial and then I'm going to show you the math that you could use if you want to drive one yourself yes explain again what a conjugate distribtion yeah do you remember when we talked about betas if you express your prior as a beta and then you did Bay theorem the posterior magically was still in the form of a beta we call that a conjugate when the form of a prior matches the form of a posterior it's really really nice in good times and so when expressing prior beliefs we often try and look for Expressions that have this property you need to come up with conjugate prior yourself in cs1 no9 no but is it nice to know that um these are really good choices for representing priors for these variables you should know that yes so does that mean re representing the distribution parameter completely as for example a beta instead of bomal yeah so the binomial has a parameter p and we now need a a random variable type for p so even though P is for binomial the variable for p itself is a beta you know Lambda is the parameter for pant that's a good time but if you were to turn Rand Lambda itself into a random variable so you could choose the most likely value of that random variable the right format for that random variable's distribution is a gamma okay which we'll talk about in a second so I understand the difference between M and and like how you're looking for um so for example in this case you're trying to maximize the likelihood of um intuitively it makes sense that the most liket give you values that are the most likely obviously that's not true but what's an example where this isn't the case yeah so I mean if the question is what's the relationship between this and if you flipped the conditional like in what case is it different asking what is the most likely value of parameters versus what parameters make the data as likely as possible I mean I'm jumping the plot line a little bit but let's replace this with data so we want to choose the parameters that are most likely given the data and using base theorem this will be data given parameters times your prior belief in the parameters divided by some normalization constants so I'll say that this is equal to some you know divid by some constant K notice this relationship if you think this constant K doesn't change an argmax which it doesn't uh multiplying everything by a constant won't change which argument maximizes this function if the argmax of this is equal to the argmax of this your question is what's the relationship between this expression and this expression and when when are they different and notice there's a mathematical relationship what's the difference between the likelihood of your parameter given your data and the likelihood of your data given your parameter to make them equal you have to multiply this one by this term that is the term that shows you how these two things are different and what is that term that's a little bit of a hard question but I'm going to ask you you ask me a hard question right so it's going to be the value of of your beta right like of that I mean more generally what does it mean to talk about the likelihood of the parameters without any data that's your prior that's your prior let's write that in all caps these two philosophies are incredibly similar because these two expressions are incredibly similar there's just one term that makes them different these things would be equal if you took what we had from mle and multiplied it by How likely you thought those parameters were before you saw any data aka the prior and if you could incorp this term then you have a way of choosing the parameters that are most likely given the data what a wild ride we're like throwing all the 109 things you've got like conditional probabilities you have parameters as random variables and you have optimization uh for parameter estimation oh my God this is like throwing everything at you guys all at once and you got some Bas theorem just for good measure to clarify the bottom equation is as if we did the likelihood thing but also Incorporated that specific graph of how how you how we thought you're you're going to put money in an envelope and then it's the same as doing it the other way yeah basically it will be the exact same formula but we now will have a term for How likely you thought parameters were before you saw any data and so we'll have a new language for talking about that prior belief and it'll hopefully make our estimation better now I actually I don't want to go over this too much because it's a little bit rote in that there's not a lot of deep Insight in these next few lines I just want to tell you that people have solved this for different parameters like if you solve this equation for if you want to come up with good ways of expressing prior so that you could come up with good posteriors I just want to show you what it look like for a couple of them if you want to choose a Lambda one of the ways is you set a prior belief and the prior belief that you choose um for your Lambda it's like I'm going to imagine that I've seen some number of imaginary time periods and in that imaginary number of time periods I saw some number of imaginary events and it's your way of representing your belief in the rate before you see any data if you had that belief you can express it in this new random variable which you've not seen before but turns out to be a useful way of expressing the prior of a Lambda which is called a gamma and it has exactly those two parameters that we just mentioned you know you could express your prior belief as a gamma below before and then in your experiment when when you're trying to update your belief about the Lambda of AA if you actually see n events in K time periods if you express your prior is a gamma your posterior will also be a gamma and it will be you know your imaginary number of events plus how many events you actually saw and your imaginary number of time periods plus how many time periods you actually saw just to give you a sense that people have solved this this is how you would end up with this is what map looks like for Lambda uh I want to skip this for a second because I do want to jump to actually solving this for ourselves so so far you know you should use a beta if you want to estimate P for a berul I just told you you should use a gamma if you want to estimate a Lambda and I just want to really quickly tell you that somebody has also solved this for a multinomial remember multinomial you're rolling a whole bunch of dice and you want to know successes of each type of outcome you can set up a prior that's like the extension of a beta for a multinomial and it has a really fancy name called a dear slate but it's just a beta for a multinomial in that prior belief you say before I look at actual dice rolles for my multinomial I'm going to imagine that I saw A1 minus one outcomes of type one A2 minus1 outcomes of type two and a minus one outcomes of Type M it's like a beta would just have two different terms here number of zeros a number of ones but a multinomial extends that to M different outcomes and the deay extends the beta to M different outcomes as well it's the exact same thing as a beta but with a few more outcomes then if you actually observe N1 outcomes of type one and two alchs of type two so like here was your prior you actually collect some data if you chose your prior Express as a deer slay your posterior will also be a deer slay and particularly won't just be any deer slay we can know what deer slay it will be and it's a imagine successes of type one plus actual success of type one imagine success of type two plus actual successive type two and then map will choose for every single parameter Pi in your multinomial it will choose it to be uh this expression which is the value which maximizes uh the probability distribution of the deer slay okay so I'm going to really quickly note that most people choose a particular deay which is that they Imagine One outcome of every type we'll talk about this a little bit more on Friday's class so I'll actually save this for Friday's class I do want to take a moment though and get a little bit deeper into the math because the plot line so far of map is this okay we have a new philosophy and this new philosophy seems elegant but so far we've seen it work in different cases but we don't have the ability to solve it ourselves so if I gave you a new case Could you actually come up with an map estimate what if you you had to come up with what to do for updating your belief in a Lambda so are you guys ready for the slide that puts together all the Cs 109 Concepts you've learned so far not all of them but a lot of them okay I would like to give you a general way of doing maximum a posterior yourselves so far we've got some intuition but now is where we see the cold hard math at the end of the day the key Insight is we want to choose the that are most likely given our data set that is the opposite condition of mle B theorem tells us what we can do we can say if we want to choose the argmax of our parameters given the data if we put this into Bas theorem it gives us that this should be the likelihood of what's on the right side of the conditional given what's on the left side so B theorem flips those two in the numerator then it has the probability of the this term on its own that was your prior before your posterior and then you have this term on the bottom which is just How likely is the data not conditioned on parameters remember that in base theorem we called this term the prior we called this the likelihood term and we called this the posterior now in base term what do we call this bottom thing yeah the normalization constant because as Theta changes values this doesn't change values like IFA equal 1 this will be the exact same as IFA equal 2 it's a constant and so if you say which value of theta maximizes this and that's just a big constant the argmax of a constant times a function is the same as the argmax of the function itself this whole normalization constant goes away one small thing for notation I am actually going to flip these two terms even though we almost always wrote Bas theorem with likelihood times prior because prior multiplication is um uh we can change the order of the multiplication and it's going to make things much easier so I'm just going to put the G over here is that okay okay I'm going to drive things step by step actually I'm going to drive what's on the board step by step the next step is because we always assume our data points are I ID when we talk about the likely of our data given the parameters this actually becomes a product of the likelihood of each data point on its own given the parameters we did the same step for mle so this is a pretty reasonable step to take me from this line to this line the next step is um we're going to use the fact that ARG Max of a constant times an expression is the same as ARG Max of an expression itself I can maybe convince you of this if you want to find argmax of this expression If instead I give you two times this function whichever argument maximizes this is going to be the same argument that maximizes two times that function so if you multiply any function by a constant the argmax ignores that constant so at this point I have a mathematical expression for saying if you want to choose an map estimate for your parameters you should be using this expression now before when we did mle we had an expression that looked like this and we wanted to choose the thetas that maximized it instead of maximizing it directly we chose the Theta that maximize the log of it it was a really cool trick and we're going to use that trick one last time if you do the argmax not of this expression but the log of this expression what these two terms will become the log of this term plus the log of this term the log of this term we already know what it is it's just the sum of the logs of each of the likelihoods and then the log of this term will just become a log of a prior this is how people do map if you want to use this other philosophical way of choosing parameters you end up with a pretty similar algorithm and that pretty similar algorithm is going to say choose the parameters that maximize this expression it has a term that just looks a lot like mle but you also have a term for your belief that Theta was the actual parameter before you started any experimentation so you're going to end up with an estimated parameter you're going to do that by using an ARG Max choose the value of your parameter that maximizes this thing it has the sum of log likelihoods but it also has a log term for a prior belief in your parameters so just to fill out this slide from before mle says if you want to choose a Theta mle you're going to AR Max over this nice little expression Maxim a posterior after we did all this different philosophical stuff and we had all this long conversation look how similar this equation ended up being we're going to choose the thetas that maximize this expression and this expression has a term that looks exactly like mle it's just got one extra term all this complicated Exposition just to say hey you want to have priors that that's fine we're just going to have a term in your log likelihood function for the prior belief that you had in your parameter and it all comes from Bas theem we can derive it step by step but it leads to this very very nice result two different philosophies for choosing parameters one requires you to have a prior and it will just become a prior term yes I don't understand what G's yeah yeah sorry sorry G is just saying like uh you had a prior belief in your parameter uh put the probability density of that prior belief here so if your prior belief was a uniform this could be a uniform if your prior belief was a beta this could be a beta probability density function let's actually put that to the test Oh and before I move on though I do want you guys to notice that those two terms are exactly the same and so mle and MP are actually very very similar okay uh I want to you can do the Bruni but I actually feel like this is going to make a lot more sense sorry when I do one last Express um I want to skip to this example okay I want to do this example and I want to do it a little bit slowly because I feel like once you can do this example then everything else will make a lot of sense let's go back to this thing we solved before before I said I give you a whole bunch of data points I tell you that all of my data points come from a Paro distribution that's something you hadn't seen before but it has a PDF from now you'll be able to do parameter estimation whether or not you've seen a random variable before or whether or not you've seen a model as long as somebody gives you a PDF or pmf you should be able to do parameter estimation the problem is going to look exactly the same but I want to use this new philosophy and this new philosophy is going to be maximum a posteriority choose the most likely parameters given the data so there's your data and I want you to choose the most likely value of the parameters the one extra piece of information I now have to give you is my belief in the parameters before I see any information this time I didn't choose a really fancy conjugate prior because I want you to show show you that this works even if you don't use a fancy conjugate prior I chose a pretty silly prior I said before I started any of this experimentation I have a belief in my parameter I believe that it's normal I believe that's normal around 2. oh I think I did my math using two uh with variance equal to three but anyways I express a prior and just like the envelope sometimes a prior can really help you come up with a good better estimate of your parameters if you can have a good prior you can do better with less data now let's see if we can do the math because if you can do the math for this that's kind of like the high water mark for understanding Maxim a posterior Maxim a posterior says that there's this thing we don't call it the log likelihood function instead we're going to call it the map function but it's just the same as the log likelihood function Plus the log of your prior belief in your parameter so this we derived earlier I don't want to lose anybody on this but this term comes from we had figured out what was uh the log of the likelihood function for the PTO and when we did this earlier in class we got this expression if you want to do maximum a posterior we're going to choose an alpha that maximizes not just this expression but that expression plus a term for the prior somebody asked earlier what is g of alpha in this problem you should be able to figure out what is g of alpha just to be clear I told you that my prior belief is that Alpha is a normal so G is supposed to be your prior belief in a particular setting of alpha yeah this is normal distribution yeah but not CDF or pedf um the pedf is exactly the PDF this will be the PDF evaluated at Alpha so remember the I think I solv this using mean equals 2 and variance equals 3 so if you know mean equals 2 and variance equal 3 you can say what is the density at Alpha and the density at Alpha would just look like putting Alpha into the PDF expression and as I said I used two for mean so this is what that expression looks like I put in the mean I put in the variance uh and I got this expression now the other terms stay the same I do want to split this up the log of this expression will be the log of this term plus the log of that term and we split it up the log of this term is going to be like some crazy constant but when you do the log of that term check out what happens you'll have log of something base e those things go away and you'll just be left with this expression so this expression saying instead of just doing log likely I'm going to do log likelihood press a prior term when we work it out we end up with we're going to choose the alpha that maximizes this thing before we're choosing the alpha that maximized just the term on the right now we're choosing the alpha that maximizes this whole term which incorporates a prior belief I will tell you that well actually you know if you want to choose an alpha that maximizes this the first thing you're going to have to do is figure out the derivative we're back to argmax all argmax functions that we know about required to do a derivative when we do this derivative we already did the derivative of the thing on the right that was n / Alpha minus the sum over all the logs of your data points but what's the derivative of K with respect to Alpha zero and you know if you expanded this term you could figure out the derivative of this term with respect to um the different values of alpha I actually believe I'm missing uh this whole thing I think should be multiplied by one over six sorry about that just a really quick check before we were choosing Alphas that maximize this term now we have this prior belief and the prior belief was that the true parameter value was gaussian centered around two notice what happens to the derivative if you put two in for Alpha this term will be zero it says oh my prior belief was it was two and so I'm not going to be changing my derivative but if you put a term that's larger than two here so you put in like a three that'll be -6 plus 4 it'll be a negative number so if you put a term that's larger than the mean of your prior belief this gradient is going to pull you closer to two and if you put a term that's smaller than two it's going to pull you closer to two the other direction and so it's this little gravity well around two that's trying to prevent your parameter from getting too far away from where you originally believed it would be so anyways once you have this derivative then you can do argmax you could use you know gradient Ascent all paths lead to gradient Ascent here if you have a derivative and just to drive that home you could have actually just you know done your derivative calculation with a slightly different starting point uh for how you calculate your gradient but you can still use gradient Ascent or gradient descent okay so to be clear we are doing all of this because we would like to build machine learning algorithms if you come back on Friday you will learn about our first machine learning algorithm in CS 109 we'll build an algorithm that can solve a general important task all of these algorithms are going to rely on the mathematics and the philosophy that you guys have worked so hard to build a foundation for we learned about unbias estim they're not so useful we talked about maximum likelihood estimation very nice math very easy to use very general but with a small problem that it sometimes overfits data and now we did beijan estimation called map maximum a posterior which basically is mle but it has a term for a prior belief on parameters now that you've worked so hard to build this Foundation we can celebrate we can have a fun time come back on Friday we will celebrate we will use this to actually solve naive Bays then we will learn about logistic aggression then we put it together into a whole neural network and it will be a party thank you guys very much for working so hard today see you on Friday