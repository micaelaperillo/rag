good afternoon cs109 how are you guys doing today oh fantastic that's what I'd love to hear and um the word of the day is happy Wednesday I hope you guys are having a fantastic day I know we're at that like mid point of the quarter wearing a lots of things start to get a little bit more intense and I just appreciate you guys for doing what you do and one of the ways of showing that appreciation is we've got a very fun class today we're going to learn something incredibly useful and also which is expansive it takes the things that you've started to learn in cs109 it's going to allow it to expand to describe more and more complexities in the world we are going to be learning about probabilistic modeling so thinking not just about one random variable not just about two random variables but a whole bunch of random variables being random together maybe like a real world scenario uh you will walk out today's class hopefully having mastered or understood some of these core techniques for modeling okay then I also have a little bit of a hook for you I want to tell you about one project that I found very interesting one of the things that I found very interesting uh especially when I was a PhD student is that computers were so bad at understanding beginner programmers and the mistakes that they would make so to give you an example there is this website called code.org and people write programs using these very simple block-based languages their programs are like six lines long and computers were a total disaster at looking at those six lines and understanding what misconceptions the student has like you look at this this block of code and be able to say um what sort of idea the student was missing and code.org has a lot of data they have tens of thousand students um you know and 37 000 unique solutions to this five line program they have lots of History they can have some teachers helping with this process and yet they were not able to understand basic mistakes that student makes I'm going to jump to the conclusion but to understand how we got to that conclusion you'll have to follow along today's class so the idea that we came up with was well first can we just use some pretty basic statistics like count conditional probabilities of mistakes given the fact that they have a for Loop or given the fact that they have an if statement in the for Loop and basic statistics didn't get us very far you might have heard of this exciting thing called Deep learning we tried a lot of deep learning on this and if green is how well humans do and you want to be high on this chart deep learning wasn't getting very close at all but we took some inspiration from this beautiful project which described a probabilistic model over all the ways that humans could write did a hand-drawn digits or characters so for this particular character you can create a whole probabilistic model that describes the process that somebody goes to write these characters we took inspiration from this and we brought to the world of beginners writing code and we thought well what if we wrote the probabilistic model for beginners writing code and we did and it was a good time and by we I mostly mean there was a student called who was a TA for 109 a year ago he wrote this beautiful generative model and it was able to outperform humans at the task of figuring out what bugs a student had um now code is a complicated thing think about imagining code in the world of random variables and by code I mean like the whole production that a student makes it's a complicated thing there's lots of moving pieces there you're going to need something more than just one random variable you'll need something more than two random variables stick around for today's class and we'll talk about how we could get to many random variables but let's build a foundation of where we left off first this whole section of cs109 probabilistic models comes down to talking about random variables being random together if you can have a deep understanding of random variables being rendered together you're in a good place and the most important fundamental Concept in this world of probabilistic models is this idea of a joint distribution a joint distribution says if you have two random variables can you give me a function an equation a piece of code a table anything where for any assignment to all my random variables together you can tell me the probability of that joint assignment the joint is the be all end all if somebody gives you a joint they've given you complete information about the probabilistic model please do enter me if you have questions or anything confusing about this is a good time to talk about it okay I'm seeing some thumbs up which makes me feel good um now if the joint was the most important thing you wanted to have the most important question was the question of inference and the question of inference says You observe some random variable and your belief about the other random variables may change we've seen inference in many forms one inference task that we saw at the beginning was you would look at the weight of an elephant and we would just infer an updated belief about the gender of the elephant this is a very simple model where we imagine the gender of an elephant was changing the distribution of their weights so if You observe weight you could infer a little bit of signal about what gender they were it's not a very practical problem no one's like trying to gender an elephant by weighing them that's a silly thing to do but a less silly thing to do is what we did in last Friday's class wait last Monday's class in last Monday's class we did this harder task but much more meaningful task of doing the visual Acuity inference and just a second while the HDMI catches up in the elephant task you are inferring a Bernoulli It could only take on two values so you only had to do one base theorem in order to update your belief but on Monday's class we inferred a random variable which wasn't Bernoulli in fact it was a random variable that was represented using a dictionary that took on way more than two variable values it could take up a hundred values and when you did inference you had to update your belief that the random variable took on any of those random variables so you didn't just need one Bayes theorem you needed a whole for Loop of Bayes theorem so that was the big upgrade that we did last Monday uh and also I just want to point out quickly that um in Monday's class we did this really cool computer science thing where we represented this random variable using a dictionary and that dictionary could map different assignments the random variable to the corresponding probability so you can think of the whole dictionary as the random variable and a single look or like you know a key would be like the little a and a lookup would give you a probability and for those of you guys who are more used to computer science maybe this is a nice little metaphor for random variables yes the reason why we didn't use Bernoulli um in the second example is because we didn't know that there's this like a 100 chance zero percent chance is a girl or a boy if you had to have a function to describe hearing and not hearing is that oh yeah going back to the first example the elephant one so in this world you would observe weight which is a very continuous thing but the thing we're inferring was the gender of an elephant and that's Bernoulli because it could only take on two values I mean what a heteronormative way of looking at elephants but you know that's what we did but it was Bernoulli because in our world of gender there was only two values oh and for this visual Acuity you mean so seeing or not seeing ah no no there's way more detail to seeing like some people can see 2020 so like you can see without glasses at 20 feet healthy vision and my vision in my left eye is 2100 which means like five times worse than an average person so there's a whole shade of gray between you're seeing like a healthy adult versus you can't see every anything and there's all these Shades of Gray and we want to keep beliefs about all those Shades of Gray in fact in order to say how well somebody can see you really do need to do that you need to keep track of all the nuance random variables and beliefs oh my God you can express yourself so much better once you master this idea it is such a powerful language that the whole world doesn't seem to speak but now you guys will now know how to speak okay so this was our big upgrade going from inferring a Bernoulli random variable to one which could take on a hundred different values and that required us to do base theorem 100 times but we had computers so it was kind of fun a number of dictionary I've mentioned this a little bit and then I wanted to point out two things you know what are these terms this term was your prior this term was How likely was an observation given the true state of ability to see which was a knowable thing and based on these two knowable things we were calculating this hard to know thing the updated posterior of ability to seize inference now this was kind of crazy when you look at code for people who do Bayes theorem on random variables that can take on more than two values you often see code that looks like this you often see code where you see references to things like the prior you often see code that has references to things like the likelihood but you hardly ever see code where they actually in the inner for Loop where you expect that they would be calculating the probability of y equals zero you hardly ever see people actually calculate probably of y equals zero and instead what they do is they calculate all the numerators and then they normalize them what an insane thing to do so when you actually look at code people don't calculate probably y equals zero instead they calculate all the numerators and normalize the reason that that works if you took probably y equals zero if you expand it using the law of total probability it basically says okay we're going to Loop over a again and for every possible value of a we're going to take one of these numerator Expressions you know the probability that y equals zero given that a equals each of the indexes in my for Loop times the prior of each of my indexes in the for Loop that's the thing that we're calculating the numerator every time through the for Loop so basically this normalization is a really really nice neat trick to skip having to calculate this and then just realizing that you can calculate all the numerators and then if you normalize it it will implicitly give you all this calculation for free because normalization will do the sum over this that gets you this term and it's going to divide every term by that sum wow do I care that you can use the normalization this really shorter version of code not so much do I care that you know Bayes theorem expanded yes but do I think that you need to know that these two things are equivalent absolutely because you're going to see code that looks like this and when you see code that looks like this I want you to know that they're calculating Bayes theorem properly yes is basically you're saying that they work out each of the numerators store it in an array and then the normalized function sums up exactly they were they work just to say what you said work out each of the numerators stored in Array and when you call normalizes it sums over all those which is that and it divides everything by that sum whoa so yes exactly you got it you spot on 100. yay hard Concepts one of the harder things that we'll do in this part of cs109 so you guys did all the hardest stuff now let's do some of the celebratory stuff and some of the celebratory stuff is now that you've mastered this hard Concept in two variables let's start expanding and just using your knowledge to be able to do more cool things learning goals for today I want you guys to get some perspective on the art form of Designing probabilistic models and it is art as much as it is a science of how you generate models when there's many random variables I'm going to teach you about this thing called correlations and covariances and then I'm going to talk a little bit about independence in random variables so we'll have a good conversation about key Concepts while learning a new ability let's talk about how to make a model there's two ways that you can represent a model their equivalent both of them describe the same thing they describe how many random variables are random together one way is to write a Python program I'll get back to this later you can write a model through python but it's easier to express it this way if you understand the mathematical way of expressing a model and one of the most important ways of expressing a model is this thing called a Bayesian Network named after our most favorite mathematician uh Reverend Thomas base and an abasian Network you're going to express complicated graphical model or complicated models of many random variables and you'll draw them out and then you'll give some probabilities and this will specify the whole model it's going to be great um I'll talk a little bit more about the joint distribution but let's go back to our motivating example our motivating example when we first started talking about probabilistic models is WebMD and as you call WebMD has the symptom checker in the symptom checker you can go and you list your symptoms and then it tells you your probability of having different diseases um and so to do wow the HDMI is really slow here so when you go through the symptom checker it keeps track of a whole bunch of random variables some of those random variables are symptoms that you could have so you have a whole bunch of symptoms but you also have some random variables for causes like actual diseases you could have so there's random variables for do you have diseases there's random variables for whether or not you have symptoms there's even random variables for things that they call you know prior conditions that can make you more likely to have diseases like maybe your uh your age or I'm going to use undergraduate for now just for a good time um maybe that could modulate probabilities as well and you have all these probabilities and they're working together and if you want to do proper inference if you want to tell somebody the probability of their fever you have to be able to think about all these random variables together the inference question is I tell you an assignment to some random variables and your job is update your belief in the other ones sometimes there's only one random variable that I care about you updating so for example I might tell you I'm nauseous and I'm tired and I want you to update your belief that I have the flu So You observe some things and you update some belief uh so here's one inference question I can say can you give me the probability that you have the flu given that you're nauseous and you're tired but that's not the only inference question oh and you know you could use chain rule you could use a couple different approaches to solve this I want to give you a sense that there's a lot of different inference questions like for example I could say I have fever and sore throat and I want you to update your belief that I have a cold oh I also want you to update your beliefs that I am an undergraduate you know like any random variable if I observe some I should be able to update my belief in any other random variable if they're in a model together so this is just that other question written in mathematical notation so c0 I think is for code that I have another C yes chest pain so c0 is cold equals one U equals one given that s equals zero and fever equals zero and again you could use chain rule or you could use Bayes theorem and start to approach this problem so if you have all the joint distribution my claim is that you can answer any of these inference problems but I just want to make sure that we're on the same page about this if there's nine random variables and they're all binary so they can take on zero or one if you try to represent this joint as a table you just had a probability table which said for any assignment of these random variables I will tell you the probability how big would that probability table have to be talk about what the person next to you but also with the person next to you see if you can come up with a question for what's confusing about this so that we can talk about it at this point talk about it with the person next to you let's come up with an answer foreign so that might not have been enough time but hopefully after a little bit of thought you got 2 to the power of n it's exponentially large in the number of random variables if you have one random variable you just need to store two probabilities well actually you can get away with one probability which is just the probability of that random variable being one if you have two random variable though you end up with a grid you have the two values that one random variable could be the two values that the other random variable could be and there's four values there technically you could remove one of them because you know they sum up to one so it's still you know 2 squared minus one if you have three random variables it will be two to the cubed I often don't think about this minus one because you could get away with not representing one random variable because you know they all sum to one but two to the three is normally High I represent if you have three random variables and if you have n random variables you're left with two to the N if you want to store it using a joint table we've learned about joint tables it's clearly not going to cut it here [Music] oh yeah so let's say you have um let's say you only have one random variable and that random variable contained on the value of zero or one so let's say I tell you the probability of 1 is 0.8 my claim is that there's always one value in your whole joint that you can just infer so here I can know that this probability is going to be 0.2 because you know I can just infer it from that it turns out if you had a grid um and let's make these numbers smaller 0.6 0.1 if I make this 0.2 this is six seven eight nine I know that this one has to be a 0.1 so I could get away without one that's such a it's such a detail but I'm glad you asked but you know long story short never in my life has this actually really changed anything it's because 2 to the power of n is so large minus one is like yeah whatever good question I wanted to motivate why we need models if you want to be able to do something like this if you want to write this web app you couldn't do it using a joint probability table you'd need a more sophisticated language for talking about how random variables can be random together so shall we learn this more sophisticated language yes okay okay uh yeah n could be large this more sophisticated language takes the name of Bayesian Networks and in Bayesian networks you don't have to give the Complete Joint you can leverage a hypothesis about how the world Works to come up with a structure where you can express the joint with fewer probabilities it's a beautiful thing I'm going to tell you this story of evasion Network through a simpler WebMD we'll call this the Vaden WebMD that's only got four random variables there's fever and tired I think of those as my symptoms and flu isn't a symptom flu is a disease you know how flu is different from fever flu may be causes a fever an undergraduate is just going to be like a pre-existing condition that might change the probability of seeing you know some of these symptoms so the great idea about a Bayesian Network is we're going to try and represent this without doing the exponential combination of all the probabilities now if there's only four random variables you might be able to get away with this exponential combination you might be able to represent the 16 2 to the fours equals 16 different assignments of all the four random variables jointly but we want to learn a more sophisticated thing because when I go beyond four random variables we know this is going to grow exponentially so here's what a Stanford flu expert would do they would try and describe the process of these random variables affecting each other using this thing called causality now I think I said this lightly I said flu causes fever in the real world we think that this random variable is the reason that this random variable would be one or zero even if it's not deterministic it really changes the probability like knowing flu is really affecting fever and we think of that as being the causal Direction if I tell you fever that would change your belief that I have a flu like if I told you I have a fever do you think I'm more likely to have a flu or not yeah so it changes your belief but I don't think it caused the flu I don't think the fever is what made me have the flu I think the flu made me have the fever so probabilities go both ways but causality we think goes in one direction do you guys buy this convention I don't think it's that controversial to say that a flu causes a fever and that a fever doesn't cause a flu someone out there is like but actually if you have a few of you you're more likely to get another condition at the same time but like that's way too complicated we're going with the simple version of the world so what a Stanford flu expert would do if they wanted to try and represent this joint probability with many random variables in a more compressed way they try and describe causality flu causes these two symptoms if you have a flu that's why we think you might have a fever and that's why we explain that you are tired similarly we think that it's the undergrad nature which is causing you to be tired and a flu expert would try and come up with a reasonable set of causalities that would explain how these random variables are causing one another now the world is a complicated thing there's lots of ways of telling this some people talk about like the butterfly effect where everything is somehow connected like somehow being undergrad really might be the thing that's causing you to have a fever causality is a complicated thing all causal models will be wrong in some way but if you can generally get the structure of causality you'll get a probabilistic model which is very close to correct and very very usable so step one describe the joint distribution using causality do you guys want to argue about my causality at all I welcome arguments there's uh we can have a debate about many of these things Will's like you definitely got that Arrow backwards yeah anyone wanna yes underground to get the flu from dorms so you're saying uh undergraduate actually change you know like you're getting the flu from dorms therefore it's the undergrad has a causal push to change whether or not you have a flu I agree with that I'm not going to model it yes fever makes you more tired yeah is making you more tired now I'm saying the flu causes the tiredness but it might be that the flu causes the fever and the fever causes the tiredness that's very reasonable I'm not going to model it that way but that's very reasonable yes um tiredness causes the flu because like immune systems are I very much agree with this if you're tired your immune system goes down and therefore we think of tiredness as causing the fever these are all so valid now if we got into it I think we could argue for almost links between any two pairs of variables because there might be some way that every variable is causing one another so our job isn't to get to all the details it's to get the broad Strokes of causality and so even though all those arguments are right these three arrows give us the broad strokes and I'm going to give you a key the more arrows you add the more your probabilistic model will start to grow exponentially if you have few arrows you'll grow closer to linearly and if you have lots of arrows you'll start to grow exponentially so I'm pretty reticent to add extra arrows such a good discussions yes let's go over the visualization again at the door distribution you mean of this one yes so in the every circle is a random variable and here I'm using lines to say that I'm hypothesizing that this random variable is causing this one's value oh um we're going to tell you I haven't told you how to express a joint using this yet I've just gotten you halfway through we got on step one but if you know step two and three we'll know how to express the joint but you're right that's your problem right now it's like I've told you this beautiful picture drawing technique I have not told you how to represent a joint which is what you need accurate every single if you could estimate every probability perfectly it would get more accurate but the more arrows I draw the harder it will be for you to estimate the many more probabilities you have to estimate and because of that it might get worse so it's actually a complicated answer and often that's kind of how you choose you might keep adding arrows until your accuracy starts to degrade there will be a peak Point there's a whole conversation about overfitting and underfitting which we can get into in 221 but that's the broad Strokes of the conversation yeah this is so cool I'm so honored to be able to show you Bayesian networks for the first time because it's uh I think one of the cooler pieces of Technologies in fact the people who worked on Bayesian networks have like won all the math Awards um and and all the big CS like uh touring Awards because this combination of mathematics and computer science was just explosive for the sorts of problems we could solve now I have told you how to draw a picture pretty picture I haven't told you how to define a joint if you want to join Define a joint using in Bayesian Network here is your next task once you've got all your arrows for every single random variable I want to give you the probability of the values it can take on given the assignment to all of its immediate parents so I've redrawn our random or our Bayesian Network over here when I say give me all the probabilities given its parents let's start with these two lovely nodes up here they have no parents in this graph I mean parents by the graphical term parents right there's no nodes that have arrows pointing into it therefore when I say give me the probability of this node given its parents I just mean give me the probability of this node not conditioned on anything and I can tell you the probability of a random person having a flu is 0.1 that's your job this note also doesn't have causal parents so if you want to specify give me the probability this random variable taking on the value 1 but not conditioned on its values of its parents because there's no parents here so what's the problem is somebody's a undergrad walking into Vaden and Vaden says 80 of the people we see are undergrads yes yeah these are I like to think of these ones at the top they look a lot like priors but they're all going to be probabilities um and yeah actually you can think of this as like we're going to describe the joint and you can kind of think about that describing your prior belief and then when you observe things like observist and undergrad I observe their fever then all your probabilities will change and that will give you a posterior so yes let's call that a prior so these two do you guys make sense if I tell you you have to provide the probability of them giving your parents these ones are pretty straightforward this one's a little harder because it's got a causal parent I have to give the probability of this random variable given its parents so I have to say what's the probably the fever equals one given flu equals zero so I have to say if flu is zero what's the chance of having a fever I also have to say if flu is one what's the chance of having a fever now notice I say for all the possible assignments to the parents and for each of them I just do the probably fever equals one I might care about the probably that fever equals zero how could I calculate the probably a fever equals zero if I know the probably a fever equals one you guys feel it in your bones I hear it murmuring in the air just one minus this you don't have to specify fever equals zero because if you specify fear equals one then if you tell me flu equals zero I can know probably fever equals zero by just doing one minus this how many parents does fever have just one all the assignments the parents are assigning a zero to the parent and assign one to a parent I have given you the probability of this random variable given all possible assignments to his parents oh I haven't actually put numbers here but I'd have to you know I don't want to get these different from what I have on the slides so I'll add those in a second either you have a fever or not I mean in the case that you want to sort of track on the radius which is be scary uh I intentionally made everything a Bernoulli to start with you are right that we are absolutely going to be able to expand this model so that fever could be something like low medium high or even it could be a real valued number there's lots of ways you can do this in fact in the elephant example we had a Bayesian Network but the thing over here was weight which was continuous so it's totally possible to represent something that's not Bernoulli but let's start with Bernoulli just because we're doing a whole bunch of random variables and that's a nice place to start good good good question okay so I'd have to give you two numbers here so zero point you know what's the probability that you have a fever given you don't have the flu Oh that's very low and if you do have the flu you might have a fever but you might not let's say there's like a 70 chance that you have a fever Yeah question most sophisticated model of its like space complexity Beijing Networks all right it's like like it like if we're trying to Model N random variables yeah uh is the Bayesian Network going to be the best model with oh whatever space complexity yeah well so the the you can imagine like how much space do you have for representing your probabilities I think yeah Bayesian Network is one of the best ways I've ever seen for representing a joint efficiently it's like the most compressed you could get that really comes down to if you are smart about your causality though like if as I said if you drew arrows between everything a Bayesian Network will degrade to a joint probability table okay so here I've given the probably this round variable and this random variable and this random variable given all their parents but I haven't given the probably this random variable given all the assignments to its parents it has two parents both of those parents can take on two values and therefore I have to give four numbers the probability of being tired give in all these four cases of assignments to his parents do you guys see how more arrows can make things exponential if this had three parents you would have two to the third if this had four parents you'd have two to the four things in this table so to specify a Bayesian or a Bayesian Network you first draw your arrows and then for every node you give the probability of the node given its parents wildness coolness powerful you're going to use this to do some neat things yes possibility would phase you oh yeah you can have like a probability of lots and lots of ways you can use base theorem you can use uh the chain rule lots of good ways you can calculate probabilities we'll go over that in a second okay maybe I'm building you guys up too slowly because you guys are getting ahead of me um we can talk about this implicit assumption now we've defined every random variable as the probability of itself given its parents so I'm just writing on the uh slides what you've got on the board here I do want to note that when you are um making a base net when you write these causalities you're doing something very sophisticated that you don't need to understand but if you did maybe that would be cool the sophisticated thing that you're doing is you're starting to talk about independencies just by drawing arrows it turns out that if this is the causal structure flu does not change your belief in undergraduate it tells me that if you change your belief in flu like if I tell you have a flu my belief that you're an undergraduate will not change and these sets of arrows are telling me that undergraduates independent of flu now as I said I'm going to leave it to a later class for you guys to do the complicated math of figuring out exactly what are all the independencies assumptions that are coming from the arrows if I want to explain where the compression is coming from the compression that you're going to get of being able to describe the joint in a lot fewer numbers is becoming is because there's implicit independencies that you get from causality basically by saying flu doesn't cause undergrad and undergrad doesn't cause flu you are going to gain some Independence and that allows you to express things more compactly one thing must be acyclic you can't have undergrad cause tired and tired cause undergrad it will completely break your whole model if you keep to that rule then you're just fine okay now my claim to you is this the Bayesian Network has this cool assumption if you give me all of these things I can reconstruct the joint the way I can reconstruct the joint it says instead of representing the joint as this whole honking probability you don't even have to do chain rule like if you wanted to try and calculate this maybe you could represent it you just from a joint table or you could have chain rule try and calculate it Bayes theorem says every time you get a random variable if you're trying to get a joint you just multiply that random variable's value given its parents what a crazy thing I think though this makes more sense when I show you a Bayesian random variable expressed through code can I show you that do you guys want to see a joint distribution of a Bayesian Network this one expressed in Python oh man Python's such an expressive thing so much more expressive than slides can be sometimes the WebMD okay I have my Bayesian Network and it's expressed as a code and particularly if you can write code that can draw a sample from The Joint distribution and by draw a sample I mean like when I run this program when I run this program it starts making fake students who either have the flu or undergraduates have a fever or are tired if you can write a program that can sample from the full joint assignment so you're giving a value for every single random variable together jointly that is an expression of the joint distribution for a Bayesian Network I'm going to start at the top and I'm going to sample each random variable and then if I get deeper in I'm going to use the values of its parents to update my belief of the future ones may I Okay so first of all we've got to start with fluent undergraduate those don't have parents so when I sample them I'm just going to sample from Bernoulli burn is a function that I wrote that just either returns a one or a zero this returns a one out of 0 with probability 0.8 and this returns a 1 over 0 with a probability of 0.1 so after I call these two functions I've decided does my current fake person have the flu as my current fake person in undergraduate after that I need to choose fever but when I choose fever I'm going to follow the Bayes theorem idea and when I choose fever its probability will be based on the probability of its parent or the value of its parents so I've chosen flu and if flu was chosen to be one then they have a high chance of having a fever if flu is chosen to be zero then they have a low chance of having a fever so at this point I've expressed a joints between these three random variables and I just need to be able to simulate this last one and then when I simulate it then I've been I've pulled one sample from The Joint probability when I choose tired it's more complicated because it depends on my previous choices for undergraduate and flu and there's four different combinations for undergraduate and flu and in each of those four different combinations I could have a different probability of whether or not somebody's tired they don't have to add up to one for example you know maybe it's guaranteed you're tired if you're an undergraduate with the flu um and you can still have positive probabilities and they don't add up to one and that's fine now at this point I've chosen all my variables and as such I've pulled from the joint and the claim of the Bayesian Network is that this is equivalent to if you had a joint probability table and you were pulling from a joint using a lookup into the joint probability table that was a lot so yes let's talk about it question what exactly is happening when you say choose I'm sampling so when I say choose fever I'm going to end up with four variables every time I make a fake person they're gonna have flu undergraduate fever tired they're all Bernoulli's they'll all be zeros or ones so when I say choose flu I mean for this particular sample for this particular person who I'm making a joint pull from their probability of the joint distribution I have to decide if they have the fever ah no this is like making a fake person and I some fake people will have fevers and some fake people won't so this is not inference you've got no information about a person some yeah yeah so so there's this is a good good thing to talk about I'm glad you asked when you're actually at Vaden and somebody walks in they will tell you some things they'll say I have a fever and I'm an undergraduate that's an inference question and in order to solve the inference question we need a different thing called a joint and the joint just says what's the probability of a complete assignment to the random variable so there are two things to keep separate the inference task of somebody's just walked in and they've told you they have a fever and they've told you they're an undergrad that's a separate thing from expressing what's the probability of these four random variables taking on any possible assignment and you need this one to solve this one we need the joint if we want to be able to solve the question of if somebody has a fever and they're an undergrad what's the probability they've got the flu such a good question so what I've got here is not doing inference it is sampling from the joint that leads to more questions yes is there something you have to do to calculate the familiar values for tired or are those given to I actually couldn't hear you it was a little too quiet there's something you have to do to calculate like the one your weight 0.9 0.1 like the Bernoulli value is required [Applause] how do people come up with those numbers is that what you're wondering yeah like did you get those numbers so this is such a great question anytime we've got probabilistic models there will be numbers and there's a great question where do those numbers come from for example where do these numbers come from the answer is I made those up based on a reasonable assumption and that's one of the places that probabilities can come from maybe an expert gives you a pretty reasonable guess a different way that you can get those numbers is from data you could say I've collected data on all the people who've come into Vaden and eight percent undergrads I've collected data on all the people who've come into Vaden and if I look at all the people who didn't have the flu and were undergraduates I can calculate what percentage had uh we're tired so you could calculate from data or sometimes experts make them up and that happens all the time and you should be wary of them cheeky experts yes a conditional table it's like condition on each of these values I'm going to tell you they're probably tired but it has a lot in common with a joint table so you know if if the parents have these values then what's the probability of tired so conditional but very similar and not just we're nearly returns one of two values zero one and it returns a one with the provided parameter so when you call Bernoulli this will give you back either a zero one and the chance of getting a one is point one and when you run this function when you run this hey wait that's not what I want you at all python sample now this is such a programmer's answer to how to express a joint I've got a joint if I can pull a whole human with assignment to all those four values and the assignment must be you know corresponding to the real likelihood of somebody having this particular assignment like the chance that I get not tired not fever not undergrad and not flu should correspond to How likely that particular assignment is and the Bayes net argument is that because I built a Bayes net and because I took each variable given its parents that this is the proper joint distribution it is implicitly making some independent assumptions because causality is not perfect here they will be wrong but this is going to be so useful so this is a wrong model but allow us to go from exponential space to express a probabilistic model to linear space to represent a problem signal so even though it's going to be a little bit wrong this is going to be such a good rep way of representing a joint boy have I said some complicated things but you know the the real complicated thing I've said here is that I told you how to make a base net it's simple draw your causality give the probability of every random variable given its parents and then my next claim is that you if you multiply the probability of every child given its parents you get the joint and or if you do a sample where you sample from the parents and then you sample from the children given the parents you get a sample from the joint and that's really the Bayes theorem assumption Bayes Nets tell what we call a generative story they say if you have a person with an assignment to all these four random variables here this generative way that you got that you started with these random variables and then you generated the children based on the assignments to the parents I really feel if you could choose one thing to focus on if you find this confusing I really do think the generative story of the code is really important it's saying this is how I think the world works I think if you were to generate a random person in this world you would generate in it in this way and that's why I call it a generative model we generate the parents then we generate the children condition on the parents and the generative model also has the probabilities of every random variable given its parents wow so much okay are you guys ready for this I have told you a lot of important things I've told you about this beautiful way of expressing a joint but that is basically useless if you can't do the cool task of inference and the cool task of inference says I want to change my belief uh given that I've seen some evidence we're gonna get to inference don't you want to get to inference because then it's like then the whole modeling was worth it in order to get to inference I first want you to be able to solve this task this is a joint assignment it's a probability but I'm asking for a particular assignment to all four of my random variables and I have a tricky task that's going to check your understanding can you tell me the probability of this particular joint assignment if you're stuck think about the generative story what is the probability that flu the person doesn't have a flu they are an undergraduate they don't have a fever and they are tired can you figure out that probability and this will be the first step towards that beautiful beautiful dream that we can do inference on any graphical model okay talk about what the person next to you let's see if we can figure out how to do this ask me lots of questions I love this conversation so okay updated factors hey Ellie foreign oh okay this is some heavy math but this is this is the hardest conceptual part before we get to inference side I do want to slow down and I want to answer any questions you may have so the Bayesian assumption says that if you want that whole joint you just take the probability of every random variable but if that random variable has causal farence you have to write it conditioned on the value of its causal appearance it's a generative story it's like first we figure out if somebody's an undergraduate what's the probability of somebody's an undergraduate yeah you guys are gonna help me out because I can't see the screen so just yell it out so give me a chance uh what's probably somebody has a flu yeah what's the problem they don't have a flu okay so this one's 0.9 so the chance that somebody's an undergraduate and they don't have a flu 's not saying the test says these two things multiply by each other that's what Independence looks like you know the probability of this particular assignment being equal to each of them that's independence now how about this one the probably that fever is equal to zero given that flu equals zero so what's the probably a fever equals one given that flu equals zero okay and then what's the problem that equals zero oh fantastic because it's just going to be one minus the probability that fever equals one now if undergrads equals one and flu equals zero what's the probability that tired equals one eight zero point eight okay good I was about to go have to look at my computer um the claim is that this joint distribution you can get by just multiplying these four probabilities together and this is saying you know this is the chance that you see all of these particular four random variables at the same time no matter which assignment to random variables I gave you you could continue to use the generative story and you get a probability of assignment to all the four random variables jointly question this is the probability that papers okay so let's look at fever's node oh wait did I get this wrong oh yeah yeah totally I didn't look at the screen okay fluid is equal to zero the problem you have a fever is 0.05 if you don't have the flu the chance you have a fever is very very low so what's the probability that you if you don't have the flu that you don't have a fever 0.95 I misheard you guys I heard 0.5 and so there should have been one minus 0.05 yeah probability yes what would it be a net so let's look one two three four five six seven eight we've represented it with eight numbers instead of 16 numbers it sounds like you've saved half the numbers but it's better than that you've gone from exponential to linear so if there is more than four random variables you go from like a billion values in your joint table to maybe like a hundred probabilities it's a number of the number of arrows it's based on the number of errors it's based on if you think of the child with the most arrows that's the one that's really going to Define it because it will have 2 to the power of how many parents it has the child with the most arrows yes another question on a table I started with the causal story I started with the nodes with the least currents and I took the probably of them given their non-existent parents and then I went down from the parents nodes with the least parents to the nodes with the most parents so the parent notes to the children notes yeah explain again how you calculate the probability of being tired um just because like it has two parents yeah so the the Bayes net idea is that we're saying tired is caused by fluid undergrad so that's why I need to know if I want to figure out somebody's tired and if you tell me that um flu is zero but undergraduates one so flu is zero but undergraduates won so if that's the assignment to its parents the chance that you're tired is 80 because even though you don't have a flu you're an undergraduate that makes us sleepy um and so that's the idea so the base net is not taking into account whether or not you have a fever so there's all these things that is not taking into account that would have implicitly been in the joint table um we're representing it in a much more compressed faction because we know that we're calculating the probability of all of those four things we do at the same time exactly the the idea of the basement is the big assumption is saying that the joint is equal to the product of all these probabilities that's and you can think of that as a Bayes net assumption it's true if the causality is perfect even though the causality is not always perfect Oh you mean like you need to someone has to give you these numbers yes yes yes absolutely and they come from one or two places experts or data you could use data to estimate them by the way if you have to estimate two to the N numbers that's hard if you have to estimate like o of n numbers that's a lot easier from a data set so you could use a data set and you could have filled in these numbers for me yeah so today if I'm using this uh Venetian strategy we only have to have like four terms and the other example we use a general like Joy probability table we'd have to have 16 terms what would those terms look like you would have a cell in your table for every assignment you'd have a selling table for this and you have a cell on the table for like if you flip this to a zero for any binary combination of the random variables you would have a cell in your table that's why it's such a disaster total disaster causalities implicitly imply independencies you don't have to know exactly which ones but that's why we're getting such reduction okay one more question then I do need to continue the conversation that's why we're using like the hand of microwave and yeah actually the joint is and The Joint says what's probably of this assignment to this random variable and this one to this other one and this one together that's what the joint's all about and that's the thing you would need to solve any probability question and speaking of which I'm Gonna Bump this up a notch for you guys oh man assume you have for any assignment you have a way to calculate the joint you now have the joint my claim is you can calculate any conditional probability are you guys ready what I really want to know is the inference problem the joint is just like Scholastic inference is real it solves people's problems I want to know the chance that I have a flu so what's the probability that I have the flu given I am not an undergraduate I am in fact much older though I might look charmingly young uh I am in fact a father so I am an undergraduate I'm not an undergrad um am I tired no I'm actually feeling kind of bouncy uh wait tired equals zero uh and I don't have a fever so this should be pretty low but inference would allow us to answer any question and my claim for you was no matter what question you have if you have a joint you can answer it may I just explain why let's first look at this using the definition of conditional probability if you use the definition of conditional probability it would say that this is equal to flu equals one my L's are are also a disaster just like joint probability tables ah l whatever no I can't use a capital as a subscript it feels wrong ah maybe I do fever whatever that's not important so definition conditional probability says it's the probability of all these things together divided by the probability of the thing on the right hand side do you remember when we first learned the conditional probability ah who would have known we would take it to the max like this that's an e so this is the definition of conditional probability luckily for us this thing is a joint could you calculate this probability you could use exactly the same method to get a joint could you use exactly the same method to get this term no because this term doesn't have flu but there is a tricky little idea that you guys might have in your head that would allow you to get this term where flu isn't existent and my key idea for you guys is I say think about two cases the case where all these things are true and somebody has the flu and the case where all these things are true and the person doesn't have the flu how could you get this one using joint assignments no no I want you guys to Let's meditate for a moment silence find your calm but not your sleepy calm you're like mindful calm okay does anyone see a way we could do this I'm looking for somebody yeah somebody hasn't spoken today yeah [Music] yes there's an even easier version of the laptop so there's two versions of lab tour probably one that's got the condition and one that's got the and the log told probably that has the and looks like this I'm going to try and write this without a sum notation though my heart really wants to use some notation I just think it's going to be easier I'm going to call this whole thing on the denominator happy symbol because I don't feel like writing it again are you guys okay with that so we've got ourselves our happy symbol divided by and we want to get an expression for this I'm going to call this whole set of equations my unhappy I don't know why it's unhappy it's really unhappy but I'm just going to do that because I don't want to write it again so it's the probability that flu equals one and all those other random variables plus the probability that flew equals zero and all those other random variables so all those three are unhappy all those ones are smiley face so this is the probably a smiley face oh my God what an emoji answer I feel like the most Millennial person right now so the law of toad probably has two versions you can say that if you want to get this thing there's a background process of whether or not somebody has flu and the probability of this is you know the chance of that assignment in both the cases where somebody has a flu and somebody doesn't have the flu and if you add up those two mutually exclusive cases you will get the probability that somebody just has this assignment of random variables and now an interesting thing has happened we have ended up with joint assignments in this expression I've got a value for every random variable in this expression I've got a value for every random variable in which case I can use this process to get them there's a fancy term for this it's called marginalization marginalization was easy to think about when we had two random variables we get to more random variables it gets a little bit harder but marginalization says you know if you want the probability if I want to write this out the way I would have liked to using some notation without smiley faces I would have said that this is equal to the probability that U equals zero uh T equals zero and fever equals zero is equal to the sum over all the assignments to flu the probability that flu equals the current thing in my for Loop and all of these things so U equals zero tired equals zero fever equals zero and those are two different pulls from The Joint distribution because of marginalization everything can be solved as a computation starting from the joint if somebody gives you the joint you can calculate your numerator and your denominator of any inference task that was a lot okay so what have we talked about first I presented a problem you want to represent many random variables but you can't using a joint probability table you want to be able to do inference but you need a joint but the joint probability table is not working then we gave you Bayesian networks it's a very compressed way of representing a joint and once you have Bayesian networks because you have a way of representing the joint anytime you have a conditional probability you can solve it by starting with the joint and marginalizing out all the things that you need to and there you have it we have a full story we now have Bayesian networks and they're salt useful for solving any probability question oh let's take a pedagogical pause if people have questions come you can ask me but let's take two minutes because that was a whole bunch to cover yeah why don't you come up to the front before the Bayesian Network if it's not Independence in random variables we have known Independence for a long time if I told you two events do you guys remember the day of events ah so humble it was like the Shire before we went on our big adventure events okay but in events world if you want to claim that two events were independent you could show it mathematically you could say that the probability of the two events happening together is equal to probably of one event times probably the other event and that was really the definition of independence uh though there was a equivalent version of chain rule if two different variable or two events are not independent they're called dependent and intuitively aside from all the mathematics if you tell me that one of independent one event's independent another it doesn't give you information in the world of random variables the exact same logic holds because as we know an assignment to a random variable is an event that's an event and that's an event so Independence is going to look exactly the same but the one increase in terminology is that if you claim two random variables are independent then this equation has to hold for any chosen assignment to values so whatever you choose for a little X and whatever you choose for little y this equation must hold if the random variables are independent so it's a much stronger claim it's saying knowing anything about this random variable will tell you nothing about this other random variable it's a much deeper claim but at the end of the day it's the exact same equation that we've known lived and loved already so let's go back to our wonderful friends after all this time still so good to us dice let's say you have dice 1 and dice two are the outcome of two rules and S is the sum so the first question I could ask you is are events that the dice one equals one and the sum of two dice equals seven independent now we looked at this uh earlier and if you figure out what's the probability that the first dice equals one well that's one over six what's the probability that the sum of two dice equals seven well it turns out there's six ways that you can make that happen out of 36 so that's one over six and what's the probability that the first nicest one and the sum is seven well that means that the first ice was one second dice is six and there's one over thirty six ways of doing that we did this earlier in class here's another event that we looked at in class we said if I tell you the first dice is a one and the sum is five are those two events independent and the answer was no if I told you the first ice was one it changes your belief that the sum will be five because if I told you the first X was six you know the sum can't be five for example so if I tell you the first ice is one all of a sudden you think okay it's more likely that maybe the two dice add up to five seven is the weirdest number like why is this independent why is knowing the value of the first ice independent of the sum being seven and that's this question and it turns out it is independent no matter um oh so basically these random variables are not independent but um this first ice being one and S being seven the reason it's independent is if you care about knowing if your sum is seven no matter what you get on the first dice there's exactly one thing on the second dice that leads to the two being seven anyways these since this is true and this is not true it's not true that the two random variables are independent for the two random variables to be independent all of these questions would have to be check marks for any assignment to D1 and S stronger claim as I said okay and this might lead to the next question and this is something we'll do if we just have a little bit of time but what if you cared about discovering causality just from data so right now we have this idea of problems models we have a way to do inference if you have problems models what you don't really have is where do those probabilities come from and how could I design the probability Network myself do you guys want to learn that okay what I have for you is a challenge I have all this data of all these kids in the UK and they're rating one to five whether or not they like all these different genres and there's like 30 different genres so you can imagine a probabilistic model of 30 different types of music all together and we could go and make a base net we definitely need something like a Bayes net because 2 to 30 is too large if you want to talk about joint probabilities so if you want to say something like was it probably somebody likes folk given that like classical any general inference we'd need a joint so we've talked about finding independencies before in class when we had the bat DNA problem um we had data like this and we could search for independencies and independencies are going to give you some hints as to how you want to draw your causality because if it tells you know like if you get your data and it says fluids independent of undergraduate then great maybe you start to make both of those your parents the bat data problem we did you guys could figure out independencies but I have a different question for you what if they weren't booleans what if there were continuous numbers how could you find independencies of random variables that take on not just true or false values so how do you find independencies in random variables not just independencies and events the bats were events now I've bumped this up to random variables and it really starts with this question here is a picture of two random variables that look independent and here's a picture of two random variables they're very much not independent and my question is what are you curious about what do you wonder and what do you notice about these two different pictures yeah I mean this one looks kind of splotchy and this one kind of is like looks a little bit like this diagonally thing that feels good yeah I told you value of a number here you could kind of have a function that could take that x value and give you a guess for the Y value whereas if I tell you the x value here you're like I don't know what your y value is your x value really doesn't tell me that much about what y value I'd predict and that's really the idea like in this picture these things look quite independent one random variable is not telling you about the other one and this one one random variable is very much telling you about another one now how about this one looks kind of like the same picture but I've shifted things but my question is semantically is the same story true here if I told you about the x value you know one value of one random variable could you predict the other one pretty well yeah you definitely update your belief if I told you the x value here would you update your belief on the Y it doesn't matter that I shifted it it's the same thing these ones look quite independent and that one looks like there's a little bit more of a relationship there is a numerical quantification for how much two random variables vary together it's like a beautiful dance you imagine you have two random variables and when one goes one way does the other one also go the same way and one one goes a different direction does the other one follow it this measure of whether or not two random variables vary together looks like this if you take any point you can say how far is this point deviating from the X's expectation and how far is it deviating from the Y's expectation and you can product those two things together and if you product those two things together it has this wonderful property that if the two points are deviating in the same direction then you'll get a positive number like if they're both negative if x is below its mean and Y is below its mean then you'll get a negative number times a negative number and that will give you a positive number and people have decided that this is a really good way of calculating numerically a simple statistic to say are my two random variables being random together is one of them telling me some information about the other now if you took other points like this point close to the middle if you calculate this term it would be close to zero and if you take this point down here if you calculate this term again it would be positive because uh the x minus this expectation is negative and the Y minus it's an expectation is negative this term if you calculate in expectation over your two random variables gives you something that you might have heard of before called the covariance and the covariance says that like if I find points where both of them are deviating from their mean in the same direction then or it'll give me like some positive product and if I find points where things are deviating in negative directions it'll give me a negative product if you took all those positive negatives and averaged it for this plot you get something close to a zero there's some positives there's some negatives but they generally cancel out but if you take something um I want another picture okay well if you took a plot and it just had points on this diagonal then all the points would either be contributing positive values or zero so in expectation you would get something positive and in fact um if X if you take a point and X and Y are both above their mean you get a positive number if X and Y are both mean you get a positive number but if they differ like if one's above the mean the other one's below the mean you get a negative number the covariance is the expectation of all these products over the whole random variables and if you calculate this then you would get what we call the covariance it's a measure of how much two random variables vary together now that covariance expression is hard to calculate it turns out it's numerically equal to this there is a whole derivation of that which you can check out in your own time I want to include it here for complete lists but I want to give you a sense of how you would calculate this the way you would calculate this is you know the covariance formula at the end of the day you could write it like that or you can write it as this expression this is how we often calculate it it turns out this measure of how much these two things vary together is the expectation of the product minus the expectation of each random variable multiplied together so if we took this data we calculate the expectation of one random variable we can calculate the expectation of the other random variable and then we can calculate the expectation of the product to calculate the expectation of product we just take the products over all the points and calculate its average and if you did this you would get a way of calculating the covariance what I'd like to show you now is what it looks like if you look at the covariance of all of those bats sorry not the bats the music this is the covariance for all of the different music tastes that I told you I wanted to challenge you to model if you look at somebody who likes Punk it has a very high covariance with rock if you look like somebody who likes a metal that also has a very high covariance with rock but it turns out like if you look at metal or hard rock it has a very negative covariance with pop so if you tell me somebody is very positive on the metal scale I think they're more likely to be negative on the pop scale it's a simple statistic it's not telling you a complete causal story but it's giving you a sense of which random variables are varying together my clue for you is this is a little bit beyond 109 but if you want to come up with causality from data this is the first step you take all your random variables and you look at how much they co-vary and from this covariance you try and find structures and you try and have the art form of Designing how you think the world works it is an art form but it will lead to something that is representable by a computer a Bayesian Network what a beautiful thing that was so cool you guys asked such amazing question I'm really appreciating you guys thank you for all the hard work you do come back on Friday we'll continue this conversation and learn rejection sampling uh thank you guys and see you guys later