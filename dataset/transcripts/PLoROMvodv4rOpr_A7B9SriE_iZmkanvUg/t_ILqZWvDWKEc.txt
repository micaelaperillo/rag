good afternoon cs109 how are you guys doing fantastic um hey welcome back from the break I hope you guys had a wonderful Thanksgiving I hope you had a chance for some rest some relaxation uh possibly some good food and some family time uh and it's really wonderful to see you I of course in the holiday spirit took some time to think about the things I'm grateful for and how lucky am I to be here doing the job that I love the most which is teaching with such a wonderful group I am getting a little sad that we're getting close to the end of the quarter I'm like ah I will miss you guys anyways that's that's not what's important right now okay it's been a while so I thought it'd be nice to just take a moment and recap where we left off before we jump into today's great topic in cs1 I'm wearing this final part of the class where we're learning about machine learning it is going to take us to the point where we're going to learn how deep learning AKA neural networks work but in order to understand that you need to know the most core classification algorithms naive base and logistic regression and those two algorithms like all of deep learning rest upon a foundation of parameter estimation if you have any probabilistic model how could you estimate the numbers that could make the probabilistic model accurate or inaccurate machine learning when we talk about it in cs109 it's generally a pretty simple process this is the process of you're going to be building a model which we sometimes think about as these black box models where somebody could give you inputs features for a particular individual your black boss module will do some work and it'll come up with a prediction and the work that the Black Box module does will be based on some numbers some key numbers that we call parameters and a lot of work and machine learning is setting these parameters so that your black box is able to make good predictions particularly in cs109 we've been talking about a specific prediction type called classification and classification is where you're not just predicting anything you're predicting discrete class labels like for example one or zero will somebody have a healthy heart one or zero will somebody like a movie or one or zero somebody coming from a particular ancestry the most interesting part of machine learning comes in what we call training so in the first step you formalize a problem you take a real world problem you make a model you come up with a formal model that has parameters but the interesting thing is when you take data and use those datas to learn the parameters uh the once you've done that though it is worth noting that it is beholden upon you to try and estimate how good your algorithm is so we will reserve some data for testing and then we can say we've got an algorithm it's trained and this is how accurate we think it is training data awesome it has this crazy notation in order to gain intelligence for classification tasks I'm going to give you a bunch of data we call that data training data and training data is going to be a combination of features and you're going to be told the label for this particular individual so you say in my training data here's one individual they've got these features and a one or zero for their class label this is generic it works regardless of what problem you're talking about but it does have some notation I don't want to lose people on this superscript is saying which individual we're talking about the X are the inputs aka the features and the Y's are the labels m is the size of the features so how many inputs do I have per individual I found that notation quite hard so I spent a little time in class making sure that we understood uh what that notation was and particularly you know if you have an individual we think of the X's as being the list of numbers for the inputs and they'll be m of those and the why being the prediction of healthier not healthy in training data you're told both X and Y and the real interesting thing is to build a machine where you could just put in inputs and it would predict the why okay maybe questions about you know this general idea of classification before we jump into the one algorithm we've got so far naive Bayes well just before the break we talked about an algorithm how you could do classification we said okay if you want to build this black box we're just going to take some inputs and make a prediction one way you could do that is you could make a little probabilistic model and that probabilistic model is going to be able to compute what's the probability that y equals one given X and the probability that y equals zero given X and then we're either going to choose our prediction to be one or zero classification has to give back one over zero based on this probability very reasonable thing to do and particularly you know imagine this if you have this probability we'll try y equals zero and we'll try y equals one and whichever one is the larger value that's the prediction we'll make that's why this ARG Max exists just so we can turn a probability into either a zero or a one naive Bayes though while this was a nice idea didn't work we couldn't make a probabilistic model that would generally work so we made this awfully incorrect but very helpful assumption that the probability for any individual of their inputs given their output is equal to the probability of each input on its own given the output so this is a whole Vector of numbers so it's the probability of X1 and X2 and X3 and X4 given y we're going to say that's going to be the product of each feature we assume the features are independent of each other given the class label now this was the derivation and the naive Bayes assumption shows up right here but importantly you know if you make this assumption then you end up with a way to make your prediction we use log probabilities because numerical making sure it works on the computer is very important and the product a bunch of probabilities could become quite small and you know we do this derivation it leads to this very simple way of how we can make our predictions all these things are learnable you are now tasked with based on your training data learning this probability the probability that y equals zero and then probably that y equals one you also have to learn each of these probabilities for every feature what's the probability that each feature takes on its value given y equals either zero or one those are all very very doable things particularly if you want to learn each of these things is just going to be counting mle says these are Bernoulli's so we can estimate these Bernoulli's base the probability P of these Bernoulli's just counting then we went really deep into this map this Bayesian way of parameter estimation and it just ended up if you had a LaPlace plier of being counting with a little bit of addition so long story short we have an algorithm it's called naive Bayes it uses this awful assumption but it turns things into just counting so training just becomes counting and then predictions just require you to put your counting through the particular function that we derived so at this point we have an algorithm which is fantastic but there are other algorithms it turns out there's other outcomes saying you absolutely should know because some of these other algorithms have changed the way that the whole world works of artificial intelligence and we're going to learn about that today but a little bit more background that's worth recalling is optimization when we were talking about parameter estimation one of the great ideas that we came up with was if you want to find parameters that maximize likelihood you're actually doing an optimization task you're saying I'm going to want you to choose parameters for me and I want those parameters to be such that the likelihood is as large as possible and we figured out that this was kind of like an optimization task and in order to do that we said you know we can use hill climbing particularly we could use gradient ascent and gradient scent tells you that if you can give me the derivative of the thing you care about with respect to each of your movable dials if you give me those derivatives then I've got a very simple algorithm to come up with very good parameters so a little bit of recall gradient ascent gradient and scent gradient descent are brother and sister algorithms one could just be optimizing the negative of the other and just to recall you have a way of optimizing parameters for a likelihood function and is gradient ascent okay so this wasn't that important okay and review that was a lot but feel free to ask questions if you forgot something and you want me to recall it this is supposed to be a conversation in fact what you're learning today you're going to have to program in your problem sets and it's something crazy important for the world so I want you guys to understand it deeply I would like to invite you to ask the questions you're curious about because other people in class will be curious about it as well it's time one of my favorite things to teach logistic regression a very simple algorithm for making a classification prediction that is going to be very impactful because it is in fact the heart and soul of deep learning if you've heard of neural networks or deep learning it is a bunch of logistic regressions put on top of each other so learn logistic regression it is critically important in order to understand logistic regression we are going to be taking advantage of some of the parameter estimation we've seen so far I know I just gave you a lot of review I'm just gonna give you a tiny bit more mathematical background because I don't want to lose people on some prerequisites that you might not have seen before well this is less of a prerequisite but it's just more of a cool thing to know there is this function that people in AI love it's called the sigmoid function which is so confusing why is it so confusing because we've seen sigmoids before when have you seen sigmoids yeah the variance of say like a normal distribution hey you guys it's an interesting day before pandemic I had this policy of bringing fruit to class and if people ask questions if they're interacted I would give out fruits and then you know the whole world stopped with covid and you couldn't through like fruits through Zoom so I stopped and then we came back but you know just giving fruits didn't feel right when we were all so worried um but I have fruits again for the first time in like three years this is a special day so anyways ask a question get a little Mandarin you can eat after class share with your friends as you like anyways as we last left off sigmoid is a confusing name because it means a very different thing this symbol is also used for variants the sigmoid function is a totally different thing the sigmoid function is just a way of writing this whole piece of math in a condensed way and this whole piece of math is saying take some input and take one divided by 1 plus e to the power of negative that input if you were to graph it for all the different possible inputs it looks like this and people in Ai and probability love this function for two reasons one reason is it's a squashing function no matter what number you put into this your output will be between zero and one and I don't want to give things away but people in probability love that because if you can guarantee that your output's between zero and one then it starts to look like something that maybe you could call a probability anyways this is just a function you should know we haven't used it for anything too deep I have written the function itself over here on the board uh caution not the same Sigma that you learned about when you're a weak kid in the first half of cs109 our background before we jump into things is some key notation you've probably seen this in like math 51 or something like that before which is the transpose of two vectors if Theta is a vector and X is a vector and they're of the same length Theta transpose X is another way of writing this sum it's saying take the first element of both of these vectors multiply them together then add that to the product of the second element and add that to the product or the third element and the product of the nth element so this is really what that is saying but you can imagine these three are three different pieces of notation for writing the exact same thing it is saying in element wise product and then the sum over the sum over the element wise product again I wrote that over here you know Theta transpose X is this sum you'll notice on the board I actually have this starting at zero not one it depends how you index your list but I'm going to start indexing by zero in a little bit which you'll see when we get there any questions on this notation don't get lost on Note would you want to get lost in background anything confusing or what is confusing about this just want a mandarin anybody just want a mandarin yeah I just want to make sure I clarify is the is Theta a matrix or a vector it's going to be a vector so if Theta is a vector and X is a vector so X could be like for example this is generally true if x is a vector but maybe it could be a list of features okay and we're going to try something because I'm afraid of breaking a camera we're going to pass this back okay fantastic now if you want to be super fancy when you do this you end up with this big calculation but if you were to actually get a computer out and evaluate this this would be a number and because this is a number you can put it into that function so you can take this calculation take the number that results and put it through the sigmoid notationally if you did that you could say that's the sigmoid of theta transpose X which would be the same as a sigmoid as that or the sigmoid of that these are three different ways of writing the same thing or you could just put that number into the sigmoid itself notation but that's just into any point of our derivations okay so background I put on the board keep it in mind and if you have questions later ask me oh the chain rule man I remember when I learned this in math and it wasn't motivated at all we're like just going to learn calculus and because we're learning calculus you needed to know the chain rule so you could do the things that were on the final I wish my math teacher had told me this was the idea that would mathematically change the world like the chain rule is the reason that we can stack things that are derivable on top of each other and have neural networks learn through the whole thing because computers can do the chain rule it's crazy important but anyways the chain rule says you know if you have some function and you can decompose it uh you could say if f of x you want to drive it with respect to X if you can decompose this into saying like f is actually some other function Z of X you can do F of Z with respect to Z so you know reference how this function changes with respect to its input and then derive F of Z or Z with respect to X just want to recall this but I think if you haven't seen chain rule a please do watch your favorite Khan Academy video on the Chain Rule and second I will show you it in action and you should particularly make sure you understand chain rule in the context I talk about it today's class okay I'm so excited we've done our hard work we've done a review we've done our background it's time to have a big party and learn about logistic regression chapter one the big picture we want to invent the next classification algorithm and classification you're gonna be building a machine where you put in inputs and it makes a prediction and we decided that a really good way to build this machine would be if we could calculate the probability of the output taking on the value 1 or 0 given the inputs if we could know that conditional probability we could make a really great machine naive Bayes tried to do this and in order to do this it made this really big assumption you know the probability of each feature given y being the product of each feature on its own given y it's helpful it made the math really simple but it's definitely wrong in some cases it's not always true that the features would be independent of each other given the output but the simple idea that is going to drive all of today is a bit of a light bulb moment what if we just allow this to be a bit more of a machine that we construct what if we just build a machine that could directly figure out the probability of Y given X and that was the simple light bulb idea that lentil logistic regression the simple idea was okay we've got our X's that's going to be a list of ones and zeros we have to predict our y That's either going to be a one or a zero and the simple idea of logistic regression is well what if we're just constructed a machine that took those x's and predicted Y and we allowed that machine to be you know a little bit more flexible than say something that had to be a naive Bayes model and particularly people started using this one specific machine they said okay I'm going to build a machine where I'm going to take each of these numbers my machine is going to have a weight for each number I'm going to weight each number sum them all up sum up all those weighted things and then I'm going to get a number which I'll squash and I'm going to call the squash thing the probability that y equals 1. craziness let me give you that picture in a little more detail logistic regression assumption is I'm just going to build you a machine that's got some parameters in it that are movable you're going to choose great values of parameters such that your machine when it takes in X's just outputs things that are close to the probability that y equals one given those particular inputs here I I think this machine is so important to understand it's one of the most important machineries to get your head around so I spent some time trying to make this machine look a little prettier than just that equation people said okay you're going to take your inputs and you're going to predict whether or not y equals one so you want the probability that y equals one given your inputs we're gonna have a probability producing machine take all your inputs and every input is going to go through this channel where it gets weighted it's going to be weighted by parameters and then every channel once it gets weighted is going to come into big summation unit we're going to weight each Channel and then sum them together you could call this sum Z if you wanted now when you take in inputs and you weight them and you sum them together there's no guarantee that this looks anything like a probability so then what we're going to do is we're going to put it through a squashing function and this squashing function will take this number and make it look like a probability and then the craziest thing is we're going to interpret that as the probability that y equals 1. craziness I use this metaphor of a soundboard where each of your parameters are movable dials and you can move them to change the thetas and that would change how your machine works okay in my cartoon just to be clear these are the inputs you know you could have a everything you're making prediction of will come with a list of inputs and we're just going to set the machine to start out with whatever those inputs are if we're making a prediction for example if you're predicting whether or not somebody likes a movie the inputs could be yes or no did they like other movies the output is also in this case going to be a one or a zero and it's going to be yes or no do they like the target movie and we would like it so that this machine when you take an individual and you set whether or not they like these three different movies once it's weighted those ones and zeros sum them up put it through its quashing function we would like the probability that comes out to be as close as possible to the true probability that this individual would like that movie can I be clear about something is this how the world really works is there something true about the universe like when probabilities are being produced it actually underneath the hood the universe is making this little machine and you know pushing Pro these inputs through these weights and then squashing it no there's nothing realistic about this it's wrong why would we use a wrong model because it turns out this wrong model is very useful so we're going to make this little machine and the most beautiful thing about this machine is it's going to end up being a Lego block and we'll be able to stack them on top of each other but I'm getting ahead of myself yes question I'm proud good question I haven't told you yet but it's going to be really important if I put random thetas I've got a random machine but my claim for you is if some you know oracle came and gave you the best setting of thetas then maybe this could be pretty reasonable but your question is where did they come from it's going to come from parameter estimation but but I haven't told you that don't you know here you go okay question question uh [Music] so yeah this washing function is going to give you a number between zero and one and yeah we are going to say like if it gives you a value of 0.8 we're going to assume that's that's the problem that y equals one if the probability that y equals one is greater than 0.5 we're just going to predict a one okay yeah he's like her Sigma function it's kind of just an arbitrary Choice like if you want to get a number between zero and one like I might have just taken the average of like so isn't our result heavily dependent on the arbitrary choice of the squashing function yes okay next question no I'm just joking okay you say like hey I could have come up with a different squashing function that could make this thing that's real valued and look make it look like a probability and people have you know sigmoid actually it's not old school but there's other different squashing functions that people now consider using and some you know neural networks will use different squashing functions other than a sigmoid so you're right it was a bit arbitrary but boy does it work do we consider different excise or whether Y is one or zero um you know the the meaning of X i's are going to stay the same like X3 will always be whether or not somebody likes Pulp Fiction in this case and the values of those X I's will depend on the individual that you're making the prediction for so if an individual comes I will put all their values of what they like for this movie and then I will predict the why so I wouldn't say that the X's depend on the Y sorry the Y the X's depend on the Y's rather I would say in our machine you set the x's and we predict the Y's and I'm afraid of throwing an orange that far so please come after class and grab your your Mandarin okay so we've got inputs we've got weights AKA parameters we have the weighted sum this will be a number where every input has been weighted we add them together get the weighted sum at this point we've calculated you know this the sum of all the inputs weighted by the particular weight added together this is a number at this point and then we squash that number and we call the resulting of the squash a probability we then make our prediction and this is it this is the logistic regression model AKA assumption now a great question was where did you get those parameters from the parameters really matter if I gave you different parameters you would probably end up with different predictions similarly if you change the inputs if you have the model if different people show up and they have different tastes like somebody doesn't like the Patriot that can also change the output of this model so this model is based on two things the input of the individual you're making the prediction for and the parameters that have been set the parameters are generally set for everybody whereas each individual input will have different what we call features different X's Okay so putting that back into the language of mathematics logistic irrigation makes this assumption the problem that y equals one given all the features is going to be sigmoid of Z and in that sigmoid of Z we're going to add up the weighted sum of all the features oh wait but there's this one extra number oh man let's go back if a user likes these three movies I say okay Independence Day we set that to zero Patriot they like it so we set it to one Pulp Fiction they like it so we set it to one what is going on with that x0 it doesn't correspond to a movie and it's always set to one huh that's a bit of a mystery here's what that mystery is about people realized this model does way better when it's allowed to have a particular parameter which can offset the results of this uh equation so if you have this weighted sum people like to have a parameter that you could just add to it for Simplicity what we would do is we would take the inputs the features and we'd always set a zeroth value and we would always set it to one because if you set a zeroth value to 1 then your weighted sum just becomes awaited sum where the first input will be a one times by Theta zero and 1 times Theta 0 is just going to give you Theta zero it just allows you to add in this intercept this is a detail obviously it's not that critical to the whole plot line but this detail is going to be critical when you actually have to go code this it makes a big big difference in whether or not this model works so the detail here is yes we're going to make this assumption and it's going to allow us to calculate this number which is the weighted sum of each input multiplied by a feature we're going to squash that but before we squash it we're going to add in some other Theta so there's a bunch of parameters in this model and one of them just corresponds to what we call the offset to make this easy in your code you're just going to take every input and you're going to make add in a one before it and then that one will be multiplied by a Theta and that will make this the offset okay I think that's a little bit confusing so somebody could ask me a question that would be wonderful yes would be true yeah it is a bit of a base case you know another terminology for exactly the same intuition is it kind of biases the machine to either predicting more people liking the movie or not liking the movie like let's say your output is a super popular movie you might want to bias it so that you predict that people like this movie more often than not and so you can think of a bit like a base case if everything else is zero this is going to be influencing the movie a lot and generally this can either make you more likely to predict one or less likely come back and get your Mandarin after class it's been a while I'm not so confident in my throwing long distance okay so anyways one thing I haven't told you this model predicts the probability that y equals one where is the model that predicts the probability that y equals zero you guys are far enough into probability that you won't be surprised for me to tell you you don't need another model if you've got a model that predicts y equals one and if you want to know what's the probability that the predictions you know that the output class is a zero you can just take advantage of the class that probably that y equals zero given X Plus probably the Y equals one given X should be one so if you want the probability that y equals zero if that was your assumption for what's the problem that y equals one probably y equals zero is just going to be one minus that okay moving and grooving and it all just comes down to this this is the plot line so far logistic regression gives us this assumption that you can figure out the probability that your output being taken on the value one can be calculated using this simple mathematical or computer programmed function the sigmoid of the weighted sum of all of our inputs sigmoid function looks like this and I think some people started to Intuit this but I want to be very clear in this model we're going to do the sigmoid of this weighted sum so it's worth understanding the sigmoid function and one thing knowing about the sigmoid function is its input's always a number if that input is positive so the thing that goes into the sigmoid is greater than zero then the output is going to be greater than 0.5 so if Z is greater than zero sigmoid of Z is greater than 0.5 if Sigma a z is interpreted as the probability that y equals one as soon as the input to the sigmoid gets positive we'll predict that this is a 1. as soon as the input to the sigmoid is negative we're going to predict zero why because if the input to the sigmoid the thing that goes in here is negative then the result of the sigmoid will be less than 0.5 and if the probability that y equals 1 is less than 0.5 we're going to be predicting the other class label which is zero again questions comments concerns as I said this is how we're going to Define one neuron in a neural network so it's worth getting your head around so if you have to wait some um why do you even need to put into the sigmoid function if you already know whether or not it's going to be above or below 0.5 such a good question so just to say that back to you you're like hey if I want to actually program this thing up could I just get this weighted sum and then just check if it's positive forget the rest and if it's positive I predict a 1 if it's negative I predicted to zero you're absolutely right for the prediction but when it comes to the other part training this theory is going to be very helpful because it will tell us how we can update um and you know even though you're right you could also still use this mechanism if you want to tell somebody not just a one or a zero but you say this is a one with probably 0.99 versus this is a one with probability you know like 0.55 so you might want to give more information to the user where is this still useful and also for the other Theory we're going to need this close no one was hurt for people watching online yes question on each Theta times x i in some data no I don't think that would give you the same answer yeah if this thing the sigmoid option to get really mathy is this thing called non-linear so you can't do a sigmoid of each Theta J times x j and then add those up that would give you a different number yeah good question that turns out to be a really really useful thing when you start putting these together so what is now annoying will later be useful question so just zooming out a little bit it seems like this method is for the most part better than the base because it because it makes an assumption that doesn't sound this is crazy okay um is that usually the case or does it really depend on the situation and so how will you know which one is better okay good good good good good so the question is hey now you've Bae has made this assumption that we thought was crazy and this feels better because the Assumption isn't as crazy which one is better it turns out in most cases where you're either using logistic regression or naive Bayes they're often pretty similar and it turns out the decision is more often defined by do you have things like continuous inputs like when you start to break out of binary values then one tends to be more useful than the other but right now if everything is binary they look very similar um the naive Bayes assumption generally is pretty good why wouldn't this be as good like why wouldn't this just like blow the pants off of naive Bayes it's still making assumption like this isn't the naive Bayes assumption but it's a pretty simplistic assumptions like probabilities in the world are nuanced maybe there is much more complexity that goes on to how y ends up being one and it's not just this simple linear sum thrown through a squashing function good question yes that's right yeah be able to be negative yeah the weights can be negative so the question is like how could you get a negative value if these thetas are negative then you can end up with a negative value and the thetas absolutely can be negative they're not constrained in any way they could be anything from negative Infinity to positive Infinity what a good question let's actually write that let's say our thetas so Theta J is going to be an element of any real number so it could be negative Infinity to positive Infinity okay a little bit more exposition not linear uh regression but linear regression is doing a different thing than classification right now we're predicting ones and zeros regression is generally what we call it when you predict real numbers classification is when you predict something like you know Y is a one or Y is a zero or why is a discrete value linear regression is a name for an algorithm that does regression I love it naive Bayes is the algorithm for the first classification that we've seen so far and I love this question it's using Bayes theorem it's making this really naive assumption it's all beautifully encapsulated in this name foreign the name of this algorithm is logistic regression and I think it's an awesome algorithm I think it is an awful name why is an awful name first of all it's not doing regression it's doing classification you know people called it regression because technically it's regressing into the real value that is the probability but I think that's very confusing and instead of using the term logistic logistic is technically a parent name for a sigmoid I would have called it Chris would have called it the sigmoid classification algorithm so I just want to point this out that it's not a very good name um it's uh really not doing regression it is doing classification but of course it's not that important right now what is important is the final mystery if you understand this model you're most of the way there you understand what logistic regression is doing but now you need to peel back to the next level of detail you need to understand under the hood this next most complicated mechanism which is where does the intelligence come from and how could you make your logistic regression algorithm as smart as possible what a mystery let's Dive In the intelligence of a logistic regression model comes from its thetas if you took a logistic regression model and you gave me random thetas it would be awful at making predictions I'd have no reason to trust it but if you gave me perfect thetas then maybe you could start to do a good job the intelligence lives in those thetas which begs the question how are we going to get good values of theta and I mentioned this earlier but the answer is we have a technique for choosing parameters this is a probabilistic model is producing a probability we can think about the likelihood of a training data set and we could ask the question which values of these parameters would maximize likelihood that's a long way of saying we can just use the parameter estimation techniques we've learned so far we can take training data and choose these parameters based on training data so we're going to do this we're going to take training data and we're going to try and learn them using this idea of Maximum likelihood remember when we first learned maximum likelihood I said ah imagine we're trying to estimate the parameters of a normal distribution and I give you data points and you can try and choose the parameters that maximize the likelihood of those data points we came up with a nice way of doing that and that's exactly the idea we're going to use for choosing the parameters I'm going to do something a little bit different though today I'm going to focus on what's most important for you for your problem set which is what are the results of the mathematics once you understand how you could use those results then we're going to drive them does that sound good so we're going to start with the end point we are going to use maximum likelihood of estimation we're going to start with the logistic assumption that for any data in a database for example or a training data set the probability that the label takes on the value 1 given the features is going to be Sigma a Theta transpose X that's a logistic regression assumption and of course that also means that we assume that the probability y equals zero is just going to be 1 minus that those are our assumptions based on those assumptions we could call this uh some measure of the probability that or this is the problem that y equals one then we can take training data set and we can figure out the likelihood based on this assumption and once you get the likelihood you can get the log likelihood and then what we're going to do is we're going to be able to figure out how we get the derivative of log likelihood now you're going to later in class understand the mathematics for how we got this equation you'll understand the mathematics for how we got this equation but before we drive into any of that mathematics you need to understand the plot line and the plot line is we're going to make this assumption then we're going to say Based on data I can say How likely does that data look under a particular set of parameters and that's this function so if you gave me any data set and you gave me a particular setting of parameters I could put those two things together through that equation number two and it will score it you'll score your parameters and say you know these parameters are making the data look really likely or these parameters look pretty bad I can now tell the difference between random parameters and smart parameters because smart parameters would make training data look really really likely again we'll understand this later I want to make sure people understand the plotline this in itself is a scoring function it doesn't tell you how to get good thetas so that's where the derivative comes in I make the Assumption I figure out a way to score your parameters and I say you want to choose good parameters pal you give me the derivative and not just any derivative I want the derivative of your score with respect to every parameter you might be changing we'll be able to calculate that numerically we'll come up with an equation for it and this equation is a thing we give to our optimization algorithm at this point we're happy to hand this over to gradient ascent and we say choose the values of theta that maximize this scoring function and hey I know gradient is sent you really want derivatives so I'm going to give you some derivatives so at this point we're willing ready to give things over to optimization and optimization will start changing values of thetas it'll start coming up with good values of thetas that'll eventually make this score really high a question though is convex so that is a very very nice property of our log likelihood function okay plot line questions you know it's Justified yeah you know I mean just to go back we're trying to choose these parameters and the idea of convexity would suggest that maybe there's really easy ways to choose these parameters um that didn't require going to gradient descent and it also has other implications if you end up with good parameters that are a local Maxima do you believe that it could be a global Maxima and it turns out all these things just end up being true it is a very nice maximization thing if you end up running gradient Ascent if you get to a peak I can guarantee you it's the global Peak it's not just a peak it will be the top of the mountain and I'm just wondering like say for example you have a problem where it maximizes so if it maximizes the score which we use the likelihood function for if you find some thetas that maximize the score they'll be good and they'll be the best status the the optimization algorithm won't actually return you back bad thetas good good question so at this point I've talked a little bit about passing this equation off to an algorithm before we get into driving each of these steps I want you to see the whole plot line and the whole plot line really ends with passing this off to an optimization algorithm what does that look like well Grady understand come on and play our old friend you remember gradient scent it says hey if you want to end up with really awesome parameters we're going to repeatedly Hill Climb and by Hill Climb I mean take your old parameter and then we're going to figure out the derivative of your score with respect to that parameter and we're going to multiply that by a step size and add that and that will become our new parameter so we're always updating it if there's just one parameter you can imagine this is like taking small Steps either making the parameter larger or maybe making the parameter smaller if this is negative step size is always positive and then just slowly making your way towards the ideal value of the parameter if there is multiple parameters not a problem we're just going to be each of them is going to be making its progress uh in sync I think this is nice to show as mathematics but I find this algorithm much easier to look at sorry I'm going to skip that a little bit and I'm going to take this Focus where we say we want to update each parameter we're going to use the idea of gradient ascent and gradient descent requires a derivative but I gave you what the derivative was we didn't prove it but I told you that this was the derivative and I want to show you how we'll use it if somebody gives you the derivative for any particular Theta J so the derivative of your score with respect to Theta J you would plug this into here and this is the equation that tells you how you should change each of your Theta J's on every step it says as you're trying to get up this mountain on every step update every Theta take the old value calculate this complicated thing you can have python do that for you it will come back as a number and that number when you multiply by step size we're going to use that to change the value the old value will get Modified by whatever this derivative is and we'll put that in a whole Loop we'll Loop this many times and the parameters get better and better and better that's the promise of hill climbing but while it's nice to give you the mathematics I think it's a little bit nicer to give you pseudocode the very simple idea of logistic regression is this two-step algorithm first you initialize your parameters and then you optimize them but that's too high level let's dive a little bit deeper when you're optimizing them you're going to repeat many times taking a step to take a step you're going to calculate all the gradients and then every single Theta is going to be I is going to be changed by the step size this n is not the number of data points this is the step size times the gradient of J we're going to take all those gradients and we're going to store them into numbers the gradients will eventually become numbers and we want to calculate those actual numbers a wild thing you know we think about derivatives as being equations but if you actually were to substitute all of your data points and all of your thetas this wouldn't just be an equation be something that would turn into a single number the derivative would actually be a value a real value and then we're going to take that real value and store it into a variable and we're going to use that to update Theta J of course this requires that we can calculate that gradient and calculate the gradient you know you're going to have to Loop for every parameter and for every parameter you're going to have to Loop over all training examples and then you have to actually update your gradient and you know using the equation we gave before this is a pretty reasonable thing to do you could take you know for every training example you could calculate this term sum them all together that's your gradient at this point I don't want you to have memorized this this will make much more sense when you sit down to actually code it and you'll look at this and be like okay yes I need to get my gradients uh we've got an equation for gradients I can calculate the gradient each step point and then I'm going to repeat many times improving but hopefully now I've shown you the whole plot line you know you've got well actually okay maybe I'll stop here we want to become smart by getting great parameters that's where the intelligence comes from first we derive a log likelihood function that's a score it tells you if your parameters are good or bad then we want to choose parameters that make that score as large as possible so then we get the derivative we calculate the derivative and then we use logistic or gradient Ascent to choose the values of the parameters that maximize our score and that uses hill climbing AKA gradient Ascent which requires you to be able to calculate derivatives so we needed to have figured out the derivative which we did and then this ends up being python when you run this whole thing it will repeat many times and at the end it will give you back good values of thetas not just the initial assignment to thetas okay that's it that's the next hardest thing we'll talk about those derivations in a second but just to make sure we're following on the plot line I want to show you this very important chart it's both meant to give you insight into what I've been talking about but it's also a great debugging technique the promise of hill climbing is that you might start out with bad thetas but over time as you do this gradient Ascent your thetas will get better and better and better better particularly in terms of the score that we call the log likelihood or the likelihood so when you start out you start out with random values of thetas and your score is going to be pretty low but if you took one step of grading Ascent it will end up moving all of your thetas it will depend on the gradient for each Theta so they could all move differently and once you get those gradients we're going to take one step and they'll change ever so slightly we've now gone from the first step of our algorithm to the next time through the loop and an important thing that's going to happen is because we changed all these parameters with respect to the derivative of the score with respect to each parameter we're guaranteed that the score should go up so the next time we calculate our score it should be higher it's not the final version of our parameters we'd like to repeat this gradient sent many steps so again we get the derivative of the score with respect to every parameter and they'll all change once again so now they've changed twice and we get to the next iteration of our algorithm and again we're guaranteed that these changes should make the score go up so at this point we've got a third incarnation of our parameter values and our score is starting to look pretty good gradient sent if you kept doing it your score should just keep going up and up and up and that's what gradient a set is doing is just giving you a way to change your parameters so that your score goes up and an important debugging idea you can get from this is let's say your algorithm is not working it could happen you could go encode this up you could run it and it could not be giving you the results you wanted one thing is you might have gotten your derivatives wrong and if you get your derivatives wrong it might not be the case that likelihood is actually going up um over different iterations so this is something I often used when I was implementing logistic regression or neural networks to make sure that I had actually coded it up correctly okay and then finally I just want to take this moment to remind you of this one detail which was don't forget we still have the intercept term remember we had a Theta zero and that Theta 0 was going to be added into our sum before we threw it into the squashing function so before we came up with the squash and function we're going to add in the Theta zero and don't forget that the way we actually Implement that is by making x0 always equal to one if you make x0 always equal to one no matter who the user is then that will be the same as adding an intercept okay so classification we've already talked about so I'm going to actually go quit pretty quickly through that you know you get the probability that y equals one if that's greater than 0.5 you predict a one otherwise you precake to zero for the actual prediction we make we call that y hat how cute it's like our guess at Y and that hat is like making us look a little silly so that you can tell that we're a guess and we're not the actual value anyways I don't think that's so surprising at this point so you get this value if it's greater than 0.5 you predict a 1. okay but um you know just to be clear once you've got good thetas you should be able to take a new user set their inputs to zero one one for this particular user then you should be able to do your forward pass get a value of Z squash it get the probability if that probability is greater than 0.5 then you predict a one oh you guys worked really hard to follow that plot line it's a pretty complicated plot line there's a lot of moving pieces but at this point hopefully you believe me when I say okay you can make this model okay we can score it and if you can score it then you can choose parameters that maximize that score using gradient ascent but what I haven't told you the missing part of the story was where did those equations come from maybe you don't need to know you know if you actually have to just code this up for your homework you could skip this final part of where those equations come from you can just look up the slides get those equations coded into python run it get the right answer you know have your good times go play with your friends but I'm going to give you a reason for why you should care about where those equations come from because logistic regression is just the start of the story this is a good idea that you can imagine is a seed and the seed has blossomed in wonderful and complicated ways so the extent that you can understand the seed is the extent to which you could understand deep learning so we should really know all the different parts especially if you want to either be the ones to make this flourish or as you guys know AI is going to require us to understand it it's having big impacts on society and we need to really really get our heads around what these powerful tools are doing for both those reasons we really want to know how this black box is going to be working so you guys ready for jumping into this math okay so I gave you these equations I said this is our assumption based on the Assumption you can get that this is the score and then we can derive score with respect to each parameter but I didn't tell you step by step how you get these two values so let's do it how do we get that log likelihood function as you recall a very funny thing happened when we wanted to do mle on a Bernoulli when we wanted to do Emiliano Bernoulli the first step is you've got training data that'll be a bunch of zeros and ones can you write How likely that training data looks based on the parameters in the case of a Bernoulli there's only one parameter that's the probability parameter P do you remember Bernoulli it's either one or zero and it's one with probability p if you're doing mle to choose a Bernoulli your your job was to choose a p and the mle step-by-step formula the first phase of mle says write down how write down a likelihood function which often would require you to write down the probability Mass function a Bernoulli probably Mass function looks like this but we have this issue that the Bernoulli probably Mass function which says you know the probability of a one should be whatever your parameter p is the probability of Z should be one minus that P this really simple thing is not drivable those bar charts weren't derivable so even though the Bernoulli is a simple concept and has a simple probability Mass function we couldn't write a likelihood function for Bernoulli's that was drivable so instead we did this super cheeky thing where we came up with a derivable version of this bar chart that derivable version of this bar chart looks like this it takes your parameter and raises it to Y remember for Bernoulli y can only be zero one and then it multiplies it by this other term which is 1 minus the parameter raised to the power of 1 minus y insanity but in sanity that we need to understand this is what people use in logistic regression and deep learning and Beyond for likelihoods of a Bernoulli because this is going to be drivable this is equivalent to the bar chart when y equals zero or one and let's just do one example of that to remind ourselves let's take this equation and plug in 1 for y if you plug in 1 for y you get P to the power of one everybody what's P to the power one oh okay good so we have P that's good multiplied by this other scary term ooh well if y was 1 what's 1 minus y and what's whatever raised to the power of zero okay so we have P times one which gives us uh yay okay so this is really nice when you put in a one for y it gives you back p so if you put in a one for y you get back p and it turns out if you put in a zero for Y into this crazy equation you'll just get back 1 minus P because a really really complicated way of writing that bar chart but we needed to do this because it made it derivable recalling that the first step of mle means take this assumption and now imagine you had a training data set and write How likely does that training data set look in the face of the current setting of parameters score the parameters by coming with a likelihood function imagine that you just had one thing in your training data point the way we could score it is we could use the Bernoulli we could say hey take your single training data point and if you put that X the inputs of the train data point if you did transpose of theta and put it through a sigmoid this whole thing here that is the probability that y equals one it's the parameter P of Y which is a Bernoulli you know this thing here that's the P of your Bernoulli the whole thing that comes out of logistic regression equation is the probability that y equals one and if we use that continuous derivable version of a Bernoulli probably Mass function says you know the likelihood that y takes on its value which could be a zero or one for a data point in your data set given the inputs for that single data point is going to be P to the power of either the one or zero that is y times 1 minus P to the power of 1 minus the one or zero that is y so to be clear your data point would have an X and the X could be something like hey remember x0 is always a one always a one but maybe there's three other features and they represent whether people like the different movies it could be a zero zero one and in this case Y is going to be a number zero or one and Theta it's going to be a list which could be like you know negative seven two zero three which will be the ways that you weight X before you squash it and get the probability that y equals one and we're going to score this combination you have your data you have your training data which at this point is hilariously just one how funny would be to train a whole machine learning algorithm with one data point now we want a lot of data points why am I starting with one because the math is easier so let's start with one so if you have one data point and you have you want to score this Theta you would calculate the probability that y equals one based on these thetas so you'd take this Theta transpose it with X that gets your weighted sum squash it and that squash will give you the probability that y equals one and if you put that into this equation for y we're going to substitute one because that's what the data point says and that means we're going to ignore this whole thing and we'll just be left with this term so it'll just be the probability that y equals 1. so if our data point had a one this likelihood function will just return you back the number which is the probability your algorithm said that the data point would be one take a moment and talk to the person next to you and see if you can ask a question about why that's true because I do want to stop here this seems like if you can follow this the rest will be just math so talk to the person next to you see if you can come up with a question what's confusing about this or what could I understand more deeply uh take a minute and a half just to have a chat foreign okay what are we doing here we want to choose good thetas mle tells us how but the first thing you have to do with LMU is you have to come up with a scoring function say how good do my thetas look like based on some real training data we thought about this in the case where you have just a single data point and my claim for you is if you have just a single data point you can calculate this equation which will return you back a single number which will be the score of how good your thetas are for that single data point but it's a very confusing thing there's a lot going on here you have logistic regression you have interpreting the output as a probability of a Bernoulli we have the continuous drivable probability Mass function of the Bernoulli and all three of those things are happening at the same time it's just so much that we should ask some good questions any questions come up yes the inputs that aren't binary good question so the question is if we have inputs that are not binary how will this get transformed I have good news for you if your inputs are non-binary you could use the exact same thing it's just the X's will happen to be not zeros in ones if the output happens to be non-binary becomes much more complicated because we're going to be interpreting the output of this logistic regression as a probability of a Bernoulli and so if you can have your why take on non-burn newly values it's going to not work so inputs can change to be anything but output we're going to say has to be zero and once good questions and I owe you mandarins I am going to keep aside the three mandarins that I owe yes sorry wait actually for naive Bayes um we were we were able to do joint distribution doing probabilities of like multiple random variables are we going to see an example of this here or do we just assume the whys of the factory so in naive Bayes we could have X be a vector and here you can also have X be a vector in both cases X can have lots of features um and in both cases the thing you're predicting is always just a zero or a one so both of them are going to have the exact same format X can be large y will be either single zero one and this is a little bit hard to see but my X is bolded just to try and show that it could be a whole list you know my X can be a whole list of numbers whereas my y will either be a zero one which is the exact same format as naive Bayes you are right that in naive Bayes there's much more interpretal way of talking about the joint likelihood of X and Y whereas this one's not trying to do a joint it's just time to talk about the probability of Y given X so an ivase is trying to do something a little more complicated that's why he has to make his assumption good good question yes why do we need to be concerned about multi-collinearity why don't you need to be concerned about it well the idea being gradient Ascent will choose good thetas even if things are collinear or not collinear it will still be able to just optimize let's talk about that after class okay yes so basically saying that the the output of our machine learning is like is it removing and it's got some p-value and we're basically assuming what p is if I like to say in our case we're saying it's signal function but you could use something else yeah that's exactly it it's just saying okay I'm assuming this Y is a Bernoulli I need a little mechanism to get the probability of it being one I've got logistic regression you could have made a different machine but logistical machine is the machine that we chose and that's what we've got and because that's what we've got because this is really a Bernoulli we can just write a likelihood function no it's squashes nicely there's not a deeper reason it's a squashing function think of it being practical uh not being driven by Deep Neuroscience or something like that yes question um the probability is the principles of theta multi you know multiplied with x yeah pass through the sigmoid and so that whole thing is to the power y right yes this is a probability this is the parameter P of our Bernoulli and why do the thetas change that's one thing that I've had trouble understanding in the previous explosion so with e23 we want the thetas to change so we first thing we're just going to come up with the scoring function but then we're going to use the scoring function to change our thetas and we want to change this thetas to make the scoring function go as large as possible so you can think about the score the thetas are movables they're things that we can move and your job is to move them and move them in a good direction so that's why they change is because it will end up making this thing smarter but we still need to derive this in order to figure out exactly how we should make them change so your first claim though I think was very helpful I think that's something I really want to highlight this whole thing we could have just called the P to our parameter and in fact you will sometimes see people give this whole thing a different symbol to represent it because it's just used so much and and this equation would have been a lot easier if we said something like we call this P hat and we just started using p hat symbol anytime you saw this it would make this maybe a little bit more readable and some people do do that okay you guys rock those are very good questions the great thing about likelihood is that if you assume all your data points are IID so each data Point's independent of other data points different from the naive Bayes assumption that's assuming the features are independent if you assume the data points are independent which is very reasonable then the likelihood of more than one point is just going to be the product over many many points of the exact same function that we had here so take that and just put it through a loop of multiplication so it says for every data point in my data set we're going to calculate its likelihood so this was the likelihood of one data point and we want the data points the product of the likelihood of every data point in our data set so then we can just substitute in here notice that if y was always one we wouldn't need this term but for some of our data points y will be one and for other data points y will be zero and don't forget that this is just picking out either P or 1 minus p okay and that's a scoring function yes yes exactly and particularly it's which data point okay so here this is saying oops pen this is not an exponent it's just saying the ith data point and this is the same notation that they use in 221 uh and 229 so I did want to start using that notation here to set you guys up if you ever take those classes but you don't have to but they're kind of fun I digress so yeah this is saying your data point has n values and we're going to loop I being the index of the curtain data point you're on and we're going to calculate this for the current data point this was one data point but in the next data point this could be like superscript zero or superscript 5 let's say and then the next data point maybe you know you have different inputs and a different y now I skipped ahead to this this is a scoring function it would have been just fine but if you try and program this for a large data set you'll have underflow problems so instead of just using the scoring function we're going to use the log of it because log is monotonic if this is a good scoring function the log of it will also be a good scoring function so let's use the log okay so at this point I've given you the mathematics for the first equation that you saw which was this is the scoring function for how good your parameters are and we're not there yet the last thing we need to do is be able to say okay that's a scoring function give me a way to make my parameters better which means we need a gradient we have to be able to drive that scoring function that thing that can take a Theta and say this is good or bad and we want the derivative of that score with respect to each parameter okay we're going to do something Brave before we do it though know this thing about sigmoids sigmoids are not just a squashing function they're a squashing function with the most beautiful derivative if you want to know the derivative of a sigmoid it is equal to the sigmoid itself times 1 minus the sigmoid if you notice the sigmoid function has this natural base e and as you know natural base e is the very special number where if you had to take its derivative of e to the power of X it has that nice property of not changing for similar reasons because sigmoid has the e natural base in it its derivative ends up being very very nice it says gorgeous that is another reason we use sigmoids by the way it's a squashing function with a beautiful easy derivative okay whoo you now have a likelihood oh before we do that I just want to show you guys what this could look like using the sigmoids derivative before we do any hard derivatives or hard likelihood derivatives so this is an aside welcome to my little aside I just told you sigmoid has a beautiful derivative that's in the top right and I want us to practice using the sigmoids derivative and particularly I want us to calculate the derivative of this expression with respect to one of the thetas just as practice just as warm up what you should do is you should use chain rule you should say this is a composition of doing step one this is step one and then once you do step two you step two is to put this through the sigmoid function if you have a two-step process it's a good candidate for something you can break apart using the chain rule particularly I'm going to say that Z is going to be equal to Theta transpose X you know Z is just the input to the sigmoid and so this is going to be the same as the derivative of sigmoid Z with respect to Z and the derivative of Z with respect to Theta J as I said you should practice chain rule you should make sure you know it watch the Khan Academy and make sure you can follow exactly this example because this is the application that is most important of chain rule now if you want to plug and chug this derivative of sigmoid Z with respect to Z is just going to be Sigma of Z times 1 minus Sigma of Z so sigmoid of Z times 1 minus sigmoid of Z remember um Z is just Theta transpose X so sigmoid of Z is that and 1 minus Sigma of Z is that so that is just that first expression their derivative sigmoid of Z with respect to C is just sigmoid Z times 1 minus sigmoid Z done and then we need this expression what's the derivative of Z with respect to a particular Theta J and I'll tell you the answer right now but let's understand why if you're deriving this expression with respect to a particular Theta let's say you're deriving this so the derivative of theta transpose x with respect to Theta 5. recall what this is you know it's Theta 0 times x 0 plus Theta 1 times X1 plus Theta 2 times X2 where does Theta 5 show up only times X5 the only part of this term where Theta 5 shows up is when it's multiplied by X5 that's the only place so if you derive this whole thing with respect to Theta 5 it would be the same as deriving Theta 5 times X5 with respect to Theta 5. and if you're driving this simple multiplication with respect to Theta 5 what is it oh you guys said it so timidly say it loud I want to hear people in recording want to hear your voices yeah that's just X5 more generally if you're driving with respect to Theta J it will be x j so if J was 5 it would be X5 if J was 3 it would be X3 Okay so a simpler way of writing the exact same thing would be is you could have declared this to be y hat and this is just you know sigmoid of Z times one minus Z times x j which is the same as this expression times 1 minus this expression times x j this is just a really cleaner way of writing what we derived before where y hat is equal to this sigmoid of Z so Sigma of Z times 1 minus Sigma a of Z times x j that is the derivative okay we don't have time for pedagogical pause we took the question before are you guys ready this is Stanford we can do this is the thing that I want you to drive in terms of theta J um okay just go and derive it [Music] we can do this here are a couple Pro tips this is going to be the derivative of a sum of the logs right so this is the thing we're driving in it's got a big sum here let's start by just doing the derivative of one of these inner things because the derivative sum is just the sum of derivatives so if you can do the derivative of the inner part your life will become really easy the derivative of sums is the sum of derivatives just tell me the derivative of this inner part so then my question becomes a little bit easier can you derive this and particularly anytime I see a sigmoid of theta transpose X every time I see this thing I'm going to call it y hat because that notation makes things so much easier to read and if you call that y hat then this is going to be just derive this with respect to a Theta J you're like where are all my thetas oh don't forget the thetas are actually in the y-hats because the y-hats are actually this expression so they're still there but that's the derivative we're trying to do okay now derive this inner thing with respect to Theta J where I'm using this shorthand for y hat which is sigmoid of theta transpose X idea of chain rule is this there are no thetas here all the thetas are hidden in these y hats because the y-hats are my shorthand for Sigma to Theta transpose X you should not try and do this derivative straight instead you should use chain rule you should say all of my thetas are in y hat so I should derive that whole expression with respect to Y hat and then I should derive y hat with respect to the Theta J chain rule is decomposition for derivatives it allows you to take a big derivative and do it in Parts it says okay just drive things with respect to Y hat then y hat with respect to Theta J do those separately and multiply them so let's start out with what is the derivative of y hat with respect to Theta J I have good news for you if y hat is this expression driving it with respect to Theta J is the thing we did in our warm-up we actually figured this out it was sigmoid of theta trans plus x times 1 minus Sigma of theta transpose x times x j and I just wrote the exact same thing here don't forget the Y hat was my new shorthand for writing sigmoid of theta transpose J hey we're halfway there now all you have to do is derive that expression with respect to Y hat and remember you're using Y hat as a variable you at this point you can forget that y hat equals this at all and just say what is the derivative of this expression with respect to some variable called y hat well if you take this first term what's the derivative of y log y hat with respect to Y hat well this doesn't have y hat so it'll just come as a term and log of Y hat drive with respect to Y hat is just one over y hat and same thing here log of 1 minus y hat will just become 1 over 1 minus y hat but because there's a you know a little thing going on inside the log you have to drive that with respect to Y hat and you get a negative sign so this positive became a negative sign and this term just came out here this whole derivative becomes this term which is not so bad and we're done we just did this crazy hard derivative it turns out by using chain Rule and breaking into the derivative of loss with respect to Y hat derivative y hat with respect to Theta it becomes not so bad you could simplify this a little bit if you actually did the math but that's not as important as knowing how to do the derivatives now if you were to do this um as the sum you know we skip this part we just did the derivative of the inside but this will just become the sum of each of those derivatives so we did this derivative and then you're just going to sum that over for all your data points which leaves us at how you get the derivative of your scoring function with respect to each of the thetas which brings us to the end of our story you guys know logistic regression you guys under understood you guys understand how we score it and now you understand how you could drive the score with respect to each Theta J which you can then put into gradient Ascent wow you work so hard and I have a treat for you come back in Wednesday and I will tell you how this becomes the blossom that is deep learning we will understand deep learning and the extent that we understand logistic regression have a fantastic day get started early on problems at six I appreciate you guys so much come back on Wednesday cheers cs109