good afternoon cs19 how are you guys doing today fantastic I you guys know this but it's our penultimate class we have class today we have a fun little lecture on Wednesday and then that's it for lectures in CS 109 I can't believe we're at that part it feels like just yesterday we were starting and we're like hey let's talk about probabilities and we were doing counting oh what good times uh we have a very fun celebratory lecture today uh we're at that part of the quarter where these aren't things that will be on the exam but it's also a really fun chance for me to just talk about what I think is interesting and part of the future of probability and I have a novel lecture for you guys one I've not given before uh and one that I'm really excited to give so I was thinking about what should we talk about and there's all these fun things to talk about I was like oh I can talk about like kale Divergence or I could talk about bounds uh and then something happened last Thursday that's really worth talking about as you might have heard there was a a a new release of this large language model called GPT just quickly has anyone ever heard of this okay it seems like something worth talking about uh interestingly it coincides this last week was the world's biggest AI conference in fact uh we had a couple papers there I decided to stay because I wanted to hang out with you guys I got sick a couple times this court and I just wanted to chill with my CS 109 fam but um that's why you're seeing so much happening in the world of AI and I thought long and hard about I just thought it'd be so neat for you guys to learn some of the the probability behind some of the the biggest movers and shakers of today so in today's class we're going to learn about how Dolly Works how gpt3 works and if we have time I can tell you a little about some of my most recent probability work if you guys find that interesting you want to do it so it's it's also going to be on the board so I I practice and everything we're going to try and do this on the board we're get rid the screen in a second but not quite yet because before we jump into thinking about the wonderful ideas Behind These algorithms that maybe are going to have a rather profound impact let's just play a little bit so if you haven't seen GPT it's a pretty simple API you can write text and it will produce text so for example here I wrote write a thought-provoking poem that explains the difference between maximum likelihood parameter estimation and map parameter estimation but it should be sung by a pirate as a sea shanty so it says IE land lovers listen to me as I sing of parameter estimation of Maximum likelihood in map you see two approaches both with their own reputation uh maximum likelihood it seeks to maximize the probability the data that's seen map adds a prior to regularize and balance this trade-off between bias and precision I feel like I got it to be more pirate other times but you can be like okay be more pirate how do you even spell pirate is that it oh man IE scurvy dogs lending near to the tail I spin a and um this is behind the scenes a machine learning Model A little bit like we've learned so that's one of the things we're going to talk about today like what's going on with this model and what are the great ideas behind it what's the difference between what we've learned in CS 109 and the techn behind GPT but we're also um going to talk about a different model that might seem pretty separate but there is similar ideas that go on behind the scenes um and this one doesn't produce text but instead it produces images so I said a photo of a beautiful Coast Live Oak these aren't real photos they don't exist they were just generated uh and you guys have probably seen this before I'm just trying to catch everyone up uh and before we go to the board there's just one more picture that I want you guys to see before we go in there because I'm going to at some point refer to gaussian noise and you need to have a visualization to go with it so this is a picture with no noise and this is a picture with some gaussian noise and when I say some Galaxy noise notice how it looks or a little bit distorted every single Pixel has its original value but we've added some gaussian sample into the pixels in this case it's on both the red or all red green and blue uh channels this is with a little bit of gaussian noise and this is with a ton of gaussian noise you can't even see the dog anymore okay questions comments thoughts before we jump into this okay okay let's make that screen go up let's get lights all on as bright as we can get it in this Nvidia [Music] Auditorium okay so this is our road map we're going to start with thinking about that second algorithm the one that can produce trees and it's a really interesting story and did you know it actually took place at Stanford like the great idea behind how you can create images from neural networks was a simple idea that happened here not too long ago uh by a fellow called Josh a soul dixin okay so where should we start maybe we could start by building a little bit of a a road map and I want to put it here in the center I want to talk about the three models we should have in our minds one model is what you're doing for homework the second model is going to be this dolly and the third model is going to be GPT and on some level they have a lot in common they all have a big neural network well maybe yours has a logistic regression but we're going to think of them all as black boxes so if we start with your algorithm so this is like your P set 6 you built a little machine learning algorithm and the Machine learning algorithm takes features which come as a list and it produces a prediction Y which is an element of 01 which is a really fancy way of saying you built an algorithm that could take some inputs and it predicts out either a 0 one binary prediction but we also want to add in do e here and Dolly does something different it's also a neural network but we're going to be making Dolly tree Style and what Dolly tree does is it doesn't take any inputs but every single time you run this neural network it's going to produce not a z one but it's going to produce an image and our version is always going to be an image of a tree so let's get out our blue marker and let's draw ourselves a little bit of a tree woohoo what you guys think not quite Dolly but it's an all right drawing right uh and then the final neural network we're going to think about is going to be GPT and GPT takes in some texts like I let's say you wrote I love uh and you throw it into the model and it produces more text so let's say cs19 uh and pancakes okay so that's our goal let's see if we can understand these three now hopefully you're understanding your own problem set six so maybe we could start by jumping into thinking about how this dolly model works can I start with just asking you to come up with it because there is a history here in this task of running a neural network and having it produce an image there's been a long history of cool creative ideas one creative idea that's no longer used is you'd have two different neural networks one neural network would create images and the other neural network would try and decide was this image produced by a computer or was it produced or is it an actual image from the internet and these two competing neural networks would train against each other and eventually one of them would get really really good at producing images that would trick the other one that was a great idea but as I said Stanford not too long ago someone had another great idea that changed the game it's a great IDE that you're going to understand deeply by the end of today's class but I wanted to just give you guys an idea to play come up with your wild ideas just make up a way like if I said you had to create a neural network that's going to generate a picture of a tree how would you do it and some of the questions you can ask yourself are what data would I use and then what would I do with the data you know how you imagine somebody gives you a big neural network how could you set the parameters to that neural network with some data have a moment just be playful make up ideas say something Sly be like I'll send my neural network to a pirate who knows so much about maximum likelihood estimation that they'll figure out all the prameters have a good time think about it for a second before we jump into what is the great idea behind dolly okay talk to the person next to you and I'll take a moment and practice my tree drawing e okay no expectations but there's a small chance that we'll just like totally revolutionize the world because like this is such an open field okay let's hear them good ideas bad ideas funny ideas we'll take them all okay what's your thoughts so for what I would do is I was like like for trees I'd scrape Google find like punch images of trees um and then have do like create a bunch of images and see which one most closely matches the data set of trees okay scrape Google produce an image and then the loss can be something like distance to closest tree great fantastic okay yes an idea can I make a network to identify trees and then in some sense run it in reverse oh okay so we're going to have like a tree Network kind of like ours over here you put in a picture and either says tree or not but we're going to then run it in reverse and we're going to say okay you're a tree mess with my pixels oh I like it reverse classification image or predictor sorry okay yes one more idea so maybe you produce like three images one of the red pixels green pixels and blue pixels and so you kind of have multiple uh variable outputs and then for each of those outputs you have 256 like values that could be and so you take all the pictures in that you scraped from Google and then evaluate what's the probability that in a picture of a tree this particular pixel has this particular level oh okay so you said a lot of good things like one is when we're making this picture we're going to have to not just output a z one but we have to Output the pixel values that's going to mean the red green and blue pixel values so that's going to be our task um and then you said you know we could set it up so that we say like the most likely pixel like the pixel that's most likely uh given what we've seen okay um okay great so we're going to have to predict predict pixel values awesome uh and we want maybe like the most likely given tree okay these are all very very good ideas shall we go into because it's the beauty of what goes behind dolly is that it's when you see it you'll be like oh okay let's get behind the probability but I can appreciate that that works now one of the things that's complicated about the world is you've got all these images but on Google say not all of them have labels you can get lots and lots and lots of pictures some of them might have um labels that say what they are but some of them won't and there is this interesting idea you could take any image picture of a tree and here's something you could do to it we'll explain why you may want to do that but here's something you can do at the moment it's a little bit unmotivated take your picture and one of the things you could do is you could break your picture and one of the ways you could break your picture is you could can add in some noise and let's say you chose gaussian noise yeah that's kind of a pretty reasonable thing to do so I'm going to add in 20% noise and my 20% noise maybe like you know one in five pixels I can add in some gaussian noise so to give you an idea of what that looks like if we zoomed into some pixel pretend for a moment that pixel all our pictures are black and white this is the black and white black and white or like The Artsy version imagine every pixel is black and white that makes it a little bit easier we don't have to think about red green and blue it's just that if you zoomed into any pixel of a tree if you looked at a pixel it would just have a number and let's say those numbers are between 0 and one so let's say this is like a 0.5 when we add in gy noise here's what I mean you know for some of these pixels we're going to add in a random sample from a gaussian and we could choose any gaussian but let's say we chose a pretty reasonable Gan that has like a mean of zero and some sort of variance right and so maybe when you sampled from this Gan you got like a 0.1 so then the resulting pixel value is 0.6 so when I say adding Gan noise it's something that you guys could do right now like you could pull up a computer and if I gave you a pixel value you could do hey scipi sample from a gaussian uh with a particular mean and a particular variance uh and then add that to my old pixel value and you could do this you could then take this image and you know what you could do You' be like not noisy enough I'm going to add a little bit more noise so you can say like I want something with 40% and then eventually you could keep going until you've just got the world's noisiest image so you know we could add 20% noise and we could add another 20% noise and now it's just starting to look pretty noisy and eventually when you get to 100% noise everything just looks like a random value from our gaussian and you just can't see any tree anymore I haven't described any neural network there's not NE Network here it's just a way that you could take all these gigantic tons of images from the internet and you could start to manipulate them but there's this growing mystery which is why would anyone want to do that to your poor images like you're making them harder to see like why is that a useful thing do you guys want to try and think about it like I've left this like tantalizing clue just reflect on it what could you do with this CU I say like when I say this is like a million-dollar question I feel like the person who figured this out is now one of the lead researchers at Google so probably was worth like a million dollars for the person who did figure it out yeah probably Jesus what a time well you could do this to a lot of images and why would that be nice okay an idea I think it's very similar to what say because you're starting with the outcome uh so you know the outcome that you adding the tees that separate it so you're constantly so it's conditioned on the outcome so constantly adding noise you condition you such a way that it keeps learning how the outcome should look so that Unity can still work yes you on to something that is really really in the right direction so yes we're going in the right direction it has this relationship between the outcome the cleaner image and a noiser image yeah maybe you could de blur the blurry images to create different trees yeah that's it this is the first great idea so the first great idea that kind of changed everything about image generation is we could build a little neural network and its job is pretty simple it doesn't have to do any complicated predictions so we're going to call this idea number one and idea number one is we're going to build a neural network it will take in an image and that image is going to be of a tree and it's going to have some some noise in it let's try and recreate that image as well as we can and we're going to build a little neural network and this little neural network when it's done it will produce a picture that has removed 20% of the noise so could you make this data set could you make your training and test set yeah you can make yourself a big one take all of Google take all the images you've got add in noise and then you can feed it into your neural network you say here's the noisy version can you predict an image and then I'll see how good a job you are at removing the galoian noise but that doesn't generate a whole tree that just removes some noise so how could you generate a tree with like this well you start here Generate random samples from a gaussian for every pixel then run that completely noisy version through your neural network you'll end up with something with 80% noise take the result put it through your neural network again you'll end up with something 60% noise put it through your neural network again you have 40% noise and you keep going and eventually it's got a tree what if you wanted a different tree you're like that's not pretty enough I want a new one you just start with different random noise if you create different random noise you put in like this completely noisy image through your neural network and it'll try and remove noise in a way that'll make it look more like a tree and you repeat this five times and you'll have a picture of a tree okay questions yes in this example does the Black Box is that strictly just step or do we have one from going to 100 to 80 one from going from 80 to 60 kind of incrementally like that just one box now one of the things I there's I'm going to try and give you guys the great ideas there are some small details that are helpful I think this black box also takes in which Step you're on as one of its X's one of its features uh but that as I said that's not the great idea and if you got rid of that it would probably be just fine yeah just one box and they call that box Dolly yes is it removing like just random 20% noise or is it more sophisticated than that like how does IT training part work like how's it getting feedback so we'll talk about in mathematical detail I promise cuz you guys could you guys actually you're totally ready to understand all the mathematical sophistication of precisely what it's doing but let me give you the big picture first it has both of these images how because it generated them right and so it puts in one and actually realistically you know you end up with your predicted one and then there's the true image of the tree and maybe the true image of the tree is slightly different because when your neural network tried to make a tree it didn't get it perfect oh all the colors you guys this is such an enjoyable marker experience so maybe you know when you made your prediction you got it slightly wrong and when I say slightly wrong what do I mean well we know what the truth is right we had the clean image and then we added the noise ourselves so we have the input and we have the truth and then we can see your prediction we can be like how good is that prediction now you as as I promised not only do I want to explain this to you I just want to give you the level of sophistication that you are ready for in CS 109 we'll talk about that detail but let's get that big picture first also I want to explain myself pedagogically I want to draw on the board for lots of reasons and one of the goals is to slow myself down um and so that we can keep it all up on the board so you can see the big picture as it unfolds plus it's kind of fun for me yeah question if we wanted to do like the full Dolly so not just making trees but making whatever would be like tret to deblur images in general and then instead of using completely random noise like blur tree or something else so it's actually pretty simple I'm going to remove that detail so we can just focus on the great idea but the simple idea is you also insert text so you say this is an oak tree whatever the prompt the user gave you and that text could help it guide its de noising uh now when we understand GPT we'll have a better sense of how you could do that and until we talk about GPT let's just assume that all it does is make some trees but it's actually a pretty small addition to make it also be guided okay question could you explain a little bit more of the intuition behind um like the benefits and drawbacks of running it um like five times um versus just like a neural network would say like five times as many layers oh that's such a good one well the the question is why do it five times instead of just going all the way so I actually used to take and I used to live in San Francisco together and our Labs were right next to each other and Stafford you're like hey Chris why don't you invent you know diffusion and I'm like that's not important right now um and you know I I guess I was hanging out with him we get on the couch we were writing a paper together around the same time so he was telling me a little about it and honestly it didn't work he just tried to do it all at once and it wasn't successful but if you did step by step it seemed to be a little bit easier um and there is a little bit of a art form to some of this stuff and I I don't think there's a deep theoretical reason why putting it through five times is better than making five times bigger model I will say this though at the time when he was a researcher he had access to a pretty big computer but we're going to talk about this in a second you know one of the other things that's happened is scale um you know Dolly is trained on massive super computers which maybe that means that in the new version of Dolly they could go straight uh it might have just been the constraint of his computer good question why do 20% yes this is kind random but is Dolly like open source does it continue to train every time people use it and get better um not open source and it does get better as people use it so I mean this is such an interesting ethical issue um so these models are expensive they cost about half a billion dollars to train and there's only a few companies in the world who have access to them and I think they're trying to figure out what this means monetarily as part of it but you know what they said they said this stuff's too powerful we can't just give it away and they're not completely wrong you know like on some level there's probably some version of GPT that maybe shouldn't be out there in the wild people could use it in really nefarious ways uh but that's what they said but it's it's hard to ignore that the value of these things even though they cost a billion dollars to train they might be worth trillions of dollars to whoever can figure out the smartest one so that's such a good question okay yeah question so are we starting with like a blurred image of another tree or just a blurred image that can become anything good question when you start you start with just a blurred image so if I give you a fully trained Dolly here's what you would do you would create the size of the tree you want and for every single Pixel you just sample from a gaussian its value would be sample from a gaan now if you want to get a little more detailed there would be a red green and blue but just assuming it's black and white every single Pixel will just be sampled from a gan so you look at at the beginning and it just looks like static on your TV but then you put it through our model once and starts to look a little bit less like static the tree it looks like will be defined by whatever random noise you started with like that seed is where this oh that's so beautiful it's the seeds where the the tree sprouts from you see what I did there not important and then you you run it five times and then you have your fully grown tree and you're like oh we went through so much together yes well is there like one reference picture that the system is trying to make it look like or is it just so this is one of like the trillion images that found on the internet one of the secrets behind all this is that the internet has a lot of data so they'll do this once for this tree and then they'll do it for the next tree and then the next tree and the next tree and we'll get every tree we get in fact there's so many trees on the internet it often won't look at the same tree twice like once it's done it can just put it away now there are different training schemes I don't have that many tree pictures I would probably use trees multiple times okay so yes um what's the advantage of using random noise as opposed to just white pixeles good question if you start with white pixels you would always get the same tree every time you ran it um and I suppose if you the idea here of like trying to figure out where the noise is I think one of the well you know what that's a cool idea I'm just going with that's a cool idea so one of the things we could have done is you could have taken your tree and you could have just like whiteed it out every single like 20% of the pixels and then white out 20% more and then your job is to fill in the 20% that sounds really good but certainly you have this problem that you'll always produce the same tree why would you always well one of the reasons you get different trees is because the noise is different each time like every time you create your image it starts with different noise like when you're in the generation phase um but if you always start with the exact same seed it'll you know when it removes it 20% there's nothing about this that is stochastic in nature the stochasticity the fact that you get different trees starts with the fact that you created a different seed but I like that and you know we should try that out an idea the D noising model uh like is it just ran on the entire image or does it only like d noise like 20% of the Piel each time well it's run on the entire image and it tries to do noise 20% but we're just at the start of things you know people are trying to use us in different ways you can imagine this is a foundation upon which you guys could build some cool ideas and one of the cool ideas is you know maybe we want to just fill in specific pixels you know maybe we just want to remove this Branch or there's lots of neat things that you could do on top of this technology okay one more question then I'm actually going to tell you there is an even better idea than this is the variance of the gausa noise chosen to ensure that the training propagates through the network there's a lot of art form I'd say that's not a closed discussion so people choose variant in lots of different ways diffusion actually comes from a physics idea like this this idea of removing gaussian noise has existed a long time in the world of physics and they've been very thoughtful about how they choose variances for today's example we're just going to use constant variance uh but if you read the uh the original diffusion model one you can see that they're changing noise in intentional ways uh throughout the training process though again it would have worked just fine if they hadn't okay now this turned out to still be a hard problem what have we talked about so far we've talked about the architecture we've talked about how you can get data so at this moment if I give you a fully trained Dolly you could use it you could put in noise and you could get progressively better images what I haven't told you is if I gave you an untrained Dolly how you could do the training yourself and one of the reasons is that it's a little bit hard to think about about the likelihood like what's the maximum likelihood of an image give another image I don't know some people could have figured things out there's like cool ideas like an elbow lower bound uh loss that was fine but somebody realized hey there's a slightly easier problem and it's very easy to think about the probability and really first principle ways if we solve this easier problem did you guys know there's an even easier problem want to hear about it okay here's how it goes same starting point the starting point is still going to be you have a picture of tree with some noise and you want to denoise it got to keep the noise consistently red I like this Christmas theme going on and this time the output it's not going to be a tree you're like what is not going to be a tree I thought we were making trees here no pal we're no longer in the tree making business we're now in the noise prediction business so this time we're not going to predict trees at all instead we're going to say if you gave me this tree instead of predicting the tree without noise I'll predict the tree the noise what would you guys do with that if I said here was my input I predict that this is the noise noise could you turn that into a tree I see some nodding just like division okay an idea somebody's got this somebody's thinking it yeah in the back oh like taking the input and subtracting the noise exactly if I gave you an image and you predicted where the noise was and this noise could be positive or negative and let's say you predicted positive negative values you could just subtract you could take this image subtract out and then you should be left with the unno image so it turns out that these two problems somebody realized were the same and this problem was a much easier and B much easier to think about probabilistically CU we still have this task of how could we train this this puppy so you could have predicted your noise and then I know exactly where I put the noise let's say you for gotten that one and now we have to think about what's the likelihood of the True Image given what we predicted so a lot like maximum likelihood estimation now what I'm going to tell you is going to extend to the whole image but can we just jump into one pixel for a hot second if we just jumped into one pixel let's see if we can get that up there if we jumped into one pixel you could imagine that if you looked at say this pixel over here you could have two numbers we have like you know y hat and I'm going to be a little clear that y hat is a result of the neural network so I'm going to say that it was a function of the parameters just like the output of a neural network was a function of its parameters and let's say y hat was 0 let's say the true one is 6 let's say y hat is 0.5 and the true y the true amount or actually we're predicting the noise sorry the true noise is 0.1 and let's say you predicted a point2 so at this pixel you made a prediction of the amount of noise you predict it was a point 2 but in truth it was a 0.1 people said you know what let's think about this as a random variable we can think about why as being a gaan because we know it a gaan and we're going to assume that the Gan takes on the mean that was your prediction so we can say it's mean is equal to the prediction you had y hat of theta uh and then we're going to say it has some fixed Gus fixed variance for now you know we can call that K and then we can ask a likelihood question we can do it for one pixel but of course we could think about every pixel as being independent because our galaxy of noise was independent um but let's think about one pixel at a time we can talk about what's the likelihood we'd say what's the likelihood with respect to our parameters and as you guys know now it should just be the probability density function of a random variable probability density function of a normal that's something we've seen before you know do you guys remember this from back in the day oh man I'm already nostalgic we're going to have to say good don't be strangers you guys should come say hi whenever you see me that's not important right now it's not goodbye yet we got a lot of GPT to talk about before that uh but you remember this little expression oh man it just like makes me sad to think that this is one of the last times we'll be talking about normals H okay but anyways um you have the value of y you observed so we observed at 0.1 we know the true mean was a 0.2 and remember this is y and this is y hat or sorry this is y hat and this is y for those people F along uh and you have 2 Sigma squ so we can say How likely does the result of our neural network look you know or How likely is our random variable assignment but of course you know we don't really think about likelihood we like to think about log likelihood because it's easier to work with and if we took the log of this you know you'd end up with a a log of this expression you know what this thing is going to look like a big old constant cuz like don't forget it's only this thing here that's got parameters the parameters went into generating this number but the parameters didn't go into anything else so when we think about you know this as a function of parameters everything that doesn't have a parameter looks like a constant so this whole thing is going to be log I'm going to draw a smiley face for my constant you're like is that a Greek symbol no it's a smiley face but it's smiley face why CU I'm so happy when I have to deal with constants what's the log of this it's a going to be a constant plus you know log of multiplication is going to be log of this plus log of this log of e raised to the power of something remember exp just means e as a base raised to the power of cancels out what a good time am I missing a negative no yeah is there a negative here in the Gan probability density function there's a negative there's a negative yes what a crazy time to be alive I forgot but now but then I remembered just in time okay so this is going be come negative and I could write 0.1 or I could write y minus y hat and remember y hat was uh something that came from our parameters squared divided by a smiley face you're like you can't divide by a smiley face oh yes I can because it's just constant now this is all looking well and good but there's just one other thing I want to point us out we want to choose our thetas to be the argmax of this function and the argmax of this function doesn't care about this constant and the argmax of the function doesn't care about this constant either so the argmax of this is just going to be the same as the argmax of Yus y hat squared that was for one pixel and because every pixel is independent the likelihood should have had a big product here and the log likelihood should have had a big sum here and this thing would just have a sum over here and if I throw that sum back in I'm feeling a little bit fancy so it's going to be pink you get an expression that you might have heard people talk about it's the sum of squared errors these are all your predictions and the these are the True Values you look at the difference between those two and square it and you sum that over all of your pixels and you're going to try and choose the parameters that maximize this now it's a maximum of a negative so it's the minimize of the sum of squared errors and so you're going to try and choose the parameters that minimize the sum of squared errors it's a beautiful piece of probability that leads to a very very simple algorithm so going back here what you're going to do you put in your noisy image it goes to the neural network you make your predictions now you have the predictions and the truth you calculate the sum of squared errors and that tells you how good a job you did but that doesn't tell you how to change the parameters in the neural network how are we going to change those parameters in the neural network gradi desent gradient descent you can't set that derivative equal to zero and then solve for because like there's going to be a big old set of parameters in here but what you then do is you just say hey there's a lot of parameters in here for every single parameter I can ask the question what's the derivative of this objective function with respect to every single parameter oh man it's time to talk about history I'm going to try and throw in some interesting histories because I can't believe I'm so old I live through some history man my field is moving fast so if this is time and this is like AI progress I would say it has kind of felt to me like like this and there was this huge moment here and this was I'd say about uh 2013 in 2013 some amazing programmer came up with this amazing piece of technology it has a name it was called autograd before that date when I made neural networks you know what I would do I would construct my neural network imagine all of the neurons all the logistic regressions connected to each other and then me and my buddy would go to a cafe and we'd spend our day doing all the derivatives and if it was a complicated neural network you know we'd start with the last layer then we get the derivative of the next layer and we get the Dera of the next layer uh we made weird neural networks weirder ones than I ever ever see now we'd have like trees and stuff and it was great it was a good time but we'd spend the entire day doing derivatives and then there was this great idea that started in a program called torch which was originally written in a language called Lua does anyone program in Lua one person a little bit yeah I used to program in this thing called L it's like a Brazilian programming language and it was the first one that had autograd it was such a big deal we all learned this Brazilian programming language named after the moon um anyway and so what did autograd do autograd meant that in Python I would say here's my neural network it has this layer this layer this layer here is my objective function and then I would say automatically do all the calculus and it would just be one line and the neural network would know how to do the chain rule it would know how to do the chain rule like the derivative of this with respect to the end layer the derivative of that with respect to the the previous layer and it did all the chain rule so no one has to do derivatives now it's important to know that they exist and that's the magic behind the scenes but autograd you guys live in the future what a time to be alive okay what that means is that as an architect what you really need is just this function you have to figure out like what is the Lost function that is probabilistically inspired so I can talk about how good a job I'm doing and then I can ask the computer to update the gradients okay wait a second that's kind of it insane now in terms of the architecture here there's a lot of things you could do but there's one thing that turned out to be really important it's got to be big and this is going to be even more true for GPT but but for GPT there's two things that people realize you know not too long after autograd they're like since we don't have to do our derivatives what if we had a bigger neural network and we just got more data and people got better and better at building special chips for doing automatic differentiation and doing the actual gradient ascent and people got bigger and bigger computers and they noticed two things number of training time I'm going to draw two graphs and on the x-axis are two different things on the xaxis is how big are our models and on the xaxis over here is how long do they train and originally when I built models I would try and make more and more parameters but my models would kind of look like this uh and you know I never got that big let's say this is like 10 to the 10th and this is like 10 to the 10th and you know my model would eventually look like that like what does that mean that means as I'm training it gets smarter and smarter and smarter but eventually I hit what we would call a plateau and then I would call it a day I'd go have like uh um an adventure with my friends and I just assume that's what would happen and for the longest time I believed that all neural networks would hit the ceiling until one day somebody was like you know what I'm just going to get a lot more parameters and when you got to a certain level all of a sudden people started to escape the plateau and they started to hit these points where it didn't seem to have a plateau it wasn't maybe you know getting that much smarter that quickly but they hit this point where if you could just take a network like this one put more and more parameters into it and give it more and more data it would just keep getting smarter and then people did the craziest things like I used to train these things like you guys think about your your homework assignment like how long do you train your gradient descent for logistic aggression I don't know five minutes at most it really does Plateau out pretty fast what you think people do longer yeah but we tell them how many steps to do so I okay it depends on your computer like maybe you train it for 10 uh 10 minutes well once people realized this they started doing something that wasn't the most ecologically sound they're like what if we just keep on all the supercomputers and not turn it off for like a year and like every time the programmers go to sleep they wake up it was still training and people just went and went and went and it started getting better and better at D noising so these are the main narratives that led to Dolly I hope you guys feel a little bit like this is demystified this was a great idea but it's a great idea you could have had right you know think about the person next to you think about the person on the other side if it's not one of them maybe it' be you who could have come up with this and there's many more ideas like that out there this second idea that was a nice one you know instead of trying to predict uh removing the noise just predict where the noise is and we can do the math and that this leads to a nice little piece of mathematics where you know if you think of the noise as being gussian when you do the log likely and the argum max of it you end up with this negative sum of squared errors now I'm not going I'm going to say autograd man I'm like such fans if I ever meet the people who did autograd I would be like total fan booy I'm like you guys are amazing uh and I wouldn't say this is a great idea I think it's just like more like okay great we can do that all but that's kind of the big story picture so I'm going to go get my blue marker which is my favorite all say that was part one of today's adventure so I'm going to give you guys the pedagogical pause now guess what we're going to do next I won't tell you I'll leave it as a mystery take your two minutes pedagogical pause and then we'll continue uh today's lecture okay I got a little Lial my yes that e e e this so what we've talked about so far is doly and that was fantastic if I had to name that part of lecture if I had to give it a title it'd be Den noising because that was really the most beautiful idea in all of this that we're actually going to be really creative with our data add noise then train a neural network that can remove that can predict the noise and since the noise is gaussian we've got a really nice likelihood function we can do mle if I had to name the next part of this lecture I would call call it attention is all you need because that was actually the name of one of the papers that led to the great idea that really transformed this and you know the original GPT there there's a lot of history here at Stanford like the original GPT was written by a person called Andre carpy who is yet again also a PhD student in Stanford so a lot of this stuff was created by people just like yourselves so language presents a problem in our previous model we dealt with generating images but now we're going to start thinking about language and there's a bunch of reasons that language is really difficult one is we subscribe so much meaning to and intelligence to language we take it very seriously as a society but there's also more practical challenges and if I could point out one practical challenge about language is that it's variable length you if you think about your features we always had a fixed length number of features that we gave on our problem set six when you think about GPT you know we could control the image size it would always be fixed size images and that just made a lot about our architecture a lot easier but one of the things I'm going to point out about language that's quite difficult besides the fact that it's such a complicated concept is that it's going to be variable length so first let me tell you a little bit about the bad ideas that didn't work so one of the bad ideas that didn't work so was to build this well when I say it's a bad idea it will be part of our final solution they build these little neural networks and the idea of the little neural network would be you would put your language through the little neural network and the nural network could produce two things it could produce both the next word so maybe you hopefully predict socks and it could also produce a vector and if you took that vector and you put this Vector into the neural network it would hopefully produce the next word and you could have different words coming as inputs and so they built these little neural networks that you could use recurrently you could put in you know CS 109 through this network it' produce rocks and it would produce a vector then you could put rocks to this network it would produce socks and it would create another vector and this Vector would then become the output or sorry would eventually become input over here so like what you produce as output then becomes this input and we could call these things State vectors so you could you have like State at time zero and one of the things that your neural network would make is State at time one it seemed like a good idea and people made language models like this for a long time and they were fine they just couldn't do that much they weren't that impressive you ask for a recipe uh and we just start like giving you gobbley cook on some level though this wasn't what the really great idea was the really great idea here was hey let's borrow some of those ideas so we're still going to have a neural network it's still going to take in a state vector and it's still going to produce the next word but the great idea here is we're going to build a very special neural network that we're going to call an attention neural network and this attention neural network is going to take in all the words we've seen so far and it's going to put them together into a really really really special format that we're going to call the context vector so we have this neural network that I have not described to you yet but what it's going to do is going to take all the words and compress them into some important piece of context now let me give you a little bit more layer of detail about what it's going to Output there's a lot of words in the English language this little neural network here is not going to just produce a word that's a little bit oversimplified instead it's going to predict for every word in the English language the probability that that word is next so I'm not going to try and draw all the words in the English language but you can imagine it creates a big old dictionary and it'll say like rocks you know that's pretty high probability it's 0.001 and you could say like Stanford maybe that's less maybe it's 10 the4 uh but for every single word we're going to produce a probability if I gave you this neural network you gave me a set of words as input it would go through the special layer make context and start with some random State and It produced these probabilities how could you go from these probabilities to picking a word I've said a lot of complexity so I want to take a moment and solidify I'm going give you guys a minute to think about it why don't you talk about with the person next to you and the question I want you guys to figure out is if you had a neural network and it reduce word probability mappings how would you choose the next word talk about it but also take it as a moment to figure out what's confusing about this and we we'll take some questions afterwards okay go for it think about it e e okay an idea what would you guys do if I gave you a neural network it took a couple words and it gave you back this dictionary where words with the probability that they come next what would you do you could choose the most likely word that is not a bad idea uh and that would be possible but here let me just tell you about one thing that would happen do you remember when we had that great idea we had like the white pixels but I said if you did that then every time you ran this you would always get the same image if you choose the most likely word then every time you write cs1 and iron rocks you'll always get that most likely word because because again there's no Randomness in the neural network itself so while that would be a good idea you'd always get the same production we want it to be a little bit different each time okay yes what if you just pick a word that's like likely enough and then yeah like sample randomly from all the words is it good enough and then put it in there it is I don't know if that's what you're going to say but sample randomly is what people do so you get this whole dictionary and you know what they kind of treat it like multinomials there's all it's like this big it's got all these sides and here's the probability of each outcome and they take this gigantic English language side Dice and they roll it and you imagine they have to have their arms like this cuz it's so big and they'll get one word and it's most likely to be the thing that has the highest probability uh but it could be something else but let's say for now when you sample you get the word rocks but you know if there was another high probability word when you roll that dice it's very possible you get something other than rocks so at this point oh sorry ah it doesn't say rocks that's what it just had CS 109 rocks socks what what did you say is great cs9 no you can't say cs9 rocks is great it's always Rock socks like what else can you rock and roll no it's definitely socks um okay so socks is the word we choose sorry now at this point we don't want to just produce one word we want to keep doing this so in the same way that we when we were doing the den noising we'd reuse our neural network to remove more and more noise we're going to reuse our neural network so at this point we have a neural network it's produced the word socks and it's also produced another Vector another set of neurons you know you can imagine you can just take the last layer of activations here and treat that as a vector and we're going to reuse that exact same neural network but now instead of having C9 rocks we have cs19 Rock socks and we're going to put that through this special thing we haven't talked about in detail called a tension layer again that's going to give us a set of neuron activations and then we're use the same neural network that neural network wants a state Vector it wanted a context vector and it's going to Output another dictionary uh and I don't know what comes after socks can somebody help me out here what could go after socks be be the large language model period yes certainly that happens with high probability uh so when you sample from this dictionary you get period and it just keeps doing that now this idea had been around for a while and people had tried it and it just didn't work that well because they hadn't figured out the magic that goes into this little mechanism and what they do with this little mechanism was kind of interesting they built a neural network that could take in any word so let's say you had CS 109 they built a little neural network and that little little neural neural network would produce a single value called an intent work let's say that this would um well actually I guess it would produce two things um let's say this is word zero because it's the first thing it would produce a little embedding of the word and it would come up with an attention value so a number between you know 0o and one and this little neural network it also took the state Vector so you would take whatever state Vector you had and and it goes into this layer here so we have state time Z and any word you can put it through this second neural network we'll call this the attention neural network and the big thing is that it produces this attention Vector this context that comes out of attention is really a simple thing that say the context would be equal to the sum over all the words of the words Vector weighted by its attention and the really simple idea here is this little neural network is like a focuser it would say before we even start to process this sentence let's have a neural network which says what we should be paying attention to for example when you're producing the word socks you probably only need to work out the word rocks it doesn't matter what came before if you put a lot of attention on rocks you can realize that the next word was socks but it might be the case that you know when you put period you have to look and realize what are you saying rocks and be like oh let's put attention on this word since this word CS 109 obviously it Rock socks period and so it can know what to focus on and it seems like a simple idea but this simple idea we think was one of the biggest changes in architecture and then all of a sudden people are like okay this stuff seems to work and then again when they start to scale up they end up with things that could speak and create things that look like meaning I'm almost done I know there's a lot going on in these two boards and I should take a lot of questions but that's kind of the big idea we're just missing one thing you could get in Python and you could Define in pytorch which is the successor of this torch that was originally written in Lua and you can say I'm going to Define all these neural networks I'm going to just Define my States and I'll have a neural network that takes us as input it's kind of like you're making a tree uh the graphical structure you make your graphical structure and then you call autograd you'd say I want you to update all of these based off words where do we get these words the dictionary is kind of good yes there's many words in the dictionary but in order to do this we're going to need a whole bunch of not just words we want to have some like Words we're going to make a prediction and then we want to know was our prediction good yeah an idea look at a bunch of written stuff yeah they get a lot of written stuff you know where they find all that written stuff internet yeah not in the library the internet's got a lot of written stuff so somewhere on the internet somebody was writing a passage and they're like baze was cool but cs19 rocked sock like uh fox fox like uh fox a fox and someone wrote this and then people would take this word on the internet and they would create training examples they would say okay I'm going to put this into my neural network and I'm going to see is it able to predict this next word the last thing we would need if we wanted to bring this thing full circle is not just how you use the neural network but how you would train it which means you need to have something like this you need to have the mle scoring function that we're trying to optimize so this says CS 109 rocks we look at the real word and it's a socks and you know what we had produced a probability we' said How likely it was and so since we had a probability of socks we could use the multinomial probability Mass function say okay I thought socks had this probability I can say you know what is my last function given that so very similar to how you did your loss function here you know here you use the beri loss function because you had a class with two labels here you've got a class with multiple labels and here You' use your probability of one in your berly probably Mass function and here you use the multinomial proba mass function and then you're done you've got a likelihood function and the next big idea was just data give it lots and lots and lots of data now to be clear what I described is probably gpt3 it's unclear what is behind gbd4 they say it's going to come out between December and February they say it's going to be as big a change from gpt2 to gpt3 and I can't tell you what's in GPT 4 maybe they're like you know what if instead of of attention we do retention structure then everything works out um who knows maybe they just came up with a better chip that could do more autograd more quickly uh I'm not sure but that's the future coming I know this is probably a lot the first time you see it but maybe you can appreciate that there's finite number of things to learn there's a lot more structure here you can appreciate for a language it's really nice that you don't have to do your derivatives that all your derivatives can come for free because there's this interesting reusing of neural networks that gets really hard to do the math round but computer can go through and do all the chain rule that you want at the end of the day you know you're producing probabilities so mle is still a tool you can use and this really simple idea of I'm going to take each input and decide how much I should care about it that simple idea was kind of the equivalent of noising and and it seems that attention is really really good in fact this single idea was so good you start to see it in all other places like in algorithms that try and learn how to drive instead of trying to do some sort of really complicated Network now they're trying to use attention where they can just say okay if I can see out my car what should I be paying attention to when I make my next decision so this simple attentional Network turns out to be very helpful again autograd is a big deal because now you have two different neural networks with parameters all over the Place doing lots of different complicated things and all you would need to do is string up your neural network and then say computer do the optimization for me okay questions comments concerns yes this is backtracking a little but I was wondering like instead of do in gradient asent or desent like why can't you just solve for like the closed form of the the log likelihood and then find your characters that way is it because like just too hard of a equation yeah it is is too hard of equation I'm prettyy sure there is no closed form so let's talk about why it's hard first of all if you had one parameter you would solve take that gradient set it equal to zero and then solve right if you had two parameters you have two equations two unknowns and then solve these things have close to a trillion parameters so you can set up a trillion equations a trillion unknowns and then try and solve but I it gets worse you know when we think about our neural networks they're all logistic regressions like at some point you have the activation of one logistic regression cell inside this massive neural network is going to be equal to something like a sigmoid uh of the weighted sum of the inputs Times by the weights so we're going to say inputs I weights I do you guys notice how the predictions we were making if this inner part ended up being positive we would just predict a one if this ended up being negative we predict is zero in your own homework but we still had the sigmoid originally the sigmoid was helpful because it allowed us to interpret the result as a probability but it turns out the sigmoid does something else that's really important if you got rid of that sigmoid you said every single neuron was just inputs multiplied by weights and you didn't put through the squashing function it turns out that you could have trillion parameters but it would still be equivalent to if you just had a linear system this sigmoid here makes it nonlinear in really complicated ways it also makes it basically impossible to do close forms um oh I want to tell you guys something and you guys tell me if I did the wrong thing how's that you're like Chris what did you do I know when I make logistic regression I use a sigmoid but sigmoid has this really nasty property that if you look at it um it has its all its values are positive it's always between zero and one that's great if you're thinking about an output but if you're somewhere in the middle of a neural network it turns out having all of your weights have positive values is really annoying and problematic for really subtle reasons and so people don't want their inner neurons to be shifted so you could do a sigmoid and subtract by .5 did you guys want to know there's another function T anyone seen this before tan yeah the hyperbola tangent it just it's not that special it doesn't have a problemistic interpretation you wouldn't use it for the final layer but you can use it in internal layers it looks a lot like the sigmoid but it's just centered nicely and it turns out this is a really big deal if you want to have inner layers and neural network there's other things you can use rather than a tan but no matter what you do sorry I went on a tangent but you need this you leave this nonlinear piece right you needed to like put through some squashing function to make sure your whole thing doesn't end up just being linear um it is important but it also makes it impossible to do the Clos form set it equal to zero you're like here's my question I'm like whoa tan and then we're back but I just I need to get this off my chest that even though we use sigmoid for the final layer in the inner layers we actually use tan which looks very similar but it's centered around zero yes the like array of parameters for attention interact with the ones for like the next word and the relative probabilities for those words like is it that you first do one and and then the other so in the forward pass you can imagine in the forward pass you first do the attention on each word and what you do on each word every word will end up with its vector and also its weight so you do this for every word that's your first thing then the second thing is you run this little function you end up with a context at this point you have a vector for context you always keep a state Vector it's just always it's like a set of neurons that are just holding on to some memory did you want to know what happens here when you have to merge those two things into this box okay wait for it if you have a neural network and it's getting inputs from the top and it's getting inputs from the side check out what we do so these are it's just a vector a little bit like your X's but now you've got two of them we do this we stack them you take your first one you take your second one you stick them together and now you got one vector going into your neural network very similar to how you have your X's going into your logistic regression so you know we use this this is just a diagram but really it's these two are both inputs into the neural network okay is that a question yeah exactly what like the state one and state two actually what makes it meaningful what is it even story they're just neurons they're just like you know you have these you have some neurons that produce values and you have a bunch of them and then you store them in memory and in theory it's holding on to memory it's holding on to ideas that it's constructing as it's going so like maybe it's trying to produce a poem in Pirate speak and it's producing it word by word and it has to keep track of where it's going in some space and it stores in these neurons and I don't even understand what's going on with those neurons but it's some sort of black magic and it just like keeps it uh so that it can make its prediction for the next we it's amazing structure this is sh uh yeah and you know you can imagine much like when we were producing images remember producing images like you could produce one pixel and then another pixel and a third pixel it's a lot like that these are just a bunch of neurons and they're Al going to be produced a little bit like their pixels it's just we're going to store them as state use in a different way we're actually wow I was like two algorithms how long could it take but I wanted to draw on the board because I wanted to go slow and go over the details and maybe if I could leave it on this is it does normally I'm you know I was the person who was kind of hoping we'd never hit general intelligence I was kind of hoping we'd stay in like the simple world where we just do our educational things but uh I must say until Thursday I was like oh yeah people are making their gpts how cute oh my God Dolly I can make a picture of a tree this is fantastic and then I started playing around this a lot over the weekend and in my heart I feel like it is a bit of a change and it's not such a big change it's not like the whole world's going to reorient around this stuff uh but it is a new technology and it you know what it reminds me of did I say I was so old I remember when mobile phones became a thing I was a computer science major and all a sudden we have mobile phones you can make apps and there was a rush everyone wanted to make an app my God I probably should have made a map if I made an app maybe I could have retired young not to self hey you want to go into like a business this we're going to do some cool GPT based neural network no let's put in the p app oh we'll put in the P set app and then we'll all do it together you know and then did mobile phones change everything yes and no you know there was a whole set of new ideas that came things were possible it was a very exciting time to be a young researcher because all of a sudden like I was I got to The Cutting Edge a lot faster you know when I look at gpt3 you I've told you a lot of what I know about gpt3 and um you know soon you guys can be on that Cutting Edge and I think that's a little bit exciting it's a little bit terrifying because there's uncertainty and uncertainty is always hard except for people with st probability we like to make decisions under certainty I don't know and I would welcome you guys to this conversation you know what do you guys think it means and and where do you want to see it go how would you like to shape it like what role would you like to play and then come back on Wednesday it's our very final class we're going to have to say goodbye I'll miss you don't be strangers have a fantastic day you guys rock see you on Wednesday