happy Friday cs19 how are you guys doing today awesome I appreciate that I also appreciate because I know it's a Friday and I know it's a Friday before the midterm so wow do I appreciate you guys coming to class thank you so much um a couple quick things before we jump into things be because it's a Friday I'll start by telling a story as always um and I think the last time I told you the story of how I met my wife is that right but where do we end up did I get in the the truck and did I make it all the way to Romania yes oh okay so then like I get to Romania and there I run into my wife again and she's saw well she's not my wife at the time I run and I say hi I made it and then I got this wonderful story to tell her about how I took all these trucks and I think she was very impressed um and and at this point it just was a great reunion uh we had about four or five days together it just felt fantastic but as life would have it she continued to have these exciting plan she was going to go working at a refugee camp in Hungary for a while uh and and we thought we were going to go different ways and we went different ways again but the most serendipitous thing happened a few days later I was in a train in Hungary in the middle of the night and it's kind of cold I'm just minding my own business and I talking a little bit to to the person in the car with me and then Laura shows up apparently she had gotten into the same train and the same random part of hungry the same night and sat down in the car right next to me and then she heard my voice she's like what the person who came all the way by the trucks to come find me in Romania how did you end up here and so at this point we're like oh my gosh the world like has brought us together in so many different ways we ran to each other on the island a bunch um we managed to make it to Romania together now we ran to each other on a train and at that point we decided hey what if we like intentionally ran into each other so even though uh she was going to a different destination than me we decided hey let's go meet up and then we met up and we started traveling together and then you know our past didn't really go that different ways uh until this day so that's the second part of the story of how I met my wife and I suppose even though there's this really cute meat cute certainly the more interesting part of the story is the one where we got to intentionally choose to spend time together and I got to know this wonderful woman and all those moments those are the really special things the meat cute is just like a nice but um that's not important right now what is important is oh boy do we have a cool lecture for you guys I appreciate you guys coming to this lecture so I want to give you something very useful very cool we're going to be learning General inference in particular you're going to be learning a partic an algorithm called rejection sampling you guys might know some of the other professors at Stanford uh but I was talking to a professor um no Goodman who's one of the more famous modern statisticians of our times happens to also be at Stanford and we were talking I said you know I think this is something I really want to bring into our inro probability class because I think it clarifies so much and he says yeah if there is one thing that I thought all interest uh probability people should see earlier it's this idea of rejection sampling so you're going to see a beautiful idea uh with no Goodman stamp of approval in today's class we're going to do two things we're going to finish our conversation about correlations and we're going to be learning about rejection sampling and it really comes around this beautiful idea that you guys saw on Wednesday called beia Network and in case people lost the plot I put the whole plot line up here on the board so the whole plot line of what we're doing is we would like to be able to solve beautiful interesting real world problems with the power of probability tools but most real world problems have a lot of random variables that in itself shouldn't be too big a problem except it leads to this interesting computational problem one of the most straightforward ways to represent many random together is a thing called a joint table and a joint table is exponentially large so as you have bigger worlds that you want to represent a joint table would need exponential numbers of probabilities that's both a space issue and you'll have to do exponential amount of work in order to estimate those big problems how sad we wanted to do this but we can't enter this great idea of B Nets that we saw on Wednesday and Bas Nets is a way to represent a probabilistic model for a real world problem but it has this beautiful uh property that it's going to be o of n in terms of n the number of random variables so B Nets allowed us to do probability for real world problems now that leaves two to-dos for you guys you would need to know how to create a baset and you need to do this task that we call inference but inference is just a fancy word for saying do conditional probability in the context of this bid random variable if you guys have plotline questions please do ask me now now towards this idea of a Bas net the key concept was if you can have some expression of which random variables cause which other random variables you can really compress how you represent the joint distribution but that would require you to be able to create a Bas net and there was this side plot in Wednesday's class and I'd like to pick up on it oh and of course there's lots of real world problems that you could think about but our running theme has been WebMD so WebMD you say a bunch of symptoms and it can tell you for all the possible diseases the probability of you having all those different diseases the beian network was defined in a few ways one of the ways it's defined is first you take all your random variables and you make them nodes in a graph and then we're going to draw lines in a directed way from nodes that we think are causally impacting other nodes so we think flu causes you to have fever and flu causes you to be tired now there is a big asteris here in the O of n is that this o of n kind of assumes that you don't have too many arrows so if you have a sparse number of arrows then you get the nice compression once you draw arrows you have to give the probability of every random variable given its causal parents and if you do that that's your compressed way of representing the entire joint distribution so for every probability give it the probability of the node given its parents so for example tired has two parents flew and undergraduate for every combination of values for its parents you have to give the probability of tired but if you do that for every random variable you defined a joint in a very compressed way wonderful there is a semantic way of thinking about these Bas Nets why they're so amazing and one way of thinking about it is that they're generative they tell you the story of how the joint comes to be then the story that's told here is first people have these priors whether or not they have the flu or whether or not they're undergrad and then we generate whether they have fever from those so it's like the world gets generated from top to bottom and I really like this idea of thinking generatively and thinking about the process which from which a joint is created it's not just for WebMD you know there's lots of different situations where people might use these like the figuring out why you have uh communication loss um or battery failure for um how systems can go down or maybe you care about you know detecting whether or not there's a chemical uh in a par or particular matter in the air for all these things you might want a b that they're very powerful tool it's just a general way of thinking about probability and what a language to have so if you want to be working on this skill creating a Bas net we have to be able to come with the structure but then we also have to Define for every random variable it's probably given its parents you can express that in a few ways you can express it mathematically or you can express it through code what a time to be alive um and then of course as we mentioned the Assumption in the Bas net is that if you take the probability of every random variable given the values of its parents if you multiply all those together that's where the joint comes from so it's a simplification of how to express the joint okay that's where we left off what we haven't really talked about is how do people in the real world design these like if I gave you a data set you could make a basan network but you maybe know the causality if you know the causality you're done just draw your nodes and and imp put the causality but I want to give you a real world data set where people might want to design the beian network the real world data set I have for you guys is all these people in the UK saying what sort of Music they want so every row here is a person in the UK saying one to five do I like dance music folk music country music classical music uh musicals pop and so on and you'd like to have a model of how all these random variables are interacting together making a joint table would be too hard but if you could come up with a beijan network which says how these different random variables might be causing each other you could express this entire complex system in O of n space if you could design this great you have an expression from which we can solve probably questions we'll start to be able to say things like okay you listen to a punk song therefore I think you like Punk which will change my belief in all these things and I'll have a better belief of whether or not you like ree or Opera and you'd be able to do all these wonderful inference tasks if you could design it now maybe you just know how music works and maybe you just sit down and be like okay people who like plot music also like you know um class music like Opera uh Dancy music they maybe like punk music too and like maybe you can just design this from scratch but one of the cool ideas we touched on at the end of yesterday's classes there are data driven things that I do when I'm given a data set to try and inform my decisions and one of them is I take every random variable and I see how much it co-varies with other random variables so if one random variable tends to if for like a person they tend to be more Rocky then how does that change my belief in whether or not they're into classical music or dance music or you know alternative folk music it would be great if we had a statistic and I do have a statistic that I calculate for every pair of random variables um that particular statistic that we talked about lightly I just want to finish the conversation on is called covariance and covariance measures for two random variables like let's say x axis is how much somebody likes heavy metal Rock and y axis is how much somebody likes let's say Punk when people tend to deviate from the mean in a positive direction so they're more likely to like heavy metal Rock than the average person they're also more like to like rock than the or sorry Punk than the average person so these two random variables when people deviate from the mean they tend to deviate in the same direction and we can measure that we can quantify what we want as this it's like the expectation of the product of the deviations of the means in both directions and if you were to then rework the mathematics of what this is you would get that it's equivalent statement to take the expectation of the product of the random variables minus the expectation of 1 * the expectation of the other something you could calculate from data and this is a measure that people use to try and sus out our two random variables related I want to talk about this a little bit because it's just a cool concept and something that you're going to need later on so it's a cool concept buried in this idea of beian networks can I just tell you a little bit about this number so there is this number you can calculate for any two random variables and when I see a high value of that random that number I think those two random variables are very related when when I see a very high negative value I think that those two random variables are related but in a negative way so if you see one random variable becoming High you expect the other random variable to go low but when I see a zero it turns out there's a complicated analysis I think that there's one of two things going on either they're not related or the relationship is somehow not captured by this number can I give you an example of two very related random VAR whose relationship is not captured by this measure of how much two random variables coary okay here it is it's not uh not a beautiful picture to go with it but it's a simple idea imagine I have a random variable that's NE 1 Z or one equal probability so one third chance of being each of those things on its own that's fine to think about and I'm going to have a second random variable and it's so related to the first random variable that if you tell me what x is I will tell you what Y is if you chose X to be zero uh then uh Y is going to be one if you chose X to be anything other than zero Y is going to be zero are these two random variables related oh yeah one is defined in terms of the other one's value so if you tell me X your belief in y will really change in fact you'll know what Y is they're not independent at all but I just want to show you the limitation of this measurement so X is 1/3 chance of being negative 1 1/3 chance of being zero 1/3 chance of being one why is going to be determined based on X and it turns out because um as long as X is zero than y is one there's a one3 chance that uh Y is one and a 2/3 chance that it's zero what you have in front of you is the full joint probability table if you were to calculate the co-variance you'd first get the expectation of X well the expectation of X you're going to take each of these numbers and wait by its value good times expectation review know how to do that by Tuesday what if you wanted the expectation of Y what a reasonable thing to want well y can take on two values Zer and one and it takes on zero with the probability 2/3 and takes on One with probably 1/3 so it's expectation of 1/3 not so bad now no matter what value X is or if x is zero then Y is something non zero but if x is non zero then Y is zero one of them will always be zero so if you multiply X and Y no matter what value X was that product will be zero so in all cases the product of X and Y is zero so the expectation of X and Y is zero now if you want to calculate this statistic we've been talking about this measure of covariance it would be 0 minus 0 * 1/3 which leaves you with zero so covariance it's supposed to be a number which captures how much these two variables coary it's in the freaking name and yet even though these things are very deterministically related coari ends up being zero what's gone wrong here covariance is one number to capture a very complicated concept if you have very large values of coari or very large negative values that is Meaningful but covariances of zero could mean one of two things either there's no relationship or it or there is a relationship but somehow the the covariance is washing out and there's a very uh fancy proof that shows that this is going to be zero if there's somehow a very nonlinear relationship uh but that's a little bit beyond the point of cs19 the point of cs19 is you need to know that there's this thing covariance between two random variables I want you to think about it when you want to construct a baset as a way to get information and I want you to be able interpret those values and I want you to know that even though coari is a signal of relation ship coari of zero could mean few things it's not a proof that they're independent yes well the value of a coari we got the of variables will the result always be between one and um it will not be what a good question so that's the first problem is this one and the second problem that we're going to talk about in a second is that it's not bounded covariance could be a large or small number so let me give you a concrete example of that uh here's weight and height for some people and you know you could calculate the coari which says to what extent do these two random variables vary together and you can get the coari formula and you can calculate each of the terms and then you can get a coari of 46 and you're like does 46 mean anything is 46 large is smaller than like 2,000 but it's bigger than like five I don't know is this a meaningful number and people thought about this for a while and they thought okay even though covariance is semantically something we care about people don't know how to interpret these numbers so come along a very fancy person who came up with a very fancy idea there is something called the kosi Schwartz inequality which the proof is beyond cs19 but the conclusion is this if you calculate the coari of any two random variables it is always going to be less than the standard deviation of one random variable multiplied by the standard deviation of the other random variable and it's also always going to be larger than the negative of that it's not possible for them to be uh bigger than this product proving that's beyond class but the result of that is a different number that you've heard about you've heard about this thing called a correlation and the correlation is just a normalized version of covariance we take the covariance and we divide it by this number why this number because we know that the covariance will always be less than this number why is that nice correlation is a scaled version of covariance which is always between negative 1 and one which is great um if you go back to this plot I actually used correlation here instead of covariance because I wanted everything between netive 1 and one so that everything was on the same scale so correlation is a scaled version of coari and coari is your measure of how much two random variables relate to each other okay and since it's a Friday I just have some like relaxing chill let's laugh at correlations so one problem I've talked about for correlations is um a correlation of zero doesn't mean that two random variables are actually independent of each other they could be very dependent just their relationship isn't captured by this particular way of talking about them deviating from means together but this is a little bit funny too um sometimes things have high correlations but they're not at all related so if you go into Data people have mind for funny correlations and here's two random variables just happen to correlate but we think there might not be a deeper reason rock music quality and US oil production so apparently like the 1960s were like the Golden Age of uh rock music and there's a whole bunch of us crude made between them this ends at 2007 um but we don't think that oil is causing rock music and we don't think that rock music is causing oil I'll ask you why there might be high correlations for these things so think about that for a second two other things that have high correlation but maybe aren't that related uh per capita consumption of cheese and number of people who died by becoming tangled up in their bed sheets unrelated but they tend to have a high correlation uh divorce rates and butter consumption butter consumption is down so our divorce rates is it a coincidence Perhaps Perhaps not um does anyone know why these things that seem unrelated might have high correlations yeah in the back there might be something else that's causing them the there could be something else that's causing that is possible turns out for these three there is a different mechanism going on of data yes can find relationship yeah if I took a random number generator and a different random number generator and I generated a data set from them they're probably going to be uncorrelated because they're all random but if I repeated this process a billion times two random number generators are going to produce two random two like a random set of data and it will just happen to be that they look correlated if you look for enough pairs of random variables even if they're all unrelated eventually just by chance you'll find two that happen to be high in the this correlation thing so there's like we are fishing through too many data sets to find correlations anyways that's just a fun aside um but the real reason I brought it up is because when I look at a big data set and I don't know how to model one of the pro tips that I would use is I calculate the correlation between every pair of random variable to try and figure out which ones are related to each other so at this point in the plot could you create a basan network well sure there's a few options for the structure you could either guess at causality or you could calculate correlations and you again try and artfully design a nice Bean Network you then need to give the probability of every child given its parents we'll talk about that in a bit but at this point sure you could create a baset if you needed to question mathematical way to determine the direction of causality is there a way to determine the direction of causality a little bit here's what's easy to do you can disprove causality or you can have a high probability that there's not causality it turns out it's very hard from historical data to say there is causality um it's you know maybe you've heard the thing like correlation does not imply causation it's very hard to pull out causality though it turns out sophisticated mathematics can disprove different causal relationships yes um last class we talked about might cause you to be tired or have you to fever how does it work where um it's like the opposite like something causes something not to happen like work yeah you still use causality so if like I don't know taking a vaccine will mean you're less likely to have flu I would still cause that call that causal even though it's like a negative cause um it's still one is really influencing the and creating the value of the other one good question but the day the real point of today's lesson is let's say you have a basan Network I want you guys to now be able to solve any probability question you might want to ask about it one option is algebra so by Algebra I mean like take this Rand this ban Network and you can just calculate using pencil and paper or maybe a piece of python any probability question we talked about this one where you can compute The Joint just by Pro calculating the probability of things in order from parent to child so probably of flu times probably of undergraduate times probably of fever given flu equals zero times the probably that tired equals 1 given that undergrad equals 1 and flu equals z if you multiply all those things together you can get the probability of the joint it's like the bean network is hiding probabilities of joints it's a way of representing joint probabilities but if you want to do a conditional probability we talked about this last time you could still do it using joint probabilities I mean using joint probabilities you could know the probability of any particular combination to all random variables and because of this you could use the definition of conditional probability this probability is equal to the probability of everything divid by the probability of the right hand side but that probability of the right hand side you can get through marginalization you can take your joint and you can think about the case of the right hand side these three where flu equals one in the case of the right hand side where flu equals zero add those two things together you Marg out flu and you're just left with the probability of the right hand side so that works it's one way to calculate but what about this probability so I have a little challenge for you guys chance for warming up chance to meet the person next to you how would you calculate this conditional probability I've given you a Bas net which means basically I've given you a way to calculate the probability of a joint could you calculate this probability and I encourage you to think well I've got the joint if I write this in terms of conditional probability could I use the fact that I've got a joint to calculate each of the things in my definition of conditional probability that I need can you go from a joint to a probability of a assignment to fewer random variables think about it talk to the person next to you and then we're going to come up with something Al together not e okay okay let's talk together questions concerns what's confusing about this I wrote the definition of conditional probability for the thing you're supposed to calculate and I want to remind you that with a basan network or any probabilistic model for many random variables the first thing is to make sure you can calculate the probability of a joint which is if you're thinking about it as like a programmer it's like I'm giving you a function if you give me assignment to all four random variables I've got a function which will give you the probability of that joint assignment what I don't have is a calculation for the probability of just two random variables so I've got this I want these things is there a way to go from one to the other you could use law of total probability yes it has another name it is the law of total probability but the law of total probability has another name often when we're talking about many random variables together does anyone remember what that other name is marginalization law of total probability is the same thing it's just law of total probability applied to joints so if you use marginalization it's a way to go from the fact that you've got a probability of many random variables to calculating a probability with fewer where you just don't know if somebody's got fever or you don't know if they've got flu so you know again if you want to know the probability of flu and fever you want to think about all combinations of undergraduate and tired so you know one way of thinking about this sorry um this is for the top bit so this is the one where you just don't have fever so for the top bit it's like you're going to add in Fever equals 1 with fever equals 2 and if you add those two probabilities together you marginalize out fever and you're just probably of this and this and this fantastic for the bottom we need to marginalize out both flu and fever and so it's like you Loop over all possible combinations of flu and fever and for each of them you calculate the probability those things you're marginalizing out with the probability of the things that you're holding constant uh and then you can end up with with just the probability oh of undergraduate and tired yes you're left with just the probability of undergraduate equals one and tired equals one marginalization allows you to go from knowing probability of many random variables to knowing the probability of fewer of them yes how do what need to be marginalized out and how do like what does it mean to marginalize something out okay so I want this I Want U and T what I've got is U and T and the two FS so I know I need to marginalize out the two fs and to marginalize out I just sum over assignments to the two FS so I'm going to call X my assignment to flu and Y my assignment to fever so you have to sum over the ones you don't want take the joint some are the ones you don't want and you're good if you did this if you did this then you would end up with a probability that somebody has a flu given these two assignments this is great and you can definitely calculate this but it does get a little bit complicated you know it can become really hard if you had thousands of random variables and you only observed two you're going to have to marginalize out every unobserved random variable and think about WebMD you can have pain in your toes you could have a headache you could have a stomach ache maybe you just feel a little bit tired today or funny like there's thousands of random variables and we have to marginalize them all up hey sorry okay um and so if you have too many random variables you have to marginalize them all out this calculation is going to become really really expensive darn it we were doing so well we want to be able to do inference and we want to be able to do it in a scalable way but right now even though we've got a mathematical way of doing inference it's not going to help us as computer scientists we need to think of another way and the beauty of the way I'm going to teach you I will teach you a more General way that is very powerful but it also covers so many intuitions about probability so if you can follow along not only are you going to learn an amazing randomized algorithm that will teach you how to do inference for any basan network or any problemistic model for that matter it's also going to just like teach us some good Hardy probability along the way the very simple idea is we're going to have this thing called rejection sampling Step Zero you have to define a basan network and you have to Define it in code to Define Bean networking code you basically need to write all these probabilities and you have to write a function which can generate a sample of the joint or calculate the probability of a joint the steps are going to be maybe you're going to get an obser observation from the user so the user is going to tell you I observe undergraduate and I observe that somebody's tired there's no algorithm here this is just the the user telling you the puzzle you have to solve and now we're going to try and figure out the probability of the flu given their observation and there's a two-step process and it's so straightforward you guys could have invented rejection sampling yourself the straightforward way is can you generate me a whole billions and billions of fake samples from your joint and we're going to put that into a big data set so give me a billion fake examples from your joint and then to calculate any probability I'm just going to resort to Counting so it's got like a little bit from every part of class you got a bit from like the problemistic models part of class you got a bit from like the the foundation part of class and a little bit of counting for the beginning of class and through all this you're going to be able to do the inference task for any problemistic model it's going to be that straightforward are you guys ready for it you should have two questions your first question should be how do I sample a ton and the second question should be how do I do counting if I've got samples to answer any probability question shall we jump into the first one how do we sample a ton it's this easy if you wanted to sample a ton you Loop many times and make one sample and add that to your list you're like oh Chris that's a very satisfying answer says nobody why is this not a satisfying answer it's like okay sampling a ton obviously requires you to be able to create samples what I really want to understand is how do you create a sample creating a sample means I'm just going to generate one fake person and this fake person we're going to observe everything about them we'll know if they have the flu we'll know if they're undergrads we're going to know if they have fever and'll know if they're tired in fact we did this a little bit on Wednesday's class I showed you a function like this and this function made me a fake person Where You observe everything about them You observe if they have a flu an undergraduate fever or tired not that they have an undergraduate whether they are an undergraduate if you run this it gives you one person and it turns out that person's probability of having this particular assignment come straight from the joint if you run this 10 times you'll get 10 people where you'll observe everything if you run this a 100,000 times you'll get a whole 100,000 fake people if you look at any particular assignment like they have a flu they are an undergraduate they have a fever and they are tired like ones to everything the probability of seeing 111 will be the true probability according to the Bas network if you generate 100,000 let's say the probability of getting a 11 one1 is truly 10% if you generate 100,000 of these you expect to see 10,000 people with a one one one one sampling is such a nice intuitive idea it sometimes weird to think about but do you guys know in problem set 2 where you guys had to write a function which would from aan like you'd run the pan function you pass Lambda and it give you back a number that's a sample and then you wrote the exponential function and you would run it and it would give you back a number every time you ran it that's a sample this is just a joint sample instead of giving you one number it's giving you a number for every random variable okay this is half of the algorithm so maybe you have questions it would be great if you had questions I would love to talk about it yes how do we guarantee that probability in the end is samp from the joint I guarantee you that it's sampled from your beijan network and if your bean network is any good this will be a really good representation of the real world if your bean network is totally made up and completely useless then so will these samples be so if your bean Network's good these samples will be good but they perfectly represent the network Yeah question how was burn implemented oh burn i c i get a probability uniformly between zero and one and so maybe when you pull a number between 0 and one you get like 0.892 and if that number is um less than the parameter then I pass return back a one otherwise I return back a zero yeah lots of good ways to implement it yeah question um how like how wrong would your sampling be if you like if you had the the children as the C parents good question you know it it's hard quantify but I tell you this is if I were to try this out I might do something like have a few beian networks and maybe I'd have some people for whom I know whether or not they had the fever and I would try seeing which Bean network is leading to the most accurate predictions but otherwise if you flip the arrows you'd be wrong it'd be so hard to come up with the probabilities like the probability of being an undergraduate given you have a fever seems like a harder probability to come up with um but even if you did it and if you came up with reasonable probabilities you would still get a reasonable answer have you guys heard this this saying all models are wrong but some models are useful turns out the more realistic your model looks the better your answers will be uh but even if you're a little bit wrong you'll probably get pretty good answers back to the C thing like since we can't tell determin causation from coari like yeah what like what's the like how does it actually help us Creator net I mean it's an art form creating a network is an art form it turns out there is this algorithm called structure learning which you'll learn about in cs228 where you can just like learn the whole thing purely from data and you try and do things like learn different causal patterns which can't be there there is there are algorithms for that but I'd say most people making basan networks it's art and love and dedication and data so they'll look at the correlations they'll come up with their understanding of how the world Works they will art create aan network and then they'll probably test it maybe they make a few ban networks and they'll test them see which ones come up with the best predictions it's a good question and the real answer is art and craft but you need it because if you don't do it you're stuck in something you can't even compute yes summarize you basically randomly generate people from your probability model and then you see if any of those people can oh yeah now now you're getting into my second step all I've done so far is make a bunch of fake people but the important thing about the fake people is that for every fake person I can see all of their variables how many hey how big is your computer I'd say 100,000 it'll just take you a second uh but if you wanted if this is like life or death then maybe you want to do like 200,000 I'm kidding twice as your uh there is a formal way to think about how many you'd need and we'll talk about that later but basically just recall if you're making a fake person start with choosing the values of the parents and the node the network and then you choose the children like flu is dependent on whether or not sorry fever is dependent on whether or not you had a flu so you want to choose flu first and tired is dependent on whether you undergraduate and flu so you want to make sure you choose tired after you choose those two random variables and after this you've sampled I have explained to you half of this beautiful randomized algorithm do you guys want to hear the under half the other half is called rejecting that's why it gets its name rejection sampling but just to be clear after I call this part of the function I'll get a whole bunch of fake people and in this in this fake data set this was flu this was undergraduate this was uh fever and this was tired everyone's tired the second half is simply rejecting and Counting the simple idea is if you want to figure out say the probability of somebody having fever given they're an undergraduate and tired you're going to go through all of your hundred ,000 fake people and you're going to throw away every data point that's not consistent with these observations recall when we taught you conditioning what was the semantics of conditioning you enter the world where these two things are true with your data set you can enter the world where undergraduate is true and tired is true and you can do that by only looking at the samples that are consistent with those two things so you take your 100,000 you throw out all the grad students you throw out all the professors you throw out all the babies but you know I just mean from your data set and you throw out everyone who's not tired and you're just left all the undergraduates who are tired that's the first step so you can call it rejecting or you can call it keeping the ones that are consistent so I'm going to Loop over every sample and I'm only going to keep the ones that are consistent with my observation so for every sample I'll see are you an undergraduate and are you tired and if so I'm just going to keep that sample after this step kept samples will be strictly smaller than 100,000 well it'll be 100,000 or smaller now you've got a small subset of data of all the people who are consistent with undergraduate tired and your simple next step is from that smaller data set count how many people have the fever so if you have let's say 30,000 people in your fake data set who are undergraduates and tired you're just going to figure out the ratio of how many of those have fevers so if 10,000 of those people have fevers then the probability of fever equaling one given undergraduate equals 1 and tired equals 1 is just going to be equal approximately equal to this ratio and I'm done and you've calculated conditional probability and the beautiful thing about this is you can change your observation you could possibly even keep the 100,000 fake people and you can calculate any conditional probability you want for any network cool or what do you guys want to see this thing in action okay let's do this no more slides take me to the code so here I have the same program it's just now I use underscores instead of camel case I fought that hard I switched to python I still did camel case everyone else had underscore snake case I fought it hard but like I've finally come around and I've I've drunk the cool and I'm I'm one of the kids who does underscores now not important what is important is rejection sampling is a cool algorithm how is this related to the rejection sampling is simulating a continuous PDF from a different PDF wait wait wait we'll talk about continuous in a hot second good question though I want to First just show you guys this thing in action so I'm going to run it it's called Joint sample you know when I first taught this I didn't want to call it rejection sample doesn't that sound so mean like I don't like to reject people that's not what I'm about I'm about solving inference task but I don't want to do any rejection so I call it joint sampling but then I realized that's not what people in literature call it so now you know here's the simple thing I I get an observation from the user and right now I just hardcoded that so I said my observation is that somebody has a fever and they're an undergraduate but I don't know if they're tired and I don't know if they're flu if I want to say I observe that they aren't tired then I would make this false but if I put none it just means I don't observe whether or not they're tired do you guys want to give me an observation we're trying to predict if somebody has the flu so we want to keep that one none okay this person do they have a fever let's just yell it and I'll just go with what the mass says do they have a fever yes oh poor them okay are they an undergraduate no no oh are they tired yes wow that's incredibly consistent okay we have someone who is not an undergraduate but they are tired and we want to know if they have a fever so we're just going to sample a ton we're going to throw out every sample that's not consistent with our observation and we'll figure out the probability that they have a flu in this case it says these people this person 95% chance definitely has a flu according to your beijia network now you could try a different one I just want to give us one other example I'm going to show us having no observation for tired in fact actually should we do this one should we do Under undergraduate equals one and tired equals one but you don't know if they have a fever okay undergraduate equals one true we don't know if they have a fever but we know that they are tired so here none means you don't know and true means you know and it's a one okay undergraduates who are tired how many of them have the flu so you're just like I'm tired you probably don't have the flu if you now if you tell me you don't have a fever so now you're saying I'm tired and I no I don't have a fever I kind of expect that probability to go down cuz now you just told me more information yeah you tell me you definitely don't have a fever I'm like it's probably just tired could be the flu 1% chance it's the flu but you're probably just tired yes was different from what we were doing like on the first P set when we trying to figure out the genes if they were causing the traits or not I feel like it was the same process yeah when you were trying to figure out for the genes if they were causing the trait what you guys were looking for were independencies you know what you guys were secretly doing you were secretly trying to find the structure of the bean Network you weren't necessarily calculating the conditional probabilities that you would need to pull out a sample like it would be hard for you guys to sample one row of that data set but once you have the data set then yeah the conditional probabilities we're just counting like the second step of this you use the exact same thing for the bat data set so first step about was just really about creating a Bas net and the Second Step was about doing the second half of rejection s sampling you already had a real data set you didn't have to simulate it yes do these like different traits have to be um independent like do you everal oh these things are very much not independent um there's a whole beian network that is explained their relationship so if you're undergraduate that very much changes our belief that you're tired if you have a flu that very much changes our belief that you're fired you're you've got a fever so you have a lot of things that are very much related so how does it like making the joint table work if it's like if they conditional prob uh the conditional probabilities are encoded here let me show you where they're encoded there is a part where you have to sample where's my sample there's my sample a high sample so there's your part of your code where you sample and this is where all the probabilities lie I'm saying the chance that you have a flu and the chance you have undergraduate the chance that you have a fever given you have a flu the chance that you have a fever given you don't have a flu the chance that you have that you're tired given this particular setting of probabilities like I've defined all these conditional probabilities I've put them into Python and I put them into python in a way that I can create one sample these conditional probabilities are true for everyone the model is universal but if you have a particular human being they might show up and you might want to do a query based on some traits the model is universal the query is for a particular individuals but the conditional probabilities in the model they'll be for everybody good question yes iiz make sure I know what Happ in this code what you're saying is that we're going to take out all the samples that don't meet the initial condition that we said that's the second part this is the first part which is generating a ton do you guys want to see the rejection part yes okay uh here so we take out for example everything that is not one and perfect and from that's the denominator and the top is from those samples to go through and what samples have the flu Yeahs one over yeah so I just go through all the samples and I count how many people have the flu and then I just return well I didn't need to do float but you know so it goes um you know the how many people had the fluid divided by how many samples I kept that's you got it exactly right okay full rejection sampling uh I imagine there's probably questions at this point though so I really appreciate them but it really comes down to get the query from the user this algorithm will handle any query for this model then we're going to sample a ton and then we're going to calculate our conditional probability just by rejecting and Counting um and then you can solve any probability query you have for this particular model and the real challenge wasn't the rejection part the real challenge is only can you write the sample and so if you have a beian network and you can write write this function which does the sample from the beia network then you can solve any inference task so I'm going to put a little check mark here I do owe you an explanation for continuous random variables but we'll get to that in a second do you have to make more samples if you know you're going to reject more yes if you know you're going to have to reject more you would need more samples and that's why continuous becomes so interesting okay so at this point you know if I gave you a thing like WebMD if you want to make a baset tell me how you think the world's organized and then you need to get your conditional probabilities you can either talk to an expert or use a data set for those then once you have a basian network we can use rejection sampling and answer any probability question we've got okay so here's the claim behind rejection sampling it's not that crazy the claim behind rejection sampling is that this conditional probability is about equal to how many samples are consistent with the observation and What proportion of those are consistent with the observation and also have flu equals one and this really harks back to the original definition of probabilities which is saying that the probability of our event of somebody having a flu is the ratio of number of times that happens this is in the universe of what we observed so in the universe of what we observed how how what fraction of people have the flu so it's like counting number of people of flu divided by the number of people in this particular universe so the universe of people who are undergraduates and are tired so the theory behind it is rather straightforward and this is why rejection sampling Works what does it require you to understand you need to know what a basan network is you need to know what it means to sample from a basan network to get a whole joint sample and then you need to know the simple idea of counting and then you can answer your inference questions yes would we be rece rection you know I might I can use Bean networks to create interesting joint probability questions so there's you know things from your uh last section where you guys talked about joint distributions and a bean network is just a really elegant way for expressing a joint um would I make you guys do rejection sampling on the midterm maybe not because you know the code for rejection sampling it's always the same like this part of the code is always the same the only part of the code which is different if you were to do it differently is to write this code where you pull a single sample um would I make you write a code that could pull a single sample I'm definitely going to ask it for you in the next problem set I think on the midterm I probably wouldn't make you do this I would possibly though you know I think this is fair game this idea that you can count to calculate a conditional probability so if you have a data set like the bats that we talked about earlier you could get probabilities by counting from subsets that's very much fair game good question okay now if you can sample Enough From The Joint distribution you can answer any probably question wow what a fun time but there is a problem with rejection sampling and the problem with rejection sampling is if you condition off something incred rare So if you condition off something that hardly ever happens and if in your 100,000 fake samples you just didn't see the thing you condition off of you'll reject everything so imagine you're conditioning off like a very rare occurrence like somebody has a fever U but you know maybe they don't have a flu like this almost never happens if you condition off that you take your 100,000 samples you look through all of them that has that particular assignment and maybe you throw away everything and if you throw away everything you've got a problem right because the definition of probability here was supposed to be you know the limit as n goes to infinity and if you thrown away everything you're left with like 0o one two or three samples this is getting pretty far away from the definition of probability that we were using so the problem with rejection sampling is if you reject everything kind of like the problem with life so let's think about this a little bit and I can give you a particular version of this which which really breaks rejection sampling and that's if I took my bean Network and I allowed for a continuous random variable fever is not something that's a one or a zero it's a continuous number you say I have a fever of 92 wait I have a c fever of 102 you don't say I just have a fever so what if we want to take a beia network and we want to allow one random variable to be continuous we have seen Bean networks with continuous random variables like for example the elephant one we would have weight as a continuous random variable or in the Gaze we how much the child's view moved that was a continuous random variable so we have seen it before and the way we saw it were things like this if somebody has the flu that's definitely wrong if somebody does have the flu then the distribution of their temperature is going to be a normal with a higher mean and it turns out there's quite a bit more variance if somebody doesn't have the flu then the distribution of their temperature is the average temperature of a human but there's also variance to the average temperature of a human some people present with like 99 degrees even when they're healthy but when people have a fever you know it could be anything from like 98 to 102 there's a whole bunch of variants so this is a more precise way of expressing fever does that make sense okay so at this point we've got a beijan network could we still write a sampling algorithm because that was the first thing we need to do as I said the most complicated thing we'd have to do is a sampling algorithm could you sample flu for a new person yeah that's good can you sample undergrad yeah it's just going to be that burn 0.8 could you sample tired well you know if they've got the flu you know if they're undergraduate and we're just using the same algorithm so yeah we could could you sample fever if you're making a joint sample talk to the person next to you how would you sample fever see if you can figure it out and we'll we'll just check or not check if you can sample fever how would you do it how would you write this in Python you I don't know what you guys came up with but it turns the answer is yes you can sample here's what happens is you go back to your sample you have to choose flu first because flu is one we're going to be sampling from different normal distributions if flu is one we're going to be sampling fever from a normal distribution with that higher mean if flu is zero we're going to sample from a normal distribution with that lower mean now Norm that's not a function here's how you sample from a normal scipi has a stats library and you can see stats. norm. RVs you just pass the mean and standard deviation it will give you a random sample from that normal distribution so can we sample absolutely the problem isn't that we can't create joint samples and if we go to our code I'm going to now create an observation my observation is somebody is an undergraduate and they are tired and we observe a fever of a 101 and I'm just going to use the exact same rejection sampling now we've got those normal distributions a sample for fever let's do this Python 3 joint sample continuous Here Comes our fake people and look they've all got temperatures how cute is that it's like so much more exciting there goes their numbers um that's not right did I might have played around with this and I must have fixed something cuz that probability is was supposed to give us the surprising result result of a null okay let's see my check observation [Music] match I'm not doing anything really fancy here am I sorry print observation D oh wait I know what I did here so cute oh my God so cute cute so cute of Me no okay back to My Demo That was supposed to fail but somehow didn't fail now it really failed probability is not defined uh terminal new terminal I feel like people go to uh watch race cars because there's a chance that they'll crash okay here comes my fake people and from these fake people we're going to reject everything that doesn't match our sample oh no it broke and our sample had this is the thing we were going to condition off so we rejected everything that didn't match this and we looked for exact matches you're like wait something definitely match that and you go over and you're like uh and you're like that one that one matches almost and you're like nope not exactly you're like that one doesn't match and you get to this one you're like that matches right no you condition off an exact match that is the proper probabilistic thing to do you went through all your samples and you looked for a 101.0 we've talked about this for a continuous random variable what's the chance that it gives you exactly 101.0 zero that's never happening sad times and so like sad times expressed in computer science is here a division by zero when I went to count I had rejected everything and I was just left alone without any samples crying in my bedroom division by zero and then unfortunately I I accidentally showed you the solution or a solution so a solution was what if every time you generate somebody with a fever you round it so does anyone remember the math the function for rounding there it is there it is so oh there's definitely a nice function for rounding but um python has a round and so you can just take any fever and if you round it now when we create our samples once they get rounded you have a non-zero chance of people actually matching 101.0 do many of these match 101.0 no you don't see a lot of people but you do see enough that you can calculate a probability if you don't see many 101 .0 if that is a very rare event how can it still be the case that the probability of fever is very high the thing I condition off is rare but the probability fever was very large sorry the probability flu was very large because the thing I condition off is rare doesn't tell you about the probability of flu what that tells you is I'm going to be rejecting a lot of my samples so maybe of a 100,000 fake people only aund of them had that particular fever temperature but when we looked at those 100 people who had that particular fever temperature a whole bunch of them actually had the flu so going back to the theory maybe I should answer questions there what you changed ah I made it round so what I changed here is when I generate my samples before I would generate a continuous number to full precision and now I just generate one to like one decimal point yeah you could have like you could have said fever is low medium high you could have said your fever is 100 or you know 90 to 95 95 to 100 100 to 100 any way you discretize would work and this is a way of discretizing I'm just discretizing to one decimal point so discretization is helping us once again yeah could you just explain how you generated the random e numbers oh yeah so I use let's check this lineup so I'm just going to pull up python so terminal python Pyon whatever from scipi import stats you want a random you want a random normal but what normal give me a mean Five O good choice standard deviation no that's too big no I'm just joking that's F okay I'm going to call this function and it gives you one sample so um if I call it again you get a different sample it's not always the same number and the distribution from which this comes is the probability density function it's but it's not uniform it's random from this particular normal it's from the PDF of the normal yes oh random Vari abl I think uh or random values and I think there might be a parameter where you can be like 10 and I it might give you back 10 of these yeah the S is for multiple and because this function allows you to get multiple how optimized is my code you're like not at all I could definitely optimize this but the point is the learning not how fast you could run this okay wait I want to talk more about this so if you guys have questions this is very fun so this is the thing we're trying to learn how to do any inference task for any Bean Network and rejection sampling is such a good idea it requires you to know how to do two things you need to know how to sample and then you just need to write rejection sampling it fails if you're doing a query where the condition thing is so rare you throw out all your samples uh and so we used a trick here to solve that problem questions comments concerns yeah that's a good question at you know I had I think I made 100,000 samples so that discretization gave me enough samples for each reasonable thing but you know if I had if I want to make a billion samples I could probably get away with a finer discretization um so it's dependent on how many samples you have it really comes down to what's the probability of your observation and if that probability gets too low it fails or you need more samples yes is there a way that we can do this without discretising via some other train yes there is next question now I'm just joking I'll I'll explain you I'll explain you how in just a second I don't know if this is like outside the scope of the problem but how exactly does the the function that creates the normal distribution people work oh how does that work ask me after class but I think I talked about it earlier um basically there's this thing called a CDF it's a beautiful thing if you take the CDF of the normal you're sampling from it turns out you choose a random value between zero and one and you inverse transform onto that CDF and you return back this goes back to the user so you start here choose your y of the CDF and figure out which X would El lead to that y give that to the user and it gives you a proper sample it's a beautiful thing okay I'm not going to teach you this but I'm going to teach you that it exists there is this thing called Markov chain Monty Carlo it is a cool algorithm if you want to go beyond rejection sample rejection sampling works for so many cases especially if you're just a little bit thoughtful about your discretizations but I did want you to know that there are other things out there and Marco CH Monte Carlo does the same thing as rejection sampling but look at my samples notice anything funny you're like ha haa everyone has a fever that's not funny Chris they've got a fever that's scary the interesting thing about this is even though I'm pulling samples every single sample that I pull from has exactly the query in it so it's like set the query to be 101 and then choose the other random variables to be correct given that the query is that number and then you don't have to reject anything how would you pull a sample like this is beyond cs19 it's super cool you can go check it out out um but you know it's the next algorithm if you were to uh learn Beyond rejection sampling rejection sampling gets you so far but there are other ones there's mcmc there's actually even a package written by Noah Goodman called pyro so that uh you can H just write this in code and then the code will choose which algorithm it should use to do inference a beautiful thing uh and then we even wrote One in our lab uh for idea to text and I'd love to tell you just a little bit about this do you guys want to hear about something cool yay okay so I'm going to motivate this by a problem that doesn't feel like WebMD at all the problem that I'm going to motivate this cool algorithm for is how can we figure out what mistakes students have made so imagine a students trying to write code to draw this thing and if you think about this like that's pretty annoying code to write you have to have like nested for Loops there's a whole bunch of things changing it's very mind-blowing for a new student uh and they're going to write it using a block based language their solutions would probably about 10 lines or less uh but you need to then be able to say what is the feedback I should give you your inference task is feedback given code those are hard random variables to think about aren't they uh and long story short we wanted to use rejection sampling something like it so what did that mean we needed to be able to write we need to write a python function called sample and when you write this python function called sample it's going to make a fake student with all of their misconceptions and all of their code and if we could write this sampling algorithm then we could sample a and then we can look out of all the sampled fake students have we ever get all the students who have exactly this code and we could look at all the students who had exactly this code and we could say okay what misconceptions did they have what's the distribution of people who didn't understand for Loops here what's the distribution of people who didn't understand if statements so all we had to do was write a sampling function and we were inspired by this picture like what is going on in this picture in 2015 somebody wrote one of the more influential papers in our field and all that they really had to do in this paper was write a function that was called sample but instead of just sampling a regular random variable it was sampling handdrawn digits so imagine you have somebody trying to write this digit and you know there's all these other characters and you might be trying to figure out what's the probability that what they've drawn is this character as opposed to this character well what they had is they had a sampling algorithm they wrote a generator boy they should have used python in their paper they wrote this crazy Insanity which is basically a mask around python where they would you know sample curves sample connections of those curves sample how people put those um components together um and then from those they could just write a forward generative process and end up with pictures of handdrawn characters and I said if they can sample handdrawn characters we can sample code they sampled a lot of hand John characters and they got very good results they beat all the neural networks out the time uh and so the idea was can you do this for code and so we just thought okay we're going to write sample and when we write sample we're just going to try and generatively think through a student's decision process okay they decide to work on this problem that's where they went wrong okay and then we said then we would set all their misconceptions and then based on their misconception we' start to generate the code probabilistically and we would sample from The Joint as much as possible it's like we made the beijan network but for code and we wrote a particular library that would allow us to write the bean Network for code and it worked really really really well and to the point where like you could write this Bean Network teachers don't know the gener of process does anyone know the beijan network for how a student thinks no there wasn't like a place I could go research and look up what is the beijan network we did have to invent it based on just what we thought students would do but generative is not so hard if you're thinking generatively it's like making the WebMD model it's not that hard you can be like okay I think flu causes fever that's not insane and similarly you can be like if people have this misconception I've seen them write this piece of code it wasn't insane for us to write this and it turned out to be very very useful and not just for code we used it on code we used it on um short answers and by we I mean some wonderful scientists which also included our own head ta actually the first time I met will we were like a a freshman and you came in you're like I want to do some research I'm like no sorry I'm just but you're like hey can I have a challenge for for winter break and I said well we're thinking a lot about these sampling things could you sample I think it was Graphics I gave you a graphics thing I said people who are in 106a they do a pyramid assignment can you sample people drawing pyramids and you sampled a ton will uh and it was part of this wonderful research project that seemed to work very well um so you know we wrote The decision- Making beijan Network and then we would sample a ton we' get a bunch of fake students with fake code uh and then we would do inference we could have used rejection sampling that would have worked pretty well I do want to be honest that we also used a little bit of a neural network to make our rejection sampling a little bit more efficient um but rediction sampling would have been fine for inference uh just there was because we could we used a slightly different one now before I let you go I just want to make this a little bit more relevant you guys should be asking yourselves this question or here's one of the weirdest things you could ask yourself while studying for a midterm what is you look at a problem and you're like what's the probability that this is a binomial distribution problem right isn't that what you're kind of doing and in fact if you want to really solve that you know what you should do you should write a beijan network you should think what's the generative process through which Chris creates binomial distribution problems and if you could write that beijan Network you would know everything about how to solve any CS 109 binomial distribution problem right wouldn't that be cool so I went ahead and did that for you guys I wrote a generative model and when you run it I think it I think currently it just creates like a 100 random binomial distribution problems uh and sorry they're written all in HTML with latex in them um but it'll say something like a ball hit a series of 30 pins where it can bounce either to left or the right the probability of a right on each pin is one3 what is the probability that the number of Rights is 28 did somebody just write the answer does it say the answer oh there is the answer oh there it is it's very very low um but anyways I sampled that I just like had a distribution for it and you know potentially using some rejection sampling or something like mcmc or something more complicated you could put this as your query and you can say what is the probability that this is binomial given the query or you could even say what's the probability of the answer you can even say what's the answer given the query um anyways but that is an interesting thing a funny way to talk about um studying for a midterm is to try and write your own that's maybe one takeaway I can't write that many problems I'm an old man I've written many midterms I'm running out of creativity and there's only so many ways you can ask about binomials and pans and all those things I probably have a generative process in my mind and if you tried to write your own midterm you would have to generate you would have to get into the generative process and maybe by thinking about that you would get really good at the inference task because one of the stories of beian networks is the ability to generate is an important PR precursor to the ability to infer okay okay um I'll just tell you what we haven't talked about as I mentioned there is a field of study where you can learn the arrows from data that's called structure learning we haven't uh talked about this and we haven't particularly talked about how you could get these probabilities basically I've told you to make them up so far but it turns out that there are data driven ways you could do this if I gave you some historical data you could infer these probabilities pretty well I hope you guys have a fantastic weekend study for the midterm we'll see you guys no class on Monday see you guys on Tuesday or Wednesday have a fantastic weekend I'll be thinking about you guys a lot come to the review session it starts in 40 minutes