good afternoon cs109 how are you guys doing today oh fantastic okay that's why I liked here I hope you guys had a wonderful wonderful weekend I was gone for a little bit last week but I am healthy I'm back I'm here for the rest of the quarter though as you know that's going to include a Thanksgiving break coming up pretty soon uh oh my gosh is this one of my favorite lectures ever you chose a fantastic lecture to come to it is one of the most interesting lectures in terms of intellectual foundations for great ideas it is one of the most used for lectures we will be building machine learning upon what we learned today uh and certainly you'll see things like this in the final in your sections uh and it also happens to be a bit independent of what we've talked about before because it's such an exciting day for cs109 we've done counting Theory we've done core probability we've talked about random variables we talked about lots of random variables being random together then we have this wonderful little aside where we went to uncertainty Theory we learned about great ideas in the world of probability like the central limit theorem and we're ready for the final section of cs109 where we take what we've learned about probability and we create something truly useful out of it uh which has this big overarching label of machine learning and today is the first class of that new section how exciting so today is the date where we Bridge from probability Theory into artificial intelligence sound good yeah let's go do it let's go learn some things okay so machine learning uh it's a story of today's class and because I was in kind of like a Disney mood when I made this we're going to use Lion King as our narrative for today so we're going to be telling you this wonderful story of machine learning on some level the questions that are posed by Machine learning have already showed up in cs109 remember when we did WebMD Health when we talk about probabilistic models we ended up with these wonderful things called Bayesian networks and we got to the point where if you knew all of these conditional probabilities you could use General inference and you could ask sir any probability question you cared about from this model but there is this one question which never answered which is where do those numbers come from and a few times in class I alluded to oh we can learn them from data and that's really what the focus of machine learning is suspense what's machine learning at this point in cs19 if you're given a model with all the probabilities necessary you can make predictions which is fantastic that's a really important thing to know how to do but what if somebody doesn't just give you a model with all the numbers uh required what would you have to do you might have to actually learn all the numbers in your model and this simple problem turns out to be so deep and complicated it is the heart and soul of artificial intelligence I want to lightly point out there is another problem which is um can we also learn the structure of a probabilistic model from data too and I just want to lightly note that that's so cool but I'm going to try and you know limit what I teach you in cs109 if it was up to me I would teach you everything I possibly know but it's it's nice to have a self-contained class so we're just going to focus on machine learning and not structure learning and that simple problem where do numbers and models come from is the heart and soul of this thing called machine learning now machine learning as you may know is related to a few other terms machine learning is one way of doing artificial intelligence artificial intelligence making programs that do something slightly interesting and then machine learning it says I'm going to learn how to do something slightly interesting by learning parameters from data I do want to lightly note that you might have heard of a thing called Deep learning and deep learning is a particular type of machine learning for a particular type of model but it all funder falls under this big umbrella of machine learning and this is what the rest of cs109 is going to look like we will teach you deep learning but before you learn deep learning you need to know two of the most classic algorithms in machine learning which are called naive Bayes and logistic regression and in order to get there we need to build you a theoretical foundation and this theoretical Foundation has a name it's called parameter estimation and it's a nice unassuming name that covers a lot of beautiful Theory I'm going to teach you three ways that you could do machine learning this foundational Theory for how you get parameters from data and then we'll build these two algorithms on top of it unbiased estimators a thing called maximum likelihood and Bayesian estimation sound good oh let's do it I do want to have a quick aside hey can't we just like jump into using tensorflow and start doing deep learning right now on some level you could every single person in this room if you wanted to just go pull up the most modern library and train a deep learning algorithm it wouldn't be that hard the hard part would be is if you try to do something creative on top of it if you wanted to debug it or if you wanted to kind of invent the next step in machine learning and for lots of reasons we think that there are going to be many new steps and in cs109 the idea I would like to give you is the theory behind what happens in something like tensorflow and it's not because I just like love Theory and I think that's the only thing that's important but because the technology is on top of the theory are moving quite fast knowing the theory will help you invent the new technologies it'll also help you stay relevant as the Technologies grow so while it'd be really cool to download tensorflow I think it's really critical that we all learn the beautiful mathematics that go into this great idea that has changed the world do I have you hooked okay let's go do this oh um there are just to give you a sense of some of the open problems machine learning what we're going to learn about traditionally uses a lot of data so if you say hey computer learn about the concept of a chair computer is like I've got to if you give me like a million examples or like okay here's a million examples of a chair and then the computer learns what a chair is but humans are so much smarter and we've talked about this before if I told you there's a new symbol you've never seen it before do you need a million examples to start recognizing other versions of the symbol no you guys can learn after just one single training example how clever are you and this tells you the story of machine learning is not done there is room for improvement there are open problems for you all to solve so let's learn some Theory oh my Segway okay and I will note that the open problems that exist in machine learning they're especially true for the human problems that you might care about it's for the places where artificial intelligence interacts with uh needs of humans there's a lot of problems that people haven't really thought of and there's a lot of open opportunities for some advancement so you guys want to push on these Grand Challenge let's understand the theory you want to understand how this thing is really working like when people say artificial intelligence what they really mean you want to demystify that let's learn the theory and so our movie begins we're going to learn parameter to our estimation parameter estimation is a simple idea we've seen lots of probability distributions and often those probability distributions whether they have single random variables or multiple ones will have these special numbers and these special numbers tell you what the distribution is those special numbers have a name they're called parameters we call these parametric models because you know if you wanted to define a whole normal you don't have to describe every single point in that bell curve you just have to give me two numbers these two parameters if you want to describe a poisson you don't have to describe every single point in the probability Mass function of a poisson you just give me the Lambda so any model which is defined by numbers we call parametric models and the parameters are where the real model comes from so I can tell you that something's poisson but if you really want to solve problems using that random variable you'd need to know what Lambda was and I can tell you I have a whole probabilistic model but if you don't have values for all of your parameters then it won't be able to do very many interesting things for you a little bit of notation we're going to now start referring to all of these different things as parameters and we're going to start using the symbol Theta to represent parameters in general does that sound good often Theta will be just like a single parameter but sometimes models like a normal might have more than one number and so you can think of theta as a vector just to be clear in this terminology of parameters because we're going to be learning parameter estimation in our beautiful Bayesian networks there is a lot of numbers and those numbers those are all parameters too and the name of the game for today's class is getting all of these parameters from data cool rocking any questions on terminology and this fits the general machine learning format the general of machine learning format has three major stages the first major stage is you have a real world problem and you model it you say I think this is a poisson or I have a Bayesian Network that's going to represent this problem and you end up with a formal model but that formal model has a bunch of numbers left to be filled in these numbers that we call parameters so that's stage one in machine learning stage two of machine learning is okay somebody just gave you a model they say that there's going to be numbers but we don't know what those numbers are but they also give you training data and from this training data we're going to choose really wonderful values of thetas so that we're end up with a model filled with numbers and then we can answer probability questions so that second stage parameter estimation has another name called training which you might sometimes hear because it's coming from it's getting its estimate for the parameters from training data okay so here's our pass and I have wild news for you guys we need to be able to estimate these parameters but we already have some methods for estimating parameters and I just want to revisit them really quickly before we jump into more General methods the first way we have for SMA parameters is called unbiased estimators so you know if I told you we have a bunch of data and they're all IID because the central limit theorem we know that this is always a sample mean and and we derived earlier that the sample variance also has a close form equation this applies for any data set so if you have a data set we already have a method for estimating the mean and the variance which means if like you had to fit a gaussian and you just had data points you kind of have a method for already estimating parameters it's not very general it'd be hard to imagine how you could use this for like your Bayesian Network but in some sense we do already have some really basic tools for estimating parameters from data we call these are unbiased estimators and that's just because in expectation this will be the right number It won't always be the right average but in expectation it will be the true average this number your guess for variance won't always be the right variance but in expectation it will be the right value and these just leave so much to be desired they just don't solve parameter estimation in general what we're going to learn about today is maximum likelihood estimation the theory behind a general method for choosing numbers in a model okay and to drive that home there's lots of different algorithms or things I could show you which are hard to fit and I do want to pull up our course reader by the way we've been writing a whole bunch of new things in the course reader there's a whole bunch of new examples but you know one example of something that'd be really hard to fit using unbiased estimators would be this new model I give you called a mixture of gaussians it says there's some gaussian a and some gaussian B and there's um every data point is more likely to come from B than a and all together this leads to one probably distribution I know that there's these five parameters I can tell you how they're related but you have to guess the numbers this is very difficult using something like unbiased estimators so we need a more General tool so enter this great idea in machine learning and to do that I want to go back to a gaussian I want to go back to the problem of estimating parameters I don't want to use unbiased estimators but I want to show you a demo which highlights a new way of thinking and this new way of thinking will give us a general way for doing parameter estimation ready let's do it I have a classic version of parameter estimation problem for you I give you all of these data points I give you a model I say that I think these data points come from a gaussian you now need to do parameter estimation which means you have to give me a mean and you have to give me a variance you have to give me the numbers of the parameters we already have a way of doing that but I want to solve this problem using a different method what I'd like to do is I'd like to focus on a simple idea see up here I have different values of these parameters and in this graph I do something interesting these vertical lines are all the data points that you're trying to come up with parameters to describe is that making sense and here are guesses for mean and variance I calculate this expression called likelihood and likelihood just says if this was the true mean and this was the true variance what's the probability of you seeing each of these points so what's the probability of seeing this point under those parameters times this point times this point times this point times this point and the crazy idea is I'm going to let people play around with these numbers and we're going to watch likelihood go up or down so if these are our data points this blue line is parameter mean equals five and standard deviation equals three do you guys want to see me make standard deviation bigger or smaller okay watch what happens when I make it smaller see that likelihood expression it's currently 4 times 10 to the negative 19th whoa it changed a lot did it get bigger or smaller yeah it got bigger so it used to be four times since the 19th now it's almost 7 times 10 to the 19th it almost doubled in likelihood there's still a very small number but that very small number got bigger so that's saying if you had those parameters your data looks more likely it's more probable that you would have seen the data you saw under these parameters doesn't say that these are the best parameters it's just more likely than the other ones we tried but we can try other ones do you guys want to make this variance bigger or smaller okay and we can keep going and I'm going to keep going and I'm going to stop when the likelihood starts to go down so it got went up it went up it went up it went up oh this is very exciting it went oh up it went up oh definitely better with smaller variances oh this is great will it just keep going up no no no it's no longer going up but you know maybe I haven't changed this parameter I can change this one oh that got worse oh that got better it got better when I made the mean a little bit higher oh fantastic no okay I like those ones did you see what I did here what a funny way of choosing a mean and variance so what did I do here I said I'm gonna just twiddle around with these numbers until the data starts to look more likely and to be clear the likelihood in this situation I'm going to use l for likelihood it's just the product over every single data point the probability density function of that data point in the world where you have that particular mean equals 5.5 and the standard deviation equals 3.0 1.1 so it's just saying you know every single data point look of its probability density function and multiply it together and then my crazy idea was we're just going to twiddle around the values of the parameters until this goes up the most crazy do you guys buy that that would work did you know that most AI has a human being behind the screens and they're just like twiddling numbers until likelihood goes up there's a secret like inside your smart watches actually a tiny little person who's being like yeah actually let's make a variance go up a little bit no that's not how it works okay so this is a really cool idea this is a powerful idea which is that for any model you have one very reasonable theme for how you could choose your parameters is choose whichever parameters makes the data look pretty likely and we haven't talked about how a computer could do this automatically we've just given you a perspective on how we could choose parameters in that perspective I like to think of this metaphor of there's this big sound board and you're like a sound engineer and when you're choosing parameters you're just trying to choose the values that make your data look as likely as possible in the world of your model and you can twiddle those sliders a little bit and each slider is a parameter's value until it's like the perfect sound and by Perfect sound I mean it makes the data look so likely did you guys oh okay the first insight then that we've got so far and this is a critically cool idea in the world of probability is that we should find which parameters maximize a measure of likelihood and that's going to require us to think about arguments that maximize something else I want to think about which values of mean and standard deviation maximize this thing arguments that maximize have you guys ever seen a thing called ARG max if not what a beautiful day in your lives we're going to be talking about arcmax first of all this is a general conversation let's take a step back from probability let me give you a function this function is negative x squared plus five and I've so helpfully drawn our function oh what a cute little function it looks like a rocket ship first of all have you guys ever seen a Max function a Max function can take in an expression and it will tell you what's the largest value that this expression can take so think about it for a second I'm going to have you shout it out what is the maximum value that this expression can take on and you can look at my handy little chart I think about it three two one all together that was like kind of like a a d flat but can we do that like a c like five okay one two three no five I was getting there let's see if we can nail it with the ARG Max now Max is a thing that we all feel comfortable it's the largest value that this can take on and we've decided that this is a five the ARG Max is a different concept it doesn't care about the largest value it asks the question what input leads to the largest value so if I took that exact same expression instead asked what is the argument that maximizes it I will get a different answer and in C in three two one zero zero okay one two three oh okay it's getting better but we should definitely be sticking to probability I kid I kid so the difference between five and zero here is five is the largest value you can take on and zero is the input which maximizes it and this is a very important concept because we're not really interested in How likely is our data what we're really interested is which input of mean and standard deviation made the data look as likely and that will be an ARG Max a beautiful piece of theory that you must know about the ARG Max is what happens if you take the ARG Max of the log of a function so imagine you cared about the ARG Max of a function mind-blowing claim for you ARG Max of a function is the exact same as ARG Max of a log of a function you're like what no it can't be the same they're close right no they're not just close they're the same so if you take your function from before so if I said what is the ARG Max of negative x squared plus five it is giving up my whole C thing and how about what's the ARG Max of this zero that is my claim for you is that if you take any function the argument which maximizes the same as the argument which maximizes the log of that function and it's a crazy thing but it comes from because of a very simple property log is monotonic if you put larger numbers into a log you'll get a larger number back and because of that whichever number led to a larger value of your function or log of a function we know that it must have been the same large output for the function itself so because log is monatomic there is this one critically important claim whichever mean and standard deviation maximizes this expression will be the exact same mean and standard deviation which maximizes the log of this expression and that expression is the pi of f of x i that is just a copy of that insanity so let me pause here for a second and see like you know what's interesting about this to people yes do you still in the case where we're dealing with smaller probabilities we can just take the log of them and get the argument yeah you're seeing exactly where I'm going can I go back to that demo I gave you did you notice how I had likelihood and it's this tiny tiny tiny number notice how below it I also had a thing called log likelihood that's just the log of this likelihood and when I was changing both of these numbers were going up like the likelihood would go up and the log likelihood would go up with it because the monotonic property of logs and this is a beautiful thing for two reasons one computers kind of are not so good at representing very very very very very very small numbers and computers are much better representing logs of those numbers and then there'll be another reason but I want to be mysterious a little bit so hold on to my other reason but you see exactly where I'm going so where have we been in this lecture a beautiful idea like we have this crazy important goal everyone in cs109 needs to how know how to choose numbers for models maximum likelihood starts with this beautiful idea choose the parameters that make your data look as likely as possible and then the next idea I've got for you is okay now we've got to choose these numbers well we'll need to do an ARG Max and we can do the ARG Max of the log likelihood instead okay so the ARG Max of some function is always the same as the ARG Max of a log and logs we love we like have a soft tender part in our hearts for logs we see logs and you're like you've done a solid so many times and like we we like let's chat a little tear for happiness when we see logs because so many things become much easier a lot of equations will simplify like log a b a times B is equal to log a plus log B um and actually it is worth noting when I mention logs I don't often write a bass because I actually mean the natural base some people would write this as Ln but I've gotten the habit of just calling this log okay we've developed a few critical pieces we're ready to start putting things together maximum likelihood estimation I wrote it out on one slide it's so important I wrote the spark notes on the board so you can always follow along the great idea is you have a general method for choosing numbers for a model it starts by defining the likelihood for one data point the second goal is to come up with a log likelihood function and then the simple idea is we're going to state that the optal parameters are which ones are the ARG Max of the log likelihood function and then finally once you've stated that you would then use an optimization algorithm and the optimization algorithm can help you find the ARG Max there's lots of cool computer science functions that can do ARG Maxes I will tell you about a few of them in cs109 now as I wrote up on the board what is this likelihood function I've been talking about well if you have IID random variables that means they're all independent so it says How likely is the data given the current status of their parameters and that will just be Loop over all your independent data points and talk about How likely each one is given the parameters we want to find the parameters that maximize this that's the simple idea when you start you will write down this expression and you will replace this with either the PDF if you have a continuous random variable the probably Mass function if you have a discrete one or the joint distribution if it's some sort of probabilistic model okay that's how you do step one and step two is just take the log of this so story so far you can choose parameters by finding the ARG Max of the log likelihood of our data and to write this out an equation the likelihood is the product of the likelihood of each data point for IID random variables the log likelihood whichever parameters make this the biggest will also be the parameters that make this the biggest but this can be much easier to work with and notice the first thing that happens when you take a log of this expression that log used to live outside of the columns but remember the log is like a tractor and the C is the columns and it's like and then you get the columns get collapsed into the summation the logs like I'm inside that's just a general property of logs if you didn't know um so log of all your data points will lead to the sum of the log of likelihood of each data point on its own and then we're going to state that the best parameters that we can guess and we put funny little hats on things that we guess is going to be the ARG Max whichever parameter makes this as large as possible I want to take a moment for conversation I want you to have a nice little chat with the person next to you take two minutes what is confusing about this what's interesting about this what questions do you have see if you can come up with a good question with the person next to you and then let's talk about this all together what a crazy set of ideas go for it services yeah okay I can guess one of your questions like what's this going to look like when we apply it to a real problem we'll get there in a second other questions that have come up curiosity is yes this isn't the artifacts of the log wouldn't that be Infinity like what exactly are we doing there so for any particular values of parameters you can calculate log likelihood of your data and notice that as I change these parameters that changes and it's not the case that Infinity so it's saying that Infinity is the ARG Max is saying if you put Infinity into these numbers then that would be your ARG Max of course this is infinity but it's getting there right but if I put Infinity in front of parameters look at my current value of log likelihood it's negative negative 4549. that feels like it's better than negative 300 but it's worse so putting in infinite values into parameters doesn't actually maximize the log of the likelihood the thing that maximizes the log of likelihood will be the same thing that maximizes the likelihood itself which will in fact be pretty good guesses of our parameters does that answer the question very very good question other questions come up yes um on this Library you had like the different steps for the maximum likely algorithm um yeah the first step was like deciding a model for the distribution of your samples how do you like how do you do that how do you decide what your model is good question so this is assuming somebody gave you a model so at this point we're saying we're telling you your data is gaussian we're telling your data is predo we're telling you that your data are joint samples from this tiny little Bayesian Network and there's these missing numbers in all of those models and your job is to find the missing numbers there is a different task of you decide what the structure is yourself and I'm going to leave that till 228. good question though we can talk about we've done that in bats we've like touched upon structure learning a little bit but this is the question of figuring out parameters you guys need to see this you need to see this live because so far it's all Theory and no application and you can solve so many problems once you know how to oh but we have a bad guy all right Max we haven't talked about how to do ARG Max this would work great and we could apply this if we knew how to do ARG Max but we haven't talked about this at all I just said do ARG Max and you're like okay great I'll do ARG Max two options I'm going to tell you the first one you learned about in Calculus class and we're going to start with that it's just straight optimization let's say I gave you a function negative x squared but this time plus four just to mix things up a little bit and I said find the ARG Max of this expression you might just be able to look at the expression and be like it's zero but this is not supposed to be using your visual cortex we're trying to solve this using math how could we solve this using math and you're like oh my calculus teacher told me something about this he she said that I should find the derivative and set it equal to zero have you guys had a calculus teacher tell you something like that thank you Mr Blanton so if you look at this part that maximizes the arguments notice that there's a derivative of zero out there must be true at a Max that the derivative must be zero so one of the things that people will classically do is they'll take the derivative oh derivative is scary but we have wool from alpha not so scary you put it in Wolfram mass that tells you the derivative of this is 2X and you're like 2x what does that mean well if x is 3 that means that the derivative at that point is six and so we can figure out the derivative oh wait where's my negative sign here should be a negative sign cheeky Chris turns out zero doesn't really mess things up negative 2x if x is 6 or 3 then your derivative at that point is negative six the slope is negative six at that point we don't care about all points we just can't swear this derivative is equal to zero so what we could do is we could say okay take this derivative and tell me the value of x which makes this equal to zero and if you solve for the value of x that makes this equal to zero you get that okay when x is 0 the derivative is zero so this is a good hypothesis for a maximum point and that is one way of doing ARG Max it has some downsides you could find the worst possible parameters because this could be a minimum uh and obviously it might not scale when you get to bigger and bigger models but this is just fine for getting us warmed up we have our first algorithm for doing ARG Max Yeah question function with multiple um Maxes and mins no if there's multiple Maxes and bins you have to do a Next Level analysis you have to decide are these Maxes and are these bins and you'd have to look at the second derivative at minimum so this works for pretty simple things this will just get us over the hump so I can show you some examples but we're going to need keep in mind I want a more satisfying ARG Max everyone should want more satisfying art Maximus okay okay and this is all the notes of like this is why this is so limited and don't worry about it we'll just get a better version later so this is your general mle formula if you wanted to find um parameter you say what's the likelihood what's the log likelihood and then you find the Der the value which maximizes loglihood you do the ARG Max let us start with kind of a medium difficulty example is not the hardest thing in the world but it's going to be a pretty good way for us to get used to doing this it won't be like once you get use that mle you can solve so many interesting problems but let's start with something that's in the Wheelhouse of what we know an mle problem would generally look like this I'm telling you my model in this case my model is that every data point has a single value and that value is drawn from a poisson and I've seen you know 12 different independent samples from this thing that I'm going to call data if this is my data set the challenge of mle is to estimate the parameters and the parameters for poisson is Lambda remember Lambda is just the word for this symbol x i when I use this notation I mean like the ith value in my data points so if I gave you these data points how could we estimate Lambda again for poisson we have some pretty reasonable ways of guessing what Lambda is from this you know maybe could imagine a way of doing it without going to a first principle approach like mle but let's use this as our first opportunity to go end to end with mle you guys ready for it you guys I'm just warning you you're going to look like really really legit mathematicians once you get used to mle because the math looks super impressive but really we're just always following this formula but when you get back to Thanksgiving I want to do a little mle and just like let your parents see and they're like wow kids learning so much at Stanford okay let's try this out so I'm going to use this notation for our data points I gave you 12 data points but now we're going to call them X1 to xn why just to use a little bit of notation I'm assuming that every single one of them are IID from the same poisson and my job is to try and estimate Lambda based on these data points and I'm going to use this recipe the mle recipe and the end results of the mle recipe is going to be an estimate for Lambda it starts very humbly you have to say How likely is a single data point if I tell you what Lambda is so if I told you Lambda was 5 How likely is a data point hardest part of Emily sometimes actually no I want people to sit with it the tension what is the likelihood of one data point okay I'm going to tell you it's very simply the probability Mass function for uh for a poisson so if you go back to your course reader and you say like I have a plus off I told you what Lambda was we can talk about How likely it is that you saw a particular value x sub I you can say what's the likelihood of seeing x sub I well it's e to the power of negative whatever your Lambda was Lambda the power of x sub I and then x sub i s factorial that's just the probability Mass function and that's really your first step so the first step um is to just write what likelihood is but that's just likelihood of one data point if we talk about the likelihood of all of our data given a particular Lambda because our data is IID the likelihood is really this expression and this expression is just going to be the product of the likelihood of each data point on its own you can't always do this it has to be independent for this to be true but because it's independent this is true and then we can just plug in this expression we had from one data point and we're just going to multiply it for different data points if you had to calculate likelihood you'd write a for Loop and for everything you look at the value you know from our previous slide you take six and plug it in for x sub one one for x sub 2 and so on and each time you'd be able to calculate this inner term product them all together and that's the likelihood if I told you Lambda would say five yes what would it look like if this data was wasn't independent like what else could be run run don't even look back just go okay uh if the data was not independent you enter a really weird theoretical world but good news almost all data is assumed to be IID we almost always assume that each data point is some independent draw condition on the parameters like uh I'm gonna give you an example but then I'm going to tell you the first thing a mathematician would do is assume it's independent anyways so um let's say these lambdas are like how many people get processed by a cashier maybe like how long it takes one person like one cashier to process a chunk in an hour influences the next one because it leaves a queue so there could be a way that they're not actually independent but guess what first thing we're going to do is assume Independence because none of the math ever works out if it's not independent so IID often assumption that we need but most data actually it's a very very very good assumption okay at this point if you gave me any value of Lambda I could plug it in with my particular data set and I could say How likely does my data look in the presence of your Lambda and we could play the game we play with the gaussians where you're just going to like twiddle on lambdas and you could try all the different values of lambdas but that's not so fun we'd like a computer to do the twiddling for us which leads us to The Next Step we're going to try and find the Lambda which maximizes this expression which is the same as the Lambda which maximizes the log of this expression and you're like I have to do logs no we get to do logs We Are The Humble users of this beautiful tool which is the log so I just write the log of the expression I had before and remember the first thing the log does is like I want to go live inside the house and I'm going to break down the columns as I go there okay great the log one inside the house and you have like the log of this nasty expression and you don't want to do it but then you're like wait a second log of e is a good time isn't it you're like yeah that's just going to be negative Lambda and you're like I can do log of this expression plus log of this expression minus log of this expression and when I do log of this expression that exponent just becomes something that goes in front and look how nice that is when I took the log it turned this nasty expression into something actually quite a lot easier to work with the log is your friend the other reason I was going to tell you why we use logs is it makes math much much easier so at this point we have a special expression we call the log likelihood and the final thing we want to do is we want to choose the Lambda which makes this expression as large as possible maybe when you put 5 in it's large as possible when you put three and it's as large as possible we would like to choose the ARG Max of this log likelihood notice how I put a derivative here is because no matter which optimization technique you use you will most likely need to derive take the derivative of this with respect to your parameters you guys are ready for some calculus definitely could use will for mouth from here but we can do this one on our own in fact this is the level of calculus that I think would be nice to know how to do on our own derive that expression with respect to Lambda okay you guys can think about it first of all the derivative of a sum is just going to be a sum of derivative so the derivative will just move right into this inner part and you're like derivative of that term seems not so bad during this term well it's got a log but we can do that seems not so bad derivative of this term looks awful what's the deliverable factorial oh my God we're gonna need like a smooth continuous approximation but wait somebody's shaking their heads why why there's no Lambda and if there's no Lambda the derivative is just zero right as Lambda changes how does this change Zero fantastic what a beautiful thing okay now we're feeling brave we go into our derivative so the derivative of that term with respect to Lambda is well the derivative is going to go inside the sum the derivative of Lambda with respect to Lambda just one so that was a negative Lambda this becomes a negative one the derivative of that log so X I log of Lambda well the derivative of log of Lambda is just one over Lambda and that x i looks like a constant and as we mentioned that negative log x i factorial just goes away so we have that this is the derivative I'm then going to do this thing where I'm going to take this derivative and I'm going to move the sum as far in as possible so that we really only deal with the terms that have eyes in them there's no I's here and there's no eyes there when I move this sum through this negative one it's like you add it up negative one n times that just becomes a negative n and when I move the sum into this inner term it's like every single term is multiplied by 1 over Lambda and then you're summing up the X size so this is how that term gets rearranged at this point we have the derivative of log likelihood with respect to Lambda your Calculus teacher Mr Blanche doesn't Mr Bland teach anyone else here I don't know my Malaysian probability I heard he went to Brazil and he taught some people maybe you no different probability or calculus features but all of our calculus teachers would have told us something like if you want to find the value which Max or maximizes you can take your derivative set it to zero so I'm going to take my derivative set it to zero and when I solve this expression for Lambda I end up with Lambda equals the sum over all my X I's divided by n where n is the size of my data set wait a second that's like a screen filled with mathematics and at the end it just says take the average of your data points right this is saying like take all your 20 values add them up together and then divide by how many values you have you're like a page of mathematics for what the sample mean and yeah for poisson the maximum likelihood estimation for Lambda says you just average your data points and that actually feels pretty good and mle is Plus off for the sound this is a this is a good time it's a good news result it was a lot of math but it led something that feels about right okay I'm gonna make us do one more simple example before we jump into solving mle for things we haven't seen the answer to yet can we do one more simple example let's drive this home Bernoulli oh man consider random variables and we're going to assume that they all come from Bernoulli and you want to figure out what is your best estimate for the parameters what's the parameters or Bernoulli ah yes p and recall that your data in this case is going to be like zero one zero zero one one you know zero and we're assuming that these are all iids pulls from the same Bernoulli with the same parameter p and we're trying to figure out what is the best value of P here again you could probably come up with a pretty reasonable formula for how you could have gotten this but I want to use this opportunity to flex our mle muscles so we're going to try and use all these steps and derive it using mle okay First Step you have to drive the formula for log likelihood it's going to be the sum of the log of the likelihood of each value and at this point you're like okay what's the likelihood of each value it's a Bernoulli what's the probability Mass function of a Bernoulli and then you look up on the course reader and you get something like this and you're like I need to substitute this into that equation if the Bernoulli says if your value is one the probability was P if your value is zero your probability was 1 minus P whatever P was and that's really cute and very helpful except it's not at all differentiable wait I promise Bernoulli would be easy oh no I'm going to show you the slickest trick and you need to learn this now because you're going to see this in two two one you're going to see in 229 and you're going to see it in 220 X whatever and they're not going to explain it to you they're just going to say it as if it was the most obvious thing in the world people don't use this as the probability Mass function for a Bernoulli instead a lot of times you'll see people write a continuous version of that same table so to be clear this table is saying the probability of one is p and the probability of zero is one minus P makes sense that's fine but this graph is not differentiable and so people use a different expression for the probability Mass function Bernoulli you're ready for it they use that you're like what is that it is particularly P raised to the power of x times 1 minus P raised to the power of 1 minus X you're like what are you guys on well it turns out this is a continuous version of that expression let's try plugging in zeros for x if you plug in a 0 for X what's P to the power of 0 this whole term goes away and one minus 0 is what one so like this whole term goes away if you put in a zero for X you're left with 1 minus p what if you put in a 1 for X well then this whole term will go away because 1 minus 1 is 0. and that whole term goes away and you're just going to be left with a p if you plug in a zero and a one this crazy formula picks up what this graph left off if you were to graph it has values for like what's the probability that your Bernoulli gave you like a 0.3 and you're like what that doesn't make sense it just happens to make sense at the value 0 and 1 and it happens to be derivable and just be clear if you put P equals 0.2 this would be a 0.2 to the power of x times 0.8 to the power of 1 minus X and that X and the 1 minus X are just choosing whether or not you should use this term or you should choose that term insanity but learn it live it love it now because people are going to just State this as if it were obvious in the future and they will not explain it with a nice little chart like this okay so before I jump back into Bernoulli's just know that this is a continuous derivable version of the probably Mass function of a Bernoulli craziness okay so if you want to do a Bernoulli using our good old friend Emily you write the likelihood and here instead of writing the bar chart we're going to write that continuous version of the probability Mass function then you say what's the likelihood of all the data it's just the products over all our IID samples so we're going to Loop over all these values and then either choose P or 1 minus P depending on whether or not the value is one or zero then we do the log and we do the log of this expression yikes yikes oh wait actually quite nice because the log of a product becomes a sum of logs you take the log of expression at this point you have log likelihood and my claim is you should choose whichever P makes that equation as large as possible Arc exit how do you ARG Max it derive it and set it equal to zero I'm going to let you do this on your own if you want but if you derive it in set it equal to zero you'll end up with this wonderful result that your best value for p is equal to dremel sum all your values divide by n a whole page of mathematics and what did we derive just choose your P to be the sample mean and it's the unbiased estimator is a really fancy way of saying the sample mean so all this math and it says you should use P to be sample mean it says add up all these values and divide by n and that's a good estimate for p that's what mle says it says that will be the value which makes this data look as likely as possible but on some level that feels pretty good for Bernoulli again it's the unbiased estimator I promise it won't always be the sample mean but first now we can just be like that's a good time we've done our math for two examples poisson Bernoulli and both times they led to a result that seems pretty reasonable and this is where we depart off into the wild unknown because we're going to be able to do this for more and more interesting problems if n was 10 for Bernoulli I just want to make this a little bit statualized so imagine you have 10 data points and you want to choose a value P just to make sure we're all following along with what we're trying to do here you're assuming that all the data points are Bernoulli you're assuming they all share the same value p and we don't know what p is what is pmle if this these were your data points and I give you a few options you can say that mle says that P should be one you can say mle is that P should be 0.5 Emily says the P should be 0.8 and M Alicia's P should be 0.2 talk about with the person next to you see if you can answer this but also as always try and come up with a really good question because a better other people in the class would benefit from that too so think about this a quick check what does mle of a Bernoulli tell us um wow .com okay in the note of C no I'm just joking okay yay or nay yes people really felt sad for either like we wanted it to be you but it's not you almost ever in this class um yeah and what we're saying is a good estimate for your parameter is you know sum up all your values in this case there's 10 values if you add them up you'll get eight and then divide by n which is how many data points you have and that's the mle estimate for what it's worth a beta distribution would give you a much richer representation of your belief in the probability so mles maybe we can recognize one of the limitations here it doesn't give us a rich representation of of expressing how certain we are instead it's just giving a single number it's just coming with single number estimate and that's what mle does Based on data at a model it will give you just a single number back okay yay you guys got it right yay um and you know we could have gotten it by our mle estimate I want to take a moment though and talk about likelihood on its own so likelihood was a step that we used to come up with our prediction for Theta but it's an interesting step on its own because it says How likely is the data in the case of our particular power setting to our parameters and the likelihood if you were to write it down it says take p and we saw eight examples of one so that will be raised to itself eight times and we saw two examples of zero so you'll see one minus P two times and that's what the likelihood is and we're going to choose the PS that make this as large as possible and the claim is that this is the value of P which makes this as large as possible just a little bit of intuition but not that important what is really important is this formula you guys are going to use it in section you're going to use it on your problem set you're going to use it on the final exam you will need to know this mle formula for starting out with some data and ending up with an estimate for parameters yeah okay there it is this is like our little mle and we're like so proud you're like welcome to the world Emily I do want to mention that it's incredibly General the point of this demo I'm about to show you where we start with data and we're going to estimate the parameters for mean and variance using mle the point of this is actually to give you two pieces of insight and the most important one being that mle works even if you have more than one number you have to estimate so for the gaussian you have to estimate both mean and variance so just to be clear my first little example this was the game we were playing we're trying to use choose the mean and the variance which made our data look as likely as possible and now we want to do that a little bit more precisely we'd like to not have to use guess and check starts out with saying How likely is a single data point if you tell me the parameters and that is just going to be the probability density function this step is almost always just plug in the PDF the pmf or the joint distribution then we're going to try to figure out what are our parameters to do that we first write log likelihood and then we're going to try and figure out what is the values of mean and variance which maximize this when we write down log likelihood it's just going to be the sum of the log of each of the likelihoods who feels like doing this log oh but and then you look at it you're like okay that'll pull this term apart and the log of e to the something is actually quite nice and when you do this it turns out to be you know negative log of that constant and that whole e disappears and you're just left with what e was raised to the power of when you do the log like logs and E's like to cancel out so in fact the log of this expression led to something quite nice to work with so at this point we have the log likelihood for a normal and the real learning experience for this one is um oh by the way and then I just distribute the sum so this expression just a little bit of algebra gives you this one when you move the sums have this sum on the side and the sum on the side the real takeaway I want you guys to get from this next example is when you have log likelihood if you have more than one parameter you can choose them simultaneously if you're using our original idea of optimization you derive them with respect to each parameter so you take this expression derive it with respect to mu and then you take this log likelihood expression and derive it with respect to Sigma which is your other parameter if you derive it with respect to Mu you'll get this equation which you can set to be equal to zero if you drive it with respect to Sigma don't worry too much about the derivations Wolfram Alpha could help you there but if you did it you would end up with this derivation which you could then set to equal equal to zero so here I have my log likelihood check then I differentiate log likelihood with respect to each parameter set to equal zero and one final thing we could do is we could solve for both mu and sigma in these two equations you have two unknowns two equations you're like hey Chris don't have more unknowns these are not unknowns these are your data points so n is known X i's are known those are all knowns the only two unknowns we have are mu and sigma and if you solve them simultaneously you would get this result you would get that uh mean should be equal to wait a second that answer we always get which is the sum over all your data points divided by n and if you were to solve for this other equation plugging in your answer for mean you would end up with your estimate for mle's variance is going to be equal to take each of your data points subtract off your mean estimate Square it and then divide that by n my big Point here is not how you do this optimization with setting it equal to zero my big point is if you have multiple parameters it's not a problem you will have to drive log likelihood with respect to each parameter but then any optimization algorithm can take it from there and the setting equal to zero trick works in this case but another trick I'll teach you in a bit works as well I did also want to highlight one funny thing here who thinks that this is a pretty reasonable statement if you're guessing the mean baselt data add all your data points together and divide by n does that seem like a pretty reasonable way to choose the mean yes the other claim that mle gives you is that if you want to choose the variance you use this equation and does anyone take any issue with this equation like I take issue with this equation yeah remember there being an equation a lot closer a couple classical or you have to divide by n minor absolutely divide by n minus one you're right and the claim was if you try and estimate the variance from data and use this equation you'll get a number which is too small because your guess for the mean was going to be wrong you always guess statistics wrong and this one is going to be closer to each of your values as such you're going to think that you're varying around the mean less and it turns out this magical minus one solves it and makes it so that you are not going to be on average underestimating we call this a bias estimator so this is a great guess for mean Based on data and that's not a really great guess for variance Based on data you're like mle you let us astray yeah sometimes it will do that so anytime we have like a variance or standard deviation if we're not dividing by n minus 1 to 10 35 by definition yeah it will always be too small for any other estimators every estimator will have an unbiased equation and so we for mean this is the unbiased equation and for variance it's got a minus one and other estimators would need to have an unbiased equation too uh oh how about that one how do you think that's not this is the biased equation okay yes yes when you say it's biased does it got to do with the sample size or something I was thinking maybe I'm using minus one music when this happens so the minus one does a magical thing it just kind of counteracts the fact that this is going to be a bad guess of me and it's going to be too close to your data points and the minus one really balances that up but you said something very important you said sample size which is a point I'd like to get deeper into if you have three data points this is pretty wrong if you have 10 000 data points for getting the minus one really doesn't matter that much and what I'm going to claim in a little bit is that while mle doesn't always give you the perfect estimate from your parameters as your number data points gets larger and larger it will get closer and closer it will converge the right number I want to drive this point home with a hilarious example let's say you want to do mle and you want to choose a uniform what parameters does a uniform have Alpha and beta Alpha and beta what do those represent ative yeah the smallest value that uniform can take on and the largest value if you wanted to do mle you'd start with data and we'd tell you what values you have for Alpha Beta it turns out I actually constructed these ones with Alpha equals zero and b equals one you can know but normally you don't know what these are and you're trying to guess if you change Alpha and you look at the law likelihood value notice how it goes up up up up up up up up up up and then it plummets to zero why is that well if you say the minimum value that your data points can take on is 0.2 and then you observe a 0.15 this looks impossible so the likelihood will just get bigger but if you get as the minimum value gets larger than your smallest value here likelihood of your data goes to zero same thing happens with the maximum the maximum will have a pretty high likelihood and always a positive likelihood but if your maximum smaller than the large value you saw then it says your data is impossible given this parameter an impossible and probabilities expressed with the probability of zero or a likelihood of zero so if you were to choose mle values what value do you think you would choose for Alpha you'd say that my Alpha here is equal to 0.15 that is the alpha which made likelihood the highest what would you choose for the max value here for beta what is the ARG Max of beta in this case of that beta likelihood with respect to Beta 0.75 I told you that the true Alpha and beta that I used to generate this data was 0 and 1. but when it had to choose its Alpha and beta it chose different values in fact I chose Alpha to be the smallest value in this list and it shows beta to be the largest value in this list I want to show you this example to drive home a point um it's this idea of when your numbers are small it does this thing that we intuitively would call overfitting it's not trying to generalize to any data it hasn't seen it's only trying to describe the data it has so if it's trying to describe these data points these are pretty reasonable choices for Alpha and beta but if you're trying to describe data points you haven't seen as well then these aren't really good choices you want to use something a little bit smaller than what the smallest value you haven't seen but anyways some qualities of mle in the limit as data points goes Infinity it's the best thing it's potentially biased that we've seen through the uniform and we've seen through the gaussian it might choose bad estimators for small data sizes but also it's one of the most popular things ever used in practice for parameter estimation okay I'm going to show you a few examples then I'll give us a minute to think about it and then we'll spend 10 minutes doing one problem together on your problem set you're going to have to do this you're going to do maximum likelihood estimation of the wind I'm going to give you a probability density function and I'm giving you data and I say choose the parameter Theta based off this data and you're going to do exactly this write the likelihood write the log likelihood and then figure out which value maximizes the log likelihood in section you're going to do a old exam problem on the final exam at one point I said here is the distribution for length of menstrual cycles if you take a particular person who has menstruation and they give you some data could you choose the parameters that would make their data look as likely as possible this was last time I gave a cs109 final I put this mle question I said how long a phone lasts is known to be coming from this thing called the reliability distribution it's a probably distribution we've never seen before but if I gave you data points like n data points could you choose the single parameter which is Alpha and then one of my favorites is this one and this problem I have put it into our lecture for today if you go to today's lecture we have this exact same problem which says I give you all these data points and you know what these data points are they're sizes radius of sand on the beach which is like I got really like artistic I was like I want to make art which fits the distribution of the sand size on the beach which is really like not that important but what is important is you might want to fit a Pareto distribution based off data So based off these dis data points could you choose the alpha for this particular random variable which is called the preto we didn't study as one of our core random variables but using mle you could figure this out this is the sort of thing you could do sort of thing that could be on your final exam in fact this was once on a final exam to give you an idea I'm going to give you guys your pedagogical pause take a minute and then I'd like to show you what this answer looks like so take a minute and talk about this with the person next to you or just take a break and think whoa mle what a time to be alive thank you whoa studies success if I told you all of these data points were pulled from this probability distribution and I said choose an alpha now I wrote this up and what you could do so you can check along at home but the answer to this starts with saying okay I'm going to figure out what Alpha is and the way to figure out what Alpha is is going to be using mle but before I jump into this any questions come up during your pedagogical pause okay how could you choose this Alpha what's the first thing that you have to do the first thing you would have to do is you'd have to say well I'm told the probability density function for one point but the first thing I really need is the likelihood function there it goes so the likelihood function and I often think of it as a function of my my parameter why because as you change your parameter the likelihood function will change since all my data points are independent it's going to be the product over all my data points of the likelihood of any one data point in the presence of this parameter so if I told you Alpha was 5 and then you took all these data points you could Loop over each of these X I's and you could plug them into here and you could get that this is going to be equal to Alpha divided by let me see if I get this right X to the power of alpha plus one but I don't just write X I write x i to represent that first I take the first data point then I take the second data point and each time I plug them into the probability Mass function that's the likelihood but we don't really need the likelihood what we really need is the log likelihood so remember the log likes to go and destroy this whole thing and go inside and then you get the log of whatever your probability density function was one of your steps in mle and if you solve this it looks a little scary at first but then you use your powers of logs and you're like okay this is going to be log of alpha this is going to be minus Alpha plus 1 times log of x i and at this point we've got our log likelihood function if we want to choose a value for Alpha what we'd want to really find is what's the derivative of this log likelihood with respect to Alpha because what we really want to figure out is which Alpha makes this the largest we want to ARG Max this over all the arguments Alpha so we have to derive this with respect to Alpha so that's going to be equal to the sum from I equals 1 to n what's the derivative of this term with respect to Alpha one over Alpha yeah fantastic and what's the derivative of this term with respect to Alpha e well if you expand this would be negative Alpha Times log of x i plus log of x i and only one of these has an alpha in it this would end up being negative Alpha Times log of x i if you drive that just becomes negative log of x i so that's the derivative and once you have the derivative you're laughing lots of optimization techniques end with this if you can write into a computer an expression for the derivative you could just pass it off to most commercial optimizers and it would just give you back a value of alpha that maximizes this we have been using this trick of setting this derivative equal to zero and if you set this derivative equal to zero I'm not going to write it on the board but you know just be clear you got the log likelihood then we took the derivative of log likelihood and we got this expression oh and just to be clear this expression I often will put the sum as far inside as I can and that would make this equal to n divided by Alpha because this is just one over Alpha added n times minus the sum of log of x I's now as I said this is a pretty satisfying answer most times I give exams this is the final answer that I'm looking for I'm like give me an expression for derivative because if you can do derivative you can do ARG Max but to just drive that point home we can say that the derivative equal to zero and then we can solve for Alpha and if you solve for Alpha you would get this expression and we've done this a whole bunch in class we've done this to the point where we can get an expression but we haven't actually coded it up if you set this derivative equal to zero can somebody help me read this off the board what's my expression for Alpha and I'll put it with a little cute hat on it okay what is it F divided by [Music] all right you know this is just an expression it doesn't seem to mean that much what I think really drives it home is we could put this into code you can take these observations and we can write our estimate of Alpha and it takes these observations um and you know we could try and write some code here that instead of returning zero estimates Alpha so if I had to code that expression up I would probably do something like calculate my log sum so I'd start equal to zero for X I in my observations I'd say log sum plus equals math.log of x i so this calculates that expression and I'll Take N divided by that expression what is n n is equal to the length of my observations and I would return n divided by log sum and if I run this it gives me back a number it says that my estimate for this parameter Alpha based on this data is equal to 2.7 oh come on okay great so this expression you know you could code it up and this would give you equal to 2.7 would be my estimate for Alpha given this particular data you know a lot of times we use these xi's but I don't want you to forget that in reality x I's will be observations it's your data set from which you're making your estimate okay that is the end of our wonderful journey I know that was hard but like what a Cornerstone of marcine learning that you've learned today uh using the Disney analogy that brings us to the end of our wonderful movie but of course there's always a sequel come back on Wednesday and we will continue this wonderful conversation you guys are fantastic go to section practice miss you we will see you on Wednesday