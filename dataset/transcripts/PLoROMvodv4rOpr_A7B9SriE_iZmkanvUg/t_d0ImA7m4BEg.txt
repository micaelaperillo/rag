that's for people who are online my microphone off for a second they didn't miss that much just basically we're gonna explain about the difference and today oh it's a special class because we're actually going to be making something real uh cs109 has the wonderful honor of being the place where the Stanford eye test came from where we think about probability under uncertainty about whether or not a person can see based on responses to whether guessing different letters at different sizes um and I always want to make this a class demo and now it is and really the probability question that you're going to have to think through is you have a belief about how well somebody can see that user that is presented with a letter we're going to decide how uh the size of that letter in a future class but the user sees a letter and then they either get it correct or wrong um there's a version of this where like there's an e up down right or left you're like what is that is that some weird W no that's supposed to be an e or facing up and then the user is supposed to guess up left right or down and if they get it wrong your job is to update your belief in how well you think that the user can see given that they just got that particular letter size wrong and of course this is very interesting uh because then you could figure out how well somebody could see over the course of a longer test we'll learn how to do exactly that today we'll do something incredibly real incredibly practical in cs109 this Monday where are we in cs109 we're learning about probabilistic models and it's such an um a wonderful time to be studying today we're going to continue our learning goal of just combining Bayes theorem and random variables in fact this task of inference is really just that what happens to random variables When You observe information about other random variables a little bit of review before we jump in uh I have this fun little Story Once Upon a Time wait was it last time no a few times ago when I was teaching 109 I was gonna have a baby and so my wife was really pregnant and I really wanted to figure out what's the chance of this baby showing on the due date or not on the due date and because I'm a total nerd I made a graph and so this graph is an interesting graph this is the probability distribution of when a baby shows up where zero is their actual due date you know babies are given due dates most babies are not born on their due dates in fact more baby like the most common date to be born is like four days after your due date am I making sense imagining babies is not that common in your guys's lives but you guys can imagine babies and due dates this is making sense okay so this is the full distribution gotten from empirical data for uh quite a lot of years there's been many babies and we feel like this is the natural distribution of when babies arrive relative to their due dates but the interesting question to me was how does this probability change when I observed that the current date is like here let's say the current date is here how does this probability change you know like if the current date is here is it possible for the baby to have been born here oh assuming that there's no baby so far no you kind of have this thing where like if the current date is here you kind of know an updated belief of all these should be zero um if the current date is here you kind of feel like these probabilities should change but it's a little bit unclear how they will change and Bayes theorem does give us this answer um so I'm going to let you guys sit with this for a second you know warm up talk to the person with next year and then I'll tell you the answer in just a moment um but let's say d is the day when the baby comes and I want to know what's the probability of this random variable that's the task we're continuing to learn about um given that you observe no child so far up to this date I'm going to give you the hint that a lot of times we're going to be using Bayes theorem so we have a prior belief about this the probability of the child being born at a particular date and what's the probability of no child so far given that the baby is actually born on a certain date is something you can think about if you think about these two things you're making good progress you're just left with this awful normalization constant and I want you to just take a moment with the person next to you and try and think about wait do I actually know how I can get this term and can I figure out how I would get that term think about what the person next to you and if you're feeling super Advanced you don't have to do this what would you do about this denominator okay go for it have a nice little conversation let's have it right okay okay this is a warm-up but I I will note it's a pretty hard warm-up um and to give you an idea of how hard it is the first time this problem was posed to me was a year before my baby was born when a colleague had a baby that was about to be born and they came to me with this problem they're like hey Chris it's past the due date and I need to know the distribution of when the baby comes so I can think about like my my midterm exam and so you imagine as a college in the Cs Department someone who thinks about this stuff and it's a little bit tricky to come up with first of all did anyone have any questions about where you would get this term from anything confused this term is the this is my prior belief before I had any information and that's what we call this term this is the prior it's just a lookup you know if D was equal to five you would be just looking up what's the value of five in your prior that's all that is it's a lookup of your belief before you see evidence now this term is a little bit wild and this definitely is more you need to think about what it means uh then there is an equation for you to find here's an easier one to think about let's say that the baby is really going to be born on day five or sorry let me let's say the baby was really going to be born on date negative 43. so put in negative 43 for a little d what's the probability of no child so far at this point so the baby was really born here what's the chance that you haven't seen the child so far zero okay that was good now what if I flip it over here what if I say the baby's really going to be born here what's the chance that I've seen no child so far at this point okay very good I thought that was actually really tricky but this value will either be one or a zero you either you know definitely haven't seen a child or you definitely have seen the child depending on whether or not you've passed the date that the child's born super hard to think about but what good practice for going between the world of probability and the real world so this is pretty great we can take every prior belief in either multiply it by zero or one and we're left with this awful awful term on the bottom no child so far not conditioned on knowing when the baby is born oh did anyone have a good way we could do this so probably no child uh is the probability of no child so far I only think about that for a second I don't know when ah no it's not one because if the baby's this is weird this is why this term is called normalization content it's very strange to Think Through you're not conditioning on when the baby is actually born so it's possible in this term this term is still accepting that it's possible that the baby could be born before so you know little capital D could equal to negative 47 in this term weird one to think about is there another way we can get rid of it um I was thinking that for this bit of the area under the curve should always be equal to your body that's a very cool insight and it's deeper than uh you might feel at first blush it must be the true that the area of this probability Mass function must equal to one you know how this term is called the normalization constant it gets that name because a lot of people find that the best way to calculus is to figure out what value makes all of this equal to one there's a different way of getting to that exact same answer which is what I have to do now is I have to do a law of total probability over D and this is going to be the log total probability of the probability of no child so far so far given D equals D if you you know law of total probability of this you'll end up with the exact same result of just treating it like a normalization constant so if you multiplied all of these you would get some distribution and the normalization constant will be whatever number it needs to be such that if you summed up all of these terms it would come to one and I think that is exactly the right way to think about it as a way I was hoping you guys would think about um oh at this point you guys might have noticed there's a massive vertical bar up here so I really cared about this like I was doing this for my wife and myself so like I wanted to get things right does anyone have a guess for why there's this like massive vertical bar and why the distribution doesn't just keep going it's not just like a truncated version yeah naturally humans can have babies after this point so this is the due date and then like after this point babies can have it can come was it the day after which she would just have a C-section yeah or it's not a C-section it turns out if a baby in in most countries if a baby is really really late they'll give you a drug that induces delivery and so they kind of make the baby come so this is the date of induction so that actually is just the sum of all the probabilities of that date or afterwards we've got a little bit too deep into babies but if there was one major takeaway here it's really this one that one way to think about this normalization term is it is the term that allows the whole probably Mass function to come to one which is a cool new angle on that term yes yeah if you think about it um the pr any probably Mass function like your belief in D at any point in time if you Loop over all values of D if you sum those up it should be equal to one because D is going to take on some value um and so one way to think about is there's some unknown term here K like if you just call this K you say there's some unknown term k but I know that probably if D given no child so far if I summed over all values of T it should come to one that would allow me to solve for what K is I can then say you know um the sum over all of these uh terms should be equal to one and then you can solve for uh k yes integrating from the left hand side up until the current date or for the current date of right and this one's going to be integrating over all the possible days that are still left um so it's it really is integrating over no child so far over all the values given the values or today today so far yes that's right okay okay good we'll come back to this in a non-gated version and but if there's one major takeaways I want you to start thinking about new ways okay and a little bit more review um we talked about Bayes theorem a long time ago we talked about random variables take on values our events and so we can think about hey what's the probability of this event that M this discrete random variable takes on the value 2 given that this other random variable takes on the value three and we could just use Bays it's just events now more generally you start to see things like what we had in the previous slide where you have random variable takes on any value given other random variable takes on any value and you have to like generally solve for whatever insertion to n and whatever's insertion to M this is more general statement but the reason that the Bayes theorem works the same is because this really is an event it's just an event with an unknown input so far shorthand will often be used when you get to these complicated statements because people don't write like to write random variable M takes on non-random input variable Little M so you'll often see this shorthand when people write this up and then on Friday we took this up a notch we started thinking not just about discrete random variables but continuous random variables and we had this nice result which is Bayes theorem Works whether or not you have a discrete random variable m or you have a continuous random variable X so if they're both discrete you just get exactly the base theorem you thought but if one of them is continuous you get Bayes theorem it just happens that when you're talking about the continuous random variable instead of using the probability Mass function you use the probably density function so the probably density function is like your expression of likelihood for a continuous random variable and probably Mass function is the expression of likelihood for a discrete random variable intuitively this feels very good and then we mathematically showed why it's true and it just doesn't matter what combination of continuous and randoms you have as long as the thing on the left hand side of the bar is continuous use density and if the thing on the left hand side of the bar is discrete then you use probability Mass so base it works in all the ways you'd want just if something's continuous use probability density okay and that works true for the law of total probability so if you want to know like the density at X you could use a lot of total probability over some discrete random variable n again you just use density when you have a continuous random variable and use probability Mass when you have a discrete random variable okay and then in class we worked on this problem we thought what's the probability of the gender of an elephant given the weight of the elephant we've told that you know gender equals one has this weight distribution gender equals zero has this weight distribution and we want to just infer the probably distribution over G given an observation to X and I want to lightly point out that I've told you before if you have multiple random variables the thing you would need to solve anything is this thing called a joint and the joint says what's the likelihood of particular combinations to random variables so like g equals one and x equals 72.3 you've given me assignment to all the random variables and you should be able to talk about the probability of those two things together and we do have that in the problem because we could decompose this like the probability of the gender and then the probability of the weight given the gender and so because the problem has given us the probability of the gender check and the probability of a weight given the gender then we could figure out exactly the joint probability I just want to note that it's implicitly there and you could do this more generally but if you want to solve this problem what we did in last class is we set up a base theorem we said we want to know probably if g equals one given a particular weight since we had probability g equals one given x equals 163. and that was equal to the density of x equals 163 given g equals one probability that g equals one and then here we had we could use a law of total probability for the classic version of Bayes that we love 163 given g equals 1. times probably g equals one plus the density of x equals 163 given g equals zero times probably g equals zero so if just beta's theorem is just our events our assignments to random variables and in this case where we have a continuous random variable on the left we use the PDF what's the PDF here oh that's a little scary why is it a little scary because you're like um what is that well X is a normal if g equals one then we know that X is this normal and you know what normals have oh sad times what do normals have a probability density function they have a cumulative density function and then probably density function and what you're going to put in right there is that normal PDF what a time to be alive um I'm going to get more into this Wednesday but I'm going to start showing you that when you have big models of lots of random variables there is this way of visually drawing them and because they gave you gender and then they gave you weight given gender this picture is actually implicitly behind the scenes that we imagine that the gender is what's causing weight and so that you can give the joint by first giving the gender and then the weight given the gender that's not that important right now but it will be more important on Wednesday okay wow what a lot I do want to give us one more piece of practice on Friday's class we did this project where um we thought about the situation of trying to decide if a baby can hear or not so play a sound and then the baby will look up or not based on whether or not they hear the sound but we have this problem that sometimes babies look up when they don't hear the sound and sometimes babies uh won't look up when they do hear the sound there's some stochasticity even though there's a probably a distribution so if you want to infer whether or not a baby can hear given their response to sound we need to use probability but now I'm going to bump this up a not CH so for babies who can hear sound when a sound is played their gaze moves by a continuous random variable so if you play a sound there's a good chance that their gaze moves by 15 units and there's some variance to it it's possible that their gaze moves by zero units possible it could be negative if a baby can't hear sounds there's still a distribution for how much they move their head it's just a different distribution so if a baby moves their head 14 degrees after you play a sound you can't conclude with certainty whether or not the baby can hear but you can update your probability so my question for you guys a little bit of practice is under this normal assumption where we're given distributions of how much babies move when the sound is played can you update your belief so again take a little time for practice I'm actually going to give you guys a minute and a half to think about this because I think it's very important in fact some of the hardest problems on the midterm often take this form so if you can if you feel uncomfortable with it great you have something to study and if you don't feel uncomfortable with it wonderful you're in a very very good place in cs109 so let's see if we can think this through how could you figure it out okay I'll race the board what we think about it sure thank you instructions how am I supposed to complete probabilities foreign but if we want to be able to do that Stanford Acuity test we need to get very comfortable with inference because right now I'm going to tell you what we're doing here is we're updating the belief of a Bernoulli given the observation of a continuous and we want to do the Stanford eye test we're going to have to be able to update a belief about something much more complicated than a Bernoulli so you really want to feel comfortable in this space before we can do something more complicated I know this is hard okay but at the same time it's not that impossible you just have to follow the recipe the recipe often involves your first step is to write your random variables like if you get to a midterm problem you don't know what to do write random variables you'll probably get some extra or like partial credit even if you don't get the answer past this point but Define your random variables really matters and once you define your random variables then write what the question is asking in terms of those random variables and in fact then you've really set yourself up for success so if I say h is a 1 if the baby can hear I'm going to say it's a Bernoulli random variable you could have had an event without a random variable here but I'm going to get in the practice of using random variables in place of events and if it's just binary I'll use a Bernoulli so what's the problem that H equals one given that x equals what was X 14. that's what we want you to calculate and the only reason this is a little bit difficult is because the thing you're conditioning on is continuous so you have this continuous belief over how much the baby's view will change and because that's continuous we're gonna have to use that other version of Bayes where we don't use the probably Mass function we use probably density function so I hope when you guys got to this point a natural thing to do was f of x equals 14 given H equals one times the probability that H equals one divided by and we can call this a normalization constant or you can expand it out um you can expand it out by thinking about the case where H equals zero and k equals H equals one there's several ways you could deal with this the probability that H equals one that's your prior belief I think I forgot to mention here but we're going to assume a one-half prior this is a pretty safe assumption if you don't have a belief otherwise but then we still need to put in this term and for this term I've told you that the child is hearing so when the condition gets flipped you're told what is the state of this unobserved hearing variable you're told that hearing is in fact one and then it's How likely is X to be 14 if hearing is one can somebody help help me out here what what is this and actually if you had to write this in Python it'd be like uh PSI Pi dot Norm dot PDF and what are the values of norms PDF you have to say what is the value it's taking on you have to say what is the mean of the distribution what's the mean of this distribution yeah because we know H equals one we know we're talking about this normal not that normal and then uh you have to put in the other parameter which is annoyingly in sci-fi the square root of the variance now you could have written that or instead of writing that you could have written what this actual mathematical equation is if you want to write what that actual mathematical equation is we have it right here it's 1 over Sigma square root Sigma is the square root of 50 and then you have the square root of 2 pi e to the power of negative 14 is X minus the mean divided by uh the standard deviation which is the square root of 50 squared did I miss anything there oh the minus one half and you can put that term in and you multiply it by one half a nice thing is if you followed through with this problem all of these terms would end up canceling out and you just get this nice little expression on the top and you get similar Expressions on the bottom we haven't dealt with the bottom turn but as I said you could just expand it out by thinking about the case for H equals one and the case where H equals zero like our classic um Bayes theorem but I know this is hard so what I want you guys to do is I want you guys to tell me what's confusing about this like what what's hard about this what could I go deeper into so how would you like good question the good question is how would I do this on a midterm like I can't calculate this by hand you don't need to calculate this by hand so there's two valid ways you could answer this problem midterm this is an answer on the midterm like that would be like the answer would be you know assuming you fill in the denominator that's the answer we would be looking for and you don't have to solve it the other thing is if there's a time where you want to use a python function we'll let you use a python function we just kind of assume you would be able to use a computer if we gave you one so if you say like you know calculate this using scipy I'm fine with that too okay yes you have you're using all pmfs in this case we're using PDFs um but PDFs don't return probabilities right that's right they give you back probability densities and it's amazing that if you multiply this probably density Pi this probability and you divide by this probability and so you're still left with a probability um and that does come from the fact um that there is this Epsilon trick we used in the derivation um and that Epsilon trick is allowing this probably density to convert to a probability and back so if you check out the derivation there's a nice reason for y but you're absolutely right it's just not a problem which is a beautiful thing and it's mind-blowing beautiful wonderful thing that you should use in your life and on the mature question yes can you explain again why p h equals one is one half oh that's a this is your prior belief in whether or not the child you hear before you see any evidence so it's like you haven't even met the child what's your belief that they can hear uh and sometimes you have reason for a deeper belief in what your prior is and sometimes you don't and if you don't have a deeper belief than like the uniform prior is pretty reasonable so the probability that H equals one equals probably H equals zero seems very reasonable to me the problem figure that otherwise it's the problem is ah so the senior isn't really helping us here this evening was such a beautiful thing if we wanted to answer probably questions about it normal like what's probably that a normal takes on a value in a particular range but the CDF doesn't even come in here um I suppose if I told you like what's the probability of hearing given that the child heard you know moved less than 14 degrees then that would allow you to use like the CDF on this side but um yeah a lot of times with Bayes theorem the PDF is more useful so the PDF you needed to know it for this reason okay you guys are wonderful I know this is hard but those two problems if you could go practice them I think uh you'll really appreciate that come Tuesday again I just want to note gently note that there is this implied way of expressing The Joint distribution here we're saying that we think it's the hearing that causes the Gaze change and so first you want to know the hearing and once you know the hearing the knowing the Gaze change is a little bit more straightforward and there's kind of this implied causality and we're going to get way deeper into that in Wednesday's class but um they give you the joint by telling you the probability of hearing and then the probability of gays change given hearing as I said if you want to solve all probability questions you want the problem ask her to have given you the full joint distribution okay are you guys ready we are going to bump this up a notch these have been hard problems but there's one thing that's made them easy here than what we're about to do the thing that makes it easier than what we're about to do is a lot of times what was on this left hand side was just a Bernoulli it's like you had one belief value and it was updating and now I want us to think about what happens when your belief distributions not a Bernoulli and we're going to be doing this by coming up with a better eye test so in cs109 we started using probability think about estimating how well somebody could see so you can go to my eyes dot Ai and you can run this cs109 algorithm and if you go to run that cs109 algorithm as you answer questions you can watch the belief distribution how well you can see change this is the old problem where we're solving has anyone ever taken one of these eye tests oh brutal right they like ask you this row and then they ask you this row and then they ask you this row and there's two big issues here like it doesn't allow for you to ask questions that are between these font sizes and then a lot of times there's stochasticity and Randomness that's not being taken into account when they score you based on this exam like maybe you kind of see the number and you're guessing maybe you make a mistake and you kind of knew the letter but you said it wrong anyways and without thinking about all this probability they actually end up in the wrong answer a lot of time so this is an explanation of like what 109 is able to do this is your normal eye exam 20 is the normal amount of letters that you have to answer and on the y-axis is how wrong it is you want to be low on the y-axis and this is pretty high this is relative error is just like the percentage difference between what the exam says you can see and how well you can actually see so you can see there's kind of some error it's not 100 but it's notable and you know longer tests don't really help that much if you want to do a clinical trial you use the much bigger version of the exact same test but it still has pretty high error there is this really cool dude in Switzerland who came up with this algorithm that we learned about later and that algorithm didn't use a lot of probability Theory but it did some nice adaptive answering but then we came up in cs109 with the one that used the nice probability theorem and you see we can get like way way way reduced uh error rates and it all just comes from a little bit of thinking about probability and you'll be surprised how simple it is like you're like hey it's just Bayes theorem with a nice little normalization mind blown and like you know from 1800 until 2000 I think it was two years ago people just didn't think about applying base theorem in this way okay so let's tell you this story and I'm going to give you a way of representing how well somebody could see we're going to represent how well somebody can see as a random variable and we're going to consistently use the random variable name a to represent how well someone can see a can you think of it as ability or if you're really fancy you can think about as a cutie Acuity is a really technical term for ability to see or that's visual acuity so this is a random variable it's a random variable and therefore you want to give the whole distribution if that random variable is continuous you should be giving a probability density if that random variable is discrete you should be giving you know some function that a probably Mass which which represents whether or not or the different assignments a little a to their probabilities this is the cheekiest thing that we did ability to see really should be continuous so if zero is like you can't see at all and one is standard vision ability to see in the natural world is continuous but to solve this problem we did something that you guys probably desire to do many times when you see continuous random variables we discretized it we said sure it's continuous but what if we represent as like 200 points so we can say like or let's say 100 points so you have a billion series zero a bill you'll see a 0.01 ability to 0.02 we took something that's truly continuous and made discrete by making a hundred little bins and we thought about you know if your true ability to see a 0.2222222 we'll just truncate that to 0.02 wild does that make sense okay then as soon as I tell you that now you have a discrete distribution and what I'm showing you is your prior belief in the ability of someone to see so this comes from historical data we've got lots of data from around the world of people's ability to see and here's what that prior belief looks like what is this prior belief thing is it saying that more people can't see or that more people can see like when someone walks into this exam are you more likely to believe that somebody can or cannot see I'm just going to give you guys a second to think about this because it's important we're going to build on this chart a lot uh in the lecture low probability here high probability here we think that most people walking into this tent this test can see you can change your priors priors are you know philosophical different people could say well I think my particular Clinic has a prior that looks like this or you can say I don't want to make any assumptions so originally I'm going to assume everything's equally likely all valid priors but what we're going to be using first is the historical prior so like this is the prior of abilities of people to see yes when doing that would you too um the value with the top probability such a good question I'm so glad you asked that okay so there's when you talk about how well somebody could see a lot of us think about number but from today on in cs109 I want you to think about the whole random variable if he says what's your somebody says what's your belief that somebody can see it's not a number it's a whole distribution and that distribution relates all possible numbers to their corresponding probabilities if you force me to choose a number I could choose expectation or I could choose mode but never force me to choose a number I would much rather have a random variable than a number because a random variable keeps tracks of all the possibilities that it could take on it keeps track my own uncertainty that maybe somebody you really could see not very well and so I love random variables and I think if you reduce it to a number you've just lost so much and there's a little tear in my eye when somebody makes me do that but uh that's not important right now okay other questions about this okay now in your computer we haven't talked too much about how this gets represented in computers but if I'm representing this probability belief in a computer and if I've discretized it there's a pretty reasonable way that I could represent this I could represent it as a dictionary so a dictionary has keys and value Pairs and so I'm gonna have a dictionary where if you put 0.09 in for the key I'm going to give you back the prior belief this one's calculated for you from historical data but this is how we're going to be representing our random variable does that make sense like Ensemble it might be intuitive but again there might be something confusing is there anything confusing about this you could have written this yourself if I gave you the data but that's not what we're going to focus on today okay you guys following along so far please ask me questions because I will just keep building so again your prior belief you can represent it as a graph you can represent it as a dictionary but these are just two different representations for the same thing they're both representing the random variable and they're both representing for any assignment what is the corresponding probability okay this is not that important but there's a doctor in the room I know you guys who love rhythmic units and I've simplified this a little bit for class so just before you use this in a real medical case just come talk to me okay now you had that prior belief so before the patient walked in you had a belief about how well they can see let's say to start off you show them this font size so there's a particular font size 0.7 and they get it wrong getting it wrong we're going to represent as there's a random variable y y equals one means they got it right y equals zero means they got it wrong You observe that they got it wrong what happens to your belief and as soon as you hear those words I want you guys to start dreaming of Bayes theorem you're like I have a random variable I observe something and I want to change my belief in the random variable that's exactly where this wonderful equation is most useful I will let you know that if we did Bayes theorem correctly you know you'll watch your distribution change they just saw a really large letter and they got it wrong so now it starts to become really believable that they can't see that well whereas in our prior we thought most people could see pretty well now that they got a big letter wrong we're like uh oh we're not going to rule out the chance that maybe you could see pretty well you just got unlucky but now we start to believe okay maybe we're talking to somebody who doesn't see it that well like me so if I could put this together all in one chart it looks like this you have a prior belief then you see an observation that's a particular font size they got it wrong and then you want to update your belief um oh well ours had something that was a little flatter because I put in two different numbers when making these slides but your updated belief looks like this is and the way we're going to get there is through base and base says this thing I care about the belief and ability to see given your observation is hard to think about but the flip is much easier the probability of seeing this observation given I tell you how well somebody can see you have to have a prior belief and you have to deal with the normalization constant but we're going to use that idea from beginning of the warm-up that this is just going to be whatever number we need it to be such that for all values of a this thing sums to one it will truly be a normalization so hopefully you guys see that this is a cool thing to have like how nice would that be so hope that's what I want you to see about this equation and I want you to see about this term that this is something we already have access to somebody has told you their prior belief if you had a particular value of 0.5 for example you would just be looking up in your Dictionary what is the probability associated with 0.5 so if this is a probably a equals 0.5 giving y equals 0 you would just look up probably equal 0.5 I'm leaving a little variable here because I don't just want the belief of ability taking on one number I want the whole belief over all possible numbers I want a whole freaking probability Mass function okay which must leave you with a question which is just like uh how are we going to get this the likelihood term How likely is your observation given I tell you the value of the thing you're trying to infer the hidden variable we often think about this thing as the hidden variable the unobserved you can't open up someone's eye and see how well they can see if instead you infer it based on evidence I'm going to leave this here because if there's one most important slide of today this would be it so we should think about it for just a second man how many times we see base there in this class how many times do you see base theorem in the world there's a beautiful book called like the theory that will not die it's just all about Bayes theorem and how it just shows up every single place in the whole world based there what a thing okay are you guys following along so far are you left with this mystery of where does this equation come from think about it semantically I will tell you exactly how well the person can see and then I want you to tell me how likely it is that they got it wrong couple key takeaways before I talk about this um that cell phone tracking problem that you guys had to do on your problem set was really warming you up for this world of random variables because if you think about it that cell phone location could have been one random variable where it takes on values of all those locations we thought about it as like 16 different events but you could really think about it as one random variable with 16 assignments and those two things are going to map very nicely onto each other um some beliefs can be represented like if you have a random variable you really want to function which is like the probably Mass function the probably density function sometimes it's hard to represent beliefs as an equation and if you're a computer scientist a dictionary is a pretty cool idea for representing a random variable I just wanted to mention that okay okay um and then I also want to mention this cool little trick I did which is I took a continuous random variable and I discretize it to make my life easier okay and then once again there is an implied causality in the world we are saying that its ability to see that causes you to get a problem correct or not correct the converse would be pretty ridiculous like whether or not you get it correct is the thing that tells you whether or not a human can see I mean knowing this allows you to estimate ability but in the real world it's ability that's causing correctness and again we'll get more into that on Wednesday so now let's take a moment and talk about this equation I'm going to tell you the real deal but I'm not going to test you on it like on the midterm not going to ask you about the education theory that explains this but I want you to understand where these could come from are you guys ready for it there is a world in education that talks about exactly this and let's write about what we're curious about we want to know the probably that y equals zero given ability equals a oh sorry probability so y equals zero given ability equals a we're going to want to be able to do this for any value of little a but let's put in a concrete number so that we can think about it so if I tell you someone's ability to see a zero point a what's the probability that they get it wrong does that make sense to people as what we're asking okay so it comes back to this thing called item response Theory item response theory is an old theory that people have had since the 1950s which says if you take a person and you match them up with a question maybe it's a question on an exam or maybe it's a question on eye test we can talk about the probability that that particular person gets it right and since the 1950s people have been making this assumption every person has an ability and every problem has a difficulty this is really the model people use they take the ability of the person subtract off the difficulty of the task and that will give you a number it's not necessarily a probability and so they take that number and put it through a squashing function that gives you a probability and if I had to plot this particular squashing function this sigmoid is a squashing function which is if you put in a really large number it gives you something close to one and if you put in a really negative number it gives you something close to a zero in fact if you put in a zero it gives you a 0.5 that's the sigmoid and the sigmoid function is basically a really cool hack to take any number and make it look like a probability and that's what people use have you guys the Adaptive GRE uses this when people normalize on the GRE they're using this um you know if you take the um I don't know any of the big tests like the TOEFL this is the theory that people have behind the scenes for probability that somebody gets things right the reason they have this theory is because they want to figure out how difficult their problems are and to figure out how strong their students are so this is a probability it says if you tell me the difficulty and you tell me the ability you just put this through a function and it gives you a probability back so the probability that y equals zero given a equals 0.8 is just going to be plugging into this equation you put 0.8 for the ability and then you put in the difficulty you're like wait what's the difficulty well if you know the size of the letter the difficulty is just going to be a function of the size of the letter so big letters are going to be thought of as easy and small little letters are going to be thought of as difficult you can just imagine this is equal to 1 minus the font size so if I tell you 0.8 and I know the font size I can take 0.8 subtract off my function of the font size put it through the squashing function and I get a probability I'm just going to go a little bit deeper if you guys bear with me for a second okay any questions about this as I said I am response Theory not on the exam that we did like a while ago or the question about multiple plus questions like taking that a step forward yeah yeah absolutely um yes and actually that does lead to this one other thing so item response Theory says the probability that person a gets I gets question J correct is this little nice little formula where sigmoid is not the very it's not the standard deviation of anything it's this particular function there's a little bit more to it if you're curious we think about guessing we add in the probability that maybe somebody just guessed and got this correct and we think about slipping too slipping is when you know the answer but you just get it wrong anyways and so you just think about cases we think about the probability of or it's like either you got it right because you guessed it right or you probably or you got it right because you actually knew the answer so you think about this as the probably that people actually know the answer and then slipping like maybe you know the answer but then you just like accidentally fill in the wrong thing just because there's a 100 chance that you just make a mistake all of this would allow us to just have a function where if you give me ability I can straight up calculate the probability that you get it right sorry all this gives you the probability that you get it right and if somebody tells you that probably you get it right and you want to know the probability that they got it wrong it's just going to be 1 minus that which kind of brings us back to you know if we go all the way back here what I'm telling you is that we just have a simple function if you give me ability and you know the font size you just put those two things into python function it'll give you the probability that they got it right back and then you can do one minus that to get the probably they got it wrong cool aside okay I didn't try I tried teaching this class once before and I didn't give people that aside I really didn't want to lose people on it but I thought actually you'd get more insight just knowing a little bit deeper what goes behind the hood you could just think of it as the problem statement gives you this term or you could know the item response theory is the the secret sauce yes or the graph that is on the slides does it go up again for one exactly because it thinks somebody has slipped exactly so here there's some probability that they're strong in my in in the graph I showed you and it's because it thinks that there's a chance that somebody came in and it was strong and then got the question wrong only because they slipped like they knew the answer and they just happened to just blurt out the wrong answer so there's still some belief that there's strong uh visually and they just got unlucky that's exactly why and turns out we we did this thing where we tested out all of our different modeling decisions and which ones matter the most and turns out putting and guessing and slipping were critical to be able to really come up with the right answer after somebody answers 20 questions so there you go okay um so we're going to practice thinking about Bayes theorem but then I also want to point out what happens if you have multiple observations so we've talked about observation one so in observation one you have a belief you see them get some letter wrong let's say it's at font size 0.7 and then you update your belief using Bayes Theory what if then you see another letter like we give them a larger letter and they get it correct how do we deal with multiple observations because we think of each observation as being independent given ability it turns out this becomes just this posterior like after you calculate this wait for it in your second observation it becomes your prior so it's like in your first observation this is your prior but then you saw something this became your belief and then your second observation that's now your belief and you get a new piece of evidence and then you change your belief if you add a third observation this would then become your prior yes because you're using like the previous step as a prior to your next step does it mean that about the order of the questions might influence what the final probability looks like because of the independence that's actually true um it turns out order won't matter so good question very good question okay this is what okay posterior becomes our new prior but really this is what I want us to be able to program whether or not somebody gets something right or wrong for a particular font size I want us to be able to update our belief did you guys get our goal and then doing multiple will be very straightforward if we can just program this Bayes theorem shall we I think we should okay to the code so what do I have for us to start out oh that's our website that's also our website to the code here's what I have for us so first of all I've got a function called get prior probability and if you call get prior probability it will give you back a function I also give you a function called plot which will take this dictionary and plot it so if I run the starter code it just plots this is your belief somebody walks in to the exam and without seeing anything about their eye this is your belief that they can see sound good we're then going to observe something we're going to update our belief given the prior and then also give an observation an observation has two parts hey people in the back can you guys see this you want font bigger just yell it my vision's bad it's mine too I really appreciate that this is going to be the test for us in fact it turns out this test is really important for people with bad Vision uh like myself okay good okay so a belief has two parts or sorry an observation has two parts when somebody answers a question you you see two things you know the font size of the question that was asked and then you see whether or not they got it correct and the correct is going to be a Boolean so are we going to want to write a function called update a belief that can take in an observation so any font size and whether or not they got it correct you know here's a different observation in this observation they saw a bigger letter and they got it correct whereas this one they saw a slightly smaller letter and they got it wrong the letters go from zero to one so these are both big letters okay Now update belief is really what we need to write what I have here creates a posterior but if you read it carefully it Loops over every value inability so every possible assignment to little a and it just looks up the prior and it says the posterior is equal to the prior it doesn't do any updating at all and then it Returns the posterior as if it had done some work so cheeky it's done nothing it's just basically copied over the dictionary and so if you call you know you can have all these changes you want oh python 3.9 I haven't installed decanter on my python 310. we saw all these observations and we haven't changed our belief at all so bad we must be more flexible we must change our belief when we see evidence we all must do this yes question about the graphs are they just stick to each layer ability to see and so they're not specific to the size of the letter at all um you know if we go back here I can look at one of these graphs and I don't need to know the history of the test that they took and I can just see that this says you think the person can see pretty badly because this is poor vision and this is good vision and it doesn't matter what the font sizes were of the question this is just an expression of how well a person can see come in different font sizes right yes so does the letter the size of the correct letter effective says I mean if you've gotten to the solution and when we get the solution right if you put in different font sizes it's going to have a really big impact on your belief so like if you get a small letter wrong that will change your belief in a really different way than if you get a big letter wrong but at the end of the day we're going to be and you know plotting these beliefs of ability to see so it's like what happens in this middle step is affected by the font size and it will certainly tell you how the the prior changes the posterior okay there's one other thing that I've got for us I wrote item response theory for you here I said if you give me the font size your difficulty is equal to one minus the font size why did they do one minus well because large letters have large font sizes but they're easy and small letters have small font sizes but they're hard and so the one minus allows us to say okay go from font size to difficulty and then the sigmoid function of a billion minus difficulty that was item response Theory and I totally recommend you guys think about why I got slip times guess plus one minus slip times the probability that came out of item response Theory that's a super interesting thing to think about if you guys want to just like push yourselves a little bit as it says not the point of this class but I think that's fascinating then also the scaling factor it's like a super extra feature of item response Theory which we don't need to get into but I do give you the probability of correct given ability but it turns out the probability of a correct given ability is super misleading because your observation might not be correct if your observation has somebody who got it correct you should be giving back that number but if the person got it wrong you need to be giving back one minus the chance that they got it correct so going back to the slides item response Theory gives you the probability that y equals one given a equals little a if you want the probability that y equals zero you have to do 1 minus that value everything the world's a potato are not a potato law of probability okay so all this means that when you package those together I give you the probability of an observation given ability so probably of observation given ability is basically writing this probability of an observation given an ability so this whole expression just becomes a function call that I give to you and come talk to me about IM response Theory you can go deeper into it so now we have problems or probably of observation given ability and our job is to write this function you saw an observation and you're not going to change one ability you're going to change your belief in the entire ability so we're going to calculate a posterior we're going to Loop over every value little a and for every value little a we can look of its prior and we can do one more thing we can now call this function we can say what's the probability with a particular ability of seeing this observation so you know we're passing in this observation and we have a function which says what's the probability observation given ability I really want you guys to think about what this for Loop is doing this for Loop is going over your entire assignments of little a so before you have this belief right this for Loop is saying okay what if a equals 0.0 you can look up the prior and you can say How likely is the observation given this value of little a and then we do 0.01 and we go through the next iteration of the for Loop you look up the prior belief of that it's just this value and then you can figure out what's the probability of this observation given a equals 0.01 and the for loop we're going to try every single value there so if we call this probability of observation in the Bayes theorem we have the prior belief I'm going to call this prior belief and these are actually I like the little eye because actually I'm going to call them little A's because I feel like that's a little bit nicer for you guys so we're going to Loop over all the assignments to little a and then now we need to say what is our posterior belief of somebody's ability to see being equal to that little a I want you guys to think about what goes in here because there's only two lines that you would really need to make this whole probabilistic thing work but you need to get those two orions perfect and so you need to go deep into your theory so in this posterior where is this coming from think about it with the person next to you I'll start writing on the board and then we can code it up okay is a way that's true okay if we were going back to our world of hearing or elephant weights you don't need a for Loop because the values of the random variable you're thinking about can only take on the value of zero or one you only need to calculate one of those so we could do Bayes theorem without a for Loop but now we're inferring our belief about a random variable called a and a can take on a hundred values 0 0.01 0.02 and so if you want to infer a you can't just use Bayes theorem once instead we're going to have a for Loop and we're going to think about for every assignment to a what is our updated belief that's what this for Loop is so we're not just going to do base theorem once we're going to do base theorem thinking about a taking on the value of 0.0 0.01 0.02 0.03 so this overall for Loop is going to Loop over all the values of little a so when I say ability a that means you know little a we have this whole thing is going to calculate the belief of capital A and we're going to for Loop over all possible values little a for every possible value little a we have the prior you can look up the graph before that is your belief before you saw this observation because don't forget we're updating our belief in capital A for an observation we know if you tell me the value of little a we can say How likely is the observation like we're given this term the likelihood of the observation given a equals little a and then Bayes theorem says you just multiply these two together you take your prior and multiply by the probably the observation given your prior and then you solve for this I'm going to do the cheekiest thing I'm going to pretend that this doesn't even exist I'm just be like I don't even see you huh like what normalization constant and I'm just going to do this numerator term and then we'll get to the wrong answer but a helpful wrong answer so I'm going to run this python this is our starter code and starter code and this looks exactly right it's not right if you were very careful and you calculated the sum of all these probabilities they won't sum to one and if this is a proper belief then either the area under this curve however you like think about area under this curve should be one and basically this looks right but if you were very careful the y-axis is wrong and it's because we didn't think about this term but the crazy thing is this term is the same for every assignment of ability little a like when ability equals 0.01 these terms change a lot but this term stays the same when ability is 0.02 these terms change a lot but this one stays the same when ability is 0.03 you guys get where I'm going these ones change a lot but how about this one stays the same you can see why people think about this as the normalization constant like it's like everything is multiplied by some unknown number if I knew that number then I would know this term but I'd also be able to calculate my updated belief but we're going to leverage the fact that we know that all these things should integrate to one the area under the curve should be one so if you sum up this Value Plus this Value Plus this Value Plus this value it should give you one there's a few ways you can do this but I chose to write a normalized function basically I Loop over the whole dictionary and I calculate the total and that total will be something different than one but I know if I divide every belief by this total then it turns out that normalizes your whole histogram so the whole thing must integrate to one you can think about how you would like to do this there's lots of correct ways to do it but at the end of the day you just need to write a function so that you take all these terms you take all of these terms in this dictionary and you scale them by some number some magical number so that if you add up all the terms it gives you one and I called mine normalize and and now we have it and you can take one belief you can add in another belief so this is they got a pretty large number wrong but then they got a bigger letter right and then you know you can think about those two letters in a row and like now we think that actually they don't see that well we are most likely a belief if you ask me for one number would be 0.3 but do you think Chris likes to be asked for one number no Chris doesn't like to represent his one number Christ likes to give you the whole probability of distribution um and there's you know and one of the nice things about that is that whole probability distribution makes it easier for me to update let's say they get um what if they got a really tiny letter and they got it right that's so weird like they got two they took a large letter and they got it wrong they took a larger letter and they got it right and then they looked at a pretty tie letter and got it right whoa can it handle that absolutely after this is like okay I have this pretty wide distribution but now I'm starting to think that you can actually see pretty well again and you can put in any combination of beliefs and you can update your belief what a good way to estimate somebody's ability yes so if you run this in with real humans after you've seen about 20 questions your probably distribution looks like this and you're like I'm pretty sure except if I don't know your person is like super drunk and they're like not paying attention like sometimes they're guessing sometimes they're slipping and after 20 questions if they've been all over the place uh you may end up with some more variants uh and that will depend a lot on the person but most people pretty low variants after 20 questions this is not that important but there's one thing I haven't told you like I've told you everything about how we update our belief about people's ability to see what I haven't told you about is how do you choose the next letter I'm going to tell you the answer in about two weeks I'll explain the deeper Theory to why this is interesting if you take any current belief and ability to see there's another reason why I like ability to C represented as a belief let's say that's my belief and ability to see it's not one number it's a whole probability distribution if I want to choose the next font size you know what I do I do this thing called optimistic sampling I take one sample from ability to C I pull a sample from this distribution and then I ask what's the perfect font size for that sample it balances out exploration of whether or not the person can see and taking advantage of what I really believe they can see and it turns out to be a really nice optimistic algorithm for coming up with the next font size and that turns out that other part of the problem is really important to you but it has a nice simple solution that nice Simple Solution though requires you not to express your belief as a number but express your belief as a whole probably distribution which is what everyone in the world should be doing we should I'll be talking about our beliefs somebody says like oh what's the like when do you think you're gonna get home I'm like okay wife here's my whole chart of all the times I can get home and my belief distribution like that's how I'd like to express everything if I could uh but that's not very good with language okay um I actually want to let us go a little bit early but how cool is that you guys have worked so hard to like build fundamentals and practice your base there I mean think about your base name with random variables and now random variables okay taking on lots of values but it leads to this ability to write programs that a lot of people seem to not be able to write which is such a cool thing have a fantastic day come back on Wednesday we'll continue this great conversation thank you guys so much for working so hard foreign