good afternoon cs19 how are you guys doing today fantastic that's what I loved here um it's Friday so as you guys are coming in I'll tell you a quick story I don't believe I've told you the story of how my grandparents ended up in the US okay let me tell you it's an interesting story this is on my mom's side so my grandmother uh was living in a country called Belgium and this is the story of how she ended up in New York which is where she met my grandfather so she was living in Belgium uh in a city called Brussels and she was living there uh just before World War II broke out and she was about 17 years old when she was living there and we were that side my family was Jewish their Jewish living in Belgium and as you can imagine that was a bad time to be in Europe and at some point her father had to do some business in Germany and he went there and he was like oh this is about to get very very very serious so as soon as it got back to Belgium he started selling all his things and he didn't know how to drive but they purchased a car and then when the Germans invaded that part of Europe they just left they're like they're a couple days ahead of what we now call the Blitzkrieg and they made all the way to France and when I got to France they're technically Dutch citizens so the queen of Holland had set up five boats and they got on one of those boats to go to the UK four of which were sunk by U-boats and they make it to London just in time for the London Blitz and then they managed to survive that too and then get on a boat that takes them all the way to New York where she meets my grandfather and then they fall in love and like that is an important part of my story another bit of fruit of the random tree uh speaking of Randomness today we are going to be learning about adding random variables which sounds a little bit dry but we're in this part of cs109 where we're learning about deep Theory and this is a one of the introductions into thinking about one of the most beautiful results in all of probability history and if we play our cards right today we will get to see that most gorgeous result in the world of theory uh in probability so that's what we're going to be talking about before we jump in though I did want to tell you guys about one of my favorite parts of cs109 I'll be putting more details about this up after class but I wanted to announce it for the first time today as you may or may not know one of my favorite things to do in cs109 is to host what I call a personal Challenge and a personal challenge goes like this there are so many ways that you could show me that you understand the material in the class midterm is one of them but for some people exam's not the best way and on the other hand one of my favorite things while learning is to give people space to be curious to be creative and to express themselves in the language of what they've just learned to that extent we Host this personal challenge you get to do anything of Interest in the in the context of what we've learned in class so maybe you thought what we learned in class was really relevant to something in your life and you want to create something and explore some topic more deeply you can do a personal challenge you'll submit um a short write-up and or a little YouTube video and then we'll give you some extra credit now this is optional in the deepest sense of the word optional let me give you my guarantee if you don't participate in this optional personal challenge your grade walking out of cs109 will be the same as if this challenge never existed so this is not like or extra credit which is going to change the curve it's extra credit that makes it optional in the truest sense of the word optional but if you do this and you're able to show that you understand this material and maybe you show that you understand the material perhaps even better than an exam uh then we'll be giving you extra credit too that will help your grade does that make sense there's a guarantee of optional in the true sense the word optional I hate when an extra credit and you're like I kind of have to do it don't I and this is not one of those but I do hope you participate because I think it's just a beautiful way to explore the material more deeply in a way that's very personal to you and that can make it relevant to your life I did want to show you guys some examples um here's one example of what somebody made to do a few years back somebody for their project made the no humans allowed club and the way this works is you have to enter 50 random numbers and it tells you if you're a computer or a human so I'm going to start entering some numbers oh I don't know I'm getting there it's definitely gonna think I'm a robot because these are completely random and it says I figured out that you are in fact a human like the reverse captcha we thought that was quite nice and showed some nice understanding the material um one person was really interested in the topic that we taught or we're going to teach today in class and they wanted to understand something that I said quickly I said you know we're going to use this result but we're not going to prove it in cs109 she really wanted to stand the proof so she went deep and understood the proof and then a YouTube video to teach other people the proof and we thought that this showed incredible understanding of a Nuance of cs109 and you guys remember um we talked about flipping Frisbees and how a student changed or showed how the way people start ultimate freezing games is unfair she then came up with another algorithm she also modeled these as beta distributions and once in the field did a lot of experiments and gave probability distributions over How likely a frisbee is to actually be a heads all good things so there's so many ways to combine what you find interesting with probability uh this is not supposed to be a representative sample instead it's just supposed to spark some creativity and excitement in you guys so there you have it the optional challenge personal challenge you're not competing against other people um though you're not competing against people the Tas will take a look and we always will find some that just make us so excited we give some sort of really nice reward to them okay any questions about logistics before we jump into things we haven't graded midterms that will happen this weekend so you'll hear more about that on Monday um questions concerns thoughts okay let's start out with a new definition so on Wednesday's class we learned about beta distributions and those were very beautiful but today we're going to shift gears into just thinking about what happens when we add random variables but we need to start with a new piece of terminology that you might have seen before but in case you hadn't I want you to know it live it learn it love it because we'll see it a lot in the rest of cs109 i i d IID stands for independent and identically distributed and you'll start seeing this a lot it's very common in the world of data science and machine learning you'll often say I have a bunch of random variables and then you'll see a claim and these random variables are IID it's a stronger claim than just saying they're independent IID includes that they're independent but it also means that they're identically distributed identically distributed is just terminology for saying that if they're discrete all their probably Mass functions are the same if they're continuous all they're probably density functions are the same as such because if all these things are the same you must have the same expectation you must have the same variance does this mean that all the random variables are the same no they're not going to be the same number they're just going to be pulled from the same distribution let's take this out for a spin I'm going to give you a few different contexts and I want you to think about whether or not they're IID so first one we're going to say there's n random variables X1 through xn x i is an exponential with a particular Lambda and I'm going to then claim that all X i's are independent so are these independent and identically distributed okay why don't you look at this list we've got the first one that says you know X i's are all exponentials and they're all independent the second one the only difference is that there's a little subscript I which means that each x i has its own Lambda and then the third one we say that X1 is equal to X2 is equal to X N and the last one we say that x i all come from binomials each with different number of Trials but the same probability p and then we claim that they're all independent so talk about what the person next to you a little bit warm up for cs109 on a Friday which of these are IID independent and identically distributed okay go talk have a good little chat and then we'll talk about this all together okay yes okay we're gonna do a Ruckus vote by loudness there's no wrong answer here uh you know this is just practice okay we're gonna yell the first one IID or not IID let's say yes for IID no for not I let's just yell yeah okay fantastic the first one is IID and this makes sense because are they independent well we've said they're all independent and are they identically disputed yeah they're all have the exact same probability density function because they're all exponentials with the same Lambda thus if you were to plot all their probably density functions they would have the same curve they won't be the same values but they have the same distribution so I ID how about this one a little cheeky no why not it looks so independent and it looks so identically distributed the PDFs are different why are the PDFs different they're all exponentials oh yeah if you have different lambdas you'll have different PDFs that's actually a different distribution so even it's the same distribution family because they have different lambdas it won't be the exact same curve so since the Lambda eyes assumed are not equal these are not identically distributed okay how about the third one feels interesting right they're definitely identically distributed but are they independent independent would mean knowing one gives you no information about the other ones and if I tell you one you now know complete information about the other one so in fact these don't fit that new terminology definition and this is a little bit cheeky but how about this last one let's take it on home no yeah because even though they have the same probability Mass function to be the same and because n is different they will in fact be different distributions um unless of course all the NIS happen to be equal okay so then I leave us with this first question to start our day we're going into this theoretical part of cs109 and we've talked about random variables we've talked about conditional beliefs of random variables conditioned on observing things but one thing that seems like it'd be interesting to do is thinking about what happens when we apply mathematical functions to random variables and the simplest one to start out with is addition what happens if you take one random variable and add another random variable recall that each random variable is going to take on a value so if I say something like you know X Plus y that's what I mean why oh that's X Plus 5. if I do X Plus y That's what I mean when I talk about adding random variables no matter what we imagine these to be numbers that take on values probabilistically so it'll be this X's number plus this wise number but the result won't be deterministic the result of adding these two numbers will depend on what value X takes and what value y takes and I do want to explain a little bit why you should care and there's a whole bunch of reasons why we can just care about addition for example at some point we talked about zero-sum games we talked about the probability that the Warriors would win and we ended up with in the world of this thing called an ELO rating and a zero-sum game is when two people are playing against each other and one person's win is the other person's loss in the world of zero-sum games we talked about how it could be represented as uh uh ELO ratings which means there's like a distribution for warriors ability and a distribution for the opponent's ability the Warriors will win if their ability was greater than their opponent's ability forgetting all the context and just leaving ourselves with this we've got what's the probability that one random variable was greater than another random variable when we hit this in class earlier we punted we said we don't have the mathematical sophistication to solve the probably that one random variable is greater than another so instead we just solved it by sampling we would just sample different pairs from the two random variables and then we would estimate the probability that one was greater than the other that's not ideal now if I knew how to add random variables one of the ways that we could think about this is we could imagine rearranging this so it's addition it would be nice if we could solve problems like this so that's our motivation we'd like to be able to add and adding will allow us to directly solve problems but then I gave you this hint learning this piece of theory is an entryway into a magical world where you're going to see some incredible math so this will be the first step on a bigger more beautiful Journey so hopefully now you understand what we want to talk about today we want to just talk about adding random variables and hopefully now you're a little bit curious It's like yeah I want to know how to random variables I want to go to magical journey with all my friends in cs109 well then come on in let's go learn a little bit of review because we will build on some older Concepts we've talked about remember at the very beginning when you're learning about probability we talked about what's the probability of sum of two dice being seven and in this wonderful space of equally likely outcomes you could just count you can think about all the outcomes and you can think about here's all the outcomes where there are two random variables added up to seven on some level this is a precursor to thinking about adding random variables and I just want to remind you of this example uh before we jump into the more General case of adding random variables and the key thing to recall is that you know if you think about all your random variables adding up to seven you kind of make a grid of all the outcomes and the cases where they add up to seven would be discrete selection amongst this grid you could say six and one adds a seven five and two adds a seven four and three as seven and like luckily because these are equally likely we could just then do some Counting okay and that's all we've got for review now we just talked about the sum of two dice and we know what happens when you add two dice um and we know the probability that it adds to seven we could also figure out what's the probability that it adds to other things so if we define y to be a random variable which is a sum of two dice we now know how to answer the question you know what's the probability that y equals seven we could also answer the probability that y equals something else like six and if we really wanted to we could solve this generally and we could figure out the probability that y equals I and this is special if you can tell me for any value of I the probability that y equals I then you understand this random variable completely so assuming that all of your dice are IID which they are two dice are independent and they have the same distribution so we have these two IID dice we're adding them together and I want to reason about the sum and we could bump this up a notch though like we've thought about two dice before in class but we could easily make this three dice we could say you know let's sum up three Dice and see what the sum is I wanted to show you what the random variables you would get if you thought about adding one dice then two dice then three dice this is ridiculous this is adding one dice you roll a dice and you add it to itself and you're left with this probably distribution over your values you know you can get anything from one to six and the probabilities are the same for all of those how about the sum of two dice well we've seen this before you know the probability of the sum of two dice um the chance of getting two as a sum if you roll two dice is just one over thirty six chance of three is two over thirty six and you know we can work this out using counting uh in the same principles we had before are you curious what the sum of three dice looked like so I've never showed you but you guys want to see oh look at that oh I saw something I heard somebody go um why is that kind of interesting it looks kind of normal yeah it looks kind of interesting well that might be just a coincidence uh but who knows and what happens if you sum 50 dice oh that'd be really interesting to think about now uh actually do you guys want to see the summer 50 dice oh let's go find it code um some of rolls okay Now quickly I'm gonna 10 100 000 times I'm going to run an experiment and in this experiment 50 times I'm going to roll a dice I'm going to come up with a total sum for 50 Dice and that will be returned from run experiment getting a random dice will either give me one two three four five or six with equally likely outcomes uh and then when I'm done I'm going to plot it just using a histogram sound fun did you do uh CD sum of variables python sum of rolls here we go oh tqdm is so cute um yeah isn't that quite interesting huh okay well we'll just we'll just leave that and think about it anyways but the point isn't we haven't gotten to the theory yet but you guys understand the problem we're thinking about adding up random variables and of course you have to understand the problem before we get into the theory and we can talk about adding two random variables we can talk about adding three random variables we could add up think about adding 50 random variables and so far actually one of the few insights I want you to have is when you add random variables the result is another random variable it is another value which will take on different numbers probabilistically okay I have a thought experiment and it will get us towards the theory of how you can think about adding two random variables in general here's the thought experiment imagine there's two people and they're playing a game where they each get points they'll get points between 0 and 100 and importantly this is not an interesting game like basketball it's a game where they like all go off on their own and independently get points so my points is unrelated to my opponent's points but there's a hundred points and the interesting thing is let's say I know the probability that player one gets zero points I know probably that player one gets two points three points Four Points any value of little X between 0 and 100. and I know the same thing for y they could be two different probability distributions but I have the probably belief of X getting on points and probably belief of why getting different numbers of points and then I ask you the question what's the probability that these two people tie now let's just review their scores are independent and I for any score I can tell you the probably that X gets that score and then probably that y gets that score so hopefully this thought experiment is something you can think through but I'm going to give you a minute just to think about it with your friend next to you so how are you gonna get this probability nine foreign okay now this doesn't seem that important but if you can get this logic it's halfway towards getting the logic of of what happens when you add two random variables so this isn't about adding random variables this is talking about the probably that two random variables take on the same value and importantly these random variables are independent so if this is player X's score and this is player y score what's the probability that they tie and you can think about all these different mutually exclusive cases for example there's the case that they tie and they both got 80 points there's also the case that they tied and they both got 79 points there's a case that they timed they both got 81 points and if you think about all the little values I which are the points that they both get since all these cases are mutually exclusive you should be able to just add them so my claim is that the probability that these two people tie is the sum over all the values of points that they could get the probably that X gets exactly those points and Y gets exactly those points so this is going to be mutually exclusive cases so we can add them you know you can't both have x equals five and also have x equals six Mutual exclusive so you can add them all and this is in this world they both have eye points therefore they tie this just gets a little bit nicer in that because they're independent we don't have to think about these things jointly we can think about the probably that x equals I times the probability that y equals I guess question I would always assume ah you cannot always it's a good question you can't always do this um let me see if I give you an example let's say that any two things that are not independent I don't know if the first if the first thing is like you get a a one on a dice and the second thing is that the sum of the two dice is like 12. then is not the problem that you get at one of dice times the probability that the two dice sum to twelve even though there's non-zero probabilities of those things happening they can't both happen at the same time this property is only true if they're independent good results yes yeah exactly exactly exactly um so any case where they're not independent you can't do this you'd have to think about them jointly but if they are independent then product of and or probably of and becomes easy follow-up yeah is there a way for us to visualize this using the graphs like can we see like how much do the two distributions overlap and is that aside and how much they're going to be attacked um yeah you know what would be a little bit easier than if you looked up there probably Mass functions on their own would be if you looked up a joint table and if you look up a joint table you know if you look up zero zero let's say this is for x and this is for y this cell here is probably the x equals zero and Y equals zero and so what you're really going to have is this thing is going to be all the values in the diagonal sum together so you can see how maybe if you made it color coded and darkness means whether or not there's high probability you know you can visualize how much weight there is on that diagonal good question okay good yes what would phase look like in the first one so in the case where Define X Y we'd have to sort of convert it to an equation so x given y y given x what would that look like in the case that they weren't Independence if they're not independent then I would probably break hmm so is bae is going to help you I guess you could if you just had one of these terms you could use a lot of taller probability which has a bunch of conditionals in it but I think with both those I can't think about how Bayes would help that might be a limit of my own imagination because Bayes is helpful when there's a conditional if there's a conditional you can flip it well I've told probably is helpful if you want to like go from you know this joint to knowing one of these on its own um but here Independence is the most helpful thing let me think about this morning come talk to me afterwards because I might be missing something very very possible okay more ideas okay so as I said that's not the thing we wanted to get to that was just a warm-up the real question we want is we have two random variables and let's imagine they could be independent or they might not be and we want to think about the probability of those some of those random variables and we want to think about the whole probability Mass function of that sum and if you want to think about the whole probably Mass function of us any random variable you have to give the probability of that random variable taking on the value n so the random variable we're thinking about is the result of adding X to Y that gives a random variable and if you can tell me what's the probability it takes on N for any possible value n you have told me what happens when you add these two random variables sound good okay here is the idea we're going to do something very similar to what we did with the tie we're going to think about many different cases that lead to these two random variables taking on a value n here's one case if x takes on the value of 0 and Y takes on the value little n what's the sum of those two values yeah great um if x equals one and Y equals n minus one what's the sum of those two values and okay you guys are very good at this I bet you can get this next one what about 2 plus n minus 2. oh man all these are different cases where these two random variables add up to n and my claim is this is all of the cases my next claim is they're mutually exclusive and then my final claim is they're countable you know you could have an index starting at zero and that will be the value of x and it can go up till n and that will be the value of x and the value of y is always going to be whatever X's or n minus whatever X is so do you guys first believe my claim that every row here adds up to n now do you believe I claim that they're mutually exclusive mutually exclusive means you can't have two rows at the same time and that makes sense because X can't be one at the same time that it's two and then the final claim I make is that this covers every possible way that you could get n if they're discrete and I guess if they're taking on whole number values okay as such every single one of these we can talk about the probability that it happens this is the problem that x equals zero and Y equals n this is probably x equals one and Y equals n minus one this is probably the x equals two and Y equals n minus two and therefore adding two random variables leads to this very nice equation what's the problem that X Plus y equals n well it's Loop over all the different ways that they could add up to n there is x equals zero and Y equals n there's x equals one and Y equals n minus one this sum is basically just expressing the addition of all these mutually exclusive cases and this could be the end of your story if you're talking about the addition of two random variables that are dependent if they're dependent then you have to think about the joint probability together if they're independent though this isn't the end of the story if they're independent then this can end up being the sum from I equals 0 to n the probability that x equals I and then probably that y equals n minus I oh sorry if they're independent though this will become a product of probabilities so the probably the x equals I times the probability that y equals n minus I if they're independent you can set up this equation in this equation you only have to think about the probabilities of each random variable on its own independence makes things a lot easier you don't have to think about random variables jointly probabilistically you can think about them one at a time okay so just putting that into slides these are the two different rules for talking about adding random variables this is the general case where they might be dependent and this is the general case where your two random variables uh are independent now we're not going to go too deep into this in cs109 but I do want to give you the Insight that this will scale nicely if you got to something like a continuous random variable if you want to talk about the sum of two continuous random variables this sum will just become an integral and these will just end up being densities but we're going to stay in the discrete world for cs109 so in some way we could have used this to drive the graph for the sum of two coin dice now we did this using counting but we could have used this Rule and this rule says that probably that X Plus y equals n is uh well the sum over all the different values that X could take on and the value that y could take on that allows the two things to be equal to one now first of all the probability of any two assignment of right dice like x equals 1 and y equals two it's one over thirty six no matter what dice combination you have there's one outcome out of 36. and then the one thing that's going to change here is the bounds you know it must be the case that whatever X is assigned to has to be between 1 and 6. you can't say x is zero because it's a dice and dices can't be zero and the same thing with y y can only take on values one two three four five and six so this exact same equation if you have probabilities of one when these things are both values between one and six and zero otherwise Sorry probably one over 36 when these are both valid assignments to dice and zero otherwise you end up with this very beautiful chart this is just the plotting of that equation so at this point do you guys now have a general rule for adding two random variables no matter what two random variables are you could apply this thinking and actually you now know how to do this thing called convolution back in the day this lecture used to be called convolution do you know that convolution is just a really fancy probability way of saying adding two random variables it's like whoever talked about adding your two random variables for the first time couldn't just call it adding two random variables they had to call it convolutions they could feel really fancy but anyways if you see convolution you should just translate it into two things they're talking about adding random variables and they feel very fancy sometimes adding two random variables doesn't require you to go all the way to this origin story of adding two random verbs do you guys want to hear about some cases where adding two random variables is easy okay here's a case we're adding two random variables is very easy you have two different binomials they have different values of n but they have the same value of P and even without going to the origin story you can think about what is the sum of X and Y and do think about it meditate it on it for a little moment what happens if you take one binomial that's the number of successes in N1 trials you take another binomial which is the number of successes in N2 trials and you add up those number of successes what will the result be it won't be a constant it will be a random variable and sometimes we call that random variable Z and if you add up those two things what is z it's a binomial but what are the parameters of the binomial well Z is number of successes in a bunch of Trials every trial has probability P of success and there's N1 plus N2 trials and that comes from it semantically you know like X's number of successes in this many trials with probability P why is the number of successes in this main trials would probably be if you add those number of successes it's equivalent to if you ran N1 plus N2 trials each with the same probability P so if you add up two binomials with the same parameter P you're in a good place I wanted to choose High because you're going to divide it by something no no no just for suspense the question was did I write N1 plus n too high because I'm going to divide by something no that is just poor typography on my part does this feel better okay yes every sum of any variables is another main event yes do you want me to repeat that or yes that is the case if you take two random variables and add them together always a random variable because that result of the sum it's not deterministic it will vary there will be probabilities to the different values we'll get because there's probabilities to the thing that goes into the sum equation will still follow the same model not always not always and I'll give you a quick example uniforms you add two uniforms you don't get a uniform what do you get if you add two uniforms huh interesting mystery well we'll get there I promise but right now we're adding two binomials which is a good time because if you add up two binomials you just end up with another binomial not everything will be so beautiful thank you binomials for making our lives easy and the intuition is exactly what we've described you know X1 had N1 trials y had M2 trials so Z has N1 plus M2 trials all with the same probability of success now here's a cool one what if you tried to add up not binomials but normal distributions and there's this beautiful property of normal distributions If X and Y are two normal distributions with different means and different variances it turns out if you add X and Y together you will still get it normal now this is not always the case that if you had two random variables you'll get the results being the same distributional type and normals turn out to be exquisitely special because it doesn't matter about their parameters you can always add to your random normals and get another random normal back and not just any random normal if you add up two normals you'll get a normal back and its mean will be the sum of the means and its variance will be the sum of the variances you may ask for me to prove that and I think that is a very good question if you're wondering but I'd say hold on to that idea until we get to this more General result from adding random variables does that sound good okay you can take this to like Nth Degree instead of adding two normals you could add n normals and no matter how many normals you add it will lead to another normal and the mean will be the sum of the means and the variance will be the sum of the variances so that's a lot of fancy notation which says no matter how many normals you're adding together you still end up with a normal with very predictable meaning variance and I have a final one that's nice for you guys what if you have two poissons so poisson X happens at rate Lambda 1 and poisson y happens at rate Lambda 2. X and Y will represent number of successes at their respective rates in a fixed time interval if you add up those number of successes you still get a poisson and the rate of this poisson will just be the sum of the previous rates and that is something that we could prove uh and I will I think there is a proof for that in the course reader but I want to first take a step back because we've got three useful rules I just gave you and I want to practice using them so the ReUse four rules I gave you is the sum of two binomials the sum of two gaussians and the sum of two poissons particularly the summit two binomials they have the same parameter p the sum of two gaussians could be any parameter and the sum of two poissons could be any parameter and then I ask you this question and I want to see if you get this configured out say you're working with the who you're planning response to initial conditions of a virus there's two exposed groups and you want to know the total probability of more than 40 infections in group 1 50 people were exposed they each are independently infected with probably 0.1 in group two a hundred people are exposed and they're independently infected with a different probability in group two you add higher probability of becoming infected and that was 0.4 and we want to think about the probability of more than 40 infections and I'll get it started you can say the number of people infected from group one is a random variable you can say the number of people infected in group two is a random variable and what we're really interested is the sum of those two random variables but then there's a puzzle for you guys you know if you think about it this should be a binomial and this should be a binomial can you reason about the sum of these two binomials why do it in your heads when you could do it the person next to you so give it a shot think about it with the person next to you because this is a classic example of what a problem on a problem set could look like with the sum of two random variables um laughs yeah okay there's many paths to answers here there's one path to be avoided and then there's two happy paths that will get you the right answer so I know let's throw up some ideas well I got bored let's brainstorm s and that's one minus probability that one minus the something that x y equals I um and then as we know that formula can be for X Plus y equals n we can substitute that in but yeah foreign so one strategy is we have this formula we think the values of X1 which is the number of people infected in group one is independent of the number of people infected in group two there are two random variables but they're not influencing each other uh and we could use this fundamental definition of adding two random variables so we can think about the probability that X1 equals like if you want to say like what's the probability that the sum of two random variables is zero well that's probably that x one equals zero and X2 equals zero if you want to know the probability the sum of two random variables is five then we have to think about looping from X one equals zero up to five and X2 is equal to whatever the complement is this first principle approach will 100 work but there's another way and there's a question or an idea well I just thought that we could convert the binomial we could approximate it to a poisson and like Lambda is PN so um in the first case where every 50 people it would be Lambda would be five for 50 people and the second case it would be like 4 400 people so basically you convert the first case into 100 people as well yes let's just add the two questions so many things that I absolutely love and I want to deconstruct first of all you said approximate the binomials and there's something very special about that Insight before you even get there you might think hey don't we have a rule for adding two binomials we've already talked about what happens if you add two binomials you get this thing I said adding two binomials is easy you just add up the number of experiments but why did you want to approximate why didn't you want to just use this equation because p is different ah p is different this simple equation I gave you for binomials only works if p is the same therefore you thought I'm going to use approximation and then you thought I'm gonna use the poisson approximation everything you said after I'm going to use a process on approximation was correct right all those ideas for how you would get lambdas but the only problem is poisson is meant for extremes extremely large and an extremely small probabilities of p and I could argue that this is Extreme but since probably 0.4 isn't that extreme your poisson approximation here will be okay and you'll get a pretty good answer but it won't be that tight in approximation but let's plus that answer let's take that answer and think okay poisson is not extreme this is an extreme enough for poissa who can play if we want to play the approximation game yeah binomials can be approximated by a poisson or normal so you could say that I will get the approximate normal for this one I'll get the approximately normal for this one and normal approximations are happy you know when everything looks moderate the variance check that we gave you works out for both of these numbers and it turns out a normal approximation does work now both of these were good paths in that there's an easy way to add up poissons and there's an easy way to add up normals so if you can turn your binomials into one of these worlds we can think about convolution Yeah question do I have another approach oh yeah fantastic okay I was thinking of doing like almost like a summation that is as we came back to the table that we had earlier yeah with all the N minus I think uh-huh and then you have like you start by like n is equal to 40. and then you go n is equal to 41 and is equal to 42 so you sum through and it's equal to 40 and then you add to that the sun through n is equal to what you want so somewhere in here there's the line and it'll actually be a zigzag because they're discreet and at this line if you take y's value and X's value it sums to 40. this is like sum to 40 line uh and your Insight is if you want to know the probability of more than 40 infections then you can just add up all the probability Mass over here and you can figure out like what's probably of every single Grid in this and that that would also work okay I love it I love it these are many good approaches to solving this problem now the first question I just have is should we use a binomial sum of random variable shortcut and hopefully at this point you're like no that binomial sum of random variable shortcut was only when P was the same yeah how do uh how do we see that it's easy to add in personas this seems non-intuitive you could actually work it through so poisson is one where we can actually go to this first principle definition and if you say they're independent poissons and you think about these probabilities if you put in the probability Mass function you will be able to factor it into a new poisson that's a cool question it is unintuitive why don't I show you the proof okay so is that fact like true more deeply because croissants are like emerged from poisson processes there's two ways to think about it mathematically if you did that first principle thing you end up with another fossil and the other way to get it is to think yeah poisson processing is mean these independencies and also like there's this convolved rate then you can get into your head about it but I believe the math when I saw it um okay if you want to go down the path of the normal approximation this is what it would look like instead of just having a which is the true binomial in B which is the true binomial for group two I had approximating normals I say x is my approximate normal and you can figure out what that is turns out mean is 5 and variance is 4.5 and Y can be approximating normal for the second group mean is 40. variance is 24. the probably that you know the sum is greater than 40 is the probability it was very similar to the probability that the sum of your normals is greater than 39.5 39.5 what happened to that half a person yeah sorry pal continue correction you're out of here it's not actually half person missing that's just because we went from a discrete and we approximate with A continuous and whenever you do that you have to think about half a unit um and which direction you have to include so this is very similar to the question what's this probably that some of these two normals greater than this value if I call my sum of two normals w that W will have a known distribution you just add up the means of the two normals you summed and you add up the variances and you get a new probability distribution W which is in fact a normal and if you want to ask the probability that W is greater than 39.5 that's asking what's the probability that this normal is greater than 39.5 I switched from W to X how embarrassing but anyways it's 1 minus the CDF at 39.5 and you can just plug that in either through the five formula or you could use the scipycdf cool yes um so how do you get the five and four point five like how do we get the parameters of the normal from just the Lambda like so so from the nnp values that we have with the binomial it's a very good question okay and I'm going to switch these to W's how did you get five here the idea is if this is a binomial and this is the approximating normal you should choose the mean here which is the same as the mean of this binomial what's the mean of a binomial well that's its expectation which is if you look up the formula n times p and 50 times point one is five what's the variance of a binomial if n times P times 1 minus P which will be 5 times 0.9 which is 4.5 so that's how you figure out the matching uh values for mean and variance if you're doing your approximating binomial okay now I showed this to some students and people went away happy and then one student came back very unhappy he says Chris there's a contradiction two things you've told me in cs109 and I cannot sleep at night can we talk about it and he posed this question to me he said Imagine X is just some normal and I'm going to add X Plus X there's two ways of thinking about X Plus X you could think about it as the adding of two random variables or you can think of it as x times two you know whatever x value is I'm going to multiply by 2. earlier in class you said if Y is a linear transform of a normal you'll end up with another normal and I gave a way of talking about what that other normal will be particularly I said if Y is equal to a linear transform of normal take that normal take the constant and multiply the mean by the constant and then the variance will be multiplied by that constant squared so I said this earlier in class but now I just told you how to think about the sum of two gaussians I said you know if Y is equal to X Plus X you can think of this as the sum of two gaussians and I just told you if you add two gaussians you'll get another gaussian well that doesn't seem to contradict yet and the mean will be the sum of the means and the variance will be the sum of the variance and you're left with this it looks so similar through these two different approaches both of them lead you to conclude that the sum of these two gaussians is normal and both of them lead you to conclude that the mean of this sum will be two times whatever the mean of X was but they have different answers for the variance and the student couldn't sleep and came as like there's something wrong in the world of mathematics this is like does this go deep is there something wrong with addition two are you lying to us and the answer is no there's just one really interesting subtle Point here and it's worth talking about because it's an assumption that we've made foreign [Music] uh well are they here x and X are they independent no that's it now X is not independent of the same random variable X if you see X Plus X and there's the same random variable what this implies is like roll your dice for X get its value and then add that exact same value to itself so if like X turns out to be four then this is also four you end up with four plus four x might turn out to be five then you get five plus five but no matter what this and this value are the same that's why we could write it as equal to 2 times x but therefore if I know the value of x does that tell me anything about the value of x yeah they're not just missing Independence they're completely dependent um and the sum of two gaussians rule that I told you assumed Independence of the gaussians the sum of two poissons that I told you assumed Independence and the sum of two binomials I told you assumed Independence okay cool moving on so anyways so now we can all sleep at night and there is a bug here which is that X is not independent of X very good okay let's go back to zero sum games so I promise that we'd be able to think about uh the probability that the Warriors win in a zero-sun game we've already talked about how this can be represented as ELO scores you can think about every team as having an ability and is drawn from a normal distribution the mean is like how good the team is and the variance depends on the sport in the original ELO paper and they set basketball to be 200 squared the idea is that both teams when they play they're going to sample from their ability of distribution and whoever has the higher sample ability wins this is the theory of Elo it's not actually how basketball Works they actually play a game with the ball and two Nets and then we ask the question what's the problem that the Warriors win and we've talked about how this is before is what we think about the Warriors ability random variable and the opponent's ability random variable and this is really asking what's the problem the Warriors pull a value that's greater than their opponents and to give you plots for what this could look like the Warriors have a random variable it could look like this normal the opponents have a random variable could look like this normal we're asking what's the probability that this random variable is larger than this other one my claim for you is that this is an addition question asking if one random variable is larger than another is the same as addition and you're like what that doesn't sound like Edition sounds like inequality and just to be clear you know like if we say that this random what's the chance of this random variable is larger than this one it says this random variable will take on a value let's say it's this and this random variable is going to take on value and it could be this one but that's very unlikely more likely it could be something like here and we can think about what's the probability that this value is greater than that one and I claim it's addition insanity okay here is why it's addition I give you your two normals for your two random variables I asked what's the probably one random variable is greater than the other and then my next claim is this is the same as if you were subtracting one random variable from both sides you get what's the probability that the difference of these two random variables is greater than zero so hopefully at this point you believe that the probability of one random variable being greater than the other one is actually a subtraction question it's probably that the subtraction of two random variables is greater than zero but if you recall if you take a normal distribution and you do a linear transform like you take that random variable and multiply by say negative one you'll again get a normal that negative one will be multiplied by the mean and the variance will be multiplied by negative one squared so negative of the important stability will be this gaussian and if you add up this gaussian and this gaussian which we just derived the sum because they're independent will be another gaussian and we can get the probability of the difference between aw and a0 as a random variable you'll add up the means the mean of aw is 1797 and the mean of negative AO is negative one five five five we add those two numbers together you get 242. you add up the variances and both of them have variants of 200 squared so adding up the variance will give you 2 times 200 squared and now you can ask what's the problem that this difference is greater than zero it'll just be equal to 1 minus the CDF and that's something you could look up and you can end up with the probability of 0.804 before we had to sample to get this answer and now we don't need to okay okay the convolution will not be televised okay now I want to mention quickly that it's not always so Rosy we've talked about binomials some was so nice if they had the same P we talked about normal if they're independent the sum is so nice and poissa they're independent the sum is so nice anybody like sums are always going to be so beautiful and I just want to for a moment look at the scary world of how this can all fall apart uniform so uh just really quickly I hope you believe me but you know if you want to talk about the sum of two discrete random variables we have this formula so the probability that sum comes to a is just going to be a minus uh or the probability that um you know one of the random variables equals whatever you're looping over in the probably the other random variable equals your target minus the index and there's a continuous version too and then the continuous version It's the exact same thing it's just become a sum becomes an integral and probability Mass function becomes probably density so what happens if you add our good friend a uniform between zero one with our other good friend a uniform between zero and one just to be clear these are the two uniforms that we're trying to add together so we're trying to add two uniforms they both have probability density that are equal to one in the range zero to one and if you think about it you could ask the question If X is uniform and why is uniform and particularly we're going to say their uniform with zero one what's the probability that X Plus Y and this should be density what's the probability that X Plus y is equal to some number and we could say something like 0.2 to start out with of course we're going to want to think about this in all the different cases but what is the probability that the X Plus Y is equal to 0.2 yeah no it could happen oh well the probably density probability is zero but the probability density is going to be non-zero um and the claim the The Continuous equivalent of this is that it's going to be the integral over some Loop and we'll think about as bounds in a second and this will be the likelihood that X equals I and Y equals 0.2 minus I so we're going to integrate will be like what's the chance that x equals you know 0.1 and then y also equals 0.1 because then they would sum up to 2. now the nice thing is both of these values are ones you know if you look at the the PDF value of both of our uniforms it's ones so it feels like we have the integral of one the tricky thing is to think of what are the bounds on I such that these two things don't give you zero because if you ask what's the probability that your uniform is less than zero it's zero and you're probably that your uniform is greater than one that's also zero so what bounds are going to make both of these non-zero well I'll tell you what from this one we know that I has to be greater than zero and less than one because if I is between those two values this is just one otherwise it's zero and zero multiplied by zero is zero so I already know that I has to go from zero to one for this value but how about this one this is a super tricky bound work but if I takes on the value 1. what is 0.2 minus one negative 0.8 it's a negative number and what's the density of our uniform at negative 0.8 it's like asking what's the density over here ah it turns out if I gets too large this thing will exit the range of being in the CDF of Y one of our uniforms so the bound over here what's the biggest value that I can be such that this doesn't exit the range of being one yeah if it ever becomes greater than 0.2 then this will become a negative number and you'll be get a density of zero if it's between 0 and 0.2 then this will be a number that is positive and less than one and this dense will be one so in this range from I equals 0 to 0.2 you end up with one times one so this will be equal to this density you know d i in this case uh and if you did this integral it would just end up being 0.2 and this is looking pretty good you're like oh that doesn't feel so bad if you want to think about this you know you could think about different points we just thought about point two and we found out that the density that the sum of the two uniforms is equal to 0.2 is equal to 0.2 and you can try a bunch of these points if you did it you'd realize that the answer is piecewise in fact if you're thinking about the in sum of two uniforms being up to one it just linearly increases as we asked what's the probably the sum of two uniforms is one half it's one half well it's probably the sum of two uniforms is three quarters it turns out to be three quarters for a very similar argument to here but then after you get to one then your bounds work changes and it starts to go down now you could do all that work piece by piece it just turns out that the uniform if you add two uniforms you certainly don't get a uniform instead you get this weird triangle thing so the sum of two random variables isn't always a clean answer and uniform really drives that point home the other thing to note is you know if you want to think about the sum of two continuous random variables it'll be just what we talked about before just with integrals and densities now I want to point out one cute thing remember we talked about the sum of two dice and dice are quite similar to uniforms and that they're equal probability of every outcome and you got the sum of two dice looks like this triangle and now we have the sum of two uniforms looks like that triangle is there a conspiracy going on question yeah if you had not zero to one you would end up with different bounds work over here and you get you know different shape triangles still a triangle though suspicious this is where they hide the treasure no okay um but then you might wonder what happens if you add up 100 uniforms and the beauty is we can do this we've got computers if we want to not just think about the sum of two uniforms what if we wanted to think about the sum of 100 uniforms if I wanted to do this um what I would probably do is I would set up this code and in this code I would run 10 000 experiments every 10 000 exp every experiment though I'm going to sum up a hundred uniforms I'm going to get a list of all those sums and then I'll plot the histogram does that sound like a cool thing to do so every experiment I do exactly this I do convolution of 100 uniforms so we know that the sum of one uniform is just this so the sum of one uniform is just that we know the sum of two uniforms is just that what is the sum of a hundred uniforms going to be oh I don't know but we can be experimentalists and we can go to sum of variables and we can do python 3.9 sum many UNIS here comes the suspense it's going it's going it's going to do this ten thousand times tqdm is so nice it does this like really cute bar check it out it's not that important right now what is important is getting really close to seeing your answer and your hypothesis oh wow it was pretty skinny but when we readjust it what does that look like what maybe there is a deeper mystery maybe there is a conspiracy all along because we added up Dice and we got a gaussian and then we added up uniforms and we got gaussian and now you're thinking there's something special about Dice and uniforms that gives you gaussian and at this point you're like I've gone crazy but I'm still gonna do it what happens if I add up a lot of beta distributions so we learned about beta distributions yesterday you know they don't look like uniforms at all and said they look like these little uh little bumps that represents distributions of probabilities that's not important right now I'm going to think about adding 100 betas together IID betas so they all have the same distribution and they're all independent I'm going to add 100 to them I'm going to do it 10 000 times and we're going to find out what happens if you add up betas you guys ready there it goes what do you think what you imagine happens if you add 100 betas beta is a weird thing to think about what's convolution is going to be I don't know maybe it's going to look like another beta yeah it could be another beta something is weird in the world of probability oh it turns out if you did 100 plus songs the same thing happens like trust me if I were to do the same experiment replace the beta with the poisson again you would see something that looks like a gaussian now let's have our moment of silence before we present one of the most beautiful results in an entire history of probability Theory a result that's profoundly important as it is profoundly beautiful foreign this is not a coincidence there is something beautiful and magical in our world and this beautiful magical thing in art world is no matter what random variables you have if they're IID if you add up enough of them they always converge to a normal distribution betas uniforms poissons just your friends random variable that they drew on a graph if you draw enough samples from that random variable in the graph and you add them up the resulting random variable will be a gaussian and not just any gaussian if you add up IID random variables no matter what they are they could be gaussians betas your friend's distribution you'll get a normal and the normal will have a mean which is equal to the mean of the underlying random variable of drawing from Times by how many experiments you ran so if you ran you added up 100 uniforms this will be a hundred and this will be the mean of your uniform and the variance will be a hundred times the variance of the uniform or replace uniform with whichever distribution it came from IID is important so the sum of niid random variables is normally distributed with a calculatable mean and variance and it's beautiful and it's gorgeous and I could talk about how beautiful and gorgeous this is in so many different ways one way to talk about is it explains why we saw this you know some of one dice roll uniform sum of two dice rolls starts to look like a triangle but you sum three dice rolls and already it's starting to look like a normal distribution we can go deeper into the proof in fact I will share that video that one one of the personal challenges back in the day and that goes over the proof for why this is true but to give you an intuition for why it's true when you add two dice world together why is there more weights in the middle because there is more ways to get a seven than there were to get it two and when you add three dice rolls together you know the same thing happens there's just way more ways to get something close to the average value on a dice times three than there are to get things that are in the extremes so that starts to give you some intuition but I'll give you that YouTube video If people want to go a little bit deeper it explains so many things it explains why when you have a binomial and you look at its distribution it looks like a gaussian that wasn't a coincidence that wasn't a hack it comes straight from the central limit theorem why does a binomial always look like a gaussian because a binomial under its hood is the sum of independent random variables you're like which independent random variables I thought a binomial was just a binomial no no a binomial is so much more if I say you have a binomial so X is a binomial with n equals 100 and P equals 0.4 or 0.5 let's say my claim for you is that X is the sum of IID random variables and particularly x equals the sum from one to a hundred of y i and what is y i y i is a single Bernoulli representing a coin flip so y i is your Bernoulli with 0.5 so if y i is a single coin flip with 0.5 these are IID random variables and the sum of Y IID random variables will always be well approximated by a normal that binomial approximation by normal comes from the central limit theorem yes part two anyways uh we talked about okay so the sum the approximation of binomial from a normal comes from this deep idea that we have on the board it's that the binomial itself is the sum of bernoullis and particularly you know if you thought about it through essential limit theorem it's say that the good approximating normal would be equal to n times the mean of your Bernoulli but the mean of a Bernoulli is just p and the variance of Bernoulli is just P times 1 minus p and you end up with the exact same approximation whether or not you use essential limit theorem or you use our normal approximation idea from before and just to drive this home this is a random variable it doesn't have a nice equation instead it has a probably Mass function defined by like some crazy person just drawing a plot but if you took samples from this and added them together you would end up with the sum of IID random variables so if you took 15 samples from this and added them together you would get something where the sum is distributed as a normal and that's just driving home the point that the central limit theorem doesn't require the underlying thing to be a clean distribution all it requires is that the things you're adding together are IID wow that's so cool fantastic whatever you take this distribution you add them together you get a normal okay what a good time now we only have five more minutes and it's a Friday and you guys have worked hard so maybe let me finish this example then I want to leave you with an idea of why I think this is so beautiful you might ask the question how good is the central limit theorem and to answer that question one thing I could do is we can take uniforms and we could take the convolution and we could compare it to the normal approximation if you talked about Central limit theorem where you thought of n equal to 2 so the sum of two independent uniforms the true answers in the dashed line and the central limit theorem is in the uh the Orange Line already this looks like a pretty good approximation but there's some error and you can figure out what's the exact answer and you can figure out what is the central libid theorem answer and you can see the central limit theorem would give you a different answer to the question like what's the probability that the sum is less than say two over three but what have we got to three if you add up three uniforms you get this dashed line but the central limit theorem would say that it's approximated by this orange line and already if you start to ask probability questions you'll get very very similar answers and if you bump it up to something like 10 the sum of 10 uniforms is so perfectly represented by the central limit theorem that any probability question you get you're going to be so incredibly close if you use the CLT okay if there's a major takeaway from today one you learned how to do convolution you learned how to add random variables you solve some cool problems with that we learned some cool tricks then we bumped it up a notch and we started to realize that the convolution of any IID random variables always gives you a normal which is a beautiful thing now if I could we'll talk about this later but if the sum of IID random variables is normal I want you to think what is the average of IID random variables because sum was just one approx one operation we did you could also think about average you could also think about a thing called a Max I'm going to leave that for you guys to think about and we can pick it up on Monday's class but I want to leave you with one a little bit of history and a quote so if you guys are curious about the history of this this comes from Abraham de Moi and in fact he was a refugee who left France because he was a hug you know who was being persecuted he ended up going to the UK because of that he wrote the doctrine of chances in English and because of that UK became the place for Bays to come up with bayesium like the whole history of mathematics starts from this person being a refugee and having to leave France and therefore not writing the doctrine of chances in French doctrine of chances is where the central limit theorem was first shown he first showed it for a sum of Bernoulli's it was then taken by LaPlace and they he did the first proof that a binomial can approximate as a normal and then finally the precise definition of the general version of the central limit theorem was given by Alexander in 1901 and then you know whatever about the Beyonce thing I'll explain that later um we're gonna play a game later on Monday we're going to start with a game where we're going to think about Central limit theorem but I would like to leave you with this idea this is a quote from Galton on the central limit theorem I know of scarcely anything so apt to impress the imagination as a wonderful form of cosmic order expressed by the central limit theorem the law would have been personified by the Greeks and deified if they had known of it it reigns with Serenity in complete self-effacements amidst the wildest confusion of adding random variables the hunger the Huger the mod the more random variables and the greater the apparent Anarchy the more the variance the more perfect is its sway it is the supreme law of unreason whenever a large sample of chaotic elements are taken in hand and marshaled in the order of their magnitude adding and unsuspected and most beautiful form of regularity proves to have been latent all along it is a beautiful thing about our universe have a fantastic weekend come back on Monday and we'll continue this beautiful Deep dive into theory of probability thank you very much cs109