okay we're live good afternoon cs109 how are you guys doing today let's try that again it's a Friday how are you guys doing yay um I appreciate like you know what are we week four I think we're week four it's like a little bit harder to like midterms are coming around the corner speaking of which we have our midterms not next week but the week after in cs109 and the one big announcement for that is please do fill up that form to let us know if you're going to take it on a Tuesday or Wednesday we need to make sure that we have enough seats uh and that will depend on how many people are showing up on each day so you're doing us a huge favor so absolutely if you could please fill out that form um and we have a wonderful class today we're going to be learning about inference how we're going to change our beliefs in the face of new information what an incredibly important thing to know how to do uh with random variables and we're going to learn how to do it today uh it's a Friday so I thought I would tell you guys a story let's see I told you guys the story of how my parents met amazing and normally I save this for the end of the quarter but I was going to start telling you guys about how I met my wife because I love Laura stories so um you know well I'll tell you guys a little about this and it's kind of interesting because you know I'd like to tell you about stories that have taken place since then so I met my wife not during my co-term year but the year after my co-term so I did a co-term at Stanford and then after I finished my co-turner I'm like I'm done I'm out of here I took a entire year off I got into a PhD program I was like don't call me for you know 365 days and in that time I went back to Kenya which was lovely and I spent a few months there and at some point I decided I had this great idea I was like in a whole year I was going to slowly make my way back to campus and so I started traveling and I made it all the way to Greece but it was 2011 and there was a economic crisis in Greece and I was on a ferry and the ferry conductor says wherever you get off you're not leaving because there's not going to be a ferry for several weeks uh so I chose an island and uh it turns out that my wife to be also got stuck on the same island for similar reasons uh and because there is no way on or off it was a small island we kept running into each other like we just met each other I was sitting on a wall and she's like it's a nice view and I'm like yeah and we had the most wonderful two-minute conversation but then the person I was waiting for came and we're supposed to go for a walk and then my wife now wife was like can I come with you and the person who I met was like no that could have been the end of the story but since we're stuck on the same island we saw each other uh a few times and like we were running each other and be like hey I remember you um but then one day the strikes ended the there was ways off the islands and we went different directions and I genuinely thought I would never see this person again until a few days later I'd made my way to Athens I got a message and it says hi Chris you know I was thinking about it it's really nice to meet you I'm going to be in Northern Romania in like three days come join or two days and I was like oh that sounds really exciting but there's no way I can cross so much of Europe in just two days um I couldn't afford the flight because I didn't have that much money I couldn't afford a train and a bus was too slow to get me there in time and that could have been the end of the story but somebody this is a bad idea don't do this at home convince me that hitchhiking was a reasonable thing and they sent me to like athenshitchhiking.com where I had like step-by-step instructions on how to do it it's like you get a sign and you stick your thumb up and they tell you where to go and I went there and nothing three hours just standing on the side of a highway and be like what am I doing with my life um but then at some point a motorcyclist took me a little bit further and then a car took me a little bit further and then another car took me a little bit further and I was really stuck in the middle of nowhere and now when I was there for three hours like this could be dangerous um but as a rest stop and a truck pulled in and he went to sleep and three hours later when the truck driver woke up I put my little sign up and he opened his door and he said come on in and he gave me a long ride and the most wonderful thing happened it was like a eight hour ride and I fell asleep which I could have died but instead of dying he woke me up he was like Chris while you're asleep I called all my friends and I told them the story I told him I was gonna go meet this girl I met which he liked uh and he said I told all my friends and tonight you're going to come to my house we'll feed you we'll give you clothes or it'll clean her clothes I'm like hey I'm pretty clean but um and then we'll get to all my friends are gonna pitched together we buy a flight so you can make it there on time and this is a stranger who's already doing a nice thing for me in the moment of a crisis like he must have felt some fear for what this Christ is going to mean for him 2011 was a pretty serious crisis for Greece and he was just so generous and it's hard to name something that was so generous in my life of course I said no though because even though I didn't have very much money that was too much for me to accept so what he did instead is he snuck 20 euros into my backpack but more importantly he set me up with his friends so like when I got off another friend took me and then they would call ahead and I got like a chain of four trucks that got me all the way to Northern Romania and I remember like like 22 years old and I walk across into Bulgaria in the middle of night in like late October around this time of year and this truck pulls up it's like are you Chris I'm like yes it's like are you gonna go me like yeah that's me and I just got in this person's truck um anyways I'll tell you the rest of the story over uh over another Friday but um we'll leave that as a little bit of suspense for now um but we know how it ends um but the details are worth recounting also worth recounting is today's topic we're going to learn such a crazy cool idea I split this amazing idea into two lectures and I did restructure cs109 a little bit because when I thought about I was like this is one of those 10 out of 10 things you really really want to know deep in your veins when you walk out of cs109 so I slowed down a little bit so I can give you guys this pretty hard idea in two different lectures so you guys are going to be putting on your pretty hard thinking hats today and we're gonna see if we can work through this idea of inference and inference is a very formal name for if you see evidence how do you change your pmf or your PDF of a random variable not just your belief in an event but the entire PDF and that's what we call inference fancy name for right relatively straightforward concept and as always I've gotten in the habit this is the first chord I've really done this of putting all the questions for the lectures uh into the pset app and you can find them on the course website but by this point of course you probably know that where are we we're in the part of the class where we're talking about multiple random variables being random together and particularly I want to give you an idea of where are we locally and what are we going to do in the world of probabilistic models first we introduce the concept of many random many variables being random together through the world of discrete models so many random variables that can just take on discrete values themselves we learn about joints in particular joint probability tables and then we were introduced to our first joint model called the multinomial today we're going to learn inference an inference is the task of how do you change a random variable's belief in a probabilistic model when you see observations but the other tasks that you're going to learn in the sections we're going to do a little bit of modeling how could you create your own probabilistic model and then finally I'm going to teach you a really cool randomized algorithm that you can use for solving inference problems for any probabilistic model so that's our journey and it starts well on Wednesday and it continues with inference today some learning goals we're going to continue our conversation about the multinomial I hope by the end of today's class you really appreciate the utility of log probabilities and then finally we're going to really focus on improving your ability to combine Bayes theorem with random variables two grade tastes that taste great together there are a few things we're going to use today and here's one of them that you probably haven't seen and you probably haven't thought of because it seems nonsensical you wouldn't really use it in a probability problem but sometimes it shows up in proofs recall when you have a continuous random variable one of the ways of representing it is using this probability density function like this could be the probably density function of let's say an exponential random variable and an exponential random variable take on continuous values we don't really think about the probability of it taking on one specific value as having too much meaning right like what's the probability that the height of a baby is four pounds .000 like to infinite precision it doesn't have meaning because all babies will be slightly different than that number no baby will be exactly equal to infinite precision that still holds true yet there is this one calculus intuition that I want to remind you of because we're going to use it in some proofs if you ask about the probability of a baby being 4 to infinite Precision zeros doesn't make sense but if you allow for a range of values you say like 4 plus or minus point zero one that does make sense you can say what's the probability that a baby is in that range does that make sense so if you have a range you get a probability and so sometimes we think about the smallest Range possible and we call this the Epsilon range if you haven't seen Epsilon before we use it in mathematics to represent the smallest number possible so the limit of a number as it goes towards zero so you can take a density and multiply it by this Epsilon and you know that is one very calculusy way to think about the probability that a continuous random variable takes on a value I'm just going to plant this seed in your mind we're going to use it again later and then we'll revisit it but I just want to start that idea germinating to show you how you might use that particular idea here's an interesting question I'm going to say the time to finish a problem set is given by a gaussian recall that a gaussian is a continuous random variable so if this is a gaussian for how long it takes to finish problem set three you could think about what's the probability density at say 10 hours or the probability density at five hours the question I'd like to think about is how much more likely are you to complete in 10 hours than in five hours at first blush you might think this is nonsensical because it's probability zero that you finish in exactly 10 hours and probability zero that you finish in exactly five hours but it turns out to the Limit this zero is zero could be interpreted differently to give it some meaning you could say actually I do think it's more likely to finish in 10 hours and five hours you know if you look at the gaussian at 10 hours the PDF is much higher than at five hours so how could we make sense of this statement in light of our idea that probability of x equals 10 is probably a zero number and probably x equals five is also a zero number well to better understand this we can use that Epsilon trick that we just talked about we can say this probably the x equals 10 is actually the density at 10 times by some really tiny range I'm not going to specify how tying the range is but I wrote one that's really really small and I can do the same thing take the density at five so five hours to finish your problem set and multiply it by the same width tiny tiny range these epsilons are totally problematic if they end up in the end of your answer there's nothing you can do with this is polluted your answer but they're helpful when they cancel and as you can see here by putting this Epsilon range on the top and putting this Epsilon range on the bottom I got meaningful statements these eplons are like infinitely small numbers but they exactly cancel each other out which leads us to this question which is hard to think about is actually equal to just the ratio of the probability densities that feels intuitive it's saying the ratio of the probability density at 10 to 5 tells us how much more likely you are to complete at 10 hours than 5 hours and at this point we're going to use this beautiful equation that we've talked a lot about but haven't used yet this is the normal probability density function looks rather scary but it's just a beautiful probability density function equation and I'm just plugging in 10 and we can use mu and the variance we can plug in those numbers too and eventually once you plug in all those numbers you end up with a reduction that goes you know e to the power of 0 divided by e to the negative 25 over 4 which is a 518. so we can give a semantic answer to how much more likely are you to complete a problem set in 10 hours than in five is that a mind-blowing way to start a Friday or what so you know the the long story short there is this trick you can use when you talk about the probability of a continuous random variable equaling a number you can either think of that as just being zero or you can think about it as the density times by the tiniest range possible questions comments concerns arguments anyone have something they want to ask about this yes what is the 500 agent represent again it's saying this 518 is the ratio of these so it's saying you're 518 times more likely to finish it in 10 hours than in five hours so it gives you some idea that like this is this is much more likely according to this gaussian um it's certainly not a probability it's a ratio of probabilities it's saying you know how much bigger is this probability than that probability some people will call that an odds but we can talk about that another day I thought in the past that probably like the probability that X is something like the PMS like yeah which is like probably x equals 10 is equal to pmf x equals okay good good question what you said is completely correct in the discrete case so if x is a discrete random variable like a binomial how many coin how many heads do I get those are countable it's discrete it can be one two three and if you want to know the probability that x equals 10 you just look up in the probability Mass function 100 correct in the continuous case though it's not just the probability Mass function you can think of it as being very similar to probably density function that's really what's happening here we're just using the ratio of the probability density functions but probably densities are not probabilities and I just wanted to talk about that so in a continuous case when you say x equals 10 it's a much more complicated claim than saying a discrete thing is takes on the value 10. because in math continuous things have infinite precision okay as I said this is just a warm up to an idea I want to let it germinate a little bit and we'll come back to this later when we talk about inference we also talked about when random variables are random together so we when introduce ourselves to this concept on Wednesday and we thought about random variables being random together and we had this fun little example of two random variables one random variable which is what year are you at Stanford and the other random variable represents well in this case I think we oh yeah we talked about uh relationship status last time but here's a new data for some excitement this is what is the room situation and this was collected a few years ago in cs109 anonymously and so this if you have these two random variables the be all endle the most useful thing you can have is what we call a joint distribution which says I have a way of telling the probability of any joint assignment to each of my random variables so what's the probability that year is junior and two room double equals or the end the room type equals two room double if you added up all these probabilities they should be one if you add up any row you marginalize out the rooms and you're left with just the probability of the year so this is probably freshman sophomore junior senior uh five plus and if you sum up the columns you marginalize out the year and you're just left with the probability of somebody having roommates two room doubles a shared partner or a single okay A little bit of review joint distribution if you have a probabilistic model if you have more than one random variable this is the thing you really care about it's the probability Mass function equivalent for multiple random variables and it's where you give me a statement which says what's the probability of this particular assignment to my first random variable and this particular assignment to other random variable remember this comma means and if you have more than two random variables you know you want to have a expression that can tell you the likelihood of all those assignments happening at the same time this joint distribution is important because if somebody gives me the joint I can calculate any probability question anyone could ever want to know about my model notation I just want to talk about it for a second hey new slides are a little bit slower on here than on my computer there you go notation I don't want to lose anybody on this but this shorthand becomes so common when you have multiple random variables that you need to know it I will often try and use the top notation when I can but often that's not possible if there's lots of random variables the top expression says exactly what I want what's the probability that this random variable equals this number and this other random variable equals the second number little Y where the little numbers the little y's represent that I expect you to put non-random values in there that takes up too much space so some people write it like this and then other people are like you know the capital x kind of implies the lowercase x and the capital Y implies the lowercase y so it's a bit redundant what if we just wrote this and everyone can just know then we write this we mean that and it's like what this is like talking about the probability of constants what are you doing but that's the notation people often use so if you see a probability like this just know that it's implying a full probability statement that has proper events and the proper events are quality events and the implied random variables are the capitals of what you see mind-blowing okay anyways joints they're really important we had this problem though we would start to represent probabilistic models with joint tables and that was a good time until we started until we started putting a couple random variables together and once you had more than two random variables we realized that the size of your joint table would grow exponentially and that's a problem for representing a computer it's also a problem for the scientists who are trying to come up with the probability values so our solution was are there ways we can model the world so that you can represent a joint without having to write a full uh probability or a joint probability table welcome the multinomial multinomial answered the question if I were to roll n dice where each dice has M different outcomes what's the probability of seeing C1 outcomes of type 1 C2 outcomes of type 2 and CM outcomes of Type M this is naturally a probabilistic model because there are naturally multiple random variables that are be random together how many outcomes of type one how many outcomes of type 2 to how many outcomes of Type M if it's a dice there's six outcomes and the probability of each outcome is one over six but the multinomial can be more General you can have more than six outcomes and you can specify the probability of each outcome similar to the binomial distribution a similar derivation leads to this beautiful equation where this is a constant it's a n choose C1 C2 CM which is same as n factorial divided by this factorial this factorial this factorial so that's a big old number multiplied by probability of outcome one to the power of how many times you saw outcome one this would let you do silly things with dice with like saying if I roll seven dice what's it probably is seeing six ones and a two and zero of the other outcomes but allows us to do more interesting things particularly we got into probabilistic text analysis probabilistic test analysis was could model a document as a multinomial we'd make a big dice which was the author's probability of writing any particular word so instead of having six sides it has one side for every word the author could write and the corresponding probability of that side was the probably the author would write that and we imagine the creation of a document as the author just getting out their massive dice rolling it seeing which word came out writing it and rolling again seeing which word came out writing it down and that's how the author produced the document if you imagine that's how we get counts of words in a document then it means that you can think about those counts of words using exactly the multinomial end of review that was a long way to get to the punch line that we all wanted to get to who wrote The Federalist Papers so on Wednesday's class we started writing this program to figure out did Alexander Hamilton or Madison write Federalist paper 53. I gave you data I gave you samples of Madison's writing I gave you samples of Hamilton's writing from those things you could calculate these two probabilities for every word I you can calculate the probability that Hamilton writes that word it's just how many times it shows up in the sample divided by a number of words and same thing with Madison for any word I or I could be like the or Congress or you know Chris he doesn't write Chris that's sad uh Mi would be the probability of Madison writing word I these two documents allow you to get h i and MI and this document is the Federalist paper we care about and that Federalist paper we can count how many times each word I appears and we're going to call those CIS and we decide we're going to use Bayes theorem we're going to say we have counts of words we know the probability of different authors writing different words uh can we figure out who wrote this document and we said we can think of it like this if I knew Hamilton was the author it's easy to think about the probability of the counts of words that's just a multinomial this is a hard probability but we can calculate using Bayes theorem using this easier one it requires us to know probably H and probably of t probably of H is How likely do we think Hamilton is to write this if we had no other information and we're just saying that's one half um and then probably if D was a problem this thing on the bottom we call it the normalization constant it's a problem a lot of the time so we have lots of tricks to deal with it the particular oh and just you know want to point out that that's a multinomial and we want to figure out what to do with this the particular trick we wrote we used you can either see the slides or I wrote it over here is I want to get rid of the probability of D I didn't want to calculate it so I use this super crafty trick use it learn it show your friends probably if Hamilton given D is this expression and the probability of Madison given D is the same expression and both of them have this probability of d and the super tricky ones out there might realize if you took this term and divide by this term we could make those probabilities of D's just go away so you don't have to ever calculate it probably if D is a weird thing we call it the normalization constant because it's a weird thing and saying like what's the probability of seeing these words without conditioning on who the author is and it's really hard to think about the processing these words if you don't know who the author is in both this term and M given or D given M we know who the author is we've been told Hamilton Madison are the authors but if we write this out check out what happens this term is divided by D and this term is divided by D so these two things just cancel out ah what a good time and it gets even better we assume that before we see anything any data the probability of Hamilton is one half and the probably of Madison is one half so these two terms just cancel out ah what good times and you're like does it get any better I tell you it gets even better why is it get even better this coefficient has to do with the counts of the words in the documents you know C1 is how many times word one shows up in the unknown document and so is this C1 in fact these two terms are exactly the same because they're both just an expression of the counts of the words in the exact same document so lo and behold these two terms just go away wait I forgot a product over here that would be funny it's we're going to Loop over all the words in the document for every word we're going to say what's the probability that Hamilton wrote that word and we're going to raise it to the power of how many times the word shows up and this term is for every word in the document figure out the probability that Madison wrote the word and then raise it to the probability or the count of the time or times that it shows up this led to this wonderful piece of code where you know I do some pre-processing on the text and I calculate Hamilton word probabilities those are all the H I's I calculate Madison word probabilities those are all the Mis and I show you a lookup I look up Congress so it's like where I is Congress I look up Hamilton word probability mass and word probably and the counts and then I calculate this term passing in the Hamilton dictionary and then I calculated this term passing in the Madison dictionary so here is passing the Hamilton dictionary that gives the Hamilton term calculate with the Madison dictionary that gives the medicine term and then I print those out this equation cheeky little bugger does that it starts with a probability of one y one if you multiply anything times zero you'll get zero so I start with a one and I'm going to multiply these terms in one at a time remember this is a for Loop over products so it's a for Loop if you haven't seen a for Loop like this in Python it says take your dictionary of count items and give me one at a time which word was written and how many times it shows up I look up in the dictionary what's the probability of the word I and I raise it to the power of CI so I do this computation and I get a running product for probabilities it seems so beautiful look it's nicely commented I use the snake case what a good time to be alive and we feel so good about ourselves um until we run the code h i where I is Congress is a number it's a probability m i where I is Congress is a number it's a probability c i where I is Congress is a count it's how many times the word Congress shows up everything seems good except when we calculate these probabilities they are zero what could go wrong now sometimes you just look at code and you know it you're like my soul speaks to me and I know exactly what the bug is but other times we debug maybe you see debugger or maybe you did a print line so I'm like what's going on here I'm going to print like this for Loop it's continually updating a probability variable and I want to print out that probability as we go and look at what happens scrolling up scrolling up that's a lot of zeros oh look first iteration it's one second iteration is pretty small third iteration it's about as small as you can imagine this says 2.5 times 10 to the negative 309. tiny and then after that your computer is like I can't even I just can't even I don't want to talk about numbers smaller than 10 to the negative 309. what's going on here yeah what's wrong [Music] but this is where you just can't get along when you multiply these small numbers together in theory it makes sense and when you divide it you'll get a tiny tiny number here like this is a small number multiplied by itself a bunch of times multiplied by a whole bunch of other small numbers this is also teenage teen and in theory dividing these two teeny tiny numbers will give us our answer but the computer doesn't want to it can't represent numbers that small so when you multiply probabilities against each other especially small ones you often get underflows which is why you guys need to know about log probabilities log probabilities don't have too much semantics except they're just taking the log of a calculation why would you just take a log of a calculation well you might just end up with something that a computer could represent so for example if you told me the answer not to this expression but log of this expression I would still be able to tell you who wrote the document I'd be able to look at this number and reason about whether or not it was Hamilton or Madison if you give me the log of this ratio and the nice thing about this is then you would do the log of these two sides and when you do the log of these two sides you actually get something nice first you get the log of this minus the log of this instead of division because divisions just become subtraction and when you do the log of this product it's like the log is a big old tractor it's going to bulldoze right through that product system and when it bulldozes it just leaves you with a beautiful sum and the log just goes right inside as a tractor the log of a product is the sum of logs as you guys know um so h i to the power of c i minus the sum of the log of m i to the power of CI so that's what happens when you do a log to this term all those products becomes additions and this division becomes a subtraction it's a little bit better than that you know the log of something raised to a power is just the log of that thing Times by the power and we have ourselves a very nice expression so if you took the log of this term you can get something that you would compute in a different way it's a lot of additions of small numbers logs of small numbers and hopefully it doesn't lead to an underflow we're going to then get an answer which isn't the ratio but rather the log of the ratio we'll have to do a little bit more work to interpret it so let's go over to code that does that exact same code but instead of calculating the probability I mean calculate the log probability so instead of starting with 1 I'm going to start with the log of 1 and then I'm going to do this sum Loop over every word get the probability of the word do the log of that probability and multiply it by how many times the word shows up so a nice one two three four five line program if we do this we're going to get log terms on the top and log terms on the bottom who wants to see what happens predict what [Music] okay back up Chris okay um instead of talking the probability of D given H we no longer have zeros we have these big negative numbers and so the log of the probability of the document given Hamilton is this negative fourteen thousand and log of the probably docking given Madison is this negative thirteen thousand if you subtract those two terms you'll get the log of this ratio and if you subtracted those two terms you get negative 1 353. so this log of this a ratio of probabilities is negative one thousand three hundred about who wrote the document Madison what how did they know it's Madison okay how does that tell me the answer to this if it wasn't a log if Madison wrote this let's say Madison was the actual author if Madison was the actual author the probably Madison given document should be greater than probably Hamilton given documents you guys agree so if Madison's truly the author this should be a number less than one because it's a smaller number divided by a bigger number it should be something like uh 0.2 or something like that you know that would be that Madison um is much more likely than Hamilton to have written it if this ratio if Hamilton wrote it then this ratio should be greater than one this number should be bigger beautiful thing about logs that you need to know can I tell you I would love to tell you this beautiful thing about logs okay A little bit of review here's a graph of the log function and notice at log of 1 something special happens if you put a number that's bigger than one into a log you get a positive and if you put a number between 0 and 1 into a log you get a negative so if you put in 0 Log of 0.2 you'll get a negative number if you put in log of 5 you'll get a positive number since this is a leg negative number I know that this ratio must be less than one let me just show you that really quickly so that you believe me so python import math.log of say we put in a number greater than one like three you get a positive let's say we put in something like 0.5 that's less than one you get a negative so anytime you see a negative expression from a log you know that something less than 1 1 in there which means I know Madison wrote that um and like if you put in a really small number you get a larger negative if you put in a really really small number you get a much larger negative this negative 13 000 like what do I have to put in here to get the log to be thirteen thousand a lot our model is very very confident that Madison Rose not like Madison Rose like Madison definitely wrote this thing Hamilton's a liar okay question yeah yes you can certainly take advantage of the fact that um you know log base e of some number so like K is saying that equals let's say n is the same as claiming that e to the power of n equals K you could take advantage of this and turn and actually calculate this exact number totally two reasonable approaches okay now we know Madison wrote it can we take a step back and think about what we were supposed to have learned so first of all we use the multinomial to come up with this probability of the document knowing who the author was it used the counts of words a lot of things canceled out and it led to a really nice expression for How likely is a document given the author we use Bayes theorem with a multinomial as a warm up to the idea that Bayes theorem can show up in lots of places and then you saw this super important concept which is that in computer science if you're doing a probability that involves a lot of products with small numbers you might get underflow and your solution to that is to use this thing called a log probabilities just put a log in front of it it's a different representation of the concept you're trying to calculate a lot of things and one cool example and there's a question in the back um so I guess a lot of business and the logifications for the information with us but it's very um succinctly I guess how the left hand side of the board relates to the right side of the board so the left board on your left oh on this one yeah so this is how you know a straightforward way to figure out if Hamilton wrote this document is to calculate this expression but this expression has this term which is easy this term which is Trivial and this term which is insanely hard now one thing we sometimes do is we take this bottom term and we turn it into the lobster probably you do something like the probability of D given h times the probability of H plus the probability of D given Madison times the probability of Mass have anyone ever done something like this yes okay good um you could have done this but if you approach I gave you a different way to get rid of the probability of D which was I was going to give you the ratio of Hamilton to Madison can I tell you cheeky why I didn't do it this way if I tried to do it this way when I apply my log this will be straightforward this will be straightforward and this addition here is going to be a total disaster because the log of a plus b doesn't turn into a nice expression uh it becomes quite hard to express you'd actually have to calculate some of these probabilities on their own it's really really hard to simplify this expression if you went straight to a log of this probability like if you're trying to do log of Hamilton given document difficult possible but difficult I use this trick got rid of the D and then when I did the log everything was very nice because I didn't have this awful summation in the in the normalizing constant so actually that is another big takeaway is that this normalization constant is a huge problem for Bayes theorem and you now know two tricks for dealing with it you can expand using log total probability or you can do this ratio and get it to cancel out yes just something different from technology that probably Innovation probability of um yeah absolutely there's lots of you know these priors your belief before you see data it's a lot of Scholastic argumentation be like I think that our belief before we see data should be like 0.8 that is Hamilton because he wrote more papers um it turns out the the multinomial is so convinced that even if you brought in different priors it gets totally overwhelmed by the evidence and it gets washed out so it turns out it didn't really matter what your prior belief was unless you say like I'm 9999999 positive it's Hamilton before you start your calculations it's going to lead to Madison being the author now it would have been fine here you would have just ended up with you know log of this would have been plus log of probability of H and this would have been you know minus this which is log of probability of Madison like the all the calculations would have worked out quite nicely okay fantastic which I feel like that summarizes the mood at this moment it's like yeah we can do it what a cool thing to be able to do at this point in cs109 okay I want to go back to a clay oh there's a bunch of things where we want to be able to do um with uh with probabilistic models so now you can kind of use a joint and I want us to get to start thinking about conditional probabilities with joint variables does that sound good I want us to get a little bit more formal about if you have a model what does conditioning mean this is inference it's one of the most important things you can learn in the whole field of probability inference really says this conditional probability is with joint variables in a model says I have a model of how the world works I observe from information and now my model will change and thinking about how your model will change when you condition on things You observe that's one of the be-alls and ends alls of why we study this whole field so let's learn how to do it and let's go back to our friends who are living with roommates or in two room doubles shared Partners cs109 from years past The Joint probability table tells you everything you need to solve conditional probability questions but let's go ahead and try solving some of them so here's a joint probability of what room peoples were in and what year they were you guys ready to go to the Internet let's go to the internet do do okay I have here your joint table it says like what does this cell mean this says like what's the chance that somebody's both a junior and has a roommate I now want us to bump up our utility and start using this joint to solve conditional probability questions and I'm going to challenge you guys with one I'm going to challenge you with I'm going to tell you the value of this random variable I'm going to tell you that this random variable is equal to sophomore there or actually let's do junior their year is a junior so I tell you that their year is a junior what's your new belief in their room situation and how could you calculate it if you want to make this simpler just start with one value if I tell you that somebody's a junior what's your updated belief that they have roommates you can solve this from first principles where we're going to generalize this so we can bring it to the world of probabilistic models but this is where it starts take a moment talk to the person next to you see if you can figure out what's the probability that somebody is in a roommate situation given they're a junior and then we'll see if we can write this in here foreign thank you this is oh man what do we do here okay you know it you might see this and think what these are conditional probabilities in the world of random variables I've only seen conditional probabilities in the world of events but in fact this is an event this is the event where the roommate situation is that you have roommates or their housing situation that you have roommates and this is the event where your year in school is a junior and if you think of these as events you can do any of the conditional probability work that you've learned in this class and really when you have conditional probability there's two ways you can approach it you can just think about the definition of conditional probability or you can use base there man that thing that shows up like a billion times cs109 and cs19 can be renamed to be like the class we use base theorem like every third problem because it's that useful Bayes theorem would say that you know this probability well actually I suppose wait I I have to forgive myself this is a situation where actually the definition of conditional probability is just fine oops okay what is the definition of conditional probability it says that this is equal to these two events together so I'm just do R equals little r for roommates or I'll say room for roommates and Y equals Junior embarrassing I said base theorem but I actually meant the definition probability like forgive myself tonight and this is it so it would say that the probability of this happening is the probability of them both happening at the same time divided by just the overall probability of somebody's a junior not taking into account what room they're in so what's the probability of these two things happening at the same time roommates and they're a junior yeah it's 0.04 you know the joint probability table gives you the probability of the and of any combination of random variables and now you're wondering do I actually have the probability that somebody's a junior yeah that's the marginal it's this is 0.175 and so oh nicely done Google filling that in for me it tells me that the probability that somebody is a junior or that their roommate situation given that they're a junior is 0.227 wait given year yeah yeah I got that right okay now if we want to know the probably something's in a two room double given that they're a junior what is this going to look like now it's going to be the same thing just rooms gonna be two room double so probably somebody's a two room double given that they're a junior is going to be that fraction the probably that somebody is a a shared partner given that they're in a tour I guess shared Partners like you live with a couple situation uh and nobody reported that Anonymous survey um but a lot of people have singles so you know if you look at the overall distribution the belief in what room situations people have very few people have singles but if you condition on the fact that people are Juniors it increases by quite a lot if I sum up all of these things stop being trying to be so intelligent it all gives me one so that's the probability that somebody's roommate given their junior plus it's probably somebody's two room double given that they're a junior plus probably a shared partner given junior plus probably a single given Junior all of that adds up to one because these four rooms span the space so if you enter the world where somebody's a junior they live in one of these four living situations now let's do the same thing with freshmen but this in this time instead of using the module I'm going to do something slightly different I'm still calculating the same probability so that probably somebody's a freshman and has roommates but in this time instead of using the module I'm going to use the definition of marginal which is the sum of this row and this is the law of total probability it is expanding this using the law of tour probably it's like Junior and or sorry freshmen and roommates plus freshmen and two room Doubles Plus freshmen and shared Partners plus freshman signal and that's what the Loft total probability is saying that's where the marginal comes from um anyways and so you could fill in this whole table to do so that equals this divided by that number this equals that divided by this number and single equals this divided by so this is very interesting if you think about the probability Mass function over rooms if you don't condition on what your people are this is what the probability Mass function says for any assignment to Rooms it gives you a probability if you condition on them being a Freshman it changes if you condition on them being a junior it changes and that's such a simple way of communicating what inferences when you tell me some information like what your somebody is you watch the whole probability distribution of another random variable change I think this is easier to see as a chart okay here is the idea as our chart this is the probability distribution of rooms if you didn't tell me what year somebody was in cs109 most people have roommates some people have singles this is a whole probably Mass function do you guys buy that now we infer now we we observe that somebody's a junior and we infer a whole new probability Mass function the probability Mass function changes quite a lot all of these likelihoods change once you tell me they're a junior and look probably if somebody being a single skyrockets people are much more likely to have a single in their junior year of college and that's a simple idea we're going to do this we're going to do it for more and more interesting models we're going to take a belief in a random variable we're going to observe some information we're going to watch the belief in that random variable change and sometimes we'll use the definition of conditional probability and sometimes equals Bayes theorem to solve for that okay just for a little bunch of these data sets for cs109 and I can show you how these distributions change over time here I'm showing you those distributions changing as people go from freshmen to sophomores to Juniors to seniors look what happens to roommates yeah we're a lot less likely to have roommates you're more likely to be in a single room as a junior than as a five plus which seems a little unfair that's one data set here's another one that I collected one year um this is how people get to class and you can imagine a lot of people bike to class especially sophomores all about the Eco travel uh but then you know seniors are all about like I don't I've got so much time to ponder the complexities of life that I'm just gonna walk uh this one's very noisy but this is where people were eating as you conditioned on different years of Stanford so if you condition on somebody being a sophomore they're very likely eating a dining hall but like grad students are like I'm never touching that food well not never but less likely um eating clubs might not exist anymore we used to like be friends and eat together and like smog is what time is Wild free pandemic uh and then people are more likely to make their own food over time uh and then you know we had that data set the other uh day in class and you can see like what's this relationship status over a year a year and you can see like uh people are really don't couple up but then all of a sudden like senior year comes and they're like I'm out of time or something I don't know laughs okay also for a little of levity because you guys are learning some complicated stuff I very much appreciate that let's play a little game little game is called number or function so if I have two random variables X and Y and if I ask a question what's probably x equals two given that y equals five is that a number or function think about it think about it think about it now yell it out loud yay cool game okay ooh here's one what's probably the X takes on the value little x given that y takes on the value 2. yeah it's a function and also it's a random variable it's saying like what is the probability Mass function of my random variable in the world where I observe this thing about another random variable and that probably Mass function is a function you can represent as a 1D table you can represent it as a python function but it is a function okay very good okay think about it think about it think about it yell it out loud oh but what it's a 2d function where you put in both the thing you're conditioning on and the outcome those are the two imposes of function uh and it gives you back a number and in fact if we went back here if we filled this whole thing in this is what a 2d function looks like for any assignment to roommates in frosh it can produce a number okay let's take our pedagogical pause have two minutes think about life and we will come back to this idea of inference that will take it up a notch okay take two minutes is foreign thank you foreign so fatherhood's this like exciting experience and one of the exciting experiences like you get to Define like what the universe looks like to another human being it's a totally Wild Thing and mostly I've just been like giving her the honest like this is what the world is like um except for I'm trying to convince her that when people ask you how are you you just say groovy or feeling groovy like that's the appropriate response I'm trying to like not tell her about like oh I'm good I'm happy it's just always groovy which is not that important I hope you guys though are feeling groovy after that pedagogy Okay so Bayes theorem Revisited you know you've seen this before we've just used it to figure out who wrote that particular document and I just want to write it out again and I want to take a moment to name the terms so if you have Bayes theorem there are names to each of these terms notice how this term in this term both have the probability of B and I really think about beta's theorem as taking a prior belief and calculating what we call an updated belief or if you're really fancy a posterior belief so this is the belief before you see the evidence and this is the belief after you saw the evidence and I really think of Bayes theorem as a function that takes a belief and updates it using evidence it has these two other terms you have to know How likely is the evidence given a particular assignment to the belief and it has this other term that we now know to dread the normalization constant and normalization constant is a pretty boring name compared to these other three like posterior likelihood of evidence probably all very semantic normalization constants not and we really do think about this is just something that we need to calculate if we want to get our probability so now we have names for those things what a nice thing and I just want to remind us of what we've stated but I just want to say it again Bayes theorem with random variables works just like you imagine it's the exact same thing as events in this case our events are just going to be assignments to random variables so this is the event where let's say m is discrete random variable this is an event where and a discrete random variable takes on the value of three and Bayes theorem if you just treat this as e and this is f you just get plugged right into your base theorem formula question not at all about base theorem is that I think there's two ways of doing it I think last time we did it with the table as the probability of something and something over the probability of the second thing happening and there's this other option and I don't often know which one is choose yes so it's a good point in the last problem there's a couple ways of expressing this numerator and a couple ways of expressing this denominator in this last problem you know what I did is I actually in this numerator had e and b instead of doing e given B times probably B I just had probably e comma B that if you wrote it out like that technically it would be the definition of conditional probability so you could instead of writing that top term or calculating that top term you could have calculated this that's the likelihood of both the events happening at the same time or if you're in the world of random variables what we call The Joint probability of those two assignments there's two ways of thinking about this just like we have one formula that's just this over that and another formula which is this over that obviously that makes those two things equal and actually if you thought about the chain rule the chain rule says that this times this equals that so if you take what's the likelihood of B to happen what's likelihood of e given B you multiply those two things together you get a chance of e and B happening so you have that option yes exclusively play into that do you have to choose one if the events no it doesn't matter neither Independence nor Mutual exclusivity matters these apply to any pairs of events for any two pairs of events Bayes rule applies and you can use either it just has to with information was given to you like in the last problem the table had this information so this was a convenient form in the Hamilton problem we didn't have this but we have the condition like probability of words given the author and then we had a belief about the author so we were given information in these two expressions yes yeah yeah no this is the point yes so that that means the joint probability of e and B is uh the numerator of the guys say like when you have a given B given me now so e given B uh and what was so um yes exactly exactly so if you multiply the probability of evidence with your prior you get the joint probability of the prior and the evidence together exactly what a nice way of putting it okay so we're bumping this up and it's not too big a leap but I don't want to lose anybody on this leap when we go to the world of random variables it's the same Bayes theorem it's just our events are made with assignments to random variables if I give you a concrete statement like this I think a lot of people follow me I think where I could lose a lot of people is when I change it to something like this where instead of writing concrete numbers with events we can imagine I use symbols so like the probably that M takes on the value Little M and the probably n takes on the value little n and I just put in a variable assignment this is true it's just using a lot more symbols in its notation so it's the exact same thing these are all events I'm just trying to write a bunch of statements with concrete numbers all in one expression so whenever you see something like this feel free to slow me down and be like whoa whoa Chris what do you mean when you say probably the M takes on the value Little M okay and I want to remind us of the shorthand I talked about at the beginning of class this becomes very clunky to write and a lot of times you'll see things like this so probably of Little M given n implies this bigger expression so it implies there is a random variable with the capital taking on this value there's a random variable with capital N taking on this value but just it's so much easier to write and almost easier to see that people love this notation so if you read a paper you'll almost always see this shorthand notation okay let's try a real problem so when babies are born they can't tell you whether or not they hear things so they do this test to see whether or not babies can hear where they'll play a sound and then they'll watch if babies actually change their gaze and it is a totally uh stochastic experience sometimes babies just look different directions for unrelated reasons and there has to be a lot of reasoning under uncertainty to try and infer is this baby hearing given how they change their gaze so I'm going to give you a lot of information about a joint relationship of two random variables the two random variables that are interesting here are whether or not the baby changes their gaze and how much they do so it's a random variable which can take on values and there's another random variable and this one is going to be Bernoulli it's either true or false and it's whether or not the baby can hear the sound I'm going to call that one y equals one just so we can enter the world of using random variables so we're going to observe x equals zero and we want to calculate what's the probability of Y equaling 1 given that x equals zero now interestingly here the information I give you is a conditional probability table here I give you the probability of x given that y equals one that's this column and here's the probability of x given that y equals zero in order to solve any conditional probability question you want to have the joint distribution but you want to know something crazy I've actually given you the joint because of giving you the probably equals y equals one I've given you the probably y equals zero that's just one over four and then the probability of x given y if you multiply the probability of x given y times probably of Y you've got the joint but that's not that important right now what is important right now is I'm going to want to calculate the probability of y equals one um so that the baby can hear the sound given that x equals zero and take a moment with that person next to you see if you can figure out how we're going to get probably of y equals one given x equals zero with the information that we've got and so this is a way to practice what we've been talking about with Bayes theorem I think divide by probability of patent thank you okay it's asking a conditional probability question so we've talked about how that's often going to give you an expression that looks like this at the top you might have this product of two different probabilities the prior and the likelihood of the evidence given the belief or if you're given different information this might be as better to represent just using the joint probability of x equals zero and Y equals one two options there but the problem really pushes you towards one of them and the reason the problem pushes you towards one is that we are given x given y equals one and x given y equals zero so we're given X in the context where y equals one that's the left hand side and we've got x square y equals zero so because the information is given to us in this format it's a nice place to start so what's the probability x equals zero given y equals one this seems like something we could just look up 0.08 it's that top thing in the row it says you know if y equals one probably x equals zero is just this number what is the probability that y equals one your belief before you walk into the experiment yeah 0.75 and you know that hopefully came from some informed belief uh and somebody didn't just make up that number though there's this whole debate and probability about like some people this just like turns some people in the mathematical worlds insane because they're like how did you get that prior and people get like really up in arms about it but that's not important right now what is important is this normalization constant what are we going to do with this normalization constant question it's actually a different number if you add up this column you should get one if I did everything right four five six seven yeah I think eight nine ten this all adds up to one and that makes sense because if you enter the world where the baby can hear then you should see one of these values if you enter the world where the baby can't hear you should see one of these values you get a full probability Mass function so it's a good question so this doesn't keep any information about the prior the prior has to be given externally yeah yeah but if it was a joint the probability of the prior would have been inside of it so this is not a joint table this is a conditional probability table so we don't we're given the probably one random variable condition on the other like it's when we go back here remember there's these two tables this is the joint table and then we start calculating this conditional table we've been given a conditional table here okay somebody help us what are we going to do about this normalization constant just forget about it we can calculate the probability of y equals zero given x equals zero and then try and divide it we could do that but we don't need to do that here there is an easier way yes law of total probability wow to the probability but what you know we've kind of got this classic probability that x equals zero given y equals one times the probability that y equals one plus the probability that x equals zero given y equals zero times the probability that y equals zero you know if you just thought of these as the world of events this is your classic version of Base that you've used a whole bunch in your homework and then again we know this term .08 we know this term 0.75 we know this term it's 0.25 and probably the Y equals zero x equals zero given that y equals zero is just 0.4 and this would just become you know 0.4 times 0.25 and this would be a 0.08 times 0.75 and we have numbers for every term so we could just calculate it um here it is written up into slides you know we just wanted to do basically a straight base and the challenge of this problem was mostly just understanding that information was given to you as this conditional probability table so we have a whole random variable in the world where y equals one and a whole random variable in the world where y equals zero with once you realize what information you've got you just plug it in chug and you get your updated belief of three over eight that the baby can hear and that's quite interesting you know like you saw an experiment and gave you results and the results might have just LED somebody to conclude that the baby can't hear because they didn't respond but because there's so much noise in this experiment there's so much stochasticity to how babies respond you're actually not that confident after you've adjusting one observation question and so let's say you instead observed x equals five how would you know if you were supposed to use the confidence in the first row oh yeah this is a little bit poorly defined you know hopefully somebody tells you if this is like inclusive or exclusive very good question so like so hopefully somebody did a better job of writing this table but also you'll never see exactly five it'll be like four point nine nine nine nine nine or five point zero zero zero zero zero zero zero zero one or larger but that's such a cop-out answer good question okay yes to range from 0 from 0 to that uh yeah this is interesting you know technically this probably should be a continuous random variable and we'll get to that later I'll have a continuous version of this but here I did kind of a hacky discretization I took my continuous space and I bend it um and people do that and normally when it's been you just treat everything and Bin as one okay good question you guys one last thing I want us to do I want us to mix some discrete and continuous I have a super interesting problem that's going to make us think about continuous random variables it kind of speaks to the question that was just asked what would you do if this wasn't discrete random variable how would you treat it if your random variable was say a gaussian so without trying to gender elephants I'm going to do this problem it turns out that girl Elephants or female elephants and male elephants at Birth have different distributions of weights and these are actually real distributions so uh male female elephants are 160 kilograms with this standard deviation and male elephants are 165 kilograms well heavy with this standard deviation and just to practice this is not the most interesting problem but I thought it was one that could help us practice updating a continuous belief given information all right sorry updating a belief given continuous information here we want to say all you know about a newborn elephant is its weight you're told that's a 163 kilograms and now I want you to change your belief about whether or not it's a boy or a girl your prior belief is that it's one half one half but your posterior belief will be something different and it's going to require us to think about the world of discrete random variables and continuous together let me just Define some things that are said in this problem there's two random variables G is going to be about whether or not the elephant is a girl and G is a Bernoulli with probability 0.5 it's one half probability prior X is the distribution the weight of the elephants if you tell me g equals one I know X is this normal distribution if you tell me g equals zero I know X is this distribution and you know maybe you take this and you start writing out your Bayes theorem you're like this problem says I want to know the probability of g equals one or G one for girl given that x equals 163. and you got probability of g equals one given x equals 163. and it seems so straightforward it seems like everything's going to be just fine and I promise it'll be just fine um so you've got probably x equals 163 given that g equals one times the probability that g equals one divided by the probability that x equals 163. and you can you know figure out that normalization constant this feels fine until you get to this expression if I tell you that an elephant is a female what's the probability that its weight is exactly 163. zero because for continuous random variables we think about 163.000 with infinite precision as being an event that never happens and so it started as a simple problem now seems terrifying but let me give you an aside that's going to make this not so terrifying um sorry we have joint distributions we have this goal of inference remember at the beginning of class I gave us this insight sure for a continuous random variable the probability that takes on a value is zero but a calculus way of interpreting that zero is it is the density so the derivative times an Epsilon since that Epsilon is basically zero you can think of this as zero or the calculus way allows you to add in an Epsilon what that means is instead of having this probability here I can put in an Epsilon not times the probability but times the probability density and this seems fine but then you get an answer with an Epsilon in it what the hell does that mean until you don't give up and you get to this part and you're like this probability it's the probability that elephant is exactly 163. again you have a continuous random variable and again you have something that should be a zero and we're dividing by zero naughty us but again you're like this probability we can represent it as an Epsilon times the density to put that more precisely let's say you have a discrete and a continuous random variable this continuous random variable probability of an exact assignment can be thought of as density times Epsilon and you have an exact assignment on both the numerator and the denominator and then a beautiful beautiful thing happens when you write it out like this your epsilon's cancel and a little tear comes to your eye because you're like I didn't know what to do with those epsilons but they just went away I wish every problem in life could be that easy and when they just go away you're left with this new version of bass you guys now have a brand new version of babes welcome to the class new version of Base it looks just like your Bayes theorem but whenever you have a continuous random variable on the left side instead of using the probability Mass function you're using the probability density function and this is the missing information that we needed in order to solve our elephant problem so there's a bunch of versions of Bays where you mix discrete and continuous but the long story short is in all of them anytime you have a continuous on the left use probably density um and so here you know if you have a continuous random variable my X and y's are continuous you use density anytime you have continuous use density every time you have discrete you use the probability Mass function and it all works out because of those Epsilon canceling so you can derive each of these independently okay going back to my elephant I want to think about this again so these epsilons are gone life seems pretty good but we still need to solve the problem and we're not done the first thing I want to think about is how do you want to expand this lower term and I'm going to say we can use the law of total probability and actually the slide in the aside that I skipped is the law of total probability also has this function where you know if n is discrete and X is continuous then you can just use the probability density in place of the probability and again you could use an Epsilon to derive this if you wanted to so here this is equal to the probability density times your prior divided by we have to expand this term and I'm going to use the law of total probability so f of x equals 163 given that g equals 1 times the probability g equals one plus the density of 163 given g equals zero so it's a boy times the prior probability that g equals zero and here you have just your standard base it's just any time you had a continuous random variable use density instead of probabilities okay what's the probability that g equals one one-half yeah so this is just going to be one half this is one half and this is one-half all those terms cancel out and we're just left with these three terms and you're like oh great three simple terms what are we going to do what is this is the probability density at 163 in the world where the gender is female well in the world where gender is female X is given by this normal distribution and if this is the normal distribution what's the probability density at 163 if only we had a probability density formula which we do have we have a probability density formula we know the like tribution taking on a particular value so if this normal distribution has a particular mean in particular variance which we know we can ask what's the likelihood of it taking on value 163 we're just going to be plugging in that bad boy in for this or a girl because g equals one putting it over there for this one and then here it's the same thing it's just we've got the distribution for whether or not it's a male and so the probability density function of the normal comes in to save the day I appreciate this is a very hard thing to do you guys worked very hard to follow a lot of complicated ideas but the power that it will lead to is worth it it's going to allow us to solve very interesting problems if you can Master the skill of inference we practice on some shorter problems we're going to finish our practice but I have a sweet promise for you guys on Monday come back on Monday we're going to use this exact same mathematics not to solve a toy problem but to solve a real problem as you guys know the Stanford Acuity with test was invented here in cs109 it does an eye test where you keep track of your belief distribution of what a person can see and it keeps updating with new evidence it's the world's most accurate eye test and we will take this math and we'll make something real come back on Monday have a fantastic weekend see you guys cs109 sorry for all the yelling