Chris Peach is sick that's right I just got the sad news yesterday evening he's feverish in fact but he's getting better not Co another happy thing however this morning we had a conversation who's going to teach this lecture and as you guys know I don't have to convince you Peach is a very good instructor so we said we need one of our best CA one of our brightest one of our most talented one of our most enthusiastic all of them were busy so you're stuck with me all right my name is swin I'm a CA for this class I teach a couple sections maybe some of you know me I'm interested in this material because I really like thinking about intelligence in the context of decision making so a lot of stuff that we deal with in this class has to do with modeling human uncertainty making the best decisions we can with limited information if you ever want to talk about stuff like that please let me know but for today we're going to continue from where we left off on Friday we saw this really cool result called the central limit theorem we're going to talk a little bit more about that and then we're going to get into the world of Statistics so let's get started first things you may have seen something either talking to your ca or an announcement there's going to be a personal challenge a contest that you can enter if you want some of the information is going to be on the course website also on Ed so you can check there but basically this is going to be an opportunity for you to try to take something that you're interested in and push on it with some of the tools that you've learned in this class you'll have a little bit more control of your grade that way also it's just a great way to learn I would really recommend everybody to think about it so a little review yeah last time we saw a little bit more about this business with convolutions so in particular whenever we have x equal to0 and y equal to n we know that their sum is going to be equal to n and if we consider all these possibilities we can write out a full probability like that and that's what a convolution is for the discret case we also talked about summing independent dice roles so first we looked at the sum of one di roll in this case we have equal probability across all six possible values we see something looking more like a triangle here and lastly when we have three things start to look a bit more like a normal distribution and then we started talking about what if we have an arbitrary large sum of these random variables as long as they're all IID and this is where we got to the central limit theorem again really impressive result kind of magical where as long as we have n independent and identically distributed random variables doesn't matter what the distribution is as long as it's the same across all of them we're able to if we know that they're mean Is mu and their variance is Sigma squ we're able to write this that the distribution of their sum is distributed according to a gaan with its parameters scaled by n it's not too often that we can say these very general things without knowing a lot more about the distributions we're talking about so this is a very cool part of the class and we're going to use some of these results to do some impressive things later on in this lecture also this explains a lot we saw this phenomenon earlier with this gon board where if we think about this as n equals 5 trials in a binomial with probability .5 each of having a success either going left or right then we saw that this starts to look like a normal distribution well now we can think about this a bit more precisely if we have a binomial random variable which is a sum of bruli IID random variables then we have this new explanation here we can say that the binomial is the sum of brly random variables and what do we do we're able to say that the distribution is just scaling the mean which is p by n and similarly we take the variance and scale it by n and you'll notice that that looks a lot like the uh normal approximation that we would use for binomials so I'd like to play a game i' like to play a game the game involves rolling 10 six-sided dice I of them here and we're going to say that you guys win in the event that the sum is less than or equal to 25 or greater than or equal to 45 and to add some Stakes I have $24 with me if you play I'll give you $4 no matter what and if you win I'll give you the 20 do I have any volunteers can I get you yes come on up okay is it clear what's going on with the game let's talk about a little bit more formally each of these di rolles is going to be one of these X1 through X10 we're looking at the sum what's your name oh I think we worked together at some point earli on cool these are yours throw them on the floor okay I'm looking at a two a three that's five 9 15 18 19 so we need you to get less than 25 or sorry less than or equal to 25 total or greater than or equal to 45 and we're at 19 right now go for it that's not good two 6 10 11 13 13 + anybody know that one that's than unfortunately I wish hey thanks very much for playing Riley give it a hand for Riley give it [Applause] up a thing that you'll learn is uh especially if you're in my section I do not play fair what chance did Riley have of winning $24 from me let's talk about it okay using the central limit theorem we're talking about a sum of 10 Dice and we can know we know we can write the distribution like this so in particular we know that mu the average value of a single di roll is 3.5 calculated this earlier on in the class similarly for the variance we know it's 352 so what's this normal distribution that we're going to be able to use we just scale them both by 10 and so if you run the numbers on the right hand side or sorry on the left hand side we're just setting up this uh this interval that we're looking at where we consider there to be a win we're doing one minus the complement of that on the right hand side we're just doing the standardization that way we can plug in Fes there was only about an 8% chance of winning this game it was possible but not probable and that's the kind of thing we can do with the CLT okay here's another problem I'm going to let you guys work on this a little bit so find one or two other people around you and just talk through this problem IR all right let's talk about this we're hitting 12 traffic lights so these are excuse me we're hitting 10 traffic lights these are going to be the random variables that we're considering now we know that they're all distributed the same way we don't know what that distribution is but we do know the mean and the standard deviation can I get a volunteer for somebody to tell me what a first step is here I've bribes I've been told I have bribes I've Starburst what's up well like the I traffic light as like the weight time is Xi the expectation average weight time right right so what's your name the suggestion giving us is we know that the average weight time across all of these is going to be 45 seconds that is right there you go so if we let T be the total weight time it's going to be the sum of all these individual random variables and they all have have as as was pointed out the same expectation and then we do something similar for the variance where we're going to scale both by this number which is 10 so we're going to end up using this distribution and now what's the event that we're wanting to check for you'll be on time if your total weight time is less than 8 minutes can somebody tell me how we modeled that using this Rand varable t that's exactly right that's exactly right okay and so if we plug in five we get this value here it's extremely likely that we're going to be on time in this case Okay so we've stated the central limit theorem but a question that you might have is why should I believe that the central limit theorem is true there are going to be times where we tell you to sort of or at least in this case I'll be telling you to sort of trust me a little bit generally not a great idea but if Chris says you should trust him then you should do that but in this case there is a proof there's an object that we don't use too much in this class it's called a characteristic function and if we do some work with that characteristic function which involves the size of our sample when we let that sample go to Infinity we're going to see that that characteristic function becomes one of a normal distribution with the parameters that you would expect if you want to see more details about this you can check out this YouTube video which I believe was made by a former CS 109 student but for now we'll just take the CLT at face value so here it is stated one last time okay so when we have a sum of IID random variables as long as they're IID doesn't matter what the distribution is exactly we know that that's going to be in the limit a normal random variable so here's a question what about for the average average okay so here's the average of X1 through xn or we're doing is summing them up dividing them by n exactly what you would expect so how would we expect this to behave well again by the central limit theorem as n goes to Infinity we're actually going to be able to say that this is normal and you might be able to tell this based on what we've learned about linear transformations of normal random variables but for the sample mean or excuse me for the mean of the IID variables instead of it being n * mu and n * Sigma it's going to be this and we're going to be using that in a moment so here's an interesting thing I've got this sure cool so let's say this distribution here is our parent population and we're going to be drawing say five random variables from this distribution and then here in the blue section these are the means of these groups of five that we're talking about so if I add another one we get more means still more means if I add 10,000 pieces from this overall distribution we see that this distribution of means starts looking normal and then if we add even more it gets more that way now this might make a lot of sense just because we're already drawing from a normal distribution but what if I just make some crazy crazy distribution like this the thing we were saying is no matter what the distribution is as long as it's the same across the random variables it should be become normal when we take the means and so let's look at that indeed we see that this happens just one more example we'll pick a skewed distribution and it's centered in a bit of a different place but we still get an approximately normal distribution of the means okay so again the average of the IID random variables is normal so next we want to ask about what if we take the maximum there's a question sorry when you're saying the average adding the expectations so when we're averaging the IID random variables we just take whatever those variables end up being so if we have 10 of them we'll have 10 different numbers kind of like in the DI roll case we count it up to 10 actual numbers that we rolled and then we just took we would take the average of that to get the mean welcome yeah why are there how do you get multiple M then would you just get one from all the variables and Sample sorry that one more time why are there multiple would you just get one right right so you would just get one mean per one sample what I was doing in that function for some of it was I would take a group of five and just look at the mean of that and then take another group of five and look at the mean of that and that random variable namely the the means or sorry the mean that is normally distributed that's what we were observing let me give you some starers got one this way all right so we want to ask the same question about the maximum of several IID random VAR oh one more question I'm just a bit confused about what exactly becomes normally distributed right um so is it the sum of all the DI right it is the sum is also going to become normally distributed and that's what we see in that sum case this is also true for the average although it's a slightly different looking normal distribution so yeah like both of these lines are referring to two different things the first one the sum of IID random variables it does become normal also the average or the mean becomes normal as n grows multiple different trials exactly and then the averag is going to be that same thing divided by the number of Trials cool so for the maximum of IID random variables it actually turns out that just like with the sum and with the average we get no it's a gumball it's a gumball I don't make the rules if I did I wouldn't make them like this we're not going to have to worry about a gumball here but the point is there are a lot of different questions we can ask about several IID random variables and functions of them we can take a function like the sum function like the mean we can take a function like the Max and these are distributed in a way that we can measure measure but in particular we're focused on the normal one so the first two okay got another problem for you guys we're going to estimate a clock running time so we know the mean is or actually we don't know the mean we're just saying we're going to represent it by some value T we do know the variance of the running time it's going to be Sigma square equals 4 seconds so what we want to do is run an algorithm repeatedly and measure the time of the clock running for each trial the question that we want to ask is how many trials do we need such that whatever estimate of T that we get is within this range either 0.5 seconds below or5 seconds above with 95% certainty take a second to talk in your groups again a bit about how this might work spe all right let's pause here any initial thoughts what are you guys thinking as first directions write the mean using the central limit theorem cool write the mean using Central limit theorem so we know that the mean is going to be this this thing T and we're going to want to here we're going to want to say something about the certainty with which whatever sample or whatever mean we get for our runs is close to that so the way we can represent an event like this is we want to say with 0.95 probability that's our certainty estimate that we were going for before we want to say that the difference between the mean of the random variables that we get that's xar minus the true mean which we're calling T we want that it's within this interval and we know that the distribution of xar minus t follows this normal distribution why do we know that because we have that they have the same standard deviation but we're taking away the uh from the sample mean or from the mean of the random variables we're taking away the uh true mean and in expectation those should both be the same so we're going to have zero as our mean of this difference and so here we can just use what we know with the CDF where we take the upper bound apply the CDF there subtract off the lower bound we do our standard our standardization that we need to for normal distributions we end up with this a little bit more algebra later and we can solve directly for n so using the central limit theorem we're able to say exactly how many samples we need to see such that whatever estimate we get of the mean of this value T that we were looking at way back here is within 05 of the actual random variable so that's our exploration of CLT y x again sure absolutely so in the top right corner there it was said before that like the expectation of the mean of IID samples from this uh distribution namely the one we're talking about in this problem setup we're saying that the expectation of that is going to be T that's what we see in this line here and so if we subtract from this random variable this constant T it's not going to change to the it's not going to make any change to the variance because T is a constant but when we look at what the new expected value is instead of it being T it'll be T minus t which will be zero does that help good okay so now we're going to move into the world of statistics let's look at a few definitions here and we'll do this by motivating an example so you guys are into the seventh week of Cs 109 and so you get reached out to people by people for help people like the king of Bhutan now what the king of Bhutan wants is to get a measurement of the true mean and the true variance of happiness in the king citizens we can't ask everyone though it's a big enough country to where that's going to be a bit of an issue so instead what we have to do is ask 200 people something way less than the total population now let's be clear if we did ask everybody then we could just straight up answer the king's question we just take the average of what everybody says for their happiness we can calculate the variance as well and then we're good to go but instead we have a data set that looks something like this we'll have 200 points and if you want to calculate the mean of this just this example that we have here we're going to get something like 83 so so we could go to the king of bhan and be like okay means 83 we're done with the first part we're going to see that that's a little bit unfulfilling we're going to ask is that really a good way to say the true mean of Happiness among the bud people so let's look at a population here we could say this is all of Bhutan this is our sample of it and so this is what we're left with as another example to sort of get your intuition going for why we wouldn't want to ask everybody or there are some cases where it's not even possible consider you're a a watermelon farmer so if you're a watermelon farmer you need to check your watermelon to make sure that they're not rancid that they taste good all that kind of thing well if you check every single one of your watermelon then you're not going to have any left to sell so you have to decide how many you want to test so that you have a good idea of what your population looks like without having to sample every member of it okay so let's talk about what a sample is is mathematically speaking it's a collection of these random variables so if we had X1 through X200 that could be the random variables representing people's happiness in bhuton so this is specifically the unspecified random variables so we call this a sample from a distribution if they're all IID from that distribution and therefore they're going to have the same expectation and variance so this is a sample of size eight now a realization of a sample is when we start getting specific values when we go ask person one through person 200 and get their actual happiness scores that's when we get a realization of the sample of size 200 for the people who Bon okay so let's say we have one sample this 200 person sample that we were talking about so how should we report our estimated statistics do we just say the mean is 83 so we're going to report 83 in particular if we wanted to say something about the error the level of confidence we have in a region around some number how we do that and then how would we later do a thing that we'll call hypothesis testing that we'll talk more about on Wednesday so we don't know the population mean and the true population variance if we knew these things we would just report them back to the king there wouldn't be a problem the issue is we don't know what these variables are or what these parameters are and we want to have some disciplined way of estimating them using only 200 samples so we need to think about what the best estimate would be in the first place so first let's talk about estimating the mean so we've got n random variables X1 through xn they're all IID they all have the same distribution they all have the same expectation specifically me so how would we estimate this one intuitive way is just to say okay let's take the average of those values that's what we were talking about before that's how we got that number 83 in our specific example but how do we say whether or not we think this is good here's one way what if we could say that the expected value of our estimate actually matched up with the true population mean well we can check this here so here's the mean of the sample that's what is this is just expanding out that definition over here we're just moving that factor to the outside the one/ N since it's a constant to get to this next line we're using the linearity of expectations and we know the expectation of XI for all XI we were saying they're all the same across all the variables right so now we can sub in for the expectation of XI so now we're summing mu n times dividing it by n and we get mu on the outside so this is a powerful statement we're saying that we would expect this estimator we're using namely the mean of the sample that we're drawing to line up with the true mean of the population and we have a term for this we call this an unbiased estimator now unbiased means a lot of different things in different context so try not to overload this term but as a statistical term this is exactly what it means when the expectation of our statistic lines up exactly with the true parameter so even if we can't report the true parameter because we don't know it we can say with a reasonable level of certainty that the 83 we were talking about before isn't too bad just because it is unbiased so we could report something like this this is just the sample mean and then we can also ask about what the variance of happiness is among the bo these people so here's the population variance this should look fairly familiar from what we looked at variants before it's just the random variable minus the mean the square of that deviation and the expectation of that population mean there and so if we only have a sample we're going to do something kind of similar we might say something like the average of the deviations from the sample mean so this is the sample mean mirroring where the population mean was before and these big XI are the instances of our sample whereas the little XI are just all the members of the population so if we take a look here notice that instead of dividing by n where n is the number of samples we're dividing by n minus one and we'll talk about that a little bit more in a second so here's a graphic to sort of tell us what's going on let's say that this is our full population here and that the true mean of this population is Mu right at that line so what we're doing is we're taking for each member of the population the deviation from mu that's what this X1 or this x IUS mu is we take the square of that and we're summing that up we do that for all of the all of the different data points and so calculation calculating this Sigma squ is just doing this precisely for all the data points so what are we trying to do instead for the sample mean those yellowed regions those are going to represent the samples that we actually have we can't in the case of puton we can't talk to everybody about their happiness level and so instead of the sample mean or excuse me instead of the population mean we have a sample mean xar and it's from this that we're measuring the deviations so we're looking at the difference from X1 to xar and similarly for the rest of our sampled points so to get a little bit more intuition for why we might be dividing by n minus one instead of just n is that we need to have a little bit more variance given that the thing that we're comparing each of our sampled points to is also an estimate this xar that we have here we not we're guaranteed that it's an unbiased estimate of the true mean but we don't know that it's going to be the true mean so you can think of n minus one in the denominator as opposed to n as increasing level of variance that we have over this now this is another thing where showing that using n minus one in the denominator instead of n knowing that that's an unbiased estimator is important it's the same reason why we wanted to use U the mean before for the unbiased estimate of the population mean but as you can see the proof is a bit complicated so this is something just for reference feel free to go through it it shouldn't be too out of range if you wanted take a look but we know that the expectation of s^2 which is the sample variance when we Define it using 1 over n minus one is going to match with the true parameter so in that sense we say that the best estimate of Sigma squ is the sample variance okay how do know this is the unique unbiased estimator unique unbiased estimator so if we had something that wasn't or yeah if we had something that wasn't um the sample variance over there and we took the expectation of it and it matched up with the true parameter then that would also be unbiased by the def that we're using so this need not be good question another question what's up um I don't I think it's pretty circular to measure accur cacy of your thing by using the true thing cuz how are you going to get the true thing if you don't have the thing gotta got you yeah that would be circular for sure where is it that we're using the true thing in order to make our estimate are you talking about what this sample mean here yeah got you so the sample mean is just the thing we were doing before like with the dice we were just rolling the dice and then we looked at the sum there but we could take the mean there or we had the specific values that were reported for those 200 people for bhon so those aren't actually the true parameters we are looking at like real samples so there's some truth there but it's only a subset of the full population so if we were using the true population parameter namely mu over here then yeah we'd be in trouble because that's exactly what we don't know that's exactly what we're trying to calculate so that's a good thought here you go okay so we have our we have a report that we can give on the average happiness we have a report we can give on the variance and we have our formula for the sample variance okay let's do a quick check let's pair up for this again I'm going to throw six quantities or sorry six objects on the board and what I want you to do within your groups once again is assign for each of these 1 through six whether or not it's a random variable or a collection of random variables or a value or an event so go ahead and think about that together a little bit s all right let's talk about this first one what do we think is it a random variable a value or an evance I hear people saying value that is right that's exactly right yep mu just stands for some value okay for two we've got a sample there random variables value or event I'm hearing random variables yeah that's exactly right so each of these X1 through X8 are random variable that can take on a certain value we don't have uh we don't have the instantiation of these random variables yet okay so what's Sigma Square population variance value value exactly exactly same reason as for one and then what is xar random variable random variable also correct because we don't know exactly what it is without getting our instantiation of the sample like two now what's five what's going on there I'm hearing value I'm hearing event it's going to be event from four if we're saying that xar is a random variable then in five we're saying this is the event that this random variable namely sample me takes on value 83 and then lastly what's going on with six I'm hearing events again yep same thing as in two we have the random variables themselves and then we have uh the instantiations of them so this is going to be an event perfect okay well here's something to think about in this plot that we had here we're sort of reporting these numbers in a very exact way we're saying okay here's our sample mean using an unbiased estimator it's 83 and then here's an unbiased estimate of the variance it's 450 but even though the expectations line up we don't exactly know that we can be super confident in these numbers let me give you another example suppose instead of asking 200 people about their happiness suppose I ask just one well if I ask just one person at least for the the mean let's say they also they also report 83 then at least for the mean we're going to have the same mean in this case and in the case with 200 people but I think you guys would agree with me that intuitively when we're asking 200 different people and aggregating what they're saying that's something we can be a bit more confident about that the mean might actually be 83 as opposed to well we just happen to find one person with that happiness value so let's think a bit about that so we know that and this is from the the CLT for for the average of IID random variables we know that the mean or the sample mean is distributed according to this distribution we don't always know what these parameters are mu and sigma squ but we know that they sort of exist out there and then we've uh we see that yeah just using the the right side over there the variance of this random variable is going to be Sig Square n so now we can talk a bit about getting a range that we feel confident about so in particular we'll start with the standard error of the mean so the variance of our sample mean that's what VAR of xar is that's a measure of how close xar is to Mu so how do we make an estimate of this so we want to estimate this parameter on the right hand side now let's get something very clear here before when we were doing our un our unbiased estimate of the variance that was the variance of like of the sample excuse me yeah that was that the variance with respect to like the whole population now we're just W to ask about the variance of specifically the sample mean this random variable that we're talking about xar so we want to estimate that thing on the right hand side and we can do this using the standard error you can think of the standard error as yeah an estimate of the standard deviation of our estimate of what the mean is so since we said that s squ which is the sample variance is an unbiased estimate of that overall parameter Sigma squ which is the population variance well we can accordingly using the relationship between standard deviation and variance we can say that the sample variance Over N is going to be an unbiased estimate of uh Sigma s n which is exactly this value that we're wanting to estimate and it'll come through accordingly so here's this worked out a little bit more this is the substitution we're making where we're saying that the sample variance is an unbiased estimator of Sigma squ so this uh this equality sign here is a little bit misleading but we're sort of making the substitution for the sake of getting an estimate because we don't know this true parameter and so then we get the standard error of xar just like that taking the square root of our estimate and so these are the numbers that we get for the B poll in particular this will come out to about 1.5 so what does this mean we now have some measure of the spread of the estimate that we're giving so we're not just saying 83 instead we can say we're thinking 83 and we're going to give a range of about two we're being a little conservative here we're extending a bit outside of the 1.5 that we calculated and saying we're reasonably sure that it's going to be within a little bit more than one standard deviation and so we get a more precise statement than just saying like Okay 83 is the mean that we got y stand so let's take a look at uh let's go back a little bit more so when we have s s here interpret that similarly to how we looking at Sigma squ where it's more of a notational thing if we take the square root of it we can call it s but what that means is uh just taking the square root of that overall that overall function here so we could write this as like s overun of n if we wanted to but it's just a matter of notation those should both be the same thing does that make sense cool let me give you some Starburst okay so now let's think about how we do this for the variance what we've been able to say is we've been able to report three things now even though we were only asked for two we were asked to be we're asked to give the true population mean and the true population variance obviously we can't get those exactly otherwise we'd have to sample everybody so instead what we said is here's our sample mean and also here's a region over which we we can be kind of confident that it's correct and so we're trying to do something similar for variance this is a trickier thing and we're going to see more about this on Wednesday but for now let's take a look at another example of this so here are some of the pet timings for the questions that you guys have worked on so we have a list for the amount of seconds that it takes each person at least as they're writing out the answers for each problem on the P set so let's take a look at some code here we've got a function analyze and what we're wanting to do is something similar that we did for the people of bhuton except with respect to the pet timings so we can start this first line is just doing some cleaning here when we're what we're taking in is the question key and then the list of times and so we're just going to remove out all of the extraneous you know all of the unclean parts so all of the parts that have zeros which doesn't really make any sense so we're left with a more accurate list we can take the length of that list and now we can just in code do the kinds of things that we've been doing in the last few slides so first we can look at the sample mean which the unbiased estimator we were using was just taking the mean over the sum of all the items in the sample that's exactly what's going on here with np. mean and to be clear NP is the shorthand we're using for numpy next we can get the sample variance and the only weird thing here is going to be this DDF now this is a parameter for degrees of freedom and setting this equal to one is how we get the minus one in this denominator it's just a thing to remember if we set this to zero then that's going to give us the case where we have 1/n which is not or is not an unbiased estimator it is a biased estimator okay here we're getting the standard error which we got just using the formula that we had before we had the sample variance divid by n square root of that and then if we want the sample standard deviation that's just taking the square root of the sample variance and so now we have all of these items and if we want to Output their values we can do it like this for each question we have the mean number of seconds that we were looking at for somebody to solve it we have this plus minus in that first row it's the 83.8 that's giving us a range around which we can be pretty comtable on our estimate of the mean that's our standard error on the right hand side we have the standard deviation of time with respect to all of the samples that we're looking at not just the mean and we have this for all the questions and so we can plot that here so we have this for each question the a bars are exactly what we were looking at in the uh in the bhuton case and if we want to look at what's the expected amount of time for future that somebody would spend on a problem set well we can just take the expectation of the sum being the sum of the expectations for each problem and what we have on the right hand side is it looks like about 2.87 hours for Pet 1 pet 3 looks more like 5.11 hours so if you're budgeting say 50 hours for pets it looks like in this case we'd be pretty happily coming in under time and these are two questions that we were a bit surprised to see if we have access to all the data why are we estimated sure right so what we don't have is some true population over all students say so we could we could get population statistics for like everybody that's in this database but if we want to say something more General about like a population of students more broadly then this is an estimate of that okay so looking back at the Baton case we're w to get the standard deviation there or sorry the standard error there and we did that but we still have a question mark about getting some kind of standard error about the sample variance there's a question can you always use the sample mean as an unbiased estimator for the mean or are there some cases where it's like if if you say like chose your sample really poorly like for example in here like if you only um measured the happiness of like like parents in bhan and not U children or like in the cs1 or9 case like if you if there was some like correlation between students who hads then you just assume that your true mean was your was your sample mean even though it's not right this is a great point and and I'll try to I'll try to repeat what you were saying uh paraphrased a little bit it's like what if we're not doing a great job of sampling like what if there's some significant dependencies between like we're getting some subset of the population that's all actually very similar to each other I'm sure you guys have run across things like you know respond responders bias or something where like if you just sort of send a pull out to people you might not get a representative sample of the people that are most likely to respond things like that so for this question this is absolutely a good concern to have the way that we get past it in terms of writing things out in terms of what is and isn't an unbiased estimator has to do with that initial assumption we were making about our sample which is that each data point that we're looking at is IID from the overall distribution so if they're all identically distributed and independent from each other that's the kind of assumption and it is an assumption that we should check as we're doing data collection this is a an empirical problem that you guys will run into if you want to work in this area but in order to make that assumption or Sorry by making that assumption we Shield ourselves from worries like oh there's actually a lot of pretty strong dependencies between the people that we're getting samples from and things along those lines all right this is going to be end ious that was pretty good right yeah okay so if we want to start getting some estimate on the error that we have for our sample variance we're going to need to go into bootstrapping and we can talk about that more on Wednesday but for now let's talk a bit about the midterm that we had last week so here's the distribution that we ended up getting got some colors here to sort of bracket things in the groups just going along the xaxis increments of Five Points so to make things maybe a little bit more concrete this green region that we're looking at we're thinking like that's a B plus a minus kind of thing the blue is going to be like a minus a the purple is going to be a A+ and then we'll say that the yellow region is like around a b and so this is the this is the performance that we had as a class uh you can our statistics there for the mean and the variance if you want to go through the lecture slides and take a look at these correlations Between You Know How likely people are to do well on certain problems in relation to others and then we can just talk about a few things for the logistics here so again take a look at that information on the contest and beyond that we'll see you on Wednesday