we're live hey good afternoon everybody welcome back to cs109 how are you guys doing today oh fantastic I love to hear that I hope you guys are having a splendid Friday and you're looking forward to a great weekend everything okay uh we're waiting for the screen to pop up oh it's probably much easier for the screen to pop up if the HDMI is plugged in I believe that and there we have it okay thank you very much wonderful wonderful people from sapd okay a couple quick announcements before get started um problem set one has its due date today though there is a grace period so if you are still making some small tweaks that's okay problem set two though will go out today so if you're still working on problem set one it'll be you know have less time to work on this new problem set that's just gone out uh and then happy Friday I'll tell you a quick story and then for those of you who went to python review session one there's now python review session two and that will be on Zoom after class and also will be recorded in case you can't make it just want to talk really quickly about the python or sorry about the problem sets so problem set one was due here you have a grace period that lasts until tomorrow if for some reason you really really need an extension you can go all the way until next Wednesday but after next Wednesday we wouldn't accept anything because we've started grading and then you'll get your feedback next Friday so one week from when you submit it to when you get your feedback though of course hopefully everyone's done by now because then you can work on problem set too and problem set do is due on the 17th and in fact you basically know everything you need to know to do problem set two if you guys were really feeling it over this weekend you could be done with problem set too and really ahead of the game for cs109 I know that will be a low percentage of people but maybe that low percentage of people could be you and if it's not for you guys then you know it would be like rocking and rolling and just enjoying and learning everything we can about probability there's some really cool things in problem set too one of my favorite things that we've added is we're gonna make you program this Bayes medical test in every possible combination you have to write a general solution to Bayes and this is a relatively new thing in cs109 but the point is to make sure that you really understand this most fundamental concept that we've studied so far the hardest thing in the problem or one of the other ways that we're going to push you is we're going to make you think about Bayes theorem in a more generalized world than we've seen in lecture in lecture we talk about Bayes theorem where an event can happen or it can happen but we're going to push you in the problem set to think about well what if there's many different possible outcomes and here's one concrete example imagine you're tracking an object and it could be in any of these nine locations I can give you a prior belief about where the object is I can give you the probability of an observation that is made given each location so I can say you know here's the chance that your object's in this location I can tell you How likely is an observation like a distance to a satellite tower or something like that given each location and then using Bayes theorem you can update your belief about where the object is now Bayes theorem should just look like this you know what's your belief of a new location giving an observation you can just use your regular base theorem the interesting thing here is you know how do you deal with this denominator when your world isn't necessarily split into location five or not location five this denominator we've always expanded using the law of total probability but I just want to clue you guys into this idea that that law of total probability if your world is split into nine nice locations you can write the law of total probability like this for every location what's the probability of the observation and the location and that's just another way to get your probability of observation but just you know on this problem set we're going to give you an opportunity to extend Bayes theorem by programming it for all situations and also extend your knowledge of Base theorem by thinking about it in a context where there could be perhaps nine outcomes instead of just two yeah okay um fantastic there's a cool problem on the problem set that extends that bat genome question we brought up yesterday or on Wednesday's class it's going to require that you actually write some python off of the browser so if you don't already have Visual Studio code or some other programming environment you're going to want to do that for problem set too questions comments concerns about logistics I have a quick story for you I really like this story it takes place in cs109 not too long ago um and a student had come to my office hours a few times it was a student who had never seen any probability before cs109 and all this was very new to the student and by the end of class found this beautiful connection between what we've been studying and her life so she had been playing this game called Ultimate Frisbee and you guys probably have heard about it and did you guys know that when you start a game of Ultimate Frisbee there's this interesting game that's played the two captains go and they both flip a frisbee and then one of the captains while it's flipping claims heads or or evens or odds if the if the person says evens if they're either both up or both down then that team starts and if it's odd that means one's up and one's down and we were thinking about this in cs109 and it turns out this game is not fair and the person who figured out this student you know went on to take this concept got published in a magazine the ultimate frisbee Magazine made a YouTube video that became quite popular and I think it tells the story of probability is not just what we do the problem sets it's something that lives all over and if you can start to understand this language you can better understand all the different things that you might find interesting in your world and so just by taking 109 you know made some small Improvement in the world of probability uh and at the end of class just for some excitement we're going to do a little bit more gambling so just a little bit of a heads up we'll talk more about that when we get there to do some review we've talked about some of the most foundational ideas and probability and you guys have worked very very hard to make sure that you understand each of those Concepts and if you've made it to this point you have this beautiful Baseline and I just want to talk about what's on that in that foundation so we've got these core ideas and probabilities just some definitional understanding as a probability we have ways that we can think about conditional probabilities relations between conditional probability is probably a band we have relationship between conditional probabilities and probabilities without condition and we have relationships between conditional probability and conditional probably written the other direction and on Wednesdays we class we really focused on understanding how you can do probably if or either in the case where you have something that's mutually exclusive in which case it's easy you just add or if you don't have mutual exclusivity in which case you can use inclusion exclusion we also talked about how probability of and is made a lot easier by this other property called Independence there are two different properties if you have events that are independent probability of n becomes multiplication and if you have events that are not independent probably and um is chain rule now we have been implicitly using this thing called de Morgan sometimes to go between the probability of or and the probability of N and de Morgan's kind of says you know the probability of e or F if you take the not of that it becomes 1 minus the probability of E and F or sorry the not of that becomes a probability of e complement and F complement people often find this a little bit intuitive a lot of people have already been using it in the problems that we've talked about but if you find it hard to go between ores and ends you probably want to revisit de Morgan's which is something you would have covered uh in some of the set theory prior to cs109 to bring this all together I did want to start practicing and in fact I was going to have us prep by thinking about that problem that frisbee problem that I mentioned do you know when you flip a frisbee it's not equally likely to be heads or tails that's what makes this so interesting so I wanted you guys to imagine a world where if you flip a frisbee we're going to call the upside heads and the downside Tails uh and I'm going to say there's a 60 chance of getting that upside so if two frisbees are considered even if either both of them are heads or both of them are Tails what's the probability that if you flip two frisbees they're even and this is just a warm-up to bring together the foundation we've talked about talk to the person next to you see if you can figure this out what's the probability that they're both that the two frisbees are even okay let's do this together I'd say one of the Hidden skills in cs19 is can you take a word problem and can you define events and once you define events then can you express the problem in terms of those events and this one I think it's a lot easier if you can to solve this problem if you can see that there's two events whether or not you get a heads on frisbee one and whether or not you get ahead and frisbee too and then you can recognize that this question is asking either H1 and H2 or H1 complement and H2 complement I don't know do you guys find it a little bit easier once you defined events certainly this is a warm-up problem but if you get to like meteor problems defining events is absolutely critical now this or would be super easy if a certain property was helped what's that property yeah and and mutual exclusively means that they can't happen at the same time are these two things mutually exclusive ah fantastic why it was you can't both get tails and both get heads that's impossible uh these are two different um of their mutually exclusive events therefore the probably we care about is just going to be the sum of these two probabilities now let's look at one of these probably if H1 and H2 we're looking for an N so what's the what's the property we care about Independence is the first frisbee independent the second frisbee yeah we think frisbees are independent which means that this just becomes the probability of H1 times the probability of H2 and we know what the probability H1 is we're given that 0.6 and we're given that this is 0.6 for similar reasons over here this is going to be equal to 0.4 squared great actually just for fun this is like really not the sort of thing you'd have to invent on the fly but just for a good time what do you guys could you guys invent an algorithm that would be fair given that I'm going to give you frisbees they're equally likely to be heads you just don't know what the probability of heads is if I give you two frisbees they're equally heads you don't know what they're probably you don't you know like maybe it's 0.6 maybe it's 0.7 it'll probably have heads could you come up with a game that would be fair so this is an unfair way to start it turns out if you plug in any number that's not 0.5 you're more likely to win if you call evens can we come up with a game that's better so that doesn't quick clarifications before I throw you guys to just think about this with the person next to you guys up for a Friday challenge you don't have to but like see if you can maybe maybe you'll come up with something so um you have these two frisbees some game that would be actually Fair and then I'll tell you about it in in about a minute and a half so talk to the person next to you okay maybe you came up with this this is a cheeky problem but I find it kind of like a fun algorithmic challenge the answer there is an answer there might be a more efficient algorithm and if you guys came up with a more affectionate efficient algorithm you should certainly talk to me but here's a rather inefficient algorithm that is fair Loop forever you're like this is supposed to be done before you start the game bear with me you're gonna flip both Frisbees if they're odd that means one was a heads and one was a Tails Whoever has the heads wins so if you think about it what's your chance of winning well in the case where they're odd either you got a head tail or a tail heads and the probability of a head tail is p times 1 minus p and the probability tail head is p times 1 minus P so both these outcomes are just as likely now there is this problem problem where like there's also the outcome of them being even but in that case if it's even you've just repeat and no one's more likely than the other person to win you could get into a deep recursive proof If you find this interesting and certainly as I mentioned you know this could go on for a long time you could flip they're both the same you flip they're both the same you flip they're both the same you flip they're different and then it's over and that seems like a pretty inefficient algorithm though fair and if somebody thinks of a more efficient one that'd be so cool what does Tails mean so that means one Frisbee's up and one Frisbee's down so you kind of keep going until the frisbees are in different orientations and then whoever had the heads in that uh situation is the person who wins and you're just as likely to have this combination what you have heads the other person has Tails as uh you have tails the other person has heads as I said not really reasonable for me to make you invent algorithms on the spot but just to show you what work in this field could look like it's both doing probability but sometimes algorithms that uh work with probabilities yes with the heads out of the tube yes exactly and if they're even what you do start over yeah you just keep playing so you just keep playing until you enter the world where they're odds and in the world where they're odds um is just as likely that one person uh wins as the other what a wild time yeah do you so the probability if you enter the world where uh this is true so like when you stop you know that you're in either of these two conditions and so you can imagine the chance that one person wins is this probability divided by the probability that you end up in this world um and then if you do love toad probably just be this Plus 1 minus p you'd end up with something like this which would give you one-half so that's like the chance that you end up in this condition condition off the fact that you've already entered the world where uh they are odd and these are the two ways you can get odd skills exactly Tails heads or his tails so you can think about this like what's the probability person one wins condition off getting odds okay great and actually I wanted to do this because I know the earlier you start problem set 2 is a great marker of you're going to stay on track with this class and I thought what if I just did one of the problems in problem set two in class and now you guys have all started problem set too so who started problem set two you can all raise your hand because that's you just go right up this this is one of your questions one of the warm-up ones okay now there was this major key that I'd like to restate it's worth restating many times you know conditional probability is a new thing to get our heads around when you condition on something you enter a world where all the rules of probability still hold so you know I guess you can say for example if you consistently condition on E for example the Bayes theorem formula would still exist in the world where e is true so once e is happen Bayes theorem still ha it still occurs you can still talk about probably a given b as the probability of B given a notice how every formula here has a condition on E in that world where e is happen rules of probability still apply uh and yeah this is now the Bayes theorem cute no okay not cute um and then you know I'm starting today by pushing you guys a little bit uh and then we'll go the second part of today we'll just go into something very definitional so bear with me for just a moment I did want to talk about these two different concepts at the same time so we've learned conditional probability and we learned Independence but what happens when you put those two things together and it turns out quite a lot happens when you put them together let's go back to that cool program we ran on Wednesday do you guys remember we ran that program Wednesday we had all these genes in this trait and after we ran the program it said here's how we think the genes influence the trait well let's break this down a little bit here's the input that the program had to work with had examples of a hundred thousand bats for every bat you knew exactly the expression of the genes and whether or not a trait was expressed the easiest thing you could do if you want to try and understand how these genes influences trait is to look for Independence Independence is something we learned last time and here's how I did it the first thing I did is I took every single Gene and I checked does this Gene look like it's independent of the trait so to do that I would calculate what's the probability of seeing the trait and the Gene and that's just counting how many times did I see these two things at the same time divide my 100 000 bats and that'll give me a probability then I could say what's the probability of the trait on its own what's the probability gene on its own and I could multiply those two things together do you guys remember Independence if two things are independent it should be the case that the probability of each event multiplied together should be the same as the probability of the and really quickly let's do this do you think trait one is independent of G1 do you think it's independent Gene 2 do you think Gene three a little controversial though they're not exactly equal but it's day a thousand we haven't seen infinite bats so like because we've only seen a hundred thousand bats the fact that these two numbers are so close I'm gonna assume yeah they're looking pretty independent um how about the trait in gene four and the trading Gene five so the trait looks like it's independent of Gene 3 and Gene four that tells us so much it's pretty reasonable to assume that knowing these genes doesn't change your belief in whether or not the trait will be expressed so we kind of know that it's these genes that are changing beliefs of whether or not the trait is expressed we learn so much by just looking at Independence and for our my Advanced concept of the day I want to think could we go a step further and it turns out there is this thing a step further it's a One Step Beyond what we teach you in cs109 but it's something that would be like one of the first Concepts you learn if you took 228 or one of the more advanced classes is this idea of conditional dependence and I want you to know it exists even if you're not going to go so deep into it in 109. conditional Independence says can we look for Independence relationships in the case where I condition on another event so here I'm going to condition on I'm going to tell you what G2 is that G2 is expressed if I tell you that G2 is expressed then I can calculate the probability of T and G5 in the world where G2 is expressed and I calculated probably G in the world where Jeep choose expressed I can talk about the probability of G5 in the world where G2 is expressed you know to do this math computation is very easy I just take my hundred thousand examples and I only look at all the ones that are consistent with G2 being expressed so maybe there's only like 20 000 examples where G2 is expressed and I'm only looking at those 20 000. then I can say what's the probability of T in those 20 000 what's the probably G5 in the 20 000 and what's probably T and G5 do you guys see how condition on G2 T looks like it's independent of G5 was T independent of G5 before no a beautiful thing sometimes happens when you condition on an event it can change Independence relationships so in the world where I tell you what G2 is it can now become the case that t and G5 are independent of one another that's not the case if I didn't tell you what G2 was and some of you are like what how can this possibly be there's a few causal structures in the world that lead to exactly this and here's one of them G5 doesn't affect the trait directly G5 makes it more likely for G2 to be expressed and G2 is what influences the trait so if I tell you whether or not G2 is expressed G5 is not giving me any more information because G5 only changes my belief that G2 is expressed so g5's causal mechanism is by making this more likely which makes T more likely we're not going to go into the depth of all the different causal structures and probability you need to build a bigger Foundation before we go there but I did want to just bring into light what happens when we mix causality and or sorry conditioning and Independence technically conditional Independence looks like this if you condition on some event consistently e and f look conditionally independent if probably both of them happening in the world where G's happen is equal to probably e happening condition on G times the probability of f happening can you send G that's exactly what we observed before uh it is equivalent to this just like independence of E and F is similarly stated as probably of e given f equals probability of E and that's conditional Independence and it looks just like the law of Independence it's just that the relationship might not be the same between conditional Independence and Independence a question so if you have some condition that's applying to pretty much that kind of hurt me is throughout your own problem and you're essentially in the world that condition you can essentially like cancel out the condition and have things to work out oh all the laws of probability would work yeah so so one way that we sometimes Express this notationally just to restate your question because it was a good question if there is a world G that permeates your whole problem sometimes we don't write down conditioned on G and let me give you an example I roll my dice there's a whole bunch of things that have happened in this universe before the dice was roll you know it's like what's the chance of me getting a two conditioned on I had a big lunch today and there's all these things that are true the laws of probability still whole even though I've had a big lunch today but you know if if that event just permeates the whole problem maybe you don't have to write it um yeah sometimes these conditionings permeate the whole problem you don't need to be explicit about it and sometimes you have to be explicit because they matter Yeah question is this guaranteed it regularly that's such a good question I want everyone to hear that question I want you guys all to be wondering about that question if two things are conditionally independent does it tell you anything about whether or not they're independent when you're not conditioning on G this is the most bizarre thing no they're totally different things things can be independent and then when you condition on G they can look dependent again things can be totally dependent but then when you condition on some other event they can start to look independent it's because causality is such a crazy thing but there is one major key is just that Independence relationships can change with conditioning the laws of probabilities still hold when your condition is just this property might change between two events let me give you an example that explains why this is so useful if I may uh it comes from Netflix so your question was like what's why is uh conditional Independence so useful and it turns out it is a powerful thing to know that's why I want to bring it to your attention now so we talked a little bit about the probability that somebody likes this movie a lot I like called life is beautiful and we said you could calculate the problem if somebody likes this movie just by counting in the Netflix data set you have Netflix has a billion users and if a hundred thousand watch this movie you can say that probably somebody watches this movie is 100 000 over a billion there's better numbers now if you cared about a conditional probability like the somebody probably that somebody watches this movie condition on the fact that they watch another movie you could use the definition of conditional probability and that would when you reduce it come out to figure out how many people watch the movie F and figure out what subset of them watch both movies and if you look at that ratio that would give you your conditional probability okay probabilities with data exciting times you'll get to try this in problem set two but what if you condition on more than one movie what if there was say four movies or three movies that you're conditioning on like you want to figure out what's the probability that somebody watches life is beautiful but you want to condition on whether or not they've watched Nairobi Half-Life Coda and three idiots because Netflix is probably using a lot of information to update its belief on whether or not you'd watch a movie like life is beautiful there's a little bit of a problem first of all do you think E4 is independent of E1 E2 E3 I see some shaking heads that means no to me and the reason I think no is a very good answer to this is knowing these three should change my belief about E4 if somebody tells me that a user likes or watched this movie in this movie in this movie my belief is going to increase that they've watched movie for so there's reason to believe that they're not independent and because they're not independent it is not true that you can just summarize probably of E4 with ignoring E1 E2 E3 you actually have to think about this using full conditional probability okay so here's what you would do you'd figure out what's the probability that somebody watches all four movies divided by probably that they watch these three that's the definition of conditional probability we can use it right here fantastic uh problems gonna come up though hmm this is my problem my problem is let's say I want a condition on somebody watching 30 movies I have a counting problem for you let's say there's 13 000 titles on Netflix a user watches 30 random titles e is the event that they specifically watched the four I care about so everybody watches 30 movies how many people actually watch the exact four I care about and that's something we can calculate using calc probability so a little accounting problem for you guys something to talk about with the person next to you should use probably equally likely outcomes how would you define your event space your sample space and then we can come up with this small small probability okay we'll go back to talking about this but I wanted to take a moment and see if somebody could help me out with the sample space see if we can get on the same page with that but what's the sample space that you guys use we got 13 000 shoes here so 13 000 choose Thirty every outcome here is a choice of 30 movies for uh for out of the 13 000 movies that are possible and we're gonna say each of those outcomes is equally likely because every movie is just as likely as the other ones these 30 movies don't necessarily contain the four we care about how many ways can we get exactly before we care about that's the event space okay take 30 more seconds think about a little bit more and then we will come up with the answer together okay okay I have a two-step procedure for you guys every single outcome here includes all the ways you could choose 30 movies but I want to find the subset of outcomes where they definitely have those four I care about so here's a two-step process for constructing an outcome that has the four I care about first put the four you care about into the set we've Chosen and next choose 26 other movies from the 12 996 remaining so this will give you the probability of seeing exactly the four movies you care about and unfortunately if you were to bring this to a calculator the probability of somebody watching exactly those four movies becomes incredibly small 10 to the negative 11th now I assume that people are equally likely to watch movies and we know people have a little bit more pattern to that but this does present a problem for us if you're conditioning on a whole bunch of movies you could very quickly enter the space where you need to do a probability of an event that looks pretty rare Netflix has this really brilliant way of leveraging conditional Independence to make this calculation a lot easier what they do is they say there is another variable there's another event and that event is this user liking foreign emotional comedies and they say it's this event that's really governing whether or not people like these four movies and then they make an assumption it's a wrong assumption but it's a very very helpful assumption they say the probability that you like this movie is conditionally dependent of you liking this other movie If I tell you that you like foreign emotional dramas the information is all coming through whether or not you have this preference and once I know if you have that preference then whether or not you like these movies start to look independent they make this crazy in conditional Independence assumption and once you make this crazy Independence assumption then the problem of E4 given all of these things and also given K1 is equal to the probability of E4 given K1 their job has become much easier they just have to infer whether or not you like this idea of a film and then they're going to base their belief of whether or not you like a particular movie within that genre on that other event that they defined super advanced stuff but I just kind of want to tease you as to where this stuff goes and to give you a little bit of a a further explanation of why I think this stuff is cool conditional Independence makes practical decomposing hard probability questions um it is true somebody said if two things are dependent they might not be dependent when you condition two things are independent they might not be independent when you condition and this idea of putting together conditioning Independence Judea Pearl who won the 2011 Turing award said exploiting conditional dependence to generate fast probably probabilistic computations is one of the main contributions CS has made to probability Theory so conditional Independence is just like Independence it's just in the world where you condition on something and the only gotcha is that Independence and conditional Independence aren't necessarily the same thing you know when you condition on events that relationship can change uh and then this fact that you can use Independence in the conditional world can make some computations way way easier so hopefully at this point I've motivated you're like okay I appreciate conditional Independence isn't just two random combinations ideas uh is something that might be worth feeling more comfortable with okay so that was my Advance that's one of the more advanced things that I'll talk about today I'm going to give you guys a pedagogical pause quite early and then we're going to go to our next cs109 episode see if you can summarize what we've talked about today before we learn about our next concept we're going to learn about random variables what a good time so take two minutes and then we will continue our journey okay let's bring it back together and I just want to point out a cool thing look I know I've thrown so much at you I Gave You Like Love told prob that I gave you Bay is theorem I gave you the definition of conditional Independence we talked about independence and mutual exclusivity so you probably or probably and and that's a whole bunch but what you guys have done now is you've come to this really important place in probability understanding you have touched what I would call Core probability you're at this interesting part in cs109 where you have seen all the fundamental pieces and now we're just going to start using those fundamental pieces to do ever more interesting things the first interesting thing we're going to do is we're going to start defining families of patterns with these core principles that we're going to call random variables and once you understand random variables that's going to be our basis for AI you guys have made a lot of progress I hope you feel good I know it's been confusing for a lot of people because it's a hard thing to get your head around but good on you for putting in all that hard work to get here the next cs109 episode is about random variables and this is not going to be as conceptually intense as thinking about conditional Independence it is instead just going to be new terminology this is like the chat that us probability experts use you're going to be learning a new lexicon for talking about things you already know I want to set out a couple learning goals for today by the end of today I want you to be able to define a random variable I want you to be able to use and produce this thing we call a probability Mass function and I want you to be able to calculate expectation this is a pretty reasonable set of things that we're going to be able to do in the next 40 minutes I'm going to start by introducing to this idea of random variables by going back to the days of defining variables in code and this is what it looks like to Define variable in C plus plus or Java and when you define a variable in code what a powerful idea imagine you're coding life before you have variables and your coding life after you have variables you know technically you could probably do a lot of those programs without any variables but they'd just be very very messy actually no not possible a variable in code what does it have it has a type you know maybe it's integer maybe it's a Boolean has a name and it has a value and it allows you to put data and think about it in a principled way here's another variable this has a different type it's a double has a different name and it has a different value there's a whole bunch of different types that you can have of variables and they're a very practical idea the one thing you don't see so explicitly in code that maybe would be really nice would be just saying I have a variable and it can take on just exactly one of these values probability has an analogy to a programming variable but it's slightly different in that we're going to be thinking about variables they're going to take on values we just don't know what value is going to take on yet and we think about there being some uncertainty as the value it takes on so for example you can have an integer and that integer could be like the number of pirate ships in our future Armada I was like going through a pirate thing when I wrote this slide just forgive all the pirate examples and you can say I have a random variable it's going to take on the values 1 2 we're definitely going to have at least one vote one two up to ten and it will be a value it will take on a number we just don't know at this point exactly what that number is and we think there's some Randomness that governs that number similarly you can say the amount of money we get after we defeat Blackbeard that's going to be maybe a double or you know continuous number and in probability we would name that as a random variable it's not a regular variable because we don't yet know how much money we're going to get and we think that there's maybe some probability distribution over all the values you could get and then you know probability random variables can also have the variable type of something like a true or false there's this idea of Okay C is going to be one if you successfully raid Ila de muerta and zero otherwise and then you can say C it's a variable it will take on the value 0 or 1. we just don't know yet if it will take on zero or one we think there's some uncertainty so so it's just going to be new terminology for something that we already for formulas that we've already seen so random variable variable it's going to have a value and there's just uncertainty as to what that value is more examples if I say I have three or I flip a coin three times I could Define a random variable to be the number of heads on the three coin flips Y is a random variable will it take on a number yeah eventually after I flip the coins do we know which number it'll take on no not yet we do know it's going to be either zero one two or three and there's a probability that it takes on zero there's going to be a probability that takes on One it probably takes on two and it probably takes on three so just like a random variable there's just uncertainty as to what value takes on to be clear we could actually count that probably takes on different values we can say what's the probability that it's zero well that means you got a Tails tail sales what's the probability that you get a one so that y random variable could take on the value of zero it could take on the value one that means you got one heads which is means you either gotten the first flip the second Flip or the third flip you could get two heads which means you need head Tails or head Tails Tails or tails head head or you could even get three heads and there's a one out of eight chance of that so random variable is just going to be a new name for a variable for whom we don't know its value yet the probability that it takes on some number greater than four is zero you can talk about the probability that takes on a negative number but that's also zero we think of it only as having non-negative probabilities for zero one two and three I think there's just one confusing thing that I'd like you guys to separate I've just introduced you to this world of random variables it's going to be a nice happy world but the thing I want you to distinguish and contrast it against is the world of events because random variables and events are different things um okay and we'll just keep that contrast in your mind now on Wednesday's class we talked about what happens if your coin doesn't come up with equal likelihood of being heads or tails so if you say you have five coin flips each is independently coming up heads with a probability of P you know from Wednesday's class we figured out that this is going to be five choose two so choose all the ways that you can put your two heads and then the outcome of each of those events is going to be the head probably the heads squared because there's two of them and the probability of Tails cubed because there's three of them if you don't remember how we got there there's that wonderful example in the course reader and it's certainly worth revisiting until you feel comfortable with the probability of exactly two heads when your coin isn't 50 50. um so if your wise number of heads on five coin flips and P is the probability of getting a heads then you can get a probability function that looks a little bit like this okay what are the three things the three most important things you can say about a random variable is somebody can give you a function somebody a very kind soul might tell you here's the probability of every outcome of a random variable and on some level this is one of the main functions of a random variable a main function of a random variable is somebody can give you a whole bunch of probabilities at once they can maybe tell you the probability of any assignment to the random variable we call that the probability Mass function when you have a random variable you can also calculate these statistics that you've probably heard about before like expectation and variance but we need to have this idea of a random variable before we can do those and our learning goals for today is really to master these first two steps so let's jump into it let's talk about this probability Mass function the relationship between the random variables values they can take on and the chance of it occurring is a function let me convince you of this here is a random variable for example this random variable could be the number of heads in five coin flips this is an event and actually I'm going to go back and forth between these two one more time because I want you to see the difference this is a random variable it's the number of outcomes on five coin flips it's not an event but if you say the number of outcomes heads on Five Point flips is two that's an event in fact any Boolean operation you do with a random variable will make an event like if I say Y is less than 2 that's an event the number of out heads being less than two that's an event but the number of outcomes on five coin flips it's not an event it's more like an experiment so maybe I want to pause here and take a question so Random variables they're variables that can take on values with probabilities and they're different than events you can go between them by asking a Boolean question of your random variable so that was the random variable this is an event any questions about this events are always Boolean you made a good observation every time probably somebody talks about an event it's either true or false it either happens or it doesn't happen that's a good observation yes exactly the probability of a random variable yeah and that's a good question turns out yes and probability of these things are different than the events themselves and you raise a good point you can ask the probability of event but you can't ask the probability of a random variable on its own so it makes sense to ask what's the probability that Y is less than two but it doesn't make sense to ask what's the probability of Y you can say what's the probability that y equals zero you can say what's probably y equals two you can say what's the problem that Y is greater than 5 but you can't say just probability of Y on its own probabilities always want to be over events that was a fantastic question okay you guys ready to go up a notch so if y equals 2 is an event then if you put two in this is going to be a probability a number between zero and one are you guys falling I'm just trying to construct this up slowly Y is a random variable y equals 2 is an event and then probably the Y equals 2 will be a number between 0 and 1. final idea if you let this be a variable what you now have is a function if you put in different values for K you should get out different probabilities if you put in one you should get probably that y equals one if you put in two you should get probably the Y equals two this is mathematically what we call a function you can think about functions in lots of ways one way is you can think of it as just being an input outreal relationship you can put k equals 5 here and you should get out of probability you can also think about functions the programming way you know if this is a function you could write it up as code and it could be a function and the parameter is K if you pass in a 1 this function should give you back the probability that y equals one if you pass in a two it should give you back the probability that y equals two okay you buy me with this function idea yes just to be clear so Y is a random variable in case just variable yeah exactly that's a good good question why is a random variable so we think about taking on values with different probabilities and K is just a like a variable in programming that you can just put in as a parameter and those are two different concepts we use a lower case for like the programming variable concept and uppercase for the random variable concept that's such a great question okay so if a random variable is discrete and by discrete I mean it takes on Integer values or whole numbers uh we call this function the probability Mass function it's possibly the most important thing to know for a random variable so if I say of a random variable the first thing you should wonder is what's its probability Mass function what's the relationship between all the values it could take on and the probabilities there's some shorthands for it some people write this some people write this in cs109 I'll almost always try and write the full event out you know this feels a little bit redundant you've got what's the probability that this random variable takes on the value of this programming variable and then some people are like well because they're both called X I could just skip the random variable and just write this shorthand you might see that in the probability paper now let me show you some probability Mass functions again since they're functions I can represent them as code I can represent them as equations I can also represent them as just drawings right I can give you a graph that gives a relationship between all the inputs of the function and the outputs here is a probably Mass function for rolling a single dice what are all the different outcomes of rolling a single dice you could get a one you get two you get a three you get a four you get five you get a six and this most important definition the relationship between those numbers and their probabilities is called the probability Mass function and in this case all of those inputs lead to a probability of 1 6. if you roll a dice and X is the outcome of your dice you probably have X taking on any value is going to be 1 6. not the most exciting example but I'm really just trying to get you into this new terminology that we're going to build on as we go towards artificial intelligence let's do something a little bit more exciting let's say x is the sum of two dice rolls we've talked a lot about two dice rolls right we've talked about how you could calculate the probably the two dice rolls is equal to two or three or four the probability Mass function is going to put all that information into one place so you don't have to individually calculate the probably the two dice roll is a seven you could just go to the probably Mass function and look it up and know that for 7 is going to be 6 over 36. the probability Mass function could be given to you like this it could also be given to you like that any way you could Define a function programming math a graph all those are valid ways of giving somebody the relationship between the values a random variable can take on and the probabilities I'll pause here for a second because that's kind of cool also you're like whoa that looks like a triangle awesome yes question what about like when the random variable isn't discrete like with the blackbeards um yeah we will get to non-discreet in about a week but for now we're just being the happy world where random variables are discrete which means all their values uh are whole numbers okay fantastic does this I'm I hope I've put enough energy into explaining this that you're like this feels kind of obvious knowing that's the I guess the original probability of rolling a seven would yeah it's a good point it's giving it to all like imagine somebody gave you this programming function it's just somebody's done the work for you it's like if I've done the work of figuring out the probability of all these different assignments to X probably you can think of probably Mass functions away from me encapsulate that work and hand it over to another person so it's allowing you to know that the probability of two dice roll being seven is six over 36 without having to do that work and you can imagine this will be nicer later when we when we derive really complicated probabilities it'll be nice to be able to put them into packages and then use them later on say your midterm so your midterm you want to use a really complicated process but you're like oh earlier in class we derived it we put into a random variable I can just refer to that random variable and directly use this probably Mass function so it's going to allow us to build off the work of others help good good good fantastic question okay I do want to make you think about something imagine those two dice rolls I'm going to sum over all values okay so 2 3 4 5 6 K are all the outcomes of the sum of two dice if I sum over each of those and add the probably that y takes on a particular value what is this sum going to come to talk to the person next to you because this is important this is fine check to make sure that you guys are following along and to push you also to think about what this means okay if you didn't get this this is fine but it certainly an important thing to know I'm asking take this probability add it to this probability added to this probability add to this probably sum over all the K's look up the probabilities and add those all together and my question is what is that sum going to be did anyone come up with an idea yes foreign yeah I'm kind of asking what's the probability of the sample space and that was one of the axons of probabilities because this is like the probability of or you're adding so it's like talking about the ore of these mutually exclusive events are they mutually exclusive can you get a sum of two and a sum of three at the same time no they're definitely mutually exclusive so this is the or is it probably that y equals two or y equals three or y equals four and if you think about that or it's saying what's the probability of any outcome and the probability of any outcome should be one so if you have a proper random variable if you sum over all the different inputs to the probability Mass function you should get a one okay fantastic we have done one of those two things that's it probably Mass functions are simply somebody representing the relationship between the value random variable can take on and its probability we now just need to learn this next concept it turns out you have a random variable there's things you can do to summarize that random variable and one of the classic things is you can give somebody an expectation and expectation is something you've probably heard called a lot of different names you might have heard it called the mean the weighted average the center of mass the first moment um most likely the mean and it's saying if this is your random variable kind of what's the most Central outcome here it summarizes your whole random variable in one statistic it's a lossy statistic but it's one that a lot of people find interesting before we talk about how we can use it let's talk about how it's defined it says take your random variable let's Loop over all the values that can take on Little X and talk about the probability that x equals Little X so you know if I had to write this in our notation I would make this probability that random variable X on the value Little X and what you saw in the slides is just shorthand for this because people get bored of writing X's so much it says take on for every value take on the value the probability that would take on that value but we're going to multiply it by the value itself so it's saying you know it can take on a one with this value a 2 with this value and we're going to take one times its probability add it to two times it's probably and add to three times its probability and that gives us the number that some people find meaningful let's just start with our favorite good old friends when we're trying to understand a concept dice here's my dice it was a boring random variable X is the boring random variable that's the outcome of the dice but we can start asking this question what's the expectation of x the probability that x equals one is the same as probably that equals two the same as probably that equals six all of those outcomes are 1 6. the probably Mass function was pretty straightforward if we want to calculate the expectation that's that statistic we can calculate our random variable we'd say take one multiply it by its probability add that to 2 multiplied by its value probability add that to three multiply by is probability four multiplied by its probability and up to six multiplied by its probability if there's other outcomes you would include them if you do all of this you don't get a function you get a number an expectation will always give you a number and this came it gave us 3.5 is 3.5 the most likely outcome of rolling a dice like what no you can't roll a 3.5 on dice you can roll a three you can run all four um but you can imagine this is kind of like the center outcome like the most Central outcome it's the average between three and four it's the value that's closest to each of the outcomes weighted by their probabilities so if one of these was really really likely it would pull this number closer to one if seven was really really or six was really really likely it would pull the outcome closer to six so it's just a weighted average of the outcomes practice let's say Y is a random variable it can take on three values one two or three if somebody asks you count the expectation can you do this even if I haven't told you anything more about the random variable like what it semantically means if I just gave you the probability Mass function written out as three statements can you figure out the expectation absolutely just take each of those outcomes and multiply it by its probability so this is what expectation looks like in a slightly more General case okay make fun with this there's three kinds of lies lies down lies and statistics so when you summarize a random variable by one number you lose a lot of information people use this to manipulate other people let me give you an example universities they try and make it sound like all their classes are super small and this is how they do it every University reports its average class size but what they report to you is they report the expectation of class side if you were to choose a random class imagine a school with three classes one class is five students one class is ten students and one class has 150 students to calculate the statistic that universities report they imagine this thing they say randomly choose a class with equal probability X is the size of the chosen class what is the expectation of x and to calculate this they say well there's three outcomes of this the size of X it could be 5 10 or 150 the chances five is one-third because there's a one-third chance you got that class the chance of 10 is one third and a chance of 150 is one-third and even though this class has or this university has a class of 150 students and most people are probably going to be in that one they report that their average class size is only 55. convenient okay let's be a little bit more cheeky let's say you randomly choose a student with equal probability and now I want to know why is the size of the class that the students in this gives you a totally different number and actually just let's think about it for a second we don't have to talk about Interest next to you but like think about how you would calculate this what's the expectation of why now the first thing you should ask yourself or what are the values that y can take on and it turns out y can stay still only take on the values 5 10 or 150. if you randomly pick a student and assuming they're only in one of those three classes you know they could be in the class of size five they can be in a class of size 10 they can be in class of size 150. but what's the probability of each of those outcomes the probability that you pick the student that's in the class of size five is five divided by the number of students in this University 165. each person is only in one class the chance that you chose a student who's in the class of size 10 is 10 over 165 and the chance that you chose the person who's in the class of 150 is 150 by 165. so if you ask this question you know what's the expectation of the size of a class of a randomly chosen student now is 137. expectations are a powerful tool one of the reasons that they're powerful is they have these very very amazing properties but I do also want you to know that a lot of people give you misleading information using expectation expectation summarizes a whole random variable in one number if I want a random variable given to me I want somebody give me the whole probably Mass function tell me the probability of each of your outcomes that's what I really care about I don't just want a number that summarizes into one value so the probability Mass function full information expectation dumbs it down into one number now that one number is very useful to computer scientists though because it has some amazing properties it turns out if you do a lot of Transformations or weird manipulations to a random variable like maybe X is the number of heads on a coin flip but you're playing a game where you're going to take that multiply it by number and add another number to get the amount of profit you have if you want to know the expectation of your profit it turns out you could calculate it just by knowing the expectation of the underlying random variable of number of heads on your coin flips so if you take any linear transformation of a random variable multiply it by a normal variable add in another normal variable you would still get something that you could calculate without doing too much extra math uh this is another amazing one now here I was adding a random variable to a non-random variable so just like a constant but it turns out if you have two random variables and you want to talk about the sum of those two random variables no matter if they're independent or not no matter if they're mutually exclusive or not the expectation of the sum of random variables is always the sum of the expectations this is amazing so if I say this is the number of heads I get if I flip 10 coins and this is the number of sixes I get if I roll two dice and if I want to talk about you know the sum of those two numbers the expectation of the sum of the numbers is equal to the expectation of the number of heads on my 10 flips plus the number of sixes on my 10 dice rolls so if you want to do the expectation of sums you have a very nice decomposition doesn't matter if they're independent or whatnot and this is something we'll use in proofs I'm going to talk about a lot more once we use it but I'm just going to list it now we'll talk about it later okay so that's it we have probably Mass functions we have expectations that's what I wanted you guys to be comfortable with in terms of random variables going out of today's class wonderful in that case it's time to play a little game we're going to play a game and it's going to involve random variables and expectations and it's a classic game that's kind of interesting to think about let me give you the setup I have a fair coin I got this thing off the internet but I flipped it a whole bunch I'm pretty sure it's fair by fair I mean it's equally likely to be heads or tails it's not one of those frisbee things I'm going to flip this coin until it comes up heads all right sorry let me see sorry I'm going to flip this coin until it comes up Tails so maybe I get a heads I flip it again I get a heads I flip it again I get a heads I flip it again I get tails the game is over we're gonna say n is the number of coin flips before my first Tales so it's how many times I flip the coin um and then I'm going to win 2 to the power of n dollars my question for you is how much would you pay to play and just my hint is people often ask answer this question by thinking about your expected winnings how much do you expect to win if you played this game and you would if the cost to play the game is less than your expected winnings you should probably pay it so think about expectation in this harder situation random variable is how much you win what are the different outcomes you could win two to the one you could win two to the squared you can win two to the third but what's the probability of each of those because you have to weight each of those outcomes by its probability add it up and then you get your expected winnings I'm gonna give you guys a second to think about it I want you to answer the question how much would you pay to play and then we're actually going to play talk to the person next to you make friends oh you guys want to know okay this is independent of the calculation you got is there anyone who would pay play this game with me for a thousand dollars in theory or in practice okay okay no no I mean in practice like you're gonna step up and we're gonna play this for a thousand dollars once I've never had somebody take me up on it I got a little sweaty there but on in theory it was a good decision right why was it such a good decision because this really pushes the definition of expectation to its limit now I do want to clarify one thing I think there's two ways of reading this if you get like a tails on your first flip do you get two to the power of one dollars you get two to the power of zero dollars depends how you think of before so some people say this is going to be the number of heads is how many is n and some people think number of coin flips is n it doesn't matter how you interpret it they both lead to the same answer okay solution Define a random variable just like we used to define events we're going to now get used to defining random variables if somebody asks you an expectation question the first thing you should do is Define the random variable that they're asking you the expectation over so X is your winnings this is the version where if you get a tail on your first coin flip you win two to the zero dollars if you get a heads than a tail you win two to the first sorry heads and tails you get you win two dollars if you get heads Heads Tails You Win two squared dollars you get heads Heads head sales you get two to the Third now what's the probability of getting a tails on your first coin flip that's one half what's the chance of getting a heads than a tails one half times one-half what's the chance of getting heads Heads Tails one half to the third heads heads Heads tails you guys get the point and you see this nice little pattern I could write this formula into this beautiful little equation I could say it's the sum from zero to Infinity of one-half to the power of I plus one times two to the power of I and that's just taking it's like oh this this term has I and I plus one and this one has I and I plus one and if you work this out I guess it really is important that well okay you would get even more money if I gave you the extra two dollars for the heads in this case it's going to be or how can we reduce this you know 2 to the I divide by 2 to the I Plus 1. is just going to be equal to 1 over 2. and now that whole term becomes the sum from 0 to Infinity of one-half and that's Infinity your theoretical winnings of this game is infinite because even though it might not be that likely that you win ten thousand dollars there's also chance that you win like 10 trillion dollars or like 20 trillion dollars or like a gazillion infinite dollars and this totally breaks this idea of expectation maybe it's another way to talk about how expectation isn't always the best summary of a random variable but then I offer people this choice I say will you play this for a thousand dollars and I've been teaching 109 for a while and this is the closest we've ever gotten to somebody taking me up on it and it's because even though in theory your expected wings are Infinity it feels too risky and I have a very concrete way of articulating the risk Chris doesn't have infinite dollars if you play this game but you win 65 536 you know what happens I don't stay here and pay you I split I just run I just leave the country you never see me you never get your winnings so really like if you won ten thousand dollars I might be like okay fair game and if you win twenty thousand dollars I'd be like fair game but you win enough money and you're never ever seeing me and you should take this into account when you think about whether or not you want to play this game so same equation that's your chance of winning to uh one dollar this is your chance of winning two dollars transfer winning three dollars in the same equation it's just that this doesn't go from zero to Infinity it's just going to go up to how many heads lead to sixty five thousand dollars and the answer to that is 16. like if you're up here getting 15 14 heads I'm like okay 15 heads I'm booking my flight 16 heads I'm gone and if you add one half to itself 16 times you are not very close to Infinity you are eight point five dollars so I would say that this game is worth about eight point five dollars to play not infinite what a way to think about expectations I know I've pushed the idea of expectation pretty hard but hopefully that gives you a solid understanding of expectation going into the weekend and hope you feel about good about the learning goals and I hope you guys have a great weekend enjoy you've worked very hard see you back on class on Monday