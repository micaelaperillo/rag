hey morning everyone welcome back um so last week you heard about uh logistic regression and um uh generalized linear models and it turns out all of the learning algorithms we've been learning about so far are called discriminative learning algorithms which is one big bucket of learning algorithms and today um what i' like to do is share with you how generative learning algorithms work um in particular you learn about Gan discre analysis so by the end of the day you know how to implement this and it turns out that uh compared to say logistic regression for classification GDA is actually a um simpler and maybe more computationally efficient algorithm to implement in some cases so um and it sometimes works better if you have uh very small data sets sometimes of some cavas um and it will talk about comparison between generative learning albums which is a new class of albums you hear about today versus The sctive Learning algorithms and then we'll talk about naive Bas and how you could use that to uh build a span filter for example okay so um we'll use bin classification as the motivating example for today and um if you have a data set that looks like this with two classes then what a discriminative learning algorithm like logistic regression would do is use GR descent to search for a line that separates the positive negative examples right so if you randomly randomly initialize parameters maybe starts with some Digis boundary like that and over the course of grade and descent you know the line migrates or evolves until you get maybe a line like that that separates the positive and negative examples and um uh logistic regression is really searching for a line searching for desision boundary that separates the positive and negative examples um and so if this was the uh malignant tumors and the benign tumors example right uh that's that's what logistic regression would do now there's a different class of algorithm which isn't searching for this separation which isn't trying to maximize the likelihood that you the way you saw last week which is um here's an alternative this is called a generative learning algorithm which is rather than looking at two classes and trying to find a separation instead the algorithm is going to look at the classes one at a time first we'll look at all of the malignant tumors right in the cancer example and try to build a model for what malignant tumus looks like so you might say oh looks like all the malignant tumus um roughly all the malignant tumors roughly live in that ellipse and then you look at all the benign tumors in isolation and say oh it looks like all the benign tumors roughly live in that ellipse and then at classification time time if there's a new you know patient in your office with those features uh it would then look at this new patient and compare it to the malignant tumor model compare it to the benign tumor model and then say in this case oh looks like this one looks a lot more like the benign tumors I had previously seen so I'm going to classify that as a benign tumor okay so um rather than uh looking at both classes simultaneous and searching for a way to separate them a generative learning algorithm uh instead builds a model of what each of the classes looks like kind of almost in isolation with some details we'll learn about later and then at test time uh it evaluates a new example against the benign model evaluates against the malignant model and tries to see which of the two models it matches more closely against so let's formalize this um a discriminative learning algorithm learns P of Y given X right um or uh or or it learns um right some mapping from X to Y directly you know learn or you can learn I think on and brief talked about the perception Al we talk about support vect machines later um we learns the function mapping from X to the labels directly so that's a discriminative learning algorithm you're trying to discriminate between positive and negative classes in contrast a generative learning algorithm it learns P of um x given y so this says what are the features like given the class right so um instead of P of Y given X we're going to learn P of x given y so in other words given that the tumor is malignant what are the features likely going to be like or given the tumor is benign what are the features X going to be like okay um and then there and then there also generative learning algorithm will also learn P of Y so this is a this is also called the class prior to this probability I guess right it's called a class prior it's just when a patient walks into your office before you've even examined them before you even seen them what are the odds that they tumorous malignant versus benign right before you see any features okay and so using Bas rule if you can build a model for p of x given Y and for p of Y um if you know if you can calculate numbers for both of these quantities then using base rule when you have a new Test example with features X you can then calculate the chance of Y being equal to one as this right uh where P of x by the okay um and so if you've learned this term if you have x given y then you can plug that in here and if you've also learned this term P of Y you can plug that in here right um so oh P of X in the denominators goes in the denominator okay so if you learned both both of those terms in the red square and in the orange Square you could plug it into all of those terms and therefore use a base rule to calculate P of y equals 1 given X so given a new patient with features X you could use this formula to calculate what's the chance that the tumor is malignant if you've estimated you know these these two quantities in the red and in the orange circles okay so um that's the framework we're use to build generative learning algorithms and in fact today you see two examples of generative learning algorithms uh one for continuous value features uh which can use for things like the tumor classification and one for uh discrete features which uh you can use for building like an email span filter right or or I don't know or if you want to download Twitter things and see how positive or negative the sentiment on Twitter is or something right you so have a natural language processing example later so um let's talk about calcian distrib analysis G um so uh let's develop this model assuming that the features X are continuous valued and when we develop um generative learning algorithms I'm going to use x and RN so you know I'm going to drop the x0 equals 1 convention right so I'm not going to we're not going to need that extra xal 1 so X is now RN rather than RN + 1 and the key assumption in gaussian discre analysis is we're going to assume that P of x given Y is distributed Gan right in other words condition on the tumor is being malignant the distribution of the features is Galan you know the feature is like size of the size of the tumor the the cell adhesion whatever features you use to measure a tumor um and condition on it being benign the distribution is also so Gan so um how many of you are familiar with a multivar gan raise your hand if you are like half of you one3 no two fifths okay cool all right oh how many of you are familiar with a uni varia like a single dimensional Gan okay cool almost everyone all right cool so let me let me go through what is a multivar gan distribution so the Gan is this familiar Bel shaped curve a multivar gan is the generalization of this familiar bell-shaped curve over one dimensional random variable to multiple random variables at the same time to to to Vector value random variables rather than Univar random variable so um if Z is distributed Gan with some mean Vector mu and some covariance Matrix Sigma um so if Z is uh in RN then mu would be RN as well and sigma The ciance Matrix would be n byn so Z is two dimensional mu is two dimensional and sigma is two dimensional and the expected value of Z is equal to um the mean and the um coari of Z well if you're familiar with multivariate ciencies uh this is the formula right um and this simplifies we shown the Elon you get this from Elon Els sorry and uh following sometimes semi-standard convention I'm sometimes going to omit the square bracket so instead of writing the expected value of Z meaning the mean of Z sometimes I just write it as EZ right and omit omit the square brackets to simplify the notation of it okay uh and the derivation from this step to this step is given electon notes um and so the PRI density function for G looks like this and this is one of those formulas that I don't know when you're implementing these algorithms you use over and over but what I've seen for a lot of people is almost no one well very few people start their machine learning memorize this formula just look at every time you need it I've used it so many times I seem to have it steered in my brain by now but most people don't but when you use it enough you you you up memorizing it uh but let me show you some pictures of what this looks like since I think that would um that might be more useful so the multiv galaan density has two parameters mu and sigma that control the mean and the variance of this density okay so this is a picture of the Gan density um this is a two-dimensional Gan bump and for now I've set the mean parameter to zero so mu is a two-dimensional parameter this is z0 which is why this gussian bump is uh centered at zero um and the cence Matrix Sigma is the identity um is the identity Matrix so uh so you know so so you have this standard this is also called the standard Gan distribution which means mean zero and Coan equals to the identity now I'm we going to take the ciance Matrix and shrink it right so take the ciance Matrix and multiply by a number less than one that should shrink the variance reduce the variability and distribution if I do that the density um the probity density function becomes taller uh this this is a probity density function that always integrates the one right the area under the curve you know is is is one and so by reducing the covariance from Identity to 0.6 times the identity it reduces the the spread of the gum density um but it also makes it taller as a result because you know the area under the curve must integrate to one now let's make it fatter uh let's make the ciance two times the identity then you end up with a wider distribution where the values of um I guess the axis here this would be the Z1 and the Z2 axis the two dimensions of gum density right increases the variance of the density so let's go back to standard Gan coari equal one one now let's try filling around with the off diagonal entries um I'm going to so right now the off diagonal entries are zero right so in this Gan density the off diagonal elements are z0 let's increase that to 0.5 and see what happens so you do that then the Gan density uh hope you can see the change right goes from this round shape to this slightly narrower thing let's increase the further to 8.8 then the density ends up looking like that um where now it's more likely that Z1 now Z1 and Z2 are positively correlated okay so let's go through all of these plots um but now looking at Contours of these gum densities instead of these 3D bmps so uh this is the Contours of the Gan density when the ciance Matrix is the identity Matrix and poiz the aspect ratio these are supposed to be perfectly round circles but the aspect ratio makes this look a little bit fatter but this is supposed to be perfectly round circles um and so uh when uh the converence Matrix is the identity Matrix you know Z1 and Z2 are uncorrelated um uh and the Contours of the Gin bump of the Gin density look like round circles and if you increase the off diagonal excuse me then it looks like that you increase it further to 8.8 it looks like that okay where now most of the probabbly M most probity density function places value on um Z1 and Z2 being positively correlated okay um next let's look at uh what happens if we set the off diagonal elements to negative values right so um actually what do you think will happen let's set the off diagonals to negative. 5.5 right oh wow people seeing people making that head gesture okay cool right great right so so so you you endow the two random variables in negative correlation so you end up with um this type of uh prity density function right uh and in Contours it looks like this okay with where it's now slanted the other way so now Z1 and Z2 have a negative correlation and that's plenty Point okay all right so so far we've been keeping the mean Vector as zero and just varing the covariance Matrix um good yeah uh yes every ciance Matrix is symmetric yeah um mat a Ser of that uh should we think of conri as interesting column vectors that point interesting directions not really um let me think maybe should yeah yeah no I I I think the cence Matrix is always symmetric and so I would usually not look at single Columns of the cience Matrix and isolation uh when we talk about principal components analysis talk about the ion vectors of the ciance Matrix which are the principal directions in which it points but yeah we'll get to that later oh yes so the AR vectors The Matrix point in the principal axis of the ellipse uh that's defined by the cont yeah cool okay um so this a standed Gan with mean zero so the Gan BMP presented at 0 0 because me is0 z uh let's move mu around so I'm going to move you know mu to 01 to 0 1.5 so that moves the Galan uh position of gine density right now let's move it to different location move it to minus 1.5 minus one and so by varying the value of mu you could also shift the center of the gum density around okay so hope this gives you a sense of um as you vary the parameters the mean and the ciance Matrix of the 2D gum density um the source of prob PR density functions you can get as a result of changing mu and sigma okay um any other questions about this all right cool so here is the GDA right model um and and uh let's see so um remember for GDA we need to model P of x given y right instead P of Y given X so I'm going to write this separately in two separate equations P of x given yal Z so what's the chance what's the appro density of the features if is a benign tumor um I'm going to assume is Gan so I'm just write out the formula for Galan okay um and then similarly I'm going to assume that if it's a malignant tumor so if Y is equal to one that the density of the features is also Gan okay and um I want to point out a couple things so the parameters of the GDA model are mu0 mu1 and sigma um and the reasons we're going into a little bit we use the same Sigma for both classes um but we'll use different means zero and one okay uh and we can come back to this later if you want you could use separate parameters you know Sigma 0o and sigma one but that's not usually done so we're going to assume that the two gaussians for the positive and negative classes have the same covariance Matrix but they they have different means uh you don't have to make this assumption but this is the way it's most commonly done and we can talk about the reason why why we tend to do that in a second um so this is a model for p of Y given X the other thing we need to do is model P of Y uh so Y is just a newly random variable right it takes on you know the value zero or one and so I'm going to write it like this 5 to the y * 1 - 5 to the 1- y okay um and you saw this kind of notation when we talked about logistic regression but all this means is that um you know probity of Y be equal to one is equal to five right because Y is either zero or one and so um this is the way of writing uh PRI yals 1 is equal to five okay and you saw a similar exponentiation notation when we're talking about um logistic rection right one week ago last Monday and so the last parameter is five so this is RN this is also RN this is r n byn and that's just a real number between zero and one okay so um for for any let's see so if you can fit mu0 mu1 Sigma and F to your data then these parameters will Define p of x given y and p of Y and so if at test time you have a new patient walked into your office and you need to compute this then you can compute right these things in the red and the orange boxes each of these is a number and by plugging all these numbers in the formula you get a number out for p of yals 1 given X and you can then predict you know malignant or benign tumor right so let's talk about how to fit the parameters so you have a training set um as usual I'm going to write the training well I'm let me write the training set like this x i Yi for IAL 1 through M right this is a usual training set um and what we're going to do in order to fit these parameters is maximize the joints like hood and in particular um let me Define the likelihood of the parameters to be equal to the product from IAL 1 through M of P of x i Yi you know parameterized by um the paramet okay um and I'm I'm just going to drop the parameters here right to simplify the notation a little bit okay and the big difference between um a generative learning algorithm like this compared to discriminative learning algorithm is that the cost function you maximize is this joint likelihood which is p of X comma y whereas for a discriminative learning algorithm we were maximizing um this other thing right uh which is sometimes also called the conditional likelihood okay so the big difference between the these two cost functions is that for logistic regression or linear regression or generalized linear models um you were trying to choose paramet data that maximize P of Y given X but for generative learning algorithms we're going to try to choose parameters that maximize p P of X and Y or P of X comma y right okay so all right so if you use um maximum like estimation right um so you choose the parameters 5 mu 0 mu1 and sigma that maximize the log likelihood right where this you define as you know log of the likelihood that we Define out there um and so uh we actually ask you to do this as a problem set in the next homework but so the way you maximize this is um look at that formula for the likelihood take logs take derivatives of this thing set the there is equal to zero and then solve for the Valu so the parameters it maximiz this whole thing and I I I'll just tell you the answer is is supposed to get uh but but but you still have to do the derivation all right um the value of five that maximizes this is you know not that surprisingly so so five is the estimate of probability of Y being equal to one right so what's the chance when the next patient walks into your doctor's office that they have a a malignant tumor and so the maximum likely estimate for five is um it's just of all of your training examples what's the fraction with label y equals 1 right so the maxim likelihood of the uh bias of a coin TOS is just well count of the fraction of his you got okay so this is it um and one other way to write this is um sum from I = 1 through M indicat okay right um let's see so software indicates a notation on Wednesday did you no uh did you did we talk did you talk about indicated notation on Wednesday no okay oh so um uh this notation is an indicator function uh where um indicator y i equals 1 is uh uh return zero or one depending on whether the thing inside is true right so this's indicator notation in in which a indicator of a true statement is equal to one and indicator of a false statement is equal to zero so that's another way of writing writing this formula right um and then the maximum likelihood estimate for mu0 is this um I'll just write out okay um and so well actually if you uh put aside the math for now what do you think is a m likely estimate of the mean of all of the uh features for the benign tumors right well what you do is you take all the benign tumors in your training set and just take their average that seems like a very reasonable way just look at look at your trading set look at all of the um look at all of the benign tumors all the O's I guess and then just take the mean of these and that you know seems like a pretty reasonable way to estimate mu0 right look at all the the negative examples and average their features so this is a way of writing out that intuition um so the denominator is sum from I equals 1 through M indicator Yi equals zero and so the denominator will count up the number of examples that have benign tumus right because every time Yi equals zero you get an extra one in this sum um uh and so the denominator ends up being the total number of benign tumors in your training set okay and the numerator uh sunal 13m indicator is a benign tumor times XI so the effect of that is um whenever a tumor is benign is one times the features whenever an example is malignant it's zero times theur features and so the numerator is summing up all the features all the feature vectors for all of the examples that have been nine does that make sense I just write this up so this is sum of feature vectors for um for all the examples with y equals z and the denominators a number of examples with y equals z okay and then if you take this ratio if you take this fraction then you're summing up all of the feature vectors for the benign tumors divide by the total number of benign tumors in the training set and so that's just the mean of the feature vectors of all of the benign examples okay um and then right Maxim like for me one no surprises is so kind of what you'd expect sum up all of the positive examples and divide by the total number of positive examples and get their mean so that's as like put mu one um and then I just write this out if you're familiar with um ciance matrices this formula might not surprise you but if you're less familiar then I guess you can see the details in the homework okay don't worry too much about that uh you can unpack the details in the lecture for the hom works okay um but the ciance Matrix basically tries to you know fit Contours to the ellipse right like we saw so so try to fit the G to both of these with these corresponding means where you want one ciance Matrix to both of these okay um so these are the so so so the way so the way I motivated this was you know I said well if you want to estimate the mean of a coin toss just counted fraction of coin tosses they came up heads uh and then it seems like the mean for Mu new one you should just look at these examples and pick the mean right so that that was the intuitive explanation of how you get these formulas but the mathematically sound way to get these formulas is not Val this intuitive argument that I just gave is instead to look at the likelihood uh take logs get the log likelihood take derivatives set deres equal to zero solve all these values and prove more formally that these are the actual values that maximize this thing right by by saying there is a zero and solving so you can see that for yourself um in the problem sets Okay so all right um finally having fit these parameters um if you want to make a prediction right so give it a new patient uh how do you make make a prediction for whether their tumor is malignant orine um so if you want to predict the most likely class label uh you choose Max over y of P of Y given X right um and by base rule this is Max of a y of P of x given y p of Y divided by P of X okay now um I want to introduce one well one more piece of notation which is uh I'm G introduce actually how how many of you are familiar with the agmax notation most of you like okay two two3 okay cool I I'll go over this quickly so um Let's do an example so the um let's see H boy all right so you know the Min over Z of Z - 5^ 2 is equal to zero because the smallest possible value of Z - 5 S is zero right and the augment over Z of zus 5^ 2 is equal to five okay so the Min is the smallest possible value obtained by the thing inside and the augment is the value you need to plug in to achieve that smallest possible value right so uh the prediction you actually want to make if you want to Output a value for y you don't want to Output a probability right you want to say well what do I think is value of y so you want to choose the value of y that maximizes this so so there's the aax of this and this would be either zero or one right um so that's equal the AUG Max of that and you notice that uh this denominator is just a constant right doesn't doesn't it's a p of X Y doesn't even appear in there it's just some positive number and so this is equal to just AR Max over y p of x given y time P of Y so when implementing um uh when when making predictions with G in discri with uh generative learning algorithm sometimes to save on computation you don't bother to calculate the denominator if all you care about is to make a prediction but if you actually need a probability then you have to normalize the probability okay so let's examine what the Aram is doing all all right so let's look at the same data set and uh compare and contrast what a discriminative learning algorithm versus a generative learning algorithm will do on this data set right um here's example of two features X1 X2 and positive and negative examples so let's start with a discriminative learning algorithm uh let's say you initialize the parameters randomly typically when you run logistic regression I almost always initialize parenthesis zero but but this just you was more interesting to start off for purpose of visualization in a random line I guess and then if you run one iteration of gradient descent on the conditional likelihood um one iteration of legis regession moves the line there there two iterations three iterations um four iterations and so on and after about 20 iterations they'll converge to that pretty decent discriminative boundary okay so that's legis really searching for a line that separates positive and negative examples how about the generative learning algorithm what it does is the following which is fit uh with gussian dis analysis what would do is fit gaussians to the positive and negative examples right and and just one one one technical detail um I described this as if we look at the two classes separately because we use the same coari Matrix Sigma for the positive and negative classes we actually don't quite look at them totally separately but we do fit two hous in densities to the positive and negative examples um and then what we do is for each point try to decide uh what is this class label using base rule using that formula and it turns out that this implies the following decision boundary right so points to the upper right of this decision boundary that that straight line I just drew you are closer to the negative class you end up classifying them as negative examples and points to the lower left of that line you end up classifying as 45 as a positive examples and um uh I've also drawn in green here the decision boundary for logistic regression so so so these two algorithms actually come up with slightly different decision boundaries okay but the way you arrive at these two digion boundaries are a little bit different so um all right let's go back to the any questions about this yeah [Music] oh sure yes good question so um why why why do we use two separate means mu0 and mu1 and single cience Matrix Sigma um it turns out that um uh well it turns out that if you choose to build the model this way the desision boundary ends up being linear and so for a lot of problems if you want to lineate Des boundary uh uh uh yeah and it turns out you could choose to use two separate um cence Matrix Sigma 0o and sigma one and that'll actually work okay right that's is actually very reasonable to do so as well but uh you double the number of parameters roughly and you end up with a desision boundary that isn't linear anymore but it's actually not reason to do that as well um now there's one now there's one very interesting property um about Gan discr analysis and it turns out that uh well let's let's let's compare GDA to logistic regression and um for fixed set of parameters right so let's say you've learn some set of parameters um I'm going to do an exercise where we're going to plot P of Y = 1 given X you know parameterized by all these things right as a function of X okay um so I'm going to do this little exercise a second but what this means is um well this formula this is equal to P of x given y equals one you know which is parameterized by right well the various parameters time P of Y = 1 parameterized by 5 divided by P of X which depends on all the paramas I guess right so uh by base rule you know this formula is equal to this little thing and uh just as we saw earlier I guess right once you have fixed all the parameters that's just a number you compute by evaluating a gan density um this is a b newly probability so actually P of yal 1 parameterized by five this is just equal to five is that second term and you similarly calculate the denominator but so for every value of x you can compute this ratio and thus get a number for the chance of yal to one given X so I'm going go through one example of uh what function you get for p of yals 1 given X for what function you get for this if you actually plot this for um different values of X okay so um let's see let's say you have a just one feature X so X is a you know uh and let's say that you have a few negative examples there and a few positive examples there right so simple data set okay and let's see what gine discu analysis will do on this data set um with just one feure so that's why all the data is POS on 1D so let me map all this data to an x-axis I just took this data and mapped it down and um if you fit a g into each of these two data sets then you end up with you know G as follows where this bump on the left is p of x given yal Z and this bump on the right is p of x given yal 1 right and and again there's a technical detail that we set the same variance to the two Gans but you know you kind of model the Gan densi of what does class zero look like what does class one look like with two gum BMS like this oh and then because the data set is spit 50/50 you know P of yal 1 is 0.5 right so one half prior okay now let's go through that exercise I described on the left of trying to plot P of yals 1 given X for different values of X so the vertical axis here is p of yal 1 given different values of X so um let's pick a point far to the left here right with this model you if if you actually calculate this ratio you find that um if you have a Point here it almost certainly came from this Gan on the left right if if if you have an unable example here you almost certainly came from the class zero Gan because the chance of this Gan generating example all the way to the left is almost zero right and so chance of p p of y equals only given X is very small so for a point like that you end up with a point you know very close to zero right um let's pick another Point all right how about this point the mid Point well if you get an example right in the midpoint you you really have no idea you really can't tell did this come from the negative or the positive calcium can't tell right so this is really 50/50 so I guess if this is a 0.5 for that midpoint you would have PF yal 1 given X is .5 um and then if you go to point way to the right if you get an example way here then you'll be pretty sure this came from the positive examples and so you know you get a point like that right now it turns out that if you repeat this exercise uh sweeping from left to right from many many points on the x- axis you find that for points far to the left the chance of this coming from uh the yals 1 CLA is very small and as you approach this midpoint it increases to 0.5 and it surpasses 0.5 and then beyond a certain point it becomes very very close to one right and you do this exercise and actually just for every point you know for a dense grid on the xaxis evaluate this formula which will give you a number between zero and one is probability and go ahead and plot you know the values you get a curve like this and it turns out that if you connect up the dots um then this is exactly a sigid function the shape of that turns out to be exactly a shap sigmoid function and you proved this in the problem sets as well right um so um both logistic regression and Gan discri analysis actually end up using a sigmoid function to calculate you know P of yals 1 given X or or or the the the outcome ends up being a sigmoid function I guess the mechanics is you actually use this calculation rather than computer sigo function right but um the specific choice of the parameters they end up choosing are quite different and you saw when I was projecting the results on the display just now in PowerPoint uh that the two algorithms actually come up with two different decision boundaries so um let's discuss when a generative algorithm like GDA is superior and when a Distributive algorithm like logistic regression is superior um let's see all right so GDA Gan disc analysis so the generative approach this assumes that x given y equals z this is Gan with mean mu 0 and coent sigma it assumes x given yal 1 this is Gan would mean mu1 and coent sigma and Y is brui with um Paramus SP right and what logistic regression does this is a discens of algorithm oh some strange wind at the back is it I see okay cool all right yeah boy no there's just a scary un report on global warming over the weekend I hope we don't already have storms here okay it's okay did you guys see the UN report on this SL slightly scary actually with the the the year for global warming but hopefully all right good hurricane stopped okay um let's see uh so what logistic regression assumes is p of y equals 1 given X you know that this is a governed by logistic function right so this you know 1 over 1 plus e Nega Theta transpose X with with some details about x0 equals 1 and so on right so just just okay so so in other words let's assume that this is um P of yal 1 given XIs logistic okay and the argument that I just described just now uh plotting you know P of yal 1 given X Point by Point really the sigo curve I drew on On The Other Board what that illustrates um it it doesn't prove it you prove it yourself in the homework problem but what that illustrates is that this set of assumptions implies that P of yal 1 given X is governed by a logistic function right but it turns out that the implication in the opposite direction is not true right so if you assume P of yal 1 given X is governed by IC function by by this shape this does not in any way shape or form assume that x given Y is Gan uh uh x given y equ z is Gan X y1 is Gan so what this means is that GDA the generative learning Alm in this case this makes a stronger set of assumptions and which is regression makes a weaker set of assumptions because you could prove these assumptions from this assumptions okay um and by the way as as a as as a let's see and so what you see in a lot of learning algorithms is that um if you make stronger modeling assumptions and if your modeling assumptions are roughly correct then your model will do do better because you're telling more information to the algorithm so if indeed x given Y is Gan then GDA will do better because you're telling the algorithm x given Y is Galan and so it can be more efficient and so even if you have a very small data set um if these assumptions are roughly correct then GDA will do better and the problem with GDA is if these assumptions turn out to be wrong so if x given Y is not AOW of Gan then this might be very bad set of assumptions to make you might be trying to fit the gaan density the data that is not at all Gan and then GDA would do more poorly okay so here's one fun fact here's another example get to question second which is um let's say the following are true let's say that x given yals 1 is plus with a parameter Lambda 1 and x given y equals z is P with a mean lambda0 or Lambda 1 Lambda 0 and Y as before is brand newly five right it turns out that this set of assumptions also imply that P of yal 1 given X is logistic okay and you can prove this and this is actually true for um any generalized linear model actually where uh where where the difference between these two distributions varies only according to the Natural parameter as the generalizing name excuse me of the exponential family distribution right and so what this means is that um if you don't know if your data is gaan or P um if you're using logistic regression you don't need to worry about it it work fine either way right so so you know maybe um your fitting data to maybe fitting uh model conation model to some data and you don't know is it data gaussian is it pong is it some other exponential family model maybe you just don't know but if you're fitting logistic regression it'll do fine under all of those scenarios right but if your data was actually pass on but you assume it was gussian then your model might do quite poorly okay so the key high level principles when you take away from this is um uh uh if you make weaker assumptions as in logistic regression then your algorithm will be more robust modeling assumptions such as accy assuming the data is gin if is not uh but on the flip side if you have a very small data set then um using a model that makes more assumptions will actually allow you to do better because by making more assumptions you're just telling the algorithm more truth about the world which is you know hey algorithm the world is Gan and if it is Gan then it will actually do do do better okay question at the back or a few questions go ahead oh uh yeah practice what of data is a go in property you know it's uh uh yeah you know it's a matter of degree right most data on this universe is Gan uh uh uh except for the spe data I guess yeah but but um so I think it's actually a matter of degree right if if you plot actually if you take continuous value data no there now there there are exceptions you could plot it and most data that you plot you know will not really be Gan but a lot of it you can convince yourself as vaguely Gan so I think a lot of is matter ofree I I actually tell you the way I choose to use um these two algorithms so I think that the whole world has moved toward using bigger data sets right Digital Society which is a lot of data and so for a lot of problems we have a lot of data I would probably use logistic regression because with more data you can overcome telling the algorithm less about the world right so so the algorithm has two sources of knowledge uh one source of knowledge is what did you tell it what the assumptions you told it to make and the second source of knowledge is learn from the data and in this era of big data we have a lot of data you know there is a strong Trend to use logistic regression which makes less assumptions and just let the a figure out whatever it wants to figure out from the data right now one practical reason why I still use algorithms like GDA General discre analysis or algorithms like this um uh is that is actually quite computationally efficient and so there's actually one use case that Landing AI than working on where we just need to fit a ton of models and don't have the patience to run theis progression over and over and it turns out Computing mean and variances of um ciance matrices is very efficient and so this is actually apart from the assumptions type of benefit uh which is a general philosophical point we'll see again later in this course right this idea about do you make strong or weak assumptions this is a general principle in machine learning that we'll see again in other places but it's a very concrete the other reason I tend to use GDA these days is less that I think could perform better from an accuracy point of view but there's actually very efficient algorithm you just compute the mean Co cence and you're done and there's no iterative process needed so these days when I use these models um is more motivated by computation and less by performance but this General principle is one that will come back to you again later we develop more sub learnings yeah the the ass oh right so what happens if the cence matrices are different it turns out that uh uh it still ends up being a logistic function but a bunch of quadratic terms in the logistic function so it's not a linear design boundary anymore you can end up with a desision boundary you know that that that looks like this right a positive negative example separated by some by some other shape and line you you could you could you could F actually if you're curious encourage you to you know uh fire up Python numpy and and play around their parames and plot this for yourself question yeah is a recommend dle test to see if it's Gan um I can tell you what's done in practice I think in practice if you have enough data to do a cical test and gain conviction you probably have enough data to just use logistic regression um uh the the I I I don't know well no that's not really fair I don't know if a very high dimensional data I I think what often happens more is people just plot the data and if it looks clearly non-an then you know that would be a reason to not use GDA but what happens often is that um uh uh sometimes you just have a very small training set and it's just a matter of judgment right like if you have if you have a a you know I don't know 50 examples of healthcare records then you just have to ask some doctors and ask well do you think the distribution is RA relatively Gan and use domain knowledge like that right I think by the way another philosophical Point um I think that uh the machine learning world has TR you know a little bit overhyped big data right and and yes it's true that when you have more data is great and I love data and having more data pretty much never hurts and usually the more data the better so all that is true and I think we did a good job telling people that high level message you know more data almost always helps but um uh I think a lot of the skill in machine learning these days is getting your alms to work even when you don't have a million in examples even don't have 100 million examples so there are lots of machine learning applications where you just don't have a million examples uh you have 100 examples and um it's then the skill in designing the learning AA matters much more um so if you take something like imet million images there are now dozens of teams maybe hundreds of teams I don't know they can get great results if you have a million examples right and so the performance difference between teams you know there are now dozens of teams that get great performance there a million examples uh for for for image classification like image them but if you have only 100 examples then the high school teams will actually do much much much much better than the low Skool teams whereas the performance Gap is smaller when you have giant data sets I think so and I think that is these types of intuitions you know what assumptions you use generative or discriminative that actually distinguishes the high school teams and the and the and the less experienced teams and drives a lot of performance differences when you have small data oh and if someone goes to you and says oh you only have 100 examples you never do anything then I don't know if if it's a competitor saying that I'll say great you know don't do it because I can make it work uh well I don't know uh but but I think there are a lot of applications where your skill of Designing a machine Learning System really makes a bigger difference when you have a makes a it makes a difference for big data and small data but it just this is very clear when you don't know much data is the assumptions you C into the a like is it Gan is it P that that skill allows you to drive much bigger performance then a lower skill team would be able to all right let's just uh I want take question go ahead oh sure so does this uh yes so what's the general statement of this yes so if uh x given yals 1 uh comes from an exponential family distribution x given yal Z comes from exponential family distribution the same exponential family distribution and if they vary only by the natural parameter of exponential family distribution then and this will be logistic yeah um I think this was once a midterm homew world problem to prove this actually but yeah all right uh actually just take one last question we'll move on go ahead oh uh does performance Improvement whole even as you increase number of classes uh I think so yes uh and the general ization of this would be the soft Max regression which I didn't talk about but yes I think a similar thing holds true for um gdf and multiple and we have so far we only talked about bind classification what if we have more than two classes but uh but yes similar similar things holds true for uh like a GDA with three classes in softmax yeah oh yes right you saw softmax the other day cool um and this this theme that when you have less data the them needs to rely more on assumptions you code in this is a recurring theme that we'll come back to as well this is one of the important principles of machine learning that when you have less data your skill at coding in your knowledge matters much more uh this is a theme we'll come back to when we talk about much more complicated learning ARS as well all right so I want a fresh board for this so you've seen GDA in the context of um continuous valued uh features X the last thing I want to do today um is talk about one more generative learning algorithm called naive Bas um and I'm going to use as a mul example email spam classification but this this is this I guess this is our first for into national language processing right but given a piece of text like given a piece of email can you classify this as spam or not spam or uh other examples uh uh actually several years ago eBay used a problem of you if someone's trying to sell something and you write a text description right hey I have a secondhand you know room I'm trying to sell it on eBay how do you take that text that someone wrote of a description and categorize it is electronic thing or are they trying to sell a TV are they trying to sell clothing uh but these these examples are text class a problems you have a piece of text and you want to classify it into one of two categories for spam or non-spam or one of maybe thousands of categories if you're trying to take a private description and classify it into one of the classes um and so the first question we will have is um uh given an email problem uh given email classification problem how do you represented as a feature vector right and so um in naive base what we're going to do is take your email take a piece of email and first map it to a feature Vector X and we'll do so as follows which is first um let's start with a let's start with the English dictionary and make a list of all the words in the English dictionary right so first word in the English dictionary is a second word English dictionary is is arvar third word is adwolf uh that's e he look it up um and then you know uh uh email spam a lot of people ask you buy stuff so they would buy right and then um uh and then the last word in my dictionary is zigar which is the technological chemistry that refers to the fermentation process in Brewing um so so again this is useful way think about it in in in practice what you do is not uh actually look at the dictionary but look at the top 10,000 words in know in your training set right so maybe you have 10,000 it's easier to think about it as if it was a dictionary but you know in practice what you the other thing that's diction has too many words but what the other way to do it is to look through your own email Corpus and just find the top 10,000 occurring words and use that as a feature set and so oh know right in your emails I guess you're getting a bunch of email about from us or maybe others about cs229 so cs229 might appear in your dictionary of building an email SPF for yourself even if it doesn't appear in the in the official uh what is it like the oate dictionary just yet just just just you way we'll get CS there somay all right um and so given an email what we would like to do is then um take this piece of text and represent to this feature vector and so one way to do this is um you can create a binary feature vector that puts a one if a word appears in the email and puts a zero if it doesn't right so if you get an email um uh that asks you to you know buy some stuff and the word a appears in email you put a one there they're not trying to sell odv odw so zero there buy and so on right so you take a take an email and turn it into a binary feature Vector um and so here the feature Vector is 01 to the N because it's a n dimensional bin feature Vector where where for the purpose of illustration let's say n is 10,000 because you're using you know take the top 10,000 words uh that appear in your email training set as the dictionary that you will use so um so in other words XI is indicator where I appears in the email right so it's either Z One depending on whether or not that word I from this list appears in your email now um in the na Bas algorithm we're going to build a generative learning algorithm um and so we want to model P of x given y right as well as P of Y okay but there are uh two to the 10,000 possible values of X right because because X is a binary Vector of this 10,000 dimensional so if we try to model P of X in the straightforward way as a multinomial distribution over you know two to the 10,000 possible outcomes then you need right uh uh you need you know two to the 10,000 parameters right which is a lot or actually technically you need 2 to 10,000 minus one parameters because that add up to one you save one parameter um but so modeling this without additional assumptions won't won't work right because uh excessive number parameters so in the Naas algorithm we're going to assume that the XIs are uh conditionally independent given y okay uh let me just write out what this means but so P of X1 up to x 10,000 given y by the chain rule of probability this is equal to P of X1 given y times P of X2 given um X1 and Y times P of x3 given X1 X2 y up to your P of x 10,000 given so on right so I haven't made any assumptions yet this is just a true statement of fact is always true by the by the chain rule of probability um and what we're going to assume which is what this assumption is is that this is equal to this first term no change but X2 given y p of x3 given y do do do p of x 10,000 given y okay so um this assumption is called a conditional Independence assumption is also sometimes called the na based assumption but you're assuming that um so long as you know why the chance of seeing the word um odv in your email does not depend on whether the word a appears in your email right um and this is one of those assumptions is definitely not a true assumption in that this is just not a mathematically true assumption just that sometimes your data isn't perfectly Gan but you assum as Gan you can kind of get away with it uh so this assumption is not true um in a mathematical sense but it may be not so horrible that you can't get away with it right um and so side if you if any of you are familiar with prob graphical models if you taken cs228 uh this assumption is summarized in this picture and if you haven't taken cs228 this picture won't make sense but don't worry about it um right that uh once you know the class label is a spam or not spam whether or not each word appears or does not appear is independent okay so this called conditional so the the mechanics of this assumption is really just captured by this equation um uh and you just use this equation that's all you need to derive naive base but the intuition is that if I tell you whether this piece if I tell you that this piece of email is Spam then whether the word buy appears in it doesn't affect your beliefs whether the word mortgage or discount or whatever spam you words appear right so just to summarize this is product from I equals 1 through n of P of x i given y all right so the parameters of this model uh are I'm going to write five subrip um J given yals 1 as the probability that XJ equal 1 given yal 1 I sub j yal z and then um f right and just to distinguish all these FES from each other I'm going to just call this five subscript y okay so this parameter says if a spam email if yal 1 is Spam y z is non spam if a spam email what's the challeng of where J appearing in the email uh if it's not spam email what's the chance of where J appearing in the email um and then also what's the class prior what's the PRI probability that the next email you receive in your uh in your in your inbox is SP email and so um to fit the parameters of this model you would similar to gmin analysis write out the Jo joint likelihood so the joint likelihood of these parameters right is the product you know given these parameters right similar to what we had for Gan discre analysis and the maximum likelihood estimates um if you take this take logs take der to set there to zero solve for the values that maximize this you find that the maximum likely estimates of the parameters are 5y is pretty much what You' expect right it's just a fraction of spam emails and F of J given yals 1 is um well all write does out an indicat a function notation oh shoot sorry okay um so that's the indicator function notation of writing out look through your uh training set find all the spam emails and of all the spam emails are examples of y equal 1 count out what fraction of them had word j in it right so your estimate of the chance of word J appearing your estim chance of the word by appearing in spam email is just where of all the spam emails in your training set what fraction of them contained the word by what fraction of them had your XJ equals one for say the word by okay um and so it turns out that if you implement this algorithm it will it will nearly work I guess uh uh but this is naive base for um for email spam classification right and I'll mention the one reason this uh and it turns out that with with one fix to this algorithm which we'll talk about on Wednesday um this is actually is actually a not too horrible spam classifier it turns out that if you use logistic regression uh for spam classification you do better than this almost all the time but this is a very efficient algorithm because estimating these parameters is just counting and then Computing probabilities is just multiplying a bunch of numbers so there's nothing iterative about this you can fit this model very efficiently and also keep on updating this model even as you get new data even as you get new new new you know users hits mark spam or whatever even you get new data you can update this model very efficiently um but it turns out that uh actually the the the biggest problem with this algorithm is what happens if uh this is zero over if uh if you get zeros in some of these equations right but but we'll come back to that when we talk about the pl moving on Wednesday okay all right any quick questions before we wrap up y okay good so now you learn about generative learning algorithms uh we'll come back on Wednesday and learn about some more fine details how to make this work that's so let's break we'll see you on Wednesday