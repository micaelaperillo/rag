hi everyone welcome welcome to the second lecture on deep learning for CST to 9 so a quick announcement before we start there is a Piazza post number 695 which is the meet quarter survey for CST to nine so fill it in when you have time ok so let's get back to deep learning so last week together we've seen what the neural network is and we started by defining the logistic regression from a neural network perspective we said that logistic regression can be viewed as a one neuron neural network where there is a linear part and an activation part which was sigmoid in that case we we've seen that sigmoid is a common activation function to be used for classification tasks because it casts a number between minus infinity and plus infinity 0 in the z-row one interval which can be interpreted as a probability and then we introduced the neural network so we started to stack some neurons inside a layer and then stack layer on top of each other and we said that the more we stack layers the more parameters we have and the more parameters we have the more our network is able to copy the complexity of our data because it becomes more flexible so we stopped at a point where we did a forward propagation we had an example during training before were propagated through the network we get the output then we compute the cost function which compares this output to the ground truth and we were in the process of back propagating the error to tell our parameters how they should move in order to detect cuts more properly does that make sense all this part so today we're going to continue that so we're in the second part neural networks we're going to derive the back propagation with the chain rule and after that we're going to talk about how to improve our neural networks because in practice it's not because you designed in your own network that is going to work there's a lot of hacks and tricks that you need to know in order to make a neural network work ok let's go so first thing that we talked about is in order to define our optimization problem and find our right parameters we need to define a cost function and usually we said we would use the letter J to denote the cost function so here when I talk about cost function I'm talking about a batch of examples it means I'm for propagating M examples at a time you remember why we do that what's the reason we use a batch instead of a single example vectorization we want to use what our GPU can do and paralyze the computation so that's what we do so we have M examples that go for propagate in the network and each of them has a loss function associated with them the average of the loss functions over the batch give us the cost function and we had defined this loss function together L of I assuming we're still and just as a reminder we're still in this network where where we had a cat remember this 1 X 1 2 X n the cat was flattened into a vector RGB matrix into one vector and then there was a neural network with three neurons then two neurons than one neuron remember fully connected here everything and then we remember this one I think that what is this one ok so now we're here we take M images of cats or non cats for propagate everything in the network compute a loss function for each of them average it and get the cost function so our last function was the binary cross-entropy or also called the loss function the logistic loss function it was the following y hi log of y hat I plus 1 minus y I log of 1 minus y hat bye so let me circle this one it's an important one and what we said is that this network has many parameters and we said the first layer has W 1 V 1 the second layer has W 2 V 2 and the third layer has W 3 B 3 where the square brackets this denotes the layer and we have to train all these parameters one thing we noticed is that because we want to make a good use of the chain rule we're going to start by by computing the derivative of these guys W 3 and V 3 and then come back and do W 2 and V 2 and then back again W 1 and B 1 in order to use our formulas of the update of the gradient descent where W would be equal to W minus alpha derivative of the cost with respect to W and this for any layer L between 1 & 3 same for B okay so let's try to do it this is the first number we want to compute and remember the reason we want to compute derivative of the cost with respect to W 3 is because the relationship between W 3 and the cost is easier than the relationship between W 1 and the cost because W 1 had much more connection going through the network before ending up in the cost computation so one thing we should not cease before starting this calculation is that the derivative is linear so this if I take the derivative of J I can just take the derivative of L and it's the same thing I just need to add the summation prior to that because derivative is a linear operation that make sense to everyone so instead of computing this I'm going to compute that and then I will add the summation if we just make our notation easier so I'm taking the derivative of a loss of one example propagated through the network with respect to W 3 so let's do the calculation together everyone I have a minus y I derivative with respect to W 3 of what we remember that Y hat was equal to Sigma of W 3x plus B or W 3 a 2 plus B because a 2 is the input to the second layer remember so I would write it down here sigmoid of W 3 a 2 plus B 3 ok yeah this is good like that it's too small W through a 2 plus B 3 it's good like that okay so we have this term and then we have the second term which is plus 1 minus y I times derivative of w3 ability with respect to W 3 of 1 oh sorry I forgot the logarithm here of log of 1 minus Sigma of W 3 a 2 plus B 3 and so just a reminder the reason we have this is because we've written the forward propagation in the previous class you guys remember the four forward propagation we had z3 which took a 2 as input and computed the linear part and sigmoid this is the activation function used in the last neuron here okay so let's try to to compute this derivative Y I so the derivative of log log prime equals 1 over log I'm ready this formula so I will just take 1 over sorry 1 over X minus 1 over X if you put your next here so block prime of X so I will take 1 over sigmoid of w3 a 2 plus B 3 I know that this thing can be written a 3 right so I will just write a 3 instead of writing the simulator game so we have 1 over a 3 times the derivative of a 3 with respect to W 3 remember is that I'm gonna write it down here if we take the derivative of sigmoid of blah blah blah let's say derivative of log of sigmoid over W what we have is 1 over the sigmoid time's the derivative with respect to w3 of the sigmoid does that make sense that's what we're using here so the derivative of sigmoid sigmoid prime of X is actually pretty easy to compute is sigmoid of x times 1 minus Sigma of X ok so I'm just going to take the derivative is going to give me 8 a 3 times 1 minus a 3 there is still one step because there is a composition of three functions here there is a logarithm there's a sigmoid and there's also a linear function WX plus B or W a 2 plus B so I also need to take the derivative of the linear part with respect to W 3 because I know that sigmoid of W 3 a 2 plus B 3 if I want to take the derivative of that with respect to W 3 I need to go inside and take the derivative of what's inside ok so this will give me the sigmoid or whatever a 3 times 1 minus a 3 times the derivative with respect to W 3 of the linear part does this make sense so I'm going to write it here bigger here I need to take the derivative of the linear part with respect to W 3 which is equal to a 2 transpose so one thing you you may want to check is when we compute when I'm trying to compute this derivative I'm trying to compute this derivative why is there a transpose that comes out how do you come up with that you look at the shape here what's the shape of w3 someone remembers one by two yeah why one by two yeah it's connecting two neurons to one your own so it has to be one by two easily flip it and in order to come back to that you can write your forward propagation make the shape analysis and find out that it's a one by two matrix how about this thing what's the shape of that hmm the scaler yeah so scaler so it's one by one how do you know is because this thing is basically z3 is the linear part of the last neuron and a3 we know that it's Y hat so it's a scalar between zero and one so this has to be a scalar as well because taking the sigmoid should not change the shape so now the question is what's the shape of this entire thing the shape of this entire thing should be the shape of w3 because you're taking the derivative of a scalar with respect to a higher dimensional matrix or vector here called a row vector then it means that the shape of this has to be the same shape of w3 so 1 by 2 and you know that when you take this simple derivative in in real like in with scalars not with high dimensional you know that this is an easy derivative it just should it should give you a 2 right but in higher dimensions sometimes you have transposed that come up and you know that the answer is a to transpose is because you know that a 2 is a 2 by 1 matrix so this is not possible it's not possible to get a 2 because otherwise it wouldn't match the derivative that you're calculating so it has to be a 2 transpose so either you you learn the formula by heart or you you learn how to analyze shapes ok any questions on that so that's why it's a 2 transpose now minus y I so I'm I'm on this one now the second term of the of the derivative and I take the derivative of this so I get 1 over 1 minus a 3 a 3 denotes the sigmoid so I'm just copying this back using the fact that the derivative a logarithm is 1 over X and then I will multiply this by the derivative of 1 minus a 3 with respect to W 3 I know that there's a minus that needs to come up so I would write it down here minus 1 and I also have the derivative of the sigmoid with respect to what's inside the sigmoid so a 3 times 1 minus a 3 and what's the last term the last term is simply the one we just talked about it's the derivative of what's inside the sigmoid with respect to W 3 so it's a 2 transpose again ok so now I will just simplify I know this scalar simplifies with this one this one simplifies with that one really going to copy back all the results - why I times 1 minus a 3 a 2 transpose plus 1 minus y I times the - I'm going to put the minus here so I'm taking the - putting it on on the front times a 3 times a 2 transpose and then quickly looking at that I see that some of the terms will cancel out right okay so I have one term here why why I time this - a 3 a 2 transpose would cancel out with + y i-83 a 2 transpose to make sense so like the term that we multiply this number we cancel out with the term we multiply this number going to continue it gives me Y I times a 2 transpose this part minus a 3 times a 2 transpose I can factor this because I have the same term a 2 transpose and gives me finally why I - a 3 times a 2 transpose okay so it doesn't look that bad actually I don't know when we take a derivative of something kind of ugly we expect something ugly to come out but this doesn't seem too bad any questions on that I let you write it quickly and then we're going to move to the rest so once I get this result I can just write down the cost for derivative with respect to W 3 I know it's just 1 - I just need to take the summation of this thing so why I - a 3 times y 2 transpose a 2 transpose and I have a minus sign coming up front so that's my derivative okay so we were done with that and we can we can just take this formula plug it in back in our gradient descent update rule and update w3 yeah now the question is you can do the same thing as as we just did but with v3 is going to be the similar difficulty we're going to do it with W - now and think how does that back propagate to W - so now it's W to stern we want to compute derivative of L the loss with respect to W of the second layer the question is how I'm gonna get this one without having too much work I'm not going to start over here as we said last time I'm going to use the chain rule of calculus so I'm going to try to decompose this derivative into several derivatives so I know that Y hat is the first thing that is connected to the loss function right the output neuron is directly connected to the last function so I'm going to take the derivative of the last function with respect to Y hat also called a tree right is the easiest one I can calculate I also know that a tree which is the output activation of the last neuron is connected with the linear part of the last neuron which is z3 so I can take derivative of a tree with respect to Z 3 you remember what this is going to be derivative of a tree with respect to Z three derivative of sigmoid I know that a tree called sigmoid of z 3 so this derivative is very simple it's just that it's just 8 3 times 1 minus a 3 right so I'm going to continue I know that Z 3 Z 3 is equal to what it's equal to W 3 a 2 plus B which path things I need do I need to take in order to back propagate I don't want to take the derivative with respect to W 3 because I went yet stuck I don't want to take the derivative with respect to B 3 because I will get stuck I will take the derivative with respect to a 2 because a 2 will be connected to Z 2 Z 2 will be connected to a 1 and I can back propagate from this path so I'm going to take the relative of Z 3 with respect to a 2 to have my error back propagate and so on I know that a 2 is equal to Sigma of Z 2 so I'm just going to do that and I know that this derivative is going to be easy as well and finally I also know that Z 2 is connected to W 2 so I'm going to take derivative of Z 2 with respect to W 2 so just what I want you to get is the thought process of this chain rule why don't we take a derivative with respect to W 3 or B threes because we will get stuck we want the error to back propagate and in order for the error to back propagate we have to go through variables that are connected to each other does it make sense so now the question is how can we use this how can we use the derivative we already have in order to to to to compute the derivative with respect to W 2 can someone tell me how we can use the results from this calculation in order not to do it again you cash it so there's another discussion on caching which is which is correct that's in order to get this result very quickly we will use cash but what I want here is to you to tell me if this result appears somewhere here yeah the first three terms so this one this one in this one yeah is it the first two terms or the first three terms the first two terms here but good intuition yeah so these results is actually the first two terms here we just calculated it okay well how do we know that it's not easy to see one thing we know based on what we've written in very big on this board is that the derivative of z3 because this is III right derivative of z3 with respect to W 3 is a 2 transpose right so I could write here that this thing is the relative of z3 with respect to W 3 is correct so I know that because I wanted to compute the derivative of the loss to W 3 I know that I could have written derivative of loss with respect to W 3 as derivative of loss with respect to Z 3 times derivative of z3 with respect to W 3 correct and I know that this is a to transpose so it means that this thing is the receive of the loss with respect to Z 3 does it make sense so I got I got my decomposition of the derivative we had if we wanted to use the chain rule from here on we could have just separated it into two terms and took the derivative here okay so I know the result of this thing I know that this thing is basically 83 minus y times a 2 transpose I just flipped it because of the minus sign okay now tell me what's disturb what is it sir let's go back yeah so sigmoid I'm just going to write it a 2 times 1 minus a 2 if that makes sense Sigma 8 times 1 minus Sigma what is this term oh sorry my bad that's not the right one this one this one is that this one is sigmoid a 2 is sigmoid of Z 2 so this result comes from this term what's what about this term sorry W 3 is it W 3 or no I heard transpose how do we know if it's W 3 or W 3 transpose so let's look at the shape of this what's D 3 it's one by one it's a scalar is the linear part of the last neuron what's the shape of that this is two one we have two neurons in the layer W 3 we said that it was the 1 by 2 matrix so we have to transpose it so the result of that is W 3 transpose and how about the last term same as here one layer before yeah someone said day 1 transpose ok yep this one there's a transpose here oh oh yeah yeah you correct you correct thank you that's what you mean yeah this one was from the z3 dw3 we didn't end up using that because we will get stuck so there's no idea to transpose here Thanks any other questions or remarks so that's cool let's write let's write down our derivative cleanly on the board so we have derivative of our last function with respect to W 2 which seems to be equal to a 3 minus y from the first term the second term seems to be equal to W 3 transpose then we have a term which is a 2 times 1 minus a 2 ok and finally finally we have another term that is a 1 transpose so are we done or not so our triggers the thing is there's two ways to compute derivatives either you go very rigorously and do what we did here for w2 or you try to do a chain moon analysis and you try to fit the terms the problem is this result is not completely correct there is a shape problem it means when we took our derivatives which should have flipped some of the terms we did it there is actually we won't have time to go in the details in this lecture because we have other things to see but there is a section note I think on the website which details the other method which is more rigorous which is like that for all the derivatives what we're going to see is how you can use chain rule plus shape analysis to come up with the results very quickly okay so let's let's analyze the shape of all that we know that the first term is a scalar into 1 by 1 we know that the second term is the transpose of 1 by 2 so it's 2 by 1 and we know that this thing here a 2 times 1 minus a 2 is 2 by 1 it's an element-wise product and this one is a 1 transpose so it's 3 by 1 transpose so it's 1 by 3 so there seem to be a problem here there is no match between these two operations for example right so the question is how can we how can we put everything together if we do it very good a city we know how to put it together if you're used to doing the chain rule you can quickly quickly do it around so after experience you will be able to to fit all these together the important thing to know is that here there is an element twice product which is here so every time you will take the derivative of the sigmoid is going to end up being an element twice product and it's the case whatever the activation that you're using is so the right result is this one so here I have my elementwise product of a 2 by 1 by a 2 by 1 so it gives me a 2 by 1 column vector and then I need something that is 1 by 1 and 1 by 3 how do I know what what do I need to have I know that the shape of this thing w3 needs to be 2 by 3 it's connecting two three neurons to neurons so w2 has to be 2 by 3 in order to end up with this I know that this has to come here a3 minus y and a1 transpose comes again and here I get my correct answer don't worry if it's the first time you do the chain rule and is going quickly don't worry read the lecture notes with the rigorous parts taking the derivative it will make more sense but I feel that usually in practice we don't compute these chain rules anymore because because programming frameworks do it for us but it's important to know at least how the chain will decomposes and also how to make this the compute this derivative if you read research papers specifically any questions on that I think I want to go back to what you mentioned with the cache so why is cache very important that was your question as well yeah yeah it has to be so it means when you take the derivative of Samoyed you take derivative with respect to every entry of the matrix which gives you an element twice product going back to the cache so one thing is it seems that during back propagation there is a lot of terms that appear that were computed during forward propagation right all these terms a 1 transpose a 2 a 3 all these we have it from the for propagation so if we don't catch anything we have to recompute them it means I'm going backwards but then I feel oh I need a 2 actually so I have to really go forward the game to get a 2 I go backwards I need a 1 I need to forward propagate my X again to get a 1 I don't want to do that so in order to avoid that when I do my for propagation I would keep in memory almost all the values that I'm getting including the W's because as you see to compute the derivative of loss with respect W to we need W 3 but also the activation or linear variables so I'm going to save them in my in my network during the for propagation in order to use it during the backward propagation that make sense and again it's all for computation efficiency it has some memory cost okay so that was the backpropagation and now I can use my formula of the cost with respect to the last function and I know that this is going to be my update this is going to be used in order to update w2 and I will do the same for w1 then you guys can do it at home if you want to make sure you understood take the derivative with respect to w1 okay so let's move on to the next part which is improving your neural network so in practice when you when you do this process of training for propagation backward propagation updates you don't end up having a good network most of the time in order to get a good network you need to improve it you need to use a bunch of techniques that will make your network work in practice the first the first trick is to use different activation functions so together we've seen one activation function which was sigmoid and we remember the graph of sigmoid is getting a number between minus infinity and plus infinity and casting it between zero and one and we know that the formula is sigmoid of z equals 1 over 1 plus exponential minus z we also know that the derivative of sigmoid is sigmoid of Z times 1 minus Sigma of Z okay another very common activation function is relu we talked quickly about it last time value of Z which is equal to 0 if the is less than zero and Z if Z is positive so the graph of relu looks like something like this with and finally another one we were using commonly as well is tan H so hyperbolic tangent and tan H of Z equals exponential Z minus exponential minus Z over exponential Z plus exponential minus Z the derivative of tan H is 1 minus tan H squared of Z and the graph looks kind of like sigmoid but but it goes between minus one and plus one so one question now that I've given you three activation function can you guess why we would use one instead of the other and and which one has more benefits so when I talk about activation functions I talk about the functions that you will put in these neurons after the linear parts what do you think is the main advantage of sigmoid yeah yeah you use it for classification between it gives you a probability what's the main disadvantage of sigmoid it's easy that should be an advantage should be a benefit yeah correct if you're at high activation if you are high Z's or low Z's your graduate is very close to zero so look here based on this graph we know that if Z is very big if Z is very big our gradient is going to be very small the slope of this graph is very very small it's almost flat same for these that are very low in the negative right what's the problem with having low gradients is when I'm back propagating if the Zi clash was big the gradient is going to be very small and it would be super hard to update my parameters that are early in the network because the gradient is just going to vanish does it make sense so sigmoid is one of these activation which which works very well in the linear regime but has trouble working in saturating regimes because the network doesn't update the parameters properly it goes very very slowly we're going to talk about that a little more how about tonnage very similar right similar like high seas and low these lead to saturation of a tannish activation relu on the other hand doesn't have this problem if Z is very big in the positives there is no saturation the gradient just passes and the gradient is one when we're here right the slope is equal to one so it's actually just directing the gradient to some entry it's not multiplying it by anything when you back propagate so you know this term here this term that I have here all the a 3 minus 8 3 times 1 minus a 3 or a 2 1 times 1 minus a 2 if we use real activations when we change these with what with with the derivative of r lu and the derivative of r lu can be written indicator function of z being positive you've seen in indicator functions so this is equal to 1 if Z is positive 0 otherwise ok so we will see why we use Rayleigh mostly yeah yeah free you remember the house prediction example in that case if you know if you only predict the price of a house based on some features you would use value because you know that the output should be a positive number between 0 and plus infinity it doesn't make sense to use one of 10 H or Samoyed yeah doesn't really matter I think if if I want my output to be between 0 and 1 I would use Samoyed if I owned my output to be between minus 1 and 1 I would use tonnage so you know there is there are some tasks where the output is kind of a reward or a minus reward that you want to get like in reinforcement learning you would use 10 H as an output activation which is because minus 1 looks like a negative reward plus 1 looks like a positive reward and you want to decide what should be the reward good question why do we consider these functions we can actually consider any functions apart from the identity function so let's see why thanks for the transition like why do we need activation functions so let's assume that we have a network which is the same as before so our network is three neurons casting into two neurons casting into one your own and we're trying to use activations or equal to identity functions so it means Z is given to Z let's try to derive the for propagation Y hat equals a tree equals Z 3 equals W 3 a 2 plus B 3 I know that a 2 a 2 is equal to Z 2 because there is no activation and Z 2 is equal to W 2 A 1 plus B 2 so I can cast here W 2 W 2 A 1 plus B 2 plus B 3 I can continue I know that a 1 is equal to Z 1 and I know that Z 1 is w 1 X plus B and B equals W three times W two times B 1 plus W 3 times B 2 plus B 3 so what's the insight here is that we need activation functions the reason is if you don't use activation functions no matter how deep is your network is going to be equivalent to a linear regression so the complexity of the network comes from the activation function in the reason we can understand if we're trying to detect cuts what we're trying to do is to train a network that will mimic the formula of detecting cuts we don't know this formula so we want to mimic it using a lot of time matters if we just have a linear regression we cannot mimic this because we're going to look at pixel by pixel and assign every way to a certain pixel if I give a new example it's not gonna work anymore yeah yeah so I think that's that that goes back to your question as well so this is why we need activation functions and then the question was can we use different activation functions and how do we how do we put them inside a layer or inside neurons usually we would use there are more activation functions I think in CS 2:30 we go over a few more but not not not today these have been designed with experience so these are the ones that that that work better and let's our networks train there are plenty of other activation functions that have been tested usually you would you would use the same activation functions inside every layer so when you it's it's a it's it's for training it doesn't have any special reason I think but when you have a network like that you would call this layer a random layer meaning it's a fully connected layer with radioactive ation this one a sigmoid layer it means it's a fully connected layer with the sigmoid activation and the last one is sigmoid I I think people have been trying a lot of putting activate different activations in different neurons in a layer in different layers and the consensus was using one activation in the layer and also using one of these three activations yeah so if someone comes up with a better activation that is obviously helping training our models on different data sets people would adopt it but right now these are the ones that work better you know last time we talked about hyper parameters a little bit these are all hyper parameters so in practice you're not going to choose these randomly you're going to try a bunch of them and choose some of them that seem to help your model train there's a lot of experimental results in deep burning and we don't really understand fully why certain activations work better than others okay let's move on okay let's go over initialization techniques okay let me use this port so another trick that you can use in order to help your network train our initialization methods and normalization methods so earlier we talked about the fact that if Z is too big or Z is too low in the negative numbers it will lead to saturation of the network so in order to avoid that you can use normalization of the input so assume that you have a network where the data is 2-dimensional x1 x2 is your two-dimensional input you can assume that x1 x2 is distributed like this thing so this is if I plot X 1 again X 2 for a lot of data I will get that type of graph the problem is if I do my W X plus B to compute my Z 1 if x's are very big it will lead to very big Z's which will lead to saturated activations in order to avoid that one method is to compute the mean of this data using mu equals 1 over the size of the batch of the internet you have in the training set sum of excise so you're just giving you the mean for x1 and the mean for x2 you would compute the operation x equals x minus mu and you will get that type of plot if you re plot the transform data let's say X 1 tilde X 2 tilde so here it's a little better but it's still not good in order to solve the problem fully you're going to compute Sigma squared which is basically the standard deviation squared so the variance of the data and then you will divide by Sigma square so you would do that and you would make the transformation of X being equal to X divided by Sigma and it will give you a graph that is centered up so you usually prefer to to work with a centered data yeah sorry oh yeah yeah sorry sorry yeah great so if we subtract the mean of X 1 and X 2 so it should look like this but be centered okay and then if you says if you standardize it it looks like something like that so why is it better because if you look at your your your loss function now before the loss function would look like something like this and after normalizing the input it may look like something something like this so what's the difference between these two loss functions why is this one easier to Train is because if you have a starting point that is here let's say your gradient descent algorithm is going to go to towards approximately the steepest slope so you're going to go there and then this one is going to go there and then you're going to go there and then you're going to go there like that and so on until you end up at the right point but the steepest slope in this loss contour is always pointing towards the middle so if you start somewhere you will directly go towards the minimum of your loss function so that's why it's helpful usually to normalize so this is one method and in practice the way you initialize your weights is very important yeah yes so exactly so here I used a very simple case but you would divide element-wise by the Sigma here okay so like every entry of your matrix you would divide it by the Sigma or one other thing that is important to notice this Sigma and mu are computed over the training set you have a training set you compute the mean of the training set the standard deviation of the training set and these Sigma and you have to be used on the test set as well it means now that you want to test your algorithm on the test set you should not compute the mean of the test set and the standard deviation of the test set and normalize your test input through the network instead you should use the mu and the Sigma that were completed on the train set because your network is used to see this type of transformation as an input so you want the distribution of the input at the first year to be always the same no matter if it's the train or the test set here likely this leads to fewer iterations okay we have a lot to see so I will I will skip a few questions so let's let's delve a little more into vanishing and exploding radius so in order to get an intuition of why we have this vanishing or exploding Radian problem we can consider a network which is very very deep and has a two dimensional input okay and so on so let's say we have let's say we have ten layers in total 10 layers plus an output layer so assume assume all the activations all the activations are identity functions and assume that these biases are equal to 0 if you compute Y hats the output of the network with respect to the input you know that Y hat would be equal to W of layer L capital L denotes the last layer times a L minus 1 plus BL but be L is 0 so we can remove it WL x al minus 1 you know that al minus 1 is W l minus 1 times a L minus 2 because the activation is an identity function and so on you can back propagate can go back and you will get that Y hat equals W L times W L minus 1 times blah blah blah times w1 times X you get something like that right so now let's let's consider two cases let's consider where the case where the WL matrices are a little bigger than the identity function a little larger than the islands function in terms of values let's say WL including all these so all these matrices which are two by two matrices right are these ones what's the consequence the consequence is that this whole thing here is going to be equal to one point five to the power L one point five to the power L zero zero it will it will make Y hat explode to make the value of y hat explode just because this number is a tiny little bit more than one same phenomena if we had zero point five instead of one point five here the value the multiplicative value of all these matrices will be zero point five to the power L here 0.5 to the power L here and Y hat will always be very close to zero so you see the issue with vanishing exploding gradient is that all the Earth's add up like multiplied each other and if you end up with numbers that are smaller than 1 you will get totally vanished gradient when you go back if you have values that are a little bigger than 1 you will get exploding gradient so we did it as a forward propagation equation we could have done it exactly the same analysis with the derivatives assuming the derivatives of the weight matrices are a little lower than the identity or a little higher than the identity so we want to avoid that one way that is not perfect to avoid this is to initialize your weight properly initialize them into the right range of values so you agree that we would prefer the weights to be around 1 as close as possible to 1 if they're very close to 1 we probably we can avoid the vanishing and exploding radiant problem so let's look at the initialization problem the first thing to look at is example of the 1 euro if you consider this neuron here which has a bunch of inputs and outputs on activation a you know that the equation inside the neuron is a equals whatever function let's say sigmoid of Z and you know that Z is equal to W 1 X 1 plus W 2 X 2 plus blah blah blah plus W and X n so it's a dot product between the W's and the X's so the interesting thing to notice is that we have n terms here so in order for Z to not explode we would like all of this term to be small if W is are too big then this term will explode with the size of the input of the layer so instead if we have a large n means the input is very large what we want is very small w i's so the larger n the smaller has to be W I so based on this intuition it seems that it would be a good idea to initialize w i's with something that is close to 1 over n we have n terms the more terms we have the more likely z is going to be big but if our initialization says the more terms you have the smaller the value of the weights we should be able to keep Zener in a certain range that is appropriate to avoid vanishing and exploding gradients so this term to be a possible initialization scheme so in practice I'm going to write a few initial ization schemes that we're not going to prove if you interested in seeing more proofs of that you can take CS 2:30 where we prove this initialization scheme I take down the board so there are a few initialization that are commonly used and again this is this is very practical and people have been testing a lot of initializations but they ended up using those so one is to initialize the weights I'm writing the code for those of you who know one on PI not going to compile it here with whatever shape you're using element twice times the square root of one over N of L minus one so what does that mean it means that I will look at the number of input I'm writing an n L minus one here and to the L minus one I'm looking at how many inputs are coming to my layer assuming we're at layer L how many inputs are coming I'm going to initialize the weights of this layer proportionally to the number of inputs that are coming in so the intuition is very similar to what we described there so this initialization has been shown to work very well for sigmoid activation so if you use sigmoid what's interesting is if you use relu it's been it's been observed that putting a two here instead of a one would make the network train better and again it's very practical it's one of the fields that that we need more Theory on it but a lot of observation has been made so far you guys want to do that as a project to see why it is happening it would be interested okay and finally there is a more common one that is used which is called Xavier initialization which which proposes to update the weights using square root of 1 over an L minus 1 for 10h this is another one and another one that is I believe called slow initialization recommends to to initialize the weights of a layer using the foreign formula so quickly the quick intuition behind the last one the last one is very often used the quick intuition is that we're doing the same thing but also for the back propagated gradients so we're saying the weights are going to multiply the back propagated gradient so we also need to look at how many inputs do we have during the back propagation and L is the number of inputs you have during back propagation and L minus 1 is the number of inputs you have during for propagation so taking an average a geometric average of those and the reason we have a random function here is because if you don't initialize your weights randomly you will end up with some problem called the symmetry problem where every neuron is going to learn kind of the same thing to avoid that you will make the neuron starts at different places and let them evolve independently from each other as much as possible so now we have two choices either we go over regularization or optimization how much have you talked about regularization so far l1 l2 early stopping all that's really stopping everybody remembers well it is know a little bit so let's go over optimization I guess and then we will do some regularization depending on the time we have so I believe so far you've seen gradient descent and stochastic gradient descent as to possible optimization algorithm in practice there is a trade-off between these two which is called mini-batch gradient descent what is the trade-off the trade-off is that batch gradient descent is cool because you can use vectorization you can give a batch of input for propagated all at once during vac using a vectorized code stochastic gradient descent advantage is that the updates are very quick and imagine that you have the data set with 1 million images 1 million images in the data set and you want to do batch gradient descent you know how long it's going to take to do one updates very long so we don't want that because maybe we don't need to go over the full dataset in order to have a good update maybe the updates based on a thousand examples might already give us the right direction for the gradient of where to go it's not going to be as good as on a minyan example where is going to be a very good approximation so that's why most people would use mini-batch gradient descent where you have a trade-off between stochasticity and also vectorization so in terms of notation I'm going to call X the matrix X 1 X 2 X m and capital y the same matrix with wise so we have M training examples and I'm going to split these into batches so I'm going to call the first batch X 1 like this until X maybe T like that and X 1 can contain probably X 1 until X 1,000 assuming it's a batch of a thousand examples X 2 then will contain X 1000 and one until X 2000 and so on so this is the notation for the batch when I use curly brackets same for y so in terms of algorithm how does the mini-batch gradient descent algorithm work we're going to iterate so for TNT from 1 to blah blah blah - how many iteration you want to do we're going to select a batch select a batch of XK 1 XT YT you will forward propagate the batch and you will back propagate the batch so by forward propagation I mean you send all the batch to the network and you compute the lost functions for every examples of the batch you sum them together and you compute the cost function over the entire batch which is the average of the loss functions and so assuming assuming the batch is of size 1,000 this will be the the formula to compute the batch over 1,000 examples and after the back propagation of course update W L and the L for all the else for all the layers this is the equation so in terms of graph what you're likely to see is that for batch gradient descent your cost function J would have looked like that if you plot it against the number of iterations on the other hand if you use a mini batch gradient descent you're most likely to see something like this so it's also decreasing as a trend but because the gradient is approximated and doesn't necessarily go straight to the to the middle of your last to the lower point of the last function you will see a kind of graph like that the smaller the batch the more stochasticity so the more noise you will have on your cost function graph and of course if you if we plot again if we plot the last function and this was gradient descent so this is the top view of the last function assuming we're in two dimensions your stochastic gradient descent or batch gradient descent would do something like that so the difference is there seem to be less iteration with the red algorithm but the iteration are much heavier to compute so each of the green iteration are going to be very very very quick while the red ones are going to be slow to compute this is the trade off now there is another algorithm that I want to go over which is called the momentum momentum algorithm sometimes called gradient descent plus momentum algorithm so what's the intuition behind momentum the intuition is let's look at this lost contour plot and I'm doing an extreme case just to illustrate the intuition assume you have a loss that is very extended in one direction so this direction is very extended and the other one is smaller you're starting at the points like this one your gradient descent algorithm itself is going to follow the following map it's going to be orthogonal to the current contour is au term contour loss is going to go there and then there and then there and then there and so on so what you would like is to move a faster on the horizontal line and slower to the vertical on the vertical side so on this axis you would like to move with smaller updates and on this axis you want to move with larger objects correct if this happened we would probably end up in the minimum much quicker than we currently are so in order to do that we're going to use a technique called momentum which is going to look at the past gradients so look at the past updates assume we're here assume we're somewhere here gradient descent doesn't look at its past at all it just will compute the fault propagation compute the backdrop look at the direction and go to that direction what momentum is going to say is look at the past updates that you did and try to consider this past update in order to find the right way to go so if you look at the past update and you take an average of the past update you would take an average of this update going up and the update after it going down the average on the vertical side is going to be small because one went up one went down but on the horizontal axis both went to the same direction so the update will not change too much on the verts on this axis so you're most likely to do something like that if you use momentum does it make sense the intuition behind it so that's the intuition why we want to use mind and for those of you who do physics sometimes you can think of momentum as friction you know like like if you if you launch a rocket and you want to move it quickly around it's not gonna move because the rocket has a certain weight and has a certain momentum you cannot change its direction very very noisily so let's see at the implementation of of momentum gradient descent oh and I believe we're almost done right yeah okay so let's look at it in the implementation quickly so gradient descent was W equals W minus alpha derivative of the loss with respect to W what we're going to do is we're going to use another variable called velocity which is going to be the average of the previous velocity and the current weight update so we're going to use that and instead of the updates being the derivative directly we're going to update the velocity so the velocity is going to be a variable that tracks the direction that we should take regarding the current update and also the past updates with a factor beta that is B going to be the weight the interesting point is that in terms of implementation it's one more line of code in terms of memory is just one additional variable and it actually has a big impact on the optimization there are much more optimization algorithms that we're not going to see together today in C su-30 we teach something called rmsprop and Adam that's our likely the the the ones that are used the most in deep learning and the reason is if you come up with an optimization algorithm you still have to prove that it works very well on a wide variety of application between bv4 researchers adopted for their research so Adam brings momentum to the OP the returning optimization algorithms okay thanks guys and that's all for deep learning in cs2 tonight so far