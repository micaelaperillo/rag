alright hey everyone morning and welcome back so what I'd like to do today is continue our discussion of naive Bayes and in particular would describe how to use naive Bayes generative learning algorithm to build a spam classifier that will almost work right and and so today you see how the paths moving is one other idea you need to add to the naive Bayes algorithm we described on Monday to really make it work for say email spam constellation or for text oxidation and then we'll talk about a different version of naive Bayes there's even better than the one we've been discussing so far it's a little bit about advice for applying machine learning algorithms so this will be useful to you as you get started on your system across projects as well this is strategy for how to choose an algorithm and what to do first what to do second and then we'll start with intro to support so to recap the naive Bayes algorithm is a generative learning algorithm in which given a piece of email or Twitter message or some piece of text psycho dictionary and put in zeros and ones depending on whether different words appear in a particular email and so this becomes your feature representation for say an email that you're trying to classify as spam on all spam so using the indicator function notation XJ I've been trying to use a subscript J not consistently to denote the indexes and features and I to invest in training examples but we'll see I'm not going mrs. J is whether or not an indicator for where the word J appears in an email and so to build a generative model for this we need to model these two terms P of x given Y and PF y so Gaussian to strengthen Alice's models these two terms with a Gaussian and a Bernoulli respectively and naive Bayes uses a different model and with naive based in particular P of X given Y is modeled as a product of the conditional probabilities of the new features given the class label Y and so the parameters the naive Bayes model are Phi subscript Y is the class prior what's the chance that Y is equal to one before you've seen any features as well as Phi subscript J given y equals 0 which is a chance of that word appearing in a non-spam as was Phi subscript J given y equals 1 which is the chance event worth that were appearing in spam email okay and so if you derive the maximum likelihood estimates you will find that the maximum likelihood estimator you know Phi Y right just a fraction of training examples that was equal to spam and this is just the indicator function notation way of writing with all of your emails with label y equals zero and counter my fraction of them did this feature X J appeared did this word x JM here and then finally a prediction time you will calculate P of Y given X this is kind of according to Bayes rule all right so it turns out this algorithm will almost work and here's where it breaks down which is um you know so actually every year there are some CST realized some machine learning students or do a class project and some people end up submitting this to an academic conference right some some actually some some 169 class projects get submitted gos conference papers pretty much every year one of the top machine learning conferences is the conference nibs name stands the neural information processing systems and let's say that in your dictionary you know you have 10,000 words in your dictionary let's say that the nibs conference the word nibs corresponds to word number 6 0 17 right in your you know 10,000 word dictionary but up until now presumably you've not had a lot of emails from your friends asking hey do you want some of the paper to the nibs conference or not and so if you use your current you know email set of emails to find these maximum likelihood estimate the parameters you will probably estimate that probability of seeing this word given that it's spam email it's probably 0 right 0 over the number of examples that you've labeled as down email so if you clean up this model using your personal email probably none of the emails you've received for the last few months had the word nips in it maybe and so if you plug in this form for the master likely estimate the numerator 0 and so you ask me that this probably is 0 and then similarly this is also 0 over you know the number of non staff right so that's what this just this formula right and statistically is just a bad idea to say that the chance of something is zero just because you haven't seen it yet and where this will cause the naive Bayes Outland to break down is if you use these as estimates of the parameters so this is your estimate parameter Phi subscript six zero one seven given y equals one this is five such of 6001 seven given y equals zero yes and if you ever calculate this probability that is equal to a product from I equals 1 through n less you have 10,000 words appear to make us I equals 1 o P of X i given the y right and so if you train your sound classifier on the email you've gotten up until today and then after cs2 to 89 your project teammates saying it starts engine email saying hey you know we like the class project shall we consists of letting this class project to the news conference news conference deadlines you know sort of May or June those years so you know finish the house projects this December work on this some more very gently very March April next year and then maybe submit to the conference May or June of 2019 do you start getting emails on your fancy let's submit our papers in its conference then when you start to see the word nips in your email maybe in March of next year this product of probabilities will have a 0 in it and so this thing that the circle will evaluate to zero because you're multiplying a lot numbers 1 which is 0 and in the same way this well this is also 0 right and this is also 0 because there'll be that one term bring that product over there and so what that means is if you train the spam classifier today using all the data you have in your email inbox so far and if tomorrow you know or two months from now whatever the first time you get an email from your teammates that has the word nips in it your spam classifier will estimate this probability as zero over zero plus zero okay now apart from the divide by zero error it turns out that this is just a bad idea right to estimate the probably of something as zero just because you have not seen it once yet right so what I want to do is describe to you Laplace smoothing which is a technique that helps address this problem okay and let's let's uh in order to motivate Laplace moving let me use a let me use a different example for now let's see all right so you know several years ago this is this is older data but several years ago so then put a sign ID based on top of the pops movie no come back to pile up house routine my base so several years ago I was tracking their progress of the Stanford football team there's a few years ago now but that year on 9/12 a football team played to Wake Forest you know I think these are all the all the state games we played that year right and we did not win that game then on Jen Jen we played Oregon State and we did not win that game and the question is these are audio way games almost all the other state games we played that yeah and so via the you know Stanford forgot his biggest fan he followed into every single out-of-state game and watch all these games the question is after this unfortunate shriek when you go on the accession game when you as if you follow them to their over Homer game what's your estimate of their chance of their winning or losing now if use maximum likelihood so let's say this is the variable X you would estimate the probability of their winning well that's one life because really count up the number of wins right and divide that by the number of wins plus the number of losses and so in this case you estimate this as 0 divided by number of wins of 0 number of losses is 4 right which is equal to 0 ok um that's kind of mean right they lost 4 games but you say no the Chancellor that they're winning is 0 absolute certain to you and then justice lee this is not this is not a good idea and so what the process moving what we're going to do is imagine that we saw the positive outcomes the number of wins you know just add one to the number of wins we actually saw and also the number of losses at 1 right so if you actually saw 0 wins pretend you saw one that we saw four losses pretend you saw one more than you actually saw and so the plus moving gonna end up adding 1 to the numerator and adding 2 to the denominator and so this ends up being 1 over 6 and that's actually a more reasonable well maybe maybe this is a more reasonable estimate for the chance of that or losing the next game and there's actually a certain set of circumstances under which there's an optimal estimate so I didn't just make the Sabbath in there but Laplace you know ancient that well-known very influential mathematician he was actually I spent the chance of the Sun rising the next day and the reasoning was well we've seen the Sun Rise a lot of times and so but but that doesn't mean we should be absolutely certain the Sun will still rise tomorrow right and so his reasoning was well we've seen the Sun Rise 10,000 times you know we can be really certain the Sun will rise again tomorrow but maybe not absolutely certain because maybe something will go wrong who knows what happened this galaxy and so his reasoning was he derived the optimal way of estimating you know really the chance the Sun will rise tomorrow and this is actually an optimal estimate under I'll say I'll save the sail assumptions we don't need to worry about it but the sense of they assumed that you are Bayesian with a uniform Bayesian prior on the chance of the Sun rising tomorrow so if the chance of Sun rising tomorrow is uniformly distributed you're in the unit interval anywhere from 0 to 1 then after the set of observations of this coin toss of whether the Sun rises this is actually based in optimist myth of the chosen Sun rising tomorrow ok if you don't stand which is in the last 30 seconds don't worry about it this is taught in sort of Bayesian statistics that bounds Bayesian statistics classes but mechanically what you should do is uh take this formula and add 1 to the number of counts you actually saw for each of the possible outcomes and more generally if Y excuse me if if you're estimating probabilities for a way random variable then you estimate the chance of X being I to be equal to you so that's the maximum likelihood estimate and for Laplace moving you'd add one to the numerator and add K to the denominator so for naive Bayes the way the small modifies your premises I'm just going to copy over right so that's the max or likely estimate and with the pasta thing you add one to the numerator and add two to the denominator and this means they us mr. probabilities probably is they're never exactly zero or exactly one which takes away that problem of field is zero over zero and so the employers algorithm yeah it's not there's not like a great spam classifier but it's not terrible either and one nice thing about this algorithm is is so simple right estimated parameters is just counting can be done you know very efficiently right just just by counting and then classification time is just multiply a bunch of properties together this very competition efficient algorithm all right any questions about this yeah oh sorry this why oh yes thank you all right oh by the way I I was actually falling inside the football team that year so okay I love a football team they're doing much better now because a few years ago all right um so um in Indy example of Tulsa fell so far the features were binary valued and so my quick generalization when the features are multinomial value then the generalization actually here's one example we talked about predicting housing prices right that was a very first moving example let's say you have a classification problem instead which is your listing house u.s. Smith once the chance that this house would be so weird in the next 30 days so it's a classification problem so if one of the features is the size of the house X right then one way to turn the feature into this speaker would be to choose a few buckets so if the size is less than 400 square feet versus you know 400 to 800 or 800 to 1200 or greater than 1200 square feet then you can set the feature X I to one of four values right so there's how you dis precise it comes in this value feature to discrete value feature and if you want to apply naive Bayes to this problem then probably of X given Y this is just same as before product from I equals 1 through n P of XJ given Y where now this can be a multinomial probability right where if X now takes on one of four values say then this can be estimated a multinomial probably so instead of a Bernoulli distribution over to pass all comes this can be a problem the probably mass function probably over for most ball comes if you discretize the size of a Houston to four values and if you ever discretized variables typical rule of thumb in machine learning often we discretize variables into ten values into 10 buckets that's just uh it often seems to work well now I grew four here so I don't to write out 10 buckets but if you ever disguising their variables you know for most people will start off with disco sizing things into 10 down use No right and so this is how you can apply now you base other problems as well including classifying for example if a house is likely to be sold in make seventy days now um this there's a different variation on naivebayes that I want to describe to you that is actually much better for the specific problem of text classification and so a future representation for X so far was the following right with a dictionary so let's see you get an email there's you know a very spammy email that's drugs by drugs now this is meant as an illustrative example I'm not selling any of my drivers so if if you have a dictionary of 10,000 words then I guess let's say a is worth one my pockets were to just to you know make this example RP let's say that whereby is worth 800 drugs is the word 1600 and let's say now is the word is the 6,200 word in your 10,000 words in a sorted dictionary then the representation for X will be you know 0 0 right and I put a one there and one there and the one there okay now one one so um one interesting thing about naive Bayes is that it throws away the fact that the word drug says appear twice right so that's losing a little bit of elevation and and in this speaker representation you know each feature is either 0 or 1 right and that's part of why it's throws way the information that what the one where drugs appear twice and maybe should be given more weight for your classifier um there's a different representation which is specific to text and I think text data has a property that they can be very long or very short you can have a five-word email or 1,000 word email and somehow you're taking very strong or very long emails and just mapping them to a feature vector there's always the same way just a different representation for this email which is for that email this is drills by drugs and how we're going to represent it as a four dimensional feature vector so this is going to be n-dimensional for an email of length and so rather than a 10,000 dimensional feature vector we now have a four dimensional feature vector but now X J is in index from 1 to 10,000 instead of just being 0 or 1 okay and and this and I guess n varies by chain example so an eye is the length of email I say the longer email this vector the feature vector X will be longer and if you're short to email this feature vector will be shorter okay so let's see just to give names the average we're going to develop these are these are really very confusing very horrible names but this is what the community calls them the model we've talked about so far is sometimes called the multivariate Bernoulli model so Bernoulli means coin tosses so multivariate means you know they're ten thousand Bernoulli random variables in this model this is a multivariate Bernoulli event model and the event comes with statistics and answers and the new representation we're gonna talk about is called the multinomial event model these two names are frankly these two names are quite confusing but these are the names that I think actually one of my friends as McCollum as far as I know wrote the paper that named these two algorithms but but I think these are these are the names me seem to use and so with this new model we're gonna build a generative model because there's a generative model or model P of X comma Y which can be factored as follows and using the naive Bayes assumption we're going to assume that P of X given Y is product from I equals 1 through n J equals 1 to N P of XJ given Y and n times you know P of Y is that second term right now one of the one of the reasons these two models were very frankly are actually very confusing to machine learning community it's because this is exactly the equation that you know you saw on Monday when we described naive Bayes for the first time like you know this P of X in wise father probabilities is exactly so this this equation looks cosmetically identical but with this new model the second model the confusingly named multinomial event model the definition of XJ and the definition of n is very different right so instead of a product from 1 through 10,000 there's a product from 1 through the number of words in your email and this is now instead a multinomial probability rather than the binary or Bernoulli probability and it turns out that with this model the parameters are same as before if I was very at one equals one and also the other parameters of this model by K given y equals zero is a chance of XJ equals K given y equals zero right and then just to make sure you understand the notation see if this makes sense so this probability is the chance of word blank being black if they were y equals zero so what goes in those two blanks actually what goes in a second bag let's see yes says a chance of the third word in the email being aware drugs which ones are the second word in the email being by or whatever and one part of what wouldn't firstly assumes me why this is tricky is that we assume that this probability doesn't depend on J right that for every position an email for the chance of the first word being drugs the same as chances of second Rabindra to say mr. worth being drugs which is why on the left hand side J doesn't actually appear on the left hand side right any questions about this and so the way you calculate the probability the way you would and so the way that given a new email a test email you would calculate this probability is by you know plugging these parameters a USB from Tainter into this formula oh and then no I wrote down and then an Indian set the parameters is this kind of just with our y equals one is that y equals zero and then for the maximum likely resume the parameters I just write out one of them your estimate of the chance of a given work there's really any word in any position being word K what's the chance of some word in a non-spam email being the word drugs let's say the chance of that is equal to you I find it well this indicates a function notation those are complex I just say in a second what this actually means so the denominator so this space means so if you figure out what the English meaning of this complicated formula is this basically says look at all the words in all of your non-spam emails all the emails of y equals zero and look at all of the words in all of the emails and so all of those words what fraction of those words is the word drugs and that's a new estimate of the chance of the word drugs appearing in the non-spam email is position in that right and so in nav the denominator is sum over your training set indicator is not spam times the number of words in that you know so the denominator ends up being the total number of words in all of your non-stemi emails in your training set and the numerator is some of your training set some from michaeles wants um indicator y equals zero so you know concept only the things for non-spam email and for the non-spam email J equals 1 through and I go over the words in that email and see how many words are that worth K right and so if in your training set you have you know a hundred thousand words in your non-spam emails and 200 of them are the word drugs the course you know 200 times then this range would be two hundred over a hundred thousand oh and then lastly to implement Laplace moving with this you would add one to the numerator as usual and then let's see actually what would you add to the denominator wait but what is K naught K right K is a variable it's okay indexes into the words what you had 1000 oh I think I've just read why you say K I think over though the notation when defining the fast moving I think I use K is the number of possible outcomes yeah but here cases in depth yeah right so CI once a numerator and add the number of possible outcomes in denominator which in this case was there at 10,000 so so this is the probability of X being equal to the value of K where K ranges from 1 the sooner 10,000 if you have a dictionary size if you're about a list of 10,000 words you're modeling and so the number of possible values for X is 10,000 so you have 10,000 simulator Oh what do you do worse around in a dictionary so um there are two approaches of that one is just throw it away just ignore it disregard it that's one second approach is to take the rare words and map them to a special token which traditionally is denoted UNK for unknown word so if in your training set you decide to take just the top 10,000 words it sends your dictionary then everything that's down the top 10,000 words you can map to a you know unknown word token unknown where a special symbol oh why they wipe their run before oh this is an indicator function notation so indicates a function boy so if so this is this notation right means so indicator you know two equals one plus one this is true and indicates her you know V equals five but this is this is no formula that's either true or false did I know whether Y is zero I guess if Y is 0 1 this is the same as not why I again saw 1 minus y all right so I think both event models including the details of mass molecular estimates are written out in more detail in lecture notes um so you know when would you use the naive Bayes algorithm it turns out now greens algorithm is actually not very competitive of other learning algorithms so for most problems you find that logistic regression we're better in terms of delivering a higher accuracy than naive Bayes but the the advantages of naive Bayes is first is completely very efficient and second is relatively quick to implement right and it also doesn't require iterative gradient descent thing and the number of lines of code needs its employee base is relatively small so if you are facing a problem where you go is to implement something quick and dirty then naive Bayes is is may be a reasonable choice and I think you know as you work on your class projects I think some of you priority minority will try to invent a new machine learning algorithm and write a research paper and and I think you know inventing new machine learning is a great thing to do helps love people on a lot of different applications so this one the majority of constructions to t9 won't try to apply a learning algorithm to a project that you care about we applied to a research project you're working on somewhere Stanford or apply it to a fun application you want to build or apply to business application for some of you taking this on SCPD taking this remotely and if your goal is not to invent a brand new learning algorithm but to take the existing algorithms and apply them then your thumb I suggest to you is when you get started on the machine learning project start by implementing something quick and dirty instead of implanting most complicated possible learning algorithm stop influencing something quickly and train the algorithm look at how it performs and then use that to deep out the algorithm and keep innovating all right so I think you know what does a Stanford so we're very good at coming up with very very complicated algorithms but if you go is to make something work for an application you brought it and invented neither in the algorithm and published a paper on a new technical you know contribution if you if you mean go is a you're working on an application on understanding news better or improving the environment or estimating prices or whatever and your primary objective is just make an algorithm work then rather than building a very complicated out and if it all set I would recommend implementing something quickly so that you can then better understand how it's performing and then to error analysis which we'll talk about later and use that to drive your development you know one one one analogy I sometimes make is that if you are let's see so if you're writing a new computer program with 10,000 lines of code right one approach is to write all 10,000 lines of code first and then they try compiling it for the first time right and that's clearly a bad idea and then sudden you know you should write small Maude use around the attested unit testing and then build up the program incremental you rather than write 10,000 lines of code and then start to see what syntax errors he gave me for the first time um and I think a similar for machine learning instead of building a very complicated algorithm from the get-go we've got a simpler algorithm tested and then and then use the see what is doing one see what's doing wrong to improve from there you often end up getting to a better performing algorithm faster um so here's here's one example there's actually something I used to work on I actually started on conference on you know and Auntie spats you know student work on spam classification many years ago and it turns out that when you start out on a new application problem oh it's hard to know what's the hardest part of the problem right so if you want to build an anti-spam crossfire the lots of things you could work on for example spammers would deliberately misspell words you know wallet mortgage that right no refinance your mortgage or whatever but instead of writing the words mortgage the spammers would write them 0 RT GA or instead of GA keaney maybe uh slash slash right but all of us as people have no trouble meaning this is aware mortgage but this would trip up a spam filter this might map the work to an unknown word talk about just Alfred it hasn't seen this before and does the line this were to slip by the spam filter so that's one idea for improving spam or action one over here see students the hall like we actually roll the paper mapping this back to worse like that so this dental they can see their words the way that humans see them all right so that's one idea another idea might be a lot of spam email spruce email headers you know spammers often tried to hide where the email truly came from by spoofing the email header the email address on from information and and and another thing you might do is try to fetch the URLs that are referred to in the email and then analyze the webpages that you get to write but a lot of things that you could do to improve a spam filter and any one of these topics could easily be three months or six months of research but when you're building say a new spam filter for the first time how do you actually know which of these is the best investment of your time so my advice to those who work on project if your primary goes just get distinctive work is to not somewhat arbitrarily dive in and spend six months on improving this or spend you know six plans on trying to analyze email headers but just that implement a more basic algorithm almost implement something quick and dirty and then look at the examples that your learning algorithm is still misclassifying if you find that if after you've implemented a quick and dirty algorithm you find that yours the anti-spam algorithm is misclassifying a lot of examples with these deliberately misspell words there's only then they get more evidence than it's worth spending a bunch of time solving the misspelled words and deliberately misspell words problem right but you implement spam filter and you see that there's not misclassifying a lot of examples of these misspelled worst and I would say don't bother go work on something else is there or these at least treat that as a lower priority so one of the uses of GDA call centers grant analysis as well as naive bayes is that is they're not going to be the most accurate algorithms if you want the highest precision accuracy there are other algorithms like which is you Russian or as well switched up walnuts or neural networks we talk about later which will others always give you higher classification accuracy than these algorithms but the advantage of Johnson's analysis and naive Bayes is that they are very quick to train there is no literature this is just counting and GDA is just computing means and covariances right so it's very calm they efficient and also there are there are simple to implement so it can help you implement that quick and dirty thing that helps you get going more quickly and so I think for your project as well I would advise most of you to you know as you start working on your project I would advise most of you to don't spend weeks designing exactly what you're going to do if you have an advocate if you have your theory of an apply project but instead get the Dana said and apply something simple start to reach a super aggression noth-nothing your network or not not something more complicated or started nowadays and then see how that performs and then and then go from there okay alright so that's it for naive Bayes and generative learning algorithms the thing I want to do is move on to a different type of classifier which is a support vector machine let me check out any questions about this with Wyman's long oh wait oh sorry oh can use logistic regression with discrete variables oh I see yeah right yes so yes right so one of the weaknesses of the naive Bayes algorithm is that it treats all the words completely you know separate from each other so right there was one and two are quite similar and whereas the only mother and father are quite similar and so but with this feature better representation it doesn't know the relation means these words so in machine learning there are other ways of representing words this technique called word embeddings in which we choose the feature representation that encodes the fact that the worst one and two are quite similar to each other are the worst mother and father question as each other you know they're worse whatever London and Tokyo are quite suit each other because they're both city names and so this is a technique that I was not planning to teach Europe but that is taught in CS 2:30 but you can also read up on word embeddings or look at some of their videos or reasons from CS 2:30 you want to so the word embeddings technique this is a technique from neural networks really will reduce the number of training examples you need to learn a good tech sauce fire because it comes in with more knowledge victim by the way I do this in the other classes too inside the other classes something got a question I go no we don't do that we just covered as ICSC jt9 so actually ICS 224 and I think also covers this yeah the NLP appreciate yeah I'm sure okay so so support vector machine says be ins um let's see the classification problem right whether they said looks like this and so you want an algorithm to find you know like a non linear decision boundary right so the support vector machine will be an algorithm to help us find potentially very very nonlinear decision boundary is like this now one way to build a classifier like this would be to use logistic regression but if this is X 1 this is X 2 right so the logistic regression will fit a straight line to Zeta a Gaussian distribution Valerie so one way to apply this is arrested like this would be to take your feature vector x1 x2 and map it to a high dimensional feature vector with you know x1 x2 x1 squared x2 squared x1 x2 may be X Y cube x2 cube and so on and have a new feature vector which we'll call the value of x that that has these high dimensional features right now it turns out if you do this and then apply logistic regression to this augmented feature vector then logistic regression can learn nonlinear decision boundaries with this other features there's just regression if you actually learn the decision boundary there's this there's a shape of an ellipse but man they'll be choosing these features is a little bit of a pain right know what I did I actually don't know what you know type of all set of features could get you a decision valve you like that right rather than just in the lips and more complex is your mouth to me and what we will see with support vector machines is that we will be able to derive an algorithm that can take say input features x1 x2 map them to a much higher dimensional set of features and then apply a linear classifier in a way similar to logistic regression but different in details that allows you to learn very nonlinear decision boundaries and I think you know a support vector machine one of the actually one of the reasons support vector machines are used today is is a relatively turnkey algorithm and what I mean by that is it doesn't have too many parameters to fiddle with even for logistic regression or for linear regression you know you might have to tune the gradient descent parameter a tune the learning rate sorry change the learning rate alpha and that's just another thing that fiddle worked very try a few values and hope you didn't mess up how you said that value Oh a support vector machine today has the very robust very mature software packages they can just download to train a support vector machine on on any on you know on a problem and you just run it and the algorithm will kind of converge without you having to worry too much about the details so I think on the grand scheme of things today I would say support vector machines are not as effective as neural networks for many problems but but one dream propria support vector machines is this is turnkey you kind of just turn the key and it works and there isn't as many parameters like the learning rate and other things that you have to fiddle with so the roadmap is we're going to develop the following set of ideas talk about the optimal margin classifier today and we'll start with the separable case and what that means is going to start off with data sets that we assume look like this and that are linearly separable and so the also margin classifier is the basic building block of a support vector machine and will first derive an algorithm don't be they'll have some similarities to a logistic regression but that allows us a scale in the important way it's that to find a linear classifier for training cells like this that we assume for now can be linearly separated so we'll do that today and then what you see on Wednesday is excuse me next Monday wait you see next Monday is an idea called kernels and the kernel idea is one of the most powerful ideas in machine learning is how do you take a feature vector X maybe this is r2 and map it too much high dimensional set of features in our example there that was our 5 right and then train an algorithm on this higher dimensional set of features and and the cool thing about kernels is that this high dimensional set of features may not be r5 it might be our 100,000 or it might even be our infinite and so with the kernel formulation we really take you know the original set of features that you were given for the houses you trying to sell you know medical conditions try to predict and map this two dimensional feature vector space into maybe an infinite dimensional so the features and what this does is it relieves us from a lot of the burden of manually picking features right like do you want to have square root of x 1 or maybe X 1 X 2 to the power of 2/3 so you just don't have the fiddle of these features too much because the kernels will allow you to choose an infinitely large set of features okay and then finally we'll talk about the inseparable case so I'm gonna do this today and then this next Monday so and by the way I you know the machine there in the world's become a little it's funny I think the if you leap well in the news the media talks a lot about machine learning the media just talks about you know neural networks all the time right and you hear about neural networks and deep learning wrote the later in this class but if you look at what actually happens in practice in machine learning the set of algorithms actually used in practice it's actually much wider than neural networks and deep learning so so we do not live in a neural networks only world we actually use many many tools in machine learning it's just that deep learning attracts the attention of the media in some this in some way there's quite disproportionate to what I find useful you know that's like I love them but but they're not they're not the only thing in the world and so yeah late last night I was talking an engineer about factor analysis which you learn about latency s39 right unsupervised learning algorithm and there's an application that one of my teams is working on in manufacturing where I'm gonna use factor analysis or something very similar to it which which is totally not a neural network technique right for so they're there all these other technique is that including support vector machines in ninety days and I think do can use are not important all right so let's start developing the optimal margin classifier so um first let me define the functional margin which is informally the functional margin of the classifier is how well how confident I and accurately do you classify an example so here's what I mean we're gonna go to binary classification and we're gonna use logistic regression right so so let's start by motivating this with logistic regression so deserve which is a classifier H of theta equals the logistic function applied to theta transpose X and so if you turn this into a binary classification if you have this algorithm predict not a probability but predict 0 or 1 then what does classifier will do is predict 1 if theta transpose X is greater than 0 right and predict 0 otherwise okay because theta transpose X is greater than 0 this means that G of theta transpose X is greater than 0.5 you can I've created emigrate to them an equal to it doesn't matter yes and exactly 0.5 it doesn't really matter what you do and so you predict 1 if theta transpose X is greater than equals 0 meaning that all probability the estimator probably over cause being 1 is greater than 50/50 and so you predict 1 and if theta transpose X is less than 0 then you predict that this class is 0 ok so this is what happen if you have largest regression output 1 or 0 rather than output or probability so in other words this means that if Y I is equal to 1 right then hope or we want that theta transpose X alright it's much greater than 0 this double greater-than sign it means much greater right because if the true label is 1 then if the album is doing well hopefully theta transpose X right will be faster there right so the output probability is very very close to and if indeed theta transpose X is much greater than zero then G of theta transpose X will be very close to one which means that is giving a very good very accurate prediction very correct and confident prediction right there goes one and if Y is equal to zero then what we want or what we hope is that theta transpose X I is much less than zero right because if this is true then the algorithm is doing very well on this example so um so the functional margin which will define in a second captures this idea that if the Kasbah has a large functional margin it means that these two statements are true um it's a local henro bit there's a different thing we define in a second there's the counted geometric margin and that's the following and for now let's assume the data is linearly separable so let's say that's the data set now that seems like a pretty good decision boundary for separating the positive and negative examples that's another decision boundary in red that also separates a positive negative examples but somehow the Green Line looks much better than the red line so why is that well the red line comes really close it's a few of the training examples whereas the Green Line you know has a much bigger separation right just as a much bigger distance from the positive a negative example so even though the red line and the Green Line both you know perfectly separate the positive and negative examples the Green Line has a much bigger separation which is called the geometric margin there's a much bigger geometric margin meaning of physical separation from the training examples even as it separates them okay and so what I'd like to do in mix several I guess with next 20 minutes is formalize definitely functional margin and formalized definition of geometric margin and it will pose that I guess the optimizing classifier which is based in algorithm that tries to maximize the geometric margin so what the rudimentary SVM does what the Hesby mlo dimensional spaces will do also called the optimal margin classifier is pose an optimization problem to try to find a green line to classify these examples so now in order to develop svms I'm going to change the notation a little bit again yeah because these algorithms have different properties using slightly different notation to strive to make something math-amazing so when developing SVM's we're going to use minus 1 and plus 1 to denote the cost labels and we're going to have output so rather than having hypothesis output a probability like you saw in logistic regression the support vector machine will output either minus 1 or plus 1 and so G of Z becomes minus 1 or 1 so I'll put 1 if Z is greater than equal to 0 and minus 1 otherwise so instead of a smooth transition from 0 to 1 we have a hard transition an abrupt transition from negative 1 to plus 1 and finally where previously we had for logistic regression right where this was our n plus 1 with x0 equals 1 for the SVM we will have h of just write this out so for the SVM the parameters of the SVM will be the parameters W and B and hypothesis applied to X will be G of this and we're dropping the X 0 equals 1 constraint so separate out W and B as follows so this is a standard notation used to develop support vector machines and one way to think about this is this if the parameters are you know theta 0 theta 1 theta 2 theta 3 then this is a new B and this isn't UW so you just separate out the theta 0 which is previously Mouse playing to n 0 and so on and so this term here becomes sum from I equals 1 through n WI x pi plus b right super conservative x 0 all right so let me formalize the definition of a functional question so so the parameters W and B to find a linear classifier right so you know what the form is just wrote down the parameters W and P defines a really defines a hyperplane but defines a line or in high dimensions it be a plane or a hyperplane but defines a straight line stepping out the positive and negative examples and so we're gonna say the functional margin so function margin of a hyperplane defined by this with respect to one training example we're going to write as this and hyperplane just means straight line right but in high dimension so this linear classifier so it's just you know functional margin of this classifier respect to one training example we're going to define as this and so if you compare this with the equations we had up there you know if y equals one we hope for that at y equals zero we hope for that so really what we hope for is for a classifier to achieve a large functional margin right and so so if y I equals 1 then what we want or what we hope for is that ee transpose X I plus B is greater than my creatinine 0 and after they both equal to minus 1 then we once I'll be hope that this is much smaller than zero and if you kind of combine these two statements if you take why I write and multiply it with that then you know these two statements together is basically saying that you hope that gamma hat I is much greater than 0 because Y I know is plus 1 or minus 1 and and so Y is equal to 1 you want this to be very very large if Y is negative 1 you want this to be a very very large negative number and so either way it's just saying that you hope this would be very large so we just hope that and and as an aside one property of this as well is that so long as gamma hat is greater than 0 that means the algorithm so so so long as the functional margin so long as this gamma hat is greater than 0 it means that either this is bigger than 0 this is less than 0 depending on the sign of the label and it means that the algorithm yes this one example corrector is right and much greater than 0 then it means you know so the scaling Zira means in in the logistic regression case it means that the prediction is at least a little bit above 0.5 and low Pitbull 0.5 probably at least gets it right and it was much greater than 0 much less than 0 and Jesus you know the probability output in the loose aggression cases are very close to one or very close to zero so one of the definition I'm going to define functional margin respect to the training set to be gamma hat equals min over I here R equals ranges over your training examples okay so this is a worst-case notion but so this definition of a function margin on the Left we define functional margin respect to a single training example which is how are you doing on that one training example and we'll define the function margin with respect to the entire training set as how are you doing on the worst example in your training set this is a little bit of a brittle notion and we're for now for today we're assuming that the training set is linearly separable so I'm gonna assume that the training set you know it looks like this and separative of a straight line or the Laxus later but because we're assuming just for today that the training set is linearly separable we'll use this kind of worst-case notion and defined the function margin to be the function margin of the worst training example now one thing about the definition of the functional margin is they're actually really easy to cheat and increase the functional margin right and one thing you can do is look at this formula is if you take W you know and multiply it by two and take B and multiply by two then everything here just multiplies by two and you've doubled the functional margin right but you haven't actually changed anything meaningful okay so so one one way to cheat on the functional margin is just by scaling the parameters by two or in seven - maybe you can multiply all your parameters by ten and then you've actually increased the functional margin of your training examples 10x but this doesn't actually change the decision boundary right it doesn't actually change any classification just to multiply all of your parameters by a factor of ten um so one thing you could do is replace one thing you could do would be to normalize the length of your parameters so for example hypothetically you can impose a constraint the normal W is equal to one another way to do that we could see W and E and replace it with W over right justify your parameters through by the magnitude by their by the Euclidean length of the parameter vector W and this doesn't change any classification is just V scaling the parameters but but but it prevents you know this way of cheating on the focused on margin okay and in fact more generally you could actually scale W and V by any other value you want and it doesn't doesn't matter could choose certain places by w over 17 and then P over 17 before any other very right and the classification stayed the same okay so we'll come back and use this property all right so to find a functional margin let's define the geometric margin and you see in a second how did your metric on the function margin relate to each other um so there's less let's define the geometric margin with respect to a single example which is um so let's see let's say you have a classifier right so given parameters W and V that defines a linear classifier and the equation W X plus B equals zero defines the equation of a straight line so the axis here I think's more than x2 and then half of this plane you know in this half of the plane you have W transpose X plus B is greater than 0 and in this half you have W transpose X plus B is less than 0 and in between this straight line but given by this equation W transpose X plus B equals 0 right and so given parameters W and B the upper right is where your classifier will predict y equals 1 and the lower left is well predict y is equal to negative 1 ok now let's say you have a one training example here so that's a training example X I comma Y I and that say is a positive example ok and so your classifier is cause find this example correctly right because and the upper right half plane you're in this half plane double transpose X plus B is greater than 0 and so this upper-right region your classifier is predicting +1 right where is it this low hot region it be predicting H of X equals negative 1 and that's why this straight line where switches from predicting negative to positive is the decision boundary so what we're going to do is define this distance to be the geometric margin of this training example it's that you couldn't distance is what we're define to be the geometric immersion so let me just write down what that is so the geometric margin you know the classifier of the hyper plane defined by WB we respect to one example X my eye this is going to be gamourai equals and let's see I'm not proving why this is the case the proof is given an election notes but the legend else shows why this is the right formula for measuring the Euclidean distance that I just drew a picture up there okay but then I'm not proving this here but the proof is giving election that was me this turns out to be the way you compute the Euclidean distance between you have an example and in the decision boundary okay um and and and this is for the positive example I guess more generally going to define the geometric margin to be equal to this and this definition applies to positive examples and to negative examples and so the relationship between the geometric margin and the functional margin is that the geometric margin is equals in a personal margin divided by the norm of W finally the geometric margin with respect to the training set is we're gain use this worst-case notion look through all your training examples and pick the worst possible training example and that is your geometric margin on the training set oh and and so I hope that certain notation is clear right so gamma hat was the functional margin and gamma is a geometric margin and so what the optimal margin classifier does is choose the parameters W and B to maximize the geometric margin okay so in other words this is the optimal margin classifiers it's the baby SVM SVM for linearly separable call data at least for today so the optimum arching crossfire would choose that straight line because that straight line maximizes the distance or maximizes the geometric margin so all of these examples know oh how you pose this mathematically they're few steps of this derivation I don't want to do but I'll just describe the beginning step and the last step and leave that in the in between steps the lecture notes but it turns out that one way to pose this problem is to maximize gamma W and B of gamma so you want to maximize the geometric margin subject to that subject to that every training example must have geometric margin greater than or equal to gamma right so you want gamma to means bigger possible subject to that every single training example must have at least as I mentioned this causes you to maximize the worst-case geometric motion and it turns out this is um not in this form this isn't a convex optimization problem so it's difficult to solve this without don't like green design initially there's no little glassware and so on but it turns out that by a few steps are be writing you can reformulate this problem as into the equivalent problem which is a minimizing normal W subject to the dramatic margin and so it turns out so I hope this problem makes sense right so this problem is just you know solve for W and B to make sure that every example test your metric margin creating equal gamma and you want gather to be as big as possible so this is a way to find the optimization problem that says maximize the geometric margin and what we show in the lecture notes is that through a few steps you can rewrite this optimization problem into the following equivalent form which is to try to minimize the normal W subject to this and maybe one piece of intuition to take away is you know the smaller W is the bigger right the less of a normalization Division effect you have right but the details are given in lecture notes ok but this turns out to be a convex optimization problem and if you optimize this then you will have the optimal margin classifier and they're very good numerical optimization packages to solve this optimization problem and if you give this a data set then you know assuming your data is separable we'll fix that assumption well reconvene next week then you have the optimal knowledge and classifier where should be the baby svf and we add kernels to it then you have the full content of C alright let's break for the day see see you guys