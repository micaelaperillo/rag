alright hey everyone welcome back so what we'll see today is additional elaborations on the e/m on the expectation-maximization harbor volley and so what you see today is go over you know quick recap of all we talked about eeehm on Monday and then describe how you can monitor if eeehm is converging and on Monday we talked about the mixture of gaussians model and started deriving Ian's for that I won't just take these two equations and map it back to specifically the E&M steps that you saw for the mixture of gaussians models to see exactly how these map - you know updating the weights of UI and so on how you should derive the m-step and then most of what I went spend today talking about is the model called the factor analysis model and this is model useful but for data there can be very high-dimensional even when you have very few training examples so what I want to do is talk a bit about properties of Gaussian distributions and then describe the factor analysis model some more about Gaussian distributions and then we'll derive e/m for the factor analysis model and once talked about factor analysis with two reasons I guess one is you know useful algorithm in an episome right and second the derivation for ian's for factor analysis is actually one of the trickier ones and there are key steps and how you actually derived in E and M steps that I think you learn better or you better master better by going through the factor analysis example ok um so just a recap last Monday or on Monday we had talked about the Ian algorithm and we wound up figuring out this estep and this n step remember that if this is the log likelihood that you trying to maximize what the estep does is it constructs a lower bound then this is a funk theta so this thing on the right hand side this is a function of the parameters theta what we proved last time was that that function is a lower bound of the log-likelihood right and depending on what you choose for Q you get different lower bound so one choice of Q you make it this little bow for a different choice Q might get that lower bound for a different choice you may get that low about and what the e set does is it uses Q together lower bound this tight that just touches the lock like here at the current value of theta and what the M set does is it chooses the parameters later that maximizes that all right so those are eeehm algorithm that we saw now um I want to step through how you would take this you know slightly abstract mathematical definition of VM and derive a concrete algorithm that you would implement right and in you know in Python and so let's let's just step through this for the mixture of gaussians model so for the mixture of gaussians model we had a model for P of X I comma Z I which given CI times P I and a model was that Z is a multinomial with some set of parameters Phi oh and so you know the probability of Z I J is equal to Phi J right so Phi is just a vector of numbers that sum to one specifying what's the chance of Z being each of the K possible to speak values and then we have that X i given the CI equals J that that is Gaussian with some mean and what we said last time was that um this is a lot like the Gaussian destroying an Alice model and the the trivial one trivial difference is this a sigma j instead of Sigma right GTA call centers analysis had the same Sigma every cost but that's not the key difference the key difference is that in this density estimation problem Z is not observable z is a latent random variable Raven which is why we have all this machinery of so now that you have this model let's see so now that you have this model this is how you would derive the E and the M steps right so the e step is you know you have Q I of CI right but let me just write this as qi of Zi equals J this is sort of the probability of Z I equals J I know this notation a little bit strange but under the Qi distribution what do you want the chance of Z being equal to J right and so in the estep you were said that to P of Z I equals J given X I parameterize by all of the parameters and we actually saw with Bayes rule right how you would fetch this out okay and what we do in the Estep is saw this number right in what we wrote as W IJ last time okay and so you remember if you a mixture of two gaussians maybe that's the first girls in the second gaussian you have an example X I here or so looks like it's more likely I come from the first and second girls yet and so this would be reflected in W IJ that that example is assigned more to the first gaussian then to second gaussian so what you implement encodes is you know you write code to compute this number and store it in W IJ and then for the m-step you will want to maximize over the parameters of the model right fine mu and signal these are the parameter parameters of the mixture of gaussians of some of the why some of the Zi and so the way you actually derived this is your write this as sum of I so Zi you know takes on the certain to speak to the value so Zi to turn turn Zi into J right so Zi can be I guess one or two of you have make sure to gaussians you sum of all the indices of the different clusters W IJ times log of the numerator is going to be times pi J that's the numerator and so you know this term is equal to this first gaussian term times that second term right because this term is P of X i given Z I write the parameters and this is just Q and then if you take this and divide it by W IJ okay so I'm gonna step you through the steps you would go through if you're deriving um using that you know you step in M said we wrote up above but if you're deriving this for the mixture of gaussians modeled and these are the steps of algebra so in order to perform this maximization what you will do is you want to maximize this formula right this big double summation with respect to each the parameters Phi mu and Sigma and so what you would do is you know take this big formula right and take the derivatives with respect to each of the parameters so you take the derivative respective mu J yeah thought that out is that big formula on the left set it to zero right and take and then and then it turns out if you do this you will derive that nu J should be equal to some K and this is what we said is how you update the means mu right the WI J's are the strength with which X I so W IJ is informally this is the strength with which Xie is a sign right - Gaussian and more formally this is really P of Z I equals J given X I the parameters and so so you end up with this formula but the way you compute this formula is by the the rigorous way to show this is the right formula updating you J is looking at this objective taking derivative saying there is equal zero to maximize and therefore deriving that equation for new J you know by by solving for the value of MU J that maximizes this expression and similarly you know you take derivatives respective this thing respective fires said it's a zero take derivatives of this thing and set that to zero and that's how you would derive the update equations in the m-step for fire and for sigma as well okay um so and and so for example when you do this you find that the optimal value for Phi is we had this near the start of Monday's lecture as well okay um so this is the process of how you would look the estep CM steps I wrote up and apply it to a specific model such as the mixtures of gaussians model and that's how you you know solve for the maximization in DM step okay and so what I'd like to do today is describe the application of VM to a more complex model called the factor of analysis model and so it's important that so I hope you understand the mechanics of how you do this because we're gonna do this today for a different model questions about this with why move on oh so in order to you know foreshadow a little bit what we'll see when it comes down to the mixture of gaussians model excuse me the factor analysis model which we talked about which is what was famous today talking about in the factor analysis model instead of Zi being discrete Zi will be continuous right and the paper zi will be distributed Gaussian so in the mixture of gaussians model we had a joint distribution for X and Z where X was a discrete random variable so in the factor analysis model will describe a different model you know for p.m. X and Z where Z is continuous and so instead of sum over Zi just be an integral over Zi of d zi right so instead of sum becomes integral and and it turns out that yeah well right yeah and it turns out that if you go through the derivation of the EEM algorithm that we worked out on Monday all of the steps with Jenson equality all of those steps work exactly that's before many of you check every single step for whether Zi was continuous it work the same as before if you have changed the sum to an integral all right so so I want to mention one other view of yam that's equivalent everything we've seen up until now which is um let me define J of theta comma Q okay says that phone that you've seen a few times now what we proved on Monday was um Ella theta is greater than or equal to J of theta comma Q right and this is true for any theta and any choice of Q okay so using using Jensen's inequality you can show that you know J for any choice of theta and Q is a lower bound for the log-likelihood of theta so it turns out that an equivalent view of PM as everything was seen before is that in the Estep what you're doing is maximize J with respect to Q and in the m-step maximize J with respect to theta right so in the East step you're picking the choice of Q that maximizes this and it turns out that the choice of Q we have will set J equal to L and then M step maximizes this respect to theta and pushes the value of L even higher so this algorithm is sometimes called coordinate ascent if you have a function of two variables and you are twice respected this also has respect to this they go back and forth and optimize the respective one at a time that's a procedure that sometimes called coordinate ascent because you're maximizing respect to one coordinate at a time and so e/m is a coordinate ascent algorithm relative to this cost function J right and and and you know and on every iteration J ends up being sent to L which is why you know that as the algorithm increases J you know that the log likelihood is increasing on every iteration and if you want to track whether the e/m algorithm is converging or how was converging you can plot you know the value of J or the value of L on successive iterations and see this very valid data is going are monotonically and then when it plateaus and isn't improving anymore then you might have a sense that the algorithm is converging all right okay so that's it the basic algorithm and make sure of gaussians what I want to do now is and it's going to talk about the factor analysis out all right so um you know that the factor analysis algorithm will work actually so over the compare and contrast mixture of gaussians with factor analysis are talking about a little bit which is uh for the mixture of gaussians let's say N equals 2 and M equals 100 right see if a dataset with two features x1 and x2 so any two or two and maybe of a data set that looks like this you know then make sure to gaussians we were a pretty good model for this data set right now say one gaussian there for the second gaussian there you can kind of capture a distribution like this with a mixture of two gaussians and this is one illustration of when when you apply make sure gaussians in this picture M is much bigger than n right you have a lot more examples then you have dimensions where I will not use mixture of gaussians and where you see the minute factor analysis will apply maybe if M is about similar to N no even n is or even M it's much less than okay and so um just for purposes of illustration let's say M equals 30 and equals 100 right so let's say you have a hundred dimensional data but only thirty examples and so to make this more concrete you know many years ago there was a Stanford PhD student that was placing temperature sensors are around different standard buildings and so what you do is model you measure the temperature at many different places right around campus but if you have a hundred sensors you know taking a hundred temperature readings around campus but only thirty days of data or maybe thirty examples then you would have a hundred dimensional data because each example is a vector of a hundred temperature readings you know at different points around this building say but you may have only thirty examples of if you say thirty thirty such vectors and so the application that the Stanford PhD student at the time was working on was he wants a model P of X right so this is X as a vector of a hundred sends a hundred temperature readings because if something goes wrong for example it has a bad case if there's a fire in one of the rooms then there'll be a very anomalous temperature reading in one place and if you can model P of X and if you ever observe a value of P of X that is very small you would say oh looks like there's an anomaly there right and we ratio let's worry about fires on Stanford the use cases was it an energy conservation if someone unexpectedly these a window open in the building you were studying you know and it was hot and was it and it's it's winter and it's warmer inside the building and ku air blows in and the temperature of one Broome Johnson and all this way you want to realize that something was going wrong with the windows or the or the temperature in part of the building okay so for the application like that you need to model P of X as a Joint Distribution over you know all of the different senses right actually if you imagine maybe just in this room let's say we have thirty senses in this room then the temperatures that the thirty different points in this room will be highly correlated with each other but how do you model this vector of a hundred hundred original vector with relatively small training set so it turns out that the problem with applying a Gaussian model right so one thing could do is model this as a single Gaussian and say that X is distributed right and if you look at your training sets of thirty examples and find the maximum likely estimate parameters you find that the maximum likelihood estimate of MU is just the average and the best of like the estimate of Sigma is this but it turns out that if M is less than equal to n then Sigma this covariance matrix will be singular and singular I just means our non-invertible I'll step for an illustration in a second but if it looks like formula for the Gaussian density right so the Gaussian density kind of looks like this right abstracting away some details and when the covariance matrix is singular then this term this determinant term will be zero so you end up with 1 over 0 and then Sigma inverse is also undefined or blows up to infinity or depending how you think about it right so you know the inverse of a matrix like um 110 right would be I guess 1 1 over 10 and an example of a non invertible matrix so singular matrix would be this and you can't actually calculate the inverse of that matrix right so it turns out that um if your number of training examples is less than the dimension of the data if you use the usual formula to derive the maximum likelihood estimate of Sigma you end up with a covariance matrix that a singular singular just means non-invertible which means the covariance ratios would look like this and so you know the Gaussian density we try to compute P of X you get you can't get infinity over 0 oh sorry not actually zero over zero doesn't matter it's all bad um and I think let me just illustrate what this looks like which is um let's say M equals 2 and N equals 2 right so you have two dimensional data x1 and x2 and so N equals two and the number of training examples 0 2 so it turns out that let's see so you see me draw contours of Gaussian densities like this right like the lips is like that it turns out that if you have two examples the two dimensional space and you compute the most likely maximum likelihood estimate the parameters of the Gaussian if it is a there then it turns out that these contours will look like that except that instead of being very thin as I'm drawing it it'll be it'll be infinitely skinny see and ever be Gaussian density where I can't draw lines you know of zero width on the whiteboard right but it turns out that the contours will be squished infinitely thin so you end up with a Gaussian density all of whose mass is on the straight line over there with infinitely thin cons was just you know this is question centers on the on the plane I guess or on the line connecting these two points and so this is so first there are practical numerical problems right as in your network so zero over zero if you try to compute P of X for any example and second this is this very poorly conditioned Gaussian density puts all the Prairie mouths on this line segments and so any example right over there just a little bit off has no probably mas because has probably loss of zero a probably density of 0 because the Gaussian is squish infinitely thin you know on that on that line okay but but you know this just not be good just this is just not a good model right for this danger so what we're gonna do is uh come up with a model that will work even for for these applications even even for a dataset like this right there's actually a I think the the origins of the factor analysis model one of the very early applications was actually a psychological testing where if you have a you know administer a psychology exam two people to measure different personality attributes right so you might measure you might have a hundred questions or measure a hundred psychological attributes but have a data set of thirty persons right okay you know doing doing psych research collecting you know running survey data is harder it's a myriad of a sample of 30 people and each person answers a hundred quiz questions and so each person is one gives you one example X and the dimension of this is hundred image now B of only thirty of these and so if you want to model P of X try to model how correlated are the different psychological attributes of people rate um Louis intelligence correlated math ability is that correlated with language ability is that correlated with other things then how do you build a model for P of X okay alright so um if the standard Gaussian model doesn't work let's look at some alternatives one thing you could do is constrained Sigma to be diagonal right so Sigma is a covariance matrix is an N by n covariance matrix so in this case be one hundred one hundred matrix but let's say we constrain it to just have diagonal entries and zeros on the off diagonals right so these giant zeroes I mean the diagonal entries of the square matrix are these values and all of the entries of the diagonals you're set to zero so that's one thing you could do and this turns out to be this turns out to correspond to constraining your Gaussian to have axis aligned contours so this is a Gaussian with zero diagonals this would be another one this would be another one so these are examples of Gaussian of contours of Gaussian densities with zero off diagonal so the axis here and thanks more than x2 right whereas you cannot model something like this if you're 0 if you're off diagonals are 0 and so if you do this the maximum likely estimate of the parameters Sigma J is pretty much what you'd expect actually the maximum like give you an estimate of the mean vector mu is the same as before and this is Mathematica estimate of Sigma J right this is kind of not a huge surprise gonna put you things back oh and it turns out that and so the covariance matrix here as n parameters instead of in spirit or about n square over 2 parameters the covariance matrix Sigma now just as in parameters which the n diagonal entries now the problem with this is that this molding assumption assumes that all of your features are uncorrelated right so you know this just assumes that any two features they connect share are completely uncorrelated and if you have temperature sensors in this room there's just not a good assumption to assume the temperature at all points of this room are completely uncorrelated completely independent each other or if you measure yes the glaciers the people is just not a great assumption to assume that you know the different different psychological measures you might have a completely independent so while this model would take care of the problem the the technical problem of the covariance matrix being singular you can't fit this model on a hundred dimensional data set with 30 examples you can fit this you won't get in you this you could build this model you won't run into numerical or singular cover and species size problems it's just not a very good model you're just assuming nothing is correlated than anything else something else that you can do is make an even stronger assumption so this is an even worse model but I go through it because there'll be a building buffer will actually do later which is constraint Sigma to be Sigma equals lowercase Sigma squared times R right and so constraint Sigma to be done not only diagonal but to have the same entry in every single element so now you've gone from I guess n parameters to just one parameter right and this means that you're constraining the covariance matrix the constraining the gaussians you used to have circular control so this is an example of what you can model and this would be another example this is another example okay so you could model things like this where every feature not only is every features uncorrelated but every feature further has the same variance as every other feature and maximum likelihood is this not a huge surprise does the average over the previous values so what we'd like to do is not quite used either of these options right which assumes really bigger problems as soon as the features are uncorrelated and what we'd like to do is build a model that you can fit even when you have very high dimensional data and the relatively small number of examples but that allows you to capture some of the correlations right so if we have 30 temperature sensors in this room you know probably there are some correlations very probably decided in the room temperature is gonna be correlated data of insertion correlated and maybe the ambient temperature in this whole building or the temperature of this room pretty girls up and down there's a whole but maybe some of the lands on the side heat up that's out of the room a bit more so different different there are correlations but maybe you don't need for covariance matrix either so what what factor analysis we'll do is give us a model that you can fit even when you have you know hundred emissions a there are thirty examples they captures some of the correlations but that doesn't run into the onion vertical covariance matrices that the naive Gaussian model does alright so let me just just track anyone let me let me describe the models check any questions by anyone oh sure yes yes there is one thing you can do a common thing to do is apply which hot Pryor and what that boils down to is as a small diagonal value to that to the maximum I could estimate it kind of in a technical sense it takes away the non-invertible matrix problem there's actually not the best algorithm from other types of danger the the the Wishart or inverse Wishart prior yeah as you know basically you take the maximum likely the Sigma and add you know some constant to the diagonal it take has the problem in a technical way but it's not it's not the best model for a lot of data says I see oh yes why do you think about option two when it's like even worse than option one um yes option two is not a good option but I need to use this as a building block for factor analysis so you see this is a small component of C I actually plan these things out and I actually took on it's just just a mention you know just mentioned some things I see yeah she's actually did the machine there any work he balls all the time we should find it fascinating but they look at all the big tech companies um a lot of the large tech companies they're all like working on exactly the same problems right every large tech company you know software AI complete his work on machine translation every one of them works on speech recognition every one of them works on face recognition and I've been part of these teams myself right and I think it's great that we have so much progress in machine translation cuz there's so many people in so many large companies work on machine translation it's actually really happy to see so much progress in these problems that every single large tech company large software a Irish tech company works on um one of the fascinating things I see is that because of all this work in the large tech companies work on very similar problems one of the really overlooked parts of the machine there any world is a small data problems right so they're all working big data representing English and French and Chinese and Spanish sentences it's in across small does it work and I think there's actually a lack of attention like a disproportionate small amount of attention on you know small data problems we're instead of a hundred million images you maybe have 100 images and so some of the teams I work with these days actually landing a I actually spent all my time thinking about small danger problems because a lot of the practical applications of machine learning including the wrong things you see in your class projects or actually small data problems right I think when when onion works with the healthcare system where our substantive Hospital force all the problems you only have 100 examples only a thousand only 10,000 you don't have a million patients or the same medical condition and so I think that a lot of these models so in the game earlier this week I was using slightly modified version factor analysis on the manufacturing problem at landing AI right and I think a lot of these small data problems are actually we're allowed the exciting work is to be done in machine learning and is somehow it feels like a blind spot or the feels like girls like a a good gap of a lot of their work done into a I world today yeah why don't we use the same arrow as a swan big data it turns out that you know it turns out if you look at computer vision world right is a dataset everyone's working on now now we're pasta we don't really use anymore called image net which had a million images and so the tons of computation architectures that have been heavily designed for the use case of if you have exactly 1 million training examples it turns out that the algorithms that work best maybe have 100 training examples is you know looks like is different than the best learning algorithm I think and so I think right now we actually I think the Machine there any world we are not very good at understanding the scaling the best algorithm for one training example you know as far as we are able to invent algorithm mister community is different than best algorithm for a thousand best earn per million is actually different than actually and Facebook published paper we see with 3.5 billion images there's no school there's very large right so was I think we don't actually have a good understanding of how to modify our algorithms set one algorithm work on every single point of the spectrum green from one example to like a billion examples and so there's a lot of work optimizing for different points of the spectrum and I think there's been a lot of work optimizing for Big Data which is great you know built some of these large systems to handle whatever I petabytes of data a day that's great but I feel like relative to the number of application opportunities there there there's a lot of work on small data as well that I find very exciting that that and I think of this as an example the reason I was using this literally well modified version of this earlier this week on the manufacturing problem is because there isn't that much data in those scenarios right all right those them off topic but let's go and describe well hopefully maybe yeah so this stuff does get used right so let's let's talk about the model so similar to your information of Gaussian is I'm going to define a model with PXE equals to P of X given Z times P of Z and Z is hidden ok so that's a framework same as mixture of gaussians so let me just define the factor analysis model so first um Z will be drawn distributed according to a Gaussian density where Z is going to be an R D where D is less than n and again to think about it maybe in think of it as on D equals 3 N equals 100 M equals 30 okay and and but I guess just did me just as a concrete example to think about it and what we're going to assume is that X is equal to mu plus lambda Z this is the capital Greek alphabet lambda plus epsilon where epsilon is disputed Gaussian with mean 0 and covariance psy so the parameters of this model are mu which is n dimensional lambda which is n by D and psy which is n by n and we're going to assume that psy is diagonal okay and so let's see the second equation an equivalent way to write that is that given the value of Z the conditional distribution of X right X given Z this is Gaussian with me given by mu plus lambda Z and kobir inside okay so once you've given Z once your sample Z so this is P of Z and this is P P of Z and SP of XE X given Z right so given Z X is computed as mu plus lambda Z so this is just some constant and then you add Gaussian noise to it and so this equation an equivalent way to defining this equation is to say that the mean of X conditioned on Z is this first term since that's the mean and the covariance of x given Z is given by this you know additional term for sy by that noise term that you had to it okay so let me go through a few examples and I think the intuition behind this model is in if you think that there are three powerful forces driving temperatures across this room maybe one powerful force is just what is the temperature you know here in Palo Alto what's the temperature here at Stanford and another powerful force is how bright are the lights on the left side of the room and how hot does it heat up the side of room and another is how hot does it heat up the right side of the group right so let's say there are three main driving factors affecting the temperature of this room then that's when D would be equal to three that you assume that you know there are three things in the world they drive the temperature of this room this three dimensional which is the temperature in Palo Alto kind of around this area how bright are the lies there and how bright otherwise they're and you try to capture that with three numbers given those three numbers right given Z the actual temperature for the 100 sensors we scatter around this room will be determined by each sensor right so plant 30 temperature sensors all over this room each sensor we plant will measure an actual temperature there's a linear function of those three powerful forces and if it sends it on that side of the room it will be affected more by how bright are the lights on that side of the room if the sensor near the door it will be more affected by the temperature outside temperature here in Palo Alto right but so X will be a linear function this first time I underlined but rather than just that term there's a little noise right so your sensor has his own noise term which is governed by this additional noise term epsilon and the assumption that this matrix psy is diagonal is saying that after you compute the mean the noise that you observe that your sensor is independent of the noise and every other sensor right there maybe maybe the sensor you know up there right maybe it's just noisy or something this gust of wind but you assume that the noise observe at different senses is independent the additional epsilon error term has a diagonal covariance matrix given by side okay so you can so you can think of that as what factor analysis is trying to model so let me um you just go through a couple of examples of the types of data factor analysis can model alright oh and again body constrains the white board I'm gonna have to go low-dimensional here so actually let me let me go through a couple examples so let's say Z is R 1 and x is r 2 so in this example I guess D is equal to 1 and is equal to 2 and let's say M is 7 right just just so won't be a typical sample generated by you know what would be an example of a type of data that disc can model so this so this would be a typical sample of Zi which is you know so this is Z is just drawn from a static alcian so I guess Z miss Gaussian with mean 0 unit variance so that's a number line and if you draw seven points from the Gaussian you know maybe you get a sample like that okay and now let's say lambda is two one and let's just say mu is zero zero okay so now let's compute lambda X plus mu so give it a sample like that if you computes lambda X plus mu this will now be an r2 right so here's X 1 is X 2 I'm gonna take those examples and map them to align as follows right wait these examples on R 1 so oh excuse me long disease okay so this is just a real number and so lambda Z plus mu is now two-dimensional right because uh lambda R it's a 2 by 1 matrix okay so you end up with so this would be a typical sample to look around the sample of lambda Z plus mu and there's a 2 dimensional data set but all of the examples lie perfectly on a straight line okay then finally let's say that sy the covariance matrix is equal to this as a diagonal covariance matrix and so this covariance matrix corresponds to x2 having a bigger Berens the next one right now so you know this this I guess the density of epsilon has ellipses that look a little bit like this is taller than Y the aspect ratio should technically be 1 over root 2 right there's a standard deviation would be root 2 oh yes and so in the last step of what we're going to do x equals lambda Z plus mu those Epsilon we're going to take each of these points we have and put a little Gaussian contour you know there's that shape there's this this I'm just doing one contour yes it is a shape and just puts it on top of this and if you sample one point from each of these gaussians there may be again this example this example this example this example so what I just did was looking into the Gaussian contours in sample a point from that Gaussian and so the red crosses here are a typical sample drawn from this model okay and so if you have data that looks like this that looks like the red crosses come disease are latent random variables right and when you get it is that you can't actually see Z so what you actually see is just you know the red crosses that's your training set and if you apply the factor analysis model with these parameters then you know by e/m and so on hopefully you can find parameters and all those this data set pretty well but hopefully this to the sense of the type of data set this could generate and so and so um and then one one way to think of this data is you have two dimensional data but most of the data lies on a 1d subspace so this is how to think about it you have two dimensional data since n is two but most of the data lies on the roughly one dimensional subspace meaning lies are up there on the line and then there's a little bit of noise off that line okay all right let me quickly do one more example because these are these are high dimensional spaces I think this I think is useful to build intuition all right um so let's go through the example where Z is 2 X is an r3 and let's use M equals 5/2 so um what about different set of parameters let's look at the type of data you can generate a factor analysis which is just e 1 is e 2 Z is distributed Gaussian standing goes in to do you so beyond you know circular Gaussian so maybe this is what a typical sample right looks like if you if you sample sort of z1 and z2 from a standard Gaussian right that would be a typical sample in z1 and z2 so now all right I'm gonna do a demo let me take these five examples and just copy them to this piece of paper okay so all right there great transfer this from the whiteboard to this piece of paper to this brown cardboard so now you have z1 and z2 in a two dimensional space what we're going to do is compute lambda Z plus mu and this will be 3 by 2 and this will be 3 by 1 so what this computation will do as you map from Z in two dimensions to lambda Z plus mu is you're going to map from two-dimensional data to three-dimensional data in other words you're going to take the two dimensional data lying on the plane of the whiteboard and map it check out the school animation into the three-dimensional space of our classroom and then the last step is for each of these points in this video space like X 1 X 2 X 3 right we'll have a little Gaussian bump that is acts as a line because epsilon is the features the the components of epsilon are uncorrelated and taking each of these five points and add the low you know be the fuzziness a little bit of Gaussian noise to it and so what you end up with is a set of red crosses and you end up with a few examples you know L above the noise you end up with except that they would have a bit of noise off this plane as well right but so what the factor analysis model can capture is if you have data in 3d right in this 3d space but most of the data set lies on this maybe roughly two dimensional pancake but there's a little bit of fuzziness off the packaging right so so this would be an example of the type of data that factor analysis can model and the intuition is really think factor analysis can take very high-dimensional data say 100 dimensional data and model the data is roughly lying on a three dimensional five dimensional subspace with a little bit of fuss will blow of noise of that low dimensional subspace so let's talk about Oh right does not work as well if the data is not lying on the load original subspace um let's see so even in 2d if you have this data set your I should the freedom to choose Gaussian noise just like that in which case you can actually model things that quite far off a subspace but yeah you know I think we're very high dimensional dataset it's actually very difficult to know what's going on because you can't visualize these very high dimensional data sets and you also don't have enough data to go very secure to models so so I feel like yes if you have if the data actually does not roughly line the subspace then this model you know may not be the best model but when you have such high dimensional data in such a small data set you can't fit very complex models to it anyway so this might be pretty reasonable so um so it turns out that the derivation of vm for factor analysis is actually is actually one of the trickiest VM derivations in terms of how you calculate at each step and how you calculate the m-step the whole algorithm is you know describe every every every single step step through in great detail the lecture notes but what I want to do is give you the flavor of how to do the derivation and to especially draw attention to the trickiest step so that if you need to derive an algorithm like this yourself or maybe a different Gaussian model that you know how to do it but I won't do every step the algebra here um so in order to set ourselves up to derive factor analysis here enm for better analysis I want to describe a few properties of multivariate gaussians so let's say that X is a vector and I'm gonna write this as a partition vector right and we jumped if there are our components there and s components there so x1 is in SS so if X is Gaussian with mean mu and covariance Sigma then let's similarly let me be written as this sort of partition vector right just break it up into two sub vectors corresponding to the first R components in the second s components and similarly lesson that the covariance matrix be partitioned into you know these four diagonal blocks where I guess this is our components this is s components this is our components this is s components so all this means is you take the covariance matrix and take the top leftmost R by R elements and call that Sigma 1 1 right and and similarly for the other sub blocks of this covariance matrix so in order to derive factor analysis one of the things you need to do is compute marginal and conditional distributions of gaussians so the marginal is you know what is P of x1 and so the the if you you know where to derive this the way you compute the marginal is to take the joint density of P of X right and you can write this as P of x1 x2 because X can be partitioned into x1 and x2 and then integrate out x2 P of X 1 X 2 right DX 2 and just to give you P of X 1 and if you plug in the Gaussian density the formula for the Gaussian density if you plug in I guess you know 1 over 2 pi to the N over 2 C it was in one half e to the minus 1/2 X 1 minus mu 1 if you plug this into P of X 1 comma X 2 and as she do the integral then you will find that the marginal distribution of X 1 is given by X 1 is a Gaussian with mean mu 1 and covariance Sigma 1 1 so it's kind of not a shocking result that the marginal distribution is given just by that and that's and then again the way to show it vigorously is to do this calculation but it's actually not shocking I guess that that's what you would get ok um and then the other property you will need to use is a conditional which is given the value of X 2 what is the conditional value of x 1 and so the way to do that would be you know in theory you would take P of X 1 comma X 2 divided by P of X 2 right and then simplify and it turns out you can show that X 1 given X 2 is itself Gaussian or some mean in some covariance which comes right at some U of 1 given 2 and Sigma of 1 given 2 over mu of 1 given 2 is and but this is one of those long is that I actually don't I actually don't manage to remember but every time I need just look what is written election I was as well so so that's how you compute modules and conditionals of a Gaussian distribution so using these properties of the multivariate Gaussian density let's go through the high-level steps of how you derive the EML rule step one is less compute actually let's let's derive what is the joint distribution of P of X and Z and in particular it turns out that if you take Z and X and stack them up into a vector like so Z and X viewed as a vector will be Gaussian wouldn't mean with some mean and some Co various because X and Z jointly will have a Gaussian density and let's try to quickly figure out what are this mean and that's covariance matrix so that was a definition of these terms and so the expected value of Z is equal to 0 because 0 Z is Gaussian with mean 0 and covariance identity and do you expected value of x is equal to the expected value of mu plus lambda Z plus epsilon but Z has 0 expected value epsilon 0 expected value so that just leaves you with mu and so this mean vector mu XZ is going to equal to 0 and so this is d dimensional and this is a n dimensional and it turns out that let's see and it turns out that you can similarly compute the covariance matrix Sigma right where this is D dimensions and this is n dimensions it turns out that if you take this partition vector and compute the covariance matrix the four blocks of the covariance matrix that can be written as follows and you can one-at-a-time derive what each of these different blocks look like and let me just do one of these and now let me just derive one Sigma 2 to the lower-right blockers and the rest are derived similarly and also fleshed out in the lecture notes so the way you derive what this block is like is that you say Sigma 2 2 is X minus y X X minus X transpose and so if I plug in the definition of X that would be a lambda Z plus mu plus epsilon minus mu the same thing so that's X minus yeah so that's X minus y X okay because the expected value of x is 2 so the Meuse cancel out and then if you do the quadratic expansion I guess this becomes expected value of let's see lambda Z times each of these two terms transpose plus it's all about you know a plus B times a plus B right is a times a times a plus B times a plus B you get four terms as a result and so the first term is lambda Z times lambda Z transpose which is this plus lambda Z epsilon transpose plus with Epsilon and so this term has your expected value because epsilon and and Z both have zero expected value and correlated so this is 0 this is 0 on expectation and so you just left with an expected value of lambda Z Z transpose lambda transpose plus the expected value of epsilon epsilon transpose and so by the linearity of expectation you can take a expectation inside the mouth matrix multiplication so this launder is the expected value of Z Z transpose times lambda transpose plus and this is just the covariance of epsilon right which is which is Sai and then because Z is drawn from a standard Gaussian with identity covariance that expectation in the middle is just the identity so there's lambda lambda transpose plus sign okay so that's how you work out what is this lower right block of this um covariance matrix I know I did that a little bit quickly but every every step is written out more slowly into lecture notes as well and it turns out that if you go through a similar process at Phaedra you know one at a time using a similar process what are the other blocks of this covariance matrix you find that the other blocks of this covariance matrix are identity lounder transpose that one we just worked out so that that's the one we just worked out but so that is the covariance matrix side so where we are is that we figured out that the joint distribution at the joint density of Z X is Gaussian would mean given by that vector and covariance given by that matrix and so what you could do is you write down write P of X I and try to take the so P of X I will be this Gaussian density and what you could do is take derivatives of the log-likelihood respect to the parameters instead on 0 0 and solve and you find that there is no known closed form solution there is actually no closed form solution for finding the values of lambda and sy and mu that maximizes life likelihood so in order to fit the parameters of the model we're instead going to resort to Y M and so in the estep so let's let's first derive what is the e step which is an e step you need to compute this now Zi here is a continuous random variable when we're fitting a mixture of gaussians distribution Zi was discrete and so you could have a list of numbers represented by you know W IJ that just just have a vector soaring what is the probability of each of the discrete values of Zi but in this case Zi is a continuous density so how do you represent qi of Zi in a computer it turns out that using the formulas we have for the marginal excuse me for the conditional distribution of a Gaussian it turns out that if you compute this right hand side you'll find that Zi given X I this is going to be Gaussian with some mean and some covariance right where it's basically those formulas mu of Z i given X I is equal to if you kind of take that foam and then apply it all thing here is 0 plus lambda transpose okay so these equations are exactly these two equations right maps to map to that big Gaussian density that we have okay so what you would do in the East step is compute this and compute this compute this vector in computers matrix and saw that sorters in you know store these as variables and your representation of the Qi is that Qi is a Gaussian density right with this mean and disco beer so this is what you actually compute to represent Qi all right so step two was to write the e step and step three is the RIVM step and the derivation of the m-step is is quite long and complicated but I want to mention just a key out your algebraic trick you need to use when deriving the m-step so you know we know from the east step that qi of zi is that gaussian density right so yes 1 over 2 pi to the D over 2 that thing and e to the negative 1/2 so that's the formula for Qi it turns out that in the m-step there'll be a few places in the derivation where you need to compute something like this and one way to approach this would be to plug in the density for Qi which is so you end up with this 1/2 pi to the T over 2 Sigma you know and so on time zi Dzi and then charlie compute this integral it turns out there's a much simpler way to computers in the draw anyone know what it is all right cool awesome right expect the value so the other way to compute this integral is to note that is that this is the expected value of CI when CI is drawn from Qi right so you know the definition of expect about value of random variables expected value of Z is equal to integral over C probably up to Z times Z DZ right that's what the expected value of a random variable is and so this integral is the expected value of Z respect to Z drawn from the QI distribution but we know that Q is Gaussian with a certain mean a certain variance and so the expected value of this this is just mu of Z i given X I is that thing that you've already computed and so when students derived the m-step you know for young implementations of gaussians one of the key things to notice is when are you actually taking an expected value respect to around in variable in which case is just the value of computer already and when do you need to plug in this big complicated integral which can lead to very complicated very intractable calculations ok so just when you're whenever you see this think about whether you need to be expanding a big complicated integral or if it can be interpreted as an expected out and so for the m-step is really you know the m-step is right so that's the m-step and if you rewrite this term as on some of I the expected value of Zi drawn from Qi it turns out that um if you go ahead and plug in the Gaussian density here actually want one rule of thumb for whether or not you should plug in a complicated in the grow-op sarcoma Gaussian density this is just a rule of thumb after doing this type of map a long time it see if there's a log in front if there's a log in front of a Gaussian density because the Gaussian density as an exponentiation right through the Gaussian density is you know 1 over e to the something so one of there's a log in front of volcanic initiation cancel out and this equations simplify so one trick as you're doing these derivations is just see if there's a log in front of a Gaussian density and when there is a plugin go ahead and plug in the formulas for Gaussian density the log will simplify that and what you end up with is the log of a Gaussian density as there being a quadratic function a quadratic function of the parameters and if you take the expected value respect to a Gaussian Jessie respect to a quadratic function this whole thing ends up being a quadratic function and then you can take derivatives of that equation with respect to the parameters respective new that whole thing set it to zero and then solve and it'll be roughly love of complexity of maximizing quadratic function okay hope that makes sense um the actual formulas are a little bit complicated so I don't I'll leave user luckily I shall for this the lecture notes but I think the takeaway is uh don't expand this in the draw and when you are deriving this plug in the Gaussian densities here because they'll all be simplified okay and details of it's an election notes so let's break for today best of luck with the midterm oh and no Suzy I hope you guys do well alright I'll see you guys in a few days