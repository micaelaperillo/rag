hello everyone welcome to CS 2 to 9 today we're going to talk about deep learning and neural networks we're going to have two lectures on that one today and a little bit more of it on Monday don't hesitate to ask questions during the lecture so stop me if you don't understand something and we'll try to build the intuition around your own Network together we will actually start with an algorithm that you guys have seen previously called logistic regression everybody remembers logistic regression ok remember it's a classification algorithm we're going to do that explain how logistic regression can be interpreted as a neural network specific case of a neural network and then we will go to neural networks sounds good so the quick intro and deep learning so deep learning is a is a set of techniques that is let's say a subset of machine learning and it's one of the growing techniques that have been used in the industry specifically for problems in computer vision natural language processing and speech recognition so you guys have a lot of different tools and and plugins on your smartphones that uses this type of algorithm the reason it came to work very well is primarily the new computational methods so one thing we're going to see today is that deep learning is really really computationally expensive and people had to find techniques in order to parallelize the code and use GPU specifically in order to graphical processing unit in order to be able to compute the computations in the punic the second part is the data available has been growing after after the internet bubble with the digitalization of the work so now people have access to large amounts of data and this type of algorithm has the specificity of be able to learn when there's a lot of data so these models are very flexible and the more you give them data the more they will be able to understand the salient feature of the data and finally algorithms so people have come up with with new techniques in order to use the data use the competition power and build models so we're going to touch a little bit on all of that but let's go with logistic regression first can you guys see in the back yeah okay so you remember what logistic regression is we're going to fix a goal for us that is a classification goal so let's try to to find cuts in images so find cuts in images meaning binary classification if there is a cut in the image we want to output a number that is close to one presence of the cut and if there is no cut in the image one output zero let's say for now we're constrained to the fact that there is maximum one cut very much there's no more if you have to draw the logistic regression model that's what you would do you would take a cut so this is an image of the cuts very bad at that sorry in computer science you know that images can be represented as 3d matrices so if I tell you that this is a color image of size 64 by 64 how many numbers do I have to represent those pixels yeah I heard it 64 by 64 by 3:3 for the RGB Channel red green blue every pixel in an image can be represented by three numbers one represents in the red filter the green filter and the blue filter so actually this image is of size 64 times 64 times 3 that make sense so the first thing we will do in order to use logistic regression to find if there is a cut on this image we're going to flatten this into a vector so I'm going to take all the numbers in this matrix and flatten them in a vector just an image to vector operation nothing more and now I can use my logistic regression because I have a vector input so I'm going to to take all of these and push them in an operation let me call this in the logistic operation which has one part that is W X plus B where X is going to be the image so W X plus B and the second part is going to be the sigmoid everybody's familiar with the sigmoid function function that takes a number between minus infinity and plus infinity maps it between 0 and 1 it's very convenient for classification problems and this we're going to call it Y hat which is sigmoid of what you've seen in class previously I think it's theta transpose X but here we will just separate the notation into W and B so can someone tell me what's the shape of W 2 matrix W vector matrix what yeah 64 by 64 by 3 as a yeah so you know that this guy here is a vector of 64 by 64 by 3 a column vector so the shape of X is going to be 64 by 64 by 3 times 1 this is the shape and this I think it's twelve twelve thousand 288 and this indeed because we want Y hat to be one by one this W has to be one by twelve 298 that make sense so we have a row vector as our parameter we're just changing the notations of the logistic regression that you guys have seen and so once we have this model we need to train it as you know and the process of training is that first we will initialize or parameters these are what we call parameters we will use the specific vocabulary of weights and bias I believe you guys have heard this vocabulary before weights and biases so we're going to find the right W and the right B in order to be able to use this model properly once we initialize them what we will do is that we will optimize them find the optimal W and B and after we found the optimal W and B we will use them to predict does this process make sense this training process and I think the important part is to understand what this is find the optimal W and B means defining your last function which is the objective and in machine learning you often have this this specific problem where you have a function that you know you want to find the network function but you don't know the values of its parameters in order to find them you're going to use a proxy that is going to be your last function if you manage to minimize the last function you will find the right parameters so you define a loss function that is the logistic loss Y log Y hat plus 1 minus y log of 1 minus y hat you guys have seen this one you remember where it comes from comes from a maximum likelihood estimation starting from a probabilistic model and so the idea is how can I minimize this function minimize because I've put a minus sign here I want to find W and B that minimize this function and I'm going to use a gradient descent algorithm which means I'm going to iteratively compute the derivative of the loss with respect to my parameters and at every step I will update them to make this loss function go a little down at every trait if set so in terms of implementation this is a for loop you will loop over a certain number of iteration and at every point you will compute the derivative of your loss with respect to your parameters everybody remembers how to compute this number take the derivative here you use the fact that the sigmoid function has a derivative that is Sigma 8 times 1 minus Sigma and you will compute the results we're going to do some derivative later today but just to set up the problem here so the few things that I want to do I want to touch on here is first how many parameters does this model have this logistic regression if you have to count them so this is the number 89 yeah correct so twelve thousand two hundred eighty eight weights and one bias that make sense so actually it's funny because you can quickly count it by just counting the number of edges on the on the on the drawing plus one every circle has a bias every Edge has a weight because ultimately this operation you can rewrite it like that right it means every weight has every weight corresponds to an edge so that's another way to count it we're going to use it a little further so we're starting with not too many parameters actually and one thing that we noticed is that the number of parameters of our model depends on the size of the input we probably don't want that at some point so we're going to change it later so two equations that I want you to remember is the first one is neurone equals linear plus activation so this is the vocabulary we will use in your networks we define in your own as an operation that has two parts one linear part and one activation part and it's exactly that this is actually a neural we have a linear part WX plus B and then we take the output of this linear part and we put it in an activation that in this case is the sigmoid function it can be other functions okay so this is the first equation not too hard the second equation that I wanna set now is the model equals architecture plus parameters what does that mean it means here we're trying to train a logistic regression in order to be able to use it we need an architecture which is the following a one-year own neural network and the parameters W and E so basically when people say we've shipped a model like in the industry what they're saying is that they found the right parameters with the right architecture they have two files and these two files are predicting a bunch of things okay one parameter file and one architecture file the architecture will be modified a lot today we will add neurons all over and the parameters will always be called W and B but they will become bigger and bigger because we have more data we want to be able to understand it you can get that it's going to be hard to understand what a cat is with only that that that many parameters we want to have more parameters any questions so far so this was just to set up the problem with logistic regression let's try to set a new goal after the first goal we have set prior to that so the second goal would be find cat lion so a little different than before only thing we changed is that we want to now to detect three types of animals if there's a cat on the image I want to know there is a cat if there is anyone on the image I want to know there is an iguana if there's a line on image I want to know it as well so how would you modify the network that we previously had in order to take this into account yeah yeah good idea so put two more circles so neurons and do the same thing so we have our picture here with the cats so the cat is going to the right 64 by 64 by 3 we flatten it from X 1 to X n let's say and represent 64 64 by 3 and what I will do is that I will use 3 neurons that are all computing the same thing they're all connected to all these inputs ok I connect all my inputs x1 to xn to each of these neurons and I will use a specific set of notation here why to hat equals a 2-1 sigmoid of w2 1 plus 2 and similarly white 3 hat equals 81 which is sigmoid of w3 1x plus 3 so I'm introducing a few notations here and we will get used to it don't worry so just just write this down and we're going to go over it so the square brackets here represents what we will call later on a layer if you look at this network it looks like there is one layer here there is one layer in which neurons don't communicate with each other we could add up to it and we will do it later on more neurons in other layers we will then note with square brackets the index of the layer the index that is the subscript to this a is the number identifying the neuron inside the layer so here we have one layer we have a 1 a 2 and a 3 with square brackets one to identify the layer does it make sense and then we have our Y hat that instead of being a single number as it was before is now a vector of size 3 so how many parameters does this network have how much okay how did you come up with that okay yeah correct so we just have three times the thing we had before because we added two more neurons and they all have their own set of parameters look like this edge is a separate edges this one so we have to replicate parameters for each of these so w11 would be the equivalent of what we had for the cat but we have to add two more parameter vectors and biases so other question when you have to train this logistic regression what data set did you need can someone try to describe the data set yeah yeah correct so we need images and labels with it's labeled as cat one or no cat zero so it's a binary classification with images and labels now what do you think should be the data set to train this network yes that's a good idea so just to repeat a label for an image that has a cat would probably be a vector with a one and two zeroes where the one should represent the present the presence of a cat this one should represent the presence of a lion and this one should represent the presence of an iguana so let's assume I use this scheme to label my dataset I train this network using the same techniques here initialize all my weights and biases with a value a starting value optimize a loss function by using gradient descent and then use y hat equals lala to predict what do you think this neuron is going to be responsible for if you have to describe the responsibilities of this neuro yes well this one yeah lion and this one iguana so basically the way you go free that's a good question we're going to talk about that now multiple image contain different animals or not so going back on what you said because we decided to label our data set like that after training this neuron is not really going to be there to detect cuts if we had changed the labeling scheme and I said that the second entry would correspond to the shut the presence of the cat then after training you will detect that this neuron is responsible for detecting the cat so the network is going to evolve depending on the way you label your dataset now do you think that this network can still be robust to different animals in the same picture so this cat now has a friend that is a lion okay I have no idea how to draw a lion but let's say there is a lion here and because there is a lion I will add a one here do you think this network is robust to this type of labeling hmm it should be the neurons aren't talking to each other that's a good answer actually another answer that's a good on intuition because the network what it sees is just one one zero and an image it doesn't see that this one corresponds to the Cal correspond to the first one and the second and the line correspond to the second one so this is a property of neural networks it's the fact that you don't need to tell them everything if you have enough data they're going to figure it out so because you will have also cats with iguanas cats alone Lions with iguanas lions alone ultimately this neuron will understand what it's looking for and it will understand that this one corresponds to this line just needs a lot of data so yes it's going to be robust and that's the reason you mentioned is going to be robust to that because the tree neurons aren't communicating together so we can totally train them independent independently from each other and in fact the sigmoid here doesn't depend on the sigmoid here and doesn't depend on the same weight here it means we can have one one and one as an output yes question you could you could you could think about it as trilogies equations so we wouldn't call that in your own network yet it's not ready yet but it's a three neural network or three logistic regression with each other now following up on that yeah go for it the question W and B are related to what oh yeah so so usually you would have theta transpose X which is sum of theta I X I correct and what I will split it is I will spit it in sum of theta I X I plus theta 0 times 1 I'll split it like that theta 0 would correspond to be and these data eyes would correspond to Wis make sense one more question good question that's the next thing we're going to see so the question is a follow-up on this is there cases where we have a constraint where there is only one possible outcome it means there is no chat in Lyon there's either a cat or a lion there is no Guana in Lyon there's either in iguana or line think about health care there are many there are many models that are made to detect if this is skin disease is present on based on cell microscopic images usually there is no overlap between this is it means you want to classify a specific this is among a large number of diseases so this model would still work but would not be optimal because it's longer to Train maybe one this is super super rare and one of the neurons is never going to be trained let's say you're working in a zoo where there is only one in wanna and there are thousands of lions and thousands of cats this guy will never train almost you know it would be super hard to train this one so you want to start with another model that where you put the constraint that okay there is only one disease that we want to predict and let the model learn with all the neurons learn together by creating interaction between them have you guys heard of soft max yes some of you I see that okay so let's look at soft max a little bit together so we set a new goal now which is we add a constraint which is unique animal on an image so at most one animal on an image so I'm going to modify the network a little bit we have our chat and there is no line on the image we flatten it and now I'm going to use the same scheme with the tree neurons a1 a2 a3 but as an output what I'm going to use is an exponent softmax function so let me be more precise let me let me actually introduce another notation to make it easier as you know the neuron is a linear part plus an activation so we're going to introduce a notation for the linear part I'm going to introduce z1 1 to represent the linear part of the first neuron z11 two to introduce the linear part of the second gyro so now when neuron has two parts one which compute Z and one which computes a equals Samoyed Ozzy now I'm going to remove all the activations and make these these and I'm going to use the specific formula so this if you recall it's exactly the softmax formula okay so now the network we have can you guys see rates too small too small okay I'm going to just write this formula bigger and then you can figure out the others I guess because e of Z 3 1 divided by sum from J equals 1 2 3 of e exponential of ZK this one so here is a for the third one if you are doing it for the first one you will add you just change this into a 2 into a 1 and for a second 1 into a 2 so why is this formula interesting and why is it not robust to this labeling scheme anymore it's because the sum of the outputs of this network have to sum up to 1 you can try it if you sum the three outputs you get the same thing in the numerator and on the denominator and you get one that makes sense so instead of getting a probabilistic output for each each of Y if each of Y hat 1 Y had to I had 3 we will get a probability distribution over all the classes so it means we cannot get 0.7 0.6 0.1 telling us roughly that there is probably a cat and a lion but no iguana we have to sum these two one so it means if there is no cut and no lion it means there is very likely an iguana the three probabilities are dependent on each other and for this one we have to label the following way 1 1 0 for a cat 0 1 0 for a lion or 0 0 1 for an iguana so this is called a softmax multi-class network you assume there is at least one of the three classes otherwise you have to add a fourth input that will represent absence of animal but this way your assume there is always one of these three animals on every picture and how many parameters does the network have the same as the second one we still have three neurons and although I didn't write it this Z 1 is equal to w1 1 X plus B 1 Z 2 thames III same so there's 3 n plus 3 parameters so one question that we didn't talk about is how do we train these parameters these these parameters the 3n plus 3 parameters how do we train them you think this scheme will work or not what's wrong what's wrong with this scheme what's wrong with the last function specifically there's only two outcomes so in this last function y is a number between 0 & 1 y hat same is a probability y is either 0 or 1 y hat is between 0 & 1 so it cannot match this labeling so we need to modify the loss function so let's call it loss trainer what I'm going to do is I'm going to just sum it up for the fingers does this make sense so I'm just doing three times this loss for each of the neurons so we have exactly three times this we sum them together and if you train this last function you should be able to train the three neurons that you have and again talking about scarcity of one of the classes if there is not many in Guana then the third term of this sum is not going to help this neuron train towards detecting an iguana it's going to push it to the technology Juana any question on the last function does this one make sense yeah yeah usually that's what will happen is that the output of this network once it's trained is going to be a probability distribution you will pick the maximum of those and you will set it one and the others to zero as your prediction one more question yeah if you use the two one if you use this labeling skin-like one one zero for this network what do you think it will happen it will probably not work and the reason is this sum is equal to two there's some of these entries while the sum of this entry is equal to one so you will never be able to match the output to the input to the label it makes sense so what the network is probably going to do is it's probably going to send this one to one half this one to one half and this one to zero probably which is not what you want okay let's talk about the last function for this softmax regression because you know what's interesting about this loss is if I take this derivative derivative of the Lost 3m with respect to W to one you thing is going to be harder than this derivative then this one or no it's going to be exactly the same because only one of these three terms depends on W want to it means the derivative of the two others are zero so we're exactly at the same complexity during the derivation but this one you think if you try to compute let's say we define a loss function that corresponds roughly to that if you try to compute the derivative of the loss with respect to W 2 it will become much more complex because this number the output here that is going to impact the loss function directly not only depends on the parameters of W 2 it also depends on the parents of W 1 and W 3 and same for this put this output also depends on the parameters W 2 doesn't make sense because of this denominator so the softmax regression needs a different loss function and a different derivative so the loss function will define is a very common one in deep learning is called the softmax first entropy cross entropy loss i'm not going to into the details of where it comes from but you can get the intuition why so it surprisingly looks like the binary croissant the binary the logistic class function the only difference is that we will sum it up on all the on all the classes now we will take a derivative of something that looks like that later but I'd say if you can try it at home on this one it would be a good exercise this way so this binary croissant ropey loss is very likely to be used in classification problems that are multi class okay so this was the first part on logistic regression types of networks and I think we're ready now with the notation that we introduced to jump on to neural networks any question on this first part before we move on so one question I would have for you let's say instead of trying to predict if there is a cat or no cat we will trying to predict the age of the cat based on the image what would you change this network instead of predicting one zero you want to predict the age of the cat what are the things you would change yes okay so I repeat I I basically make several output nodes where each of them corresponds to one edge of cats so would you use this network or the third one would use the tree neurons your own network or the softmax regression the third one why you have a unique age you cannot have two ages right so we would use a soft max one because we want a probability distribution along the edge the age okay that makes sense that's a good approach there is also another approach which is using directly regression to predict an age an age can be between 0 and plus in feet not plus infinity 0 in a certain number and so let's say you want to do a regression how would you modify your network change the sigmoid the sigmoid puts the Z between 0 & 1 we don't want this to happen so I'd say we will change the sigmoid into what function would you change the Samoyed yes so the second one you said was or to get a plus-one type of distribution okay so let's let's go with linear you mentioned linear we could just use a linear function right for the sigmoid but this becomes a linear regression the whole network becomes a linear regression another one that is very common in in deep learning is called the rayleigh function it's a function that is almost linear but for every input that is negative it's equal to zero because we cannot have negative age it makes sense to use this one okay so this is called rectified linear units really it's a very common one in different now what else would you change we talked about linear regression you remember the last function you were using a linear regression what was it it was probably one of these two y hat minus y just comparison between the output label and Y hats the prediction or it was the l2 loss Y hat minus y in l2 norm so that's what we would use we would modify our loss function to fit the regression type of problem and the reason we would use this loss instead of the one we have for a regression test is because in optimization the shape of this loss is much easier to optimize for a regression task than it is for a classification task and vice versa not going to go into the details of that but that's the intuition ok let's go have fun with neural networks so we we stick to our first goal I've given an image tell us if there is cat or no cat this is one this is you but now we're going to make a network a little more complex we're going to add some parameters so I get my teacher of the cat cat is moving okay and what I'm going to do is that I'm going to put more neurons than before maybe something like that so using the same notation you see that my square bracket here is 2 indicating that there is a layer here which is the second layer while this one is the first air and this one is the third layer everybody's up to speed with the notations cool so now notice that when you make a choice of architecture you have to be careful of one thing is that the output layer has to have the same number of neurons as you want the number of classes to be for a classification and one for a regression so how many parameters does need this network have can someone quickly give me the thought process so how much here yeah like 3n plus 3 let's say yeah correct so here you would have three any weights plus three biases here you would have two times three weights plus two biases because you have three neurons connected to two neurons and here you will have two times one plus one bias this is the total number of characters so you see that we didn't add too much parameters most of the parameters are still in the input layer let's define some vocabulary the first word is layer layer denotes neurons that are not connected to each other these two neurons are not connected to each other these three neurons are not connected to each other we call this cluster of neurons a layer and this has three layers we would use input layer to define the first layer output layer to define the third layer because it directly sees the output and we would call the second layer a hidden layer and the reason we call it hidden is because the input and the output are hidden from this layer it means the only thing that this layer sees as input is what's the previous layer again it so it's an abstraction of the inputs but it's not the input doesn't make sense and say it doesn't see the output it just gives what it understood to the last neuron that will compare the output to the ground truth so now why our neural network interesting and why do we call this hidden layer is because if you train this network on cats classification with a lot of images of cats you would notice that the first layers are going to understand the fundamental concepts of the image which is the edges this neuron is going to be able to detect this type of edges this your own probably going to detect some other type of edge this neuron may be this type of edge then what's going to happen is that this neuron are going to communicate what they found on the image to the next layers new and this room is going to use the edges that these guys found to figure out that oh there is a their ears while this one is going to figure out oh there is a mouth and so on if you have several neural and they're going to communicate what they understood to the output neuron that is going to construct the face of the cat based on what it received and be able to tell if there is a cat or not so the reason it's called hidden layer is because we don't really know what it's going to figure out but with enough data it should understand very complex information about the data the deeper you go the more complex information the neurons are able to understand let me give you another example which is a house prediction example house price prediction so let's assume that our inputs are number of bedrooms size of the house zip code and wealth of the neighborhood let's say what we will build is a network that has twin neurons in the first layer and one your own in the output layer so what's interesting is that as a human if you were to build this network and like hand engineer it you would say that okay zip code and wealth or or sorry zip code and wealth are able to tell us about the school quality in the neighborhood the quality of the school that is next to the house probably as a human you would say these are probably good features to predict that the zip code is going to tell us if the neighborhood is walkable or not probably the size and the number of bedrooms is going to tell us what's the size of the family that can fit in this house and these three are probably better information than these in order to finally predict the price so that's a way to hand engineer that by hand as a human in order to give human knowledge to the network to figure out the price in practice what we do here is that we use a fully connected layer fully connected what does it mean it means that we connect every input of a layer every every input to the first layer every output of the first layer to the input of the third layer here and so on so all the neurons among like from one layer to another are connected with each other what we're saying is that we will let the network figure these out we will net the neurons of the first layer figure out what's interesting for the second layer to make the price prediction so we will not tell these to the network instead we will fully connect the network and so on okay we'll fully connect the network and let it figure out what are the interesting features and often time the network is going to be able better than humans to find these what are the features that are representative sometimes you may hear neural networks referred as blackbox models the reason is we will not understand what this edge would correspond to it's it's hard to figure out that this neuron is detecting a weighted average of the input features does it make sense another word you might hear is end to end learning the reason we talk about end to end learning is because we have an input a ground truth and we don't constrain the network in the middle we let it learn whatever it has to learn and we call it end to end learning because we're just training based on the input and the output let's delve more into the math of this network the neural network that we have here which has an input layer a hidden layer and an output layer let's try to write down the equations that run the input and pour propagated through the output we first have z1 that is the linear part of the first layer that is computed using w1 times X plus B then this z1 is given to an activation let's say it's sigmoid which is sigmoid of z1 z2 is then the linear part of the second neuron which is going to take the output of the previous layer multiplied by its weights and add the bias the second activation is going to take the sigmoid of z2 and finally we have the third layer which is going to multiply its weights with the output of the layer present in it and add its bias and finally we have the third activation which is simply the simulate so what is interesting to notice between these equations and the equations that we wrote here is that we put everything in matrices so it means this 8/3 that I have here sorry this here for three neurons I wrote three here for three neurons in the second layer I just wrote a single equation to summarize it but the shape of these things are going to be vectors so let's go over the shapes let's try to define them z11 is going to be X which is n by 1 times W which has to be 3 by n because it connects three neurons to the input so this Z has to be 3 by 1 it makes sense because we have three neurons now let's go let's go deeper a 1 is just the sigmoid of z1 so it doesn't change the shape it keeps the 3 by 1 Z 2 we know it it has to be 2 by 1 because there are two neurons in the second layer and it helps us figure out what W 2 would be we know a 1 is 3 by 1 it means that W 2 has to be 2 by 3 and if you count the edges between the first and the second layer here you will find 6 ages 2 times 3 a 2 same shape as z2 z3 1 by 1 a 3 1 by 1 w3 it has to be 1 by 2 because a 2 is 2 by it's same for me B is going to be the number of neurons so 3 by 1 2 by 1 and finally 1 by 1 so I think it's usually very helpful even when coding this type of equations to know all the shapes that are involved are you guys like totally ok with the shapes super easy to figure out ok cool so now what is interesting is that we will try to vectorize the code even more does someone remember the difference between stochastic gradient descent and gradient descent what's the difference exactly so Cassie gradient descent is update the weights and the bias after you see every example so the direction of the gradient is quite noisy doesn't represent very well the entire batch while gradient descent or batch gradient descent is update after you've seen the whole batch of examples and the gradient is much more precise it points to the direction you want to go to so what we're trying to do now is to write down these equations if instead of giving one single cat image we had given a bunch of images that either have a cat or another cat so now our input X so what happens for an input batch of examples so now arm or X is not anymore a single column vector it's a matrix with the first image corresponding to X 1 the second image corresponding to X 2 and so on until the enth image corresponding to X and I'm introducing a new notation which is the parentheses superscript corresponding to the ID of the example so square brackets for the layer round brackets for the idea of the example we're talking about so just to give more context on what we're trying to do we know that this is a bunch of operations we just have a network with inputs hidden and output layer we could have a network with a thousand layer the more layers we have the more computation and it quickly goes up so what we want to do is to be able to paralyze our code or our computation as much as possible by giving batches of input and parallelizing these equations so let's see how these equations are modified when we give it a bash of M inputs I will use capital letters to denote the equivalent of the lowercase letters but for a batch of input so z1 as an example would be w1 let's use the same w1 times X plus b1 so let's analyze what z1 would look like z1 we know that for every for every input example of the batch we will get one Z 1 it should look like this then we have to figure out what has to be the shapes of this equation in order to end up with this we know that Z one was three by one it mean it means Capital Z one has to be three by M because each of these column vectors are three by one and we have M of them because for each input we forward propagate through the network we get these equations so for the first cut image we get these equations for the second cut image we get again equations like that and so on so what is the shape of X we have it above we know that it's n by M what is the shape of W one it didn't change the ability one doesn't change it's not because I will give a thousand inputs to my network that the parameters are going to be more so the parameter number stays the same even if I give more inputs and so this has to be 3 by n in order to match now the interesting thing is that there is a an algebraic problem here what is the algebraic problem we said that the number of parameters doesn't change it means that W has the same shape as it has before as it had before B should have the same shape as it had before right should be 3 by 1 what's the problem of this equation exactly we're summing a 3 by M matrix to a 3 by 1 vector this is not possible in that it doesn't work doesn't match when you do some summations or subtraction you need the two terms to be the same shape because you will do an element-wise addition of them an element-wise subscription so what's the trick that is used here it's a it's a technique called broadcasting broadcasting is that is the fact that we don't want to change the number of parameters it should stay the same but we still want this operation to be able to be written in parallel version so we still want to write this equation because we want to paralyze our code but we don't want to add more parameters it doesn't make sense so what we're going to do is that we're going to create a vector B tilde 1 which is going to be B 1 repeated three times sorry repeated M times so we just keep the same number of parameters but just repeat them in order to be able to write my code in parallel is this called broadcasting and what is convenient is that for those of you who do not do homeworks are in max hub or Python MATLAB okay so in MATLAB know Python Python so in Python there is a package that is often used to code these equations it's non pipe some people call it dumpy not sure so numpy basically numerical Python we directly do the broadcasting it means if you sum this three by m matrix with a three by one parameter vector is going to automatically reproduce the parameter vector M times so that the equation works it's called broadcasting it make sense so because we're using this technique we're able to rewrite all these equations with capital letters you want to do it together or do you want to do it on your own wants to do it on their own okay so let's do it on their own on your own so rewrite these with capital letters and figure out the shapes I think you can do it at home where we're not going to date here but make sure you understand all the shapes yeah so the question is how is this different from principle component analysis this is a supervised learning algorithm that will be used to predict the price of a house principle component analysis doesn't predict anything it gets an input matrix X normalizes it compute the covariance matrix and then figures out what are the principal components by doing the eigenvalue decomposition but the outcome of pca is you know that the most important features of your data set X are going to be these features here we're not looking at the features we're only looking at the output that's what is important to us so the question is can you explain why the first layer would see the edges is there any tuition behind it it's not always going to see the edges but it's often time going to see edges because in order to detect a human face let's say you will train an algorithm to find out whose face it is so it has to understand the faces very well you need the network to be complex enough to understand very detailed feature of the face and usually this neuron what it sees as input or pixels so it means every edge here is the multiplication of the weight by a pixel so it sees pixels it cannot understand the face as a whole because it sees only pixels it's very granular information for it so it's going to check if pixels nearby have the same color and understand that there is an edge there okay but it's too complicated to understand the whole face in the first layer however if it understands a little more than a pixel information it can give it to the next neuron this neuron will receive more than pixel information it would receive a little more complex like edges and then it will use this information to build on top of it and build the features of the face so what I'm trying to sum up is that these neurons only see the pixels so they're not able to build more than the edges that's the minimum thing that they can the maximum thing they can build and it's it's a complex topic like interpretation of neural network is very highly researched topic the big research topic so nobody figured out exactly how all the neurons evolved yeah one more question and then we move on so the question is how how do you decide how many neurons per layer how many layers what's the architecture of their neural network there are two things to take into a consideration I would say first and nobody knows the right answer so you have to test it so you guys talked about training set validation set and test set so what we would do is we would try 10 different architectures train its train the network on this look at the validation set accuracy of all these and decide which one seems to be the best that's how we figure out what's the right network size on top of that using experience is often valuable so if you give me a problem I try always to gauge how complex is the problem like CAD classification do you think it's easier or harder than day-and-night classification so then a classification is I give you an image I ask you to predict if it was taken during the day or during the night and on the other hand you want there is a cat on the image or not which one is easier which one is harder who thinks cat classification is harder ok I think people are great at classification seems harder why because there are many breeds of cats can look like different things there's not many breeds of nights one thing that might be challenging in the image classification is if you want also to figure it out in house like inside you know maybe there is a tiny window there and I'm able to tell that is the day but for a network to understand it you will need a lot more data than if only you wanted to work outside different so these problems all have their own complexity based on their complexity I think the network should be deeper become the more complex usually is the problem the more data you need in order to figure out the output the more deeper should be the network that's an intuition I think ok let's move on guys because I think we have about what 12 more minutes okay let's try to write the lost function for this problem so now that we have our network we have written this propagation equation and I will call it for propagation phase going forward it's going from the input to the output later on when we will do we will derive these equations we will call them backward propagation because we're starting from the loss and going backwards so let's let's talk about the optimization problem optimizing w1 w2 w3 b1 b2 Mitri we have a lot of stuff to optimize right we have to find the right values for these and remember model equals architectural parameter we have our architecture if we have our parameters we're done so in order to do that we have to define an objective function sometimes called loss sometimes cost cost function so usually we would call it loss if there is only one example in the batch and cost if there is multiple examples in a match so the last function that let's define the cost function the cost function J depends on Y hat and Y okay so Y hat Y hat is a 3 ok it depends on Y hat and Y and we will set it to be the sum of the loss functions Li and I will normalize it it's not mandatory but normalize it with one over so what does this mean is that we're going for batch gradient descent we want to compute the loss function for the whole batch paralyze our code and then calculate the cost function that will be then derived to give us the direction of the gradient that is the average direction of all the the derivation with respect to the whole input batch and Li will be the last function corresponding to one parameter so what's the error on this specific one input sorry not parameter and it will be the logistic loss you've already seen these equations I believe so now is it more complex to take a derivative with respect to J like of J with respect to the parameters or of L what's the most complex between this one let's say we're taking derivative with respect to W to compare to this one which one is the hardest who thinks J is the hardest we think it doesn't matter it doesn't matter because derivation is a linear operation right so you can just take the derivative inside and you will see that if you know this you just have to take the sum over this so instead of computing or derivatives on J we will come compute them on L but it's totally equivalent there's just one more step at the end okay so now we defined our loss function super we define our loss function and the next step is optimize so we have to compute a lot of derivatives and that's called backward propagation so the question is why is it called backward propagation it's because what we want to do ultimately is this for any N equals one to three we want to do that WL equals W L minus alpha derivative of J with respect to W and BL equals V L minus alpha derivative of J with respect so we want to do that for every parameter in layer 1 2 & 3 so it means we have to compute all these derivatives we have to compute derivative of the cost with respect to W 1 W 2 W 3 B 1 B 2 B 3 you've done it with logistic regression we're going to do it with a neural network and you're going to understand why it's called backward propagation which one you want to start with which derivative you want to start with the derivative with respect W 1 W 2 or W 3 they say assuming we'll do the bias later W what W want you think that value one is a good idea I don't want to do W 1 and I think we should do W 3 and the reason is because if you look at this loss function do you think the relation between W 3 and this loss function is easier to understand or the relation between W 1 and this loss function is the relation between W 3 and this last function because W 3 happens much later in the in the network so if you want to understand how much should we move W 1 in order to make the last move it's much more complicated than answering the question how much should W 3 move to move the loss because there is much more connections if you want to compute with W 1 so that's why we call it backward propagation is because we will start with the top layer the one that's the closest to the last function derive the derivative of J with respect to w1 okay and once we computed this derivative which we are going to do next week once we completed this number we can then tackle this one oh sorry yeah thanks yeah once we computed this number we will be able to compute this one very easily why very easily because we can use the chain rule of calculus so let's see how it works we're I'm just going to give you a one minute pitch on on backdrop but we'll do it next week together so if we had to compute this derivative what I will do is that I will separate it into several derivative that are easier I will separate it into derivative of J with respect as something with this something with respect the w3 and the question is what should this something be I will look at my equations I know that J depends on Y hat and I know that Y hat depends on Z 3 Y hat is the same thing as a 3 I know it depends on Z 3 so why don't why don't I include these three in my equation I also know that Z 3 depends on W 3 and the derivative of Z 3 with respect to W 3 super easy it's just a 2 transpose so I will just make a quick hack and say that this derivative is the same as taking it with respect to a 3 taking the derivative of 83 with respect to Z 3 and taking the derivative of Z 3 with respect to W 3 so you see same same derivative calculated in different ways and I know this I know these are pretty easy to compute so that's why we call it back propagation is because we use the chain rule to compute the derivative w3 and then one I want to do it for w2 I'm going to insert I'm going to insert the derivative with Z three times the derivative of Z three with respect to a two times the derivative of a two with respect to Z 2 and their relative of Z 2 with respect to W 2 does this make sense that this thing here is the same thing as this it means if I want to compute the derivative of W 2 I don't need to come to this anymore I already did for W 3 I just need to compute those which are easy ones and so on if I want to compute the derivative of J with respect to W 1 I'm going to I'm not going to decompose all the thing again I'm just going to take the derivative of J with respect to Z 2 which is equal to this whole thing and then I'm going to multiply it by derivative of Z 2 with respect to a 1 times derivative of a 1 with respect to Z 1 times the derivative of Z 1 with respect to W 1 and again this thing I know it already I computed it previously just for this one so what's what's interesting about it is that I'm not going to redo the work I did I'm just going to store the right values while back propagating and continue to derivate one thing that you need to notice though is that look you need this forward propagation equation in order to remember what should be the path to take in your chain rule because you know that this derivative of J with respect to W 3 I cannot use it as it is because W 3 is not connected to the previous layer if you look at this equation e 2 doesn't depend on W 3 it depends on Z 3 sorry like my bad it depends no sorry what I wanted to say is that Z 2 is connected to W 2 but a1 is not connected to w2 so you want to choose the path that you're going through in the proper way so that there is no cancellation in these derivatives you cannot compute derivative of W 2 with respect to 2 a1 right you cannot compute that you don't know it okay so I think we're done for today so one thing that I'd like you to do if you have time is just think about the things that can be tweaked in a neural network when you build a neural network you are not done you have to tweak it you have to tweak the activations you have to take the loss function there's many things you can tweak and that's what we're going to see next which ok thanks