all right everyone Pontius Sandoval 8 so welcome to the final lecture of sis 229 this quarter or I guess to the home viewers welcome to the season finale so what like to do today is wrap up our discussion on reinforcer learning and then and it will conclude the class so I think you know over the last few lectures you saw a lot of we saw a lot at nav so maybe as a brief interlude here are some videos so sample autonomous helicopter you know there's a project that I know Peter view Adam coats some some former students here now some of the machine learning greats were on when they were PhD students here and oh and and and I think using algorithms similar to the ones you learned in this class how do you make a helicopter fly so it just have fun there's a video shot on top of one of the Stanford soccer fields I was actually a cameraman that day and zooming out the camera see the trees planted in the sky say it turns out there's a small radio control helicopter it turns out that when you're very far away you can't tell if this is a small radio control helicopter if there is like a helicopter with people sitting there named so there was actually this um you know Foods is on a kind of soccer field the big grass field off Santo Road it turns out across Sand Hill Road and what that high-rises there was a those an elder lady that lives in one of those apartments and when if she saw this she would call 911 and say hey this copter about the crash and then the the firemen would come out and I and I think they were Polly relief probably disappointed that there was no one for us for them to save I think um and so and and I think um let's see one of the things I promise to do in the debugging learning algorithms lecture was to just go over the reinforcement learning example again so let me just do that now but with notation that I think you now understand compared to oh yes oh you as an aerobatic stunt yeah that I I don't think there's a good reason for fine how it drops upside down other than that you can there are a lot of videos of samples on the side cough to find all sorts of stunts go to heli stanford.edu Akio I don't stanford.edu and the stem photons are cogs did a lot more than fine upside down I mean make some maneuvers that look aerodynamically impossible such as a helicopter that looks like a stumbling just spinning randomly but staying the same place in the air right now it's called the chaos maneuver and if you look how to go wow this work was turning upside down spinning around the area same direction but it's just staying right there in the air and not crashing and so the maneuvers like that that um the very best human pilots in the world can fly with helicopters and I think this was just demonstration yes and I think a lot of this work wound up influencing something later work on the quadcopter drones and a few research labs and yeah I think it was a difficult control problem and it was it was one of those things you do when you're you when you're a university you want to solve on the hardest problems round um but one that step through of you the debugging process that we went through as we were you're building a helicopter like this so when you're trying to get a helicopter to fly upside down fly stunts you don't wanna crash too often so step one is build a model or build a simulator of the helicopter right much much as you saw we start to talk about fitted value iteration and then choose the reward function like that and it turns out that specifies the reward function for staying a place is not that hard you know like a quadratic function like that works okay but if you want a helicopter to fly aggressive maneuvers it's actually quite tricky to specify what is a good turn for a helicopter and then what you do is you run the course to learning algorithm to try to maximize say the final horizon MTP formulation maximize some rewards over T time steps you get a policy pie and then whenever you do this the first time you do this you find that the resulting controller does much worse than the human pilot and the question is whether you do Nix right this is better this is almost I think this is almost exactly the slide I showed you last time because I might clean up the slide using reinforcement or any notation rather than it slightly simplified notation you saw before you learned about reinforcement or anything and so the question is and and again if you work on the reinforcer learning problem yourself you know there's a good chance you have to answer this question yourself for whatever robot or other reinforcement learning or factory automation or stock trading system or whatever it is you are trying to get to work with enforcement learning but do you want to improve the modelsim model or doing a modified reward function or do you want to modify the reinforcement learning algorithm okay and multiply the reports of learning album includes things like playing with the descritization that you're using if you are taking a continuously MDP and discretizing it's a solve of a finite state MVP formulation or modifying the reinforcement learning algorithm and Cruz also may be choosing new features to use in physicality iteration small things we could try or maybe instead of using a linear function approximator instead of fitting a linear function for fit evaluation maybe you want to use a bigger you know deep neural network right but so which of these steps is the most useful thing to do so this is the analysis of those three things you know if I give you a second meters right but if these three statements are true then the learn controller should have flown well on the helicopter right and so those three sentences correspond to the three things in yellow that you could work on is a problem that you know statement one is false that the assimilator isn't good enough for his problem that statement two is false that oh sorry I think actually two and three are reverse right but the three statements correspond to three things in yellow I think two and three are and are in opposite order right is the arrow alpha maximizing some rewards is a reward function actually the right thing to maximize and so here the Diagnostics you could use to see this helicopter simulator is accurate well first check if the policy flies well in simulation if your policy flies one simulation but not in real life then this shows that the problem is with your simulator and you should try to learn a better model for your helicopter right and if you're using a linear model this with the matrices a and B if you know st plus 1 equals a st plus ba t if you try try anymore or maybe try a nonlinear model but if you find it the problem is not your simulator if you find that your policy is flying poorly in simulation and flying poorly in real life right then this is the diagnostic I will use so I shall show these two lines so let's that human be the human control policy so hire a human pilot which we did we're fortunate that one of the best what one of them America is tall you know aerobatic helicopter pilots working with us and he using his control signals radio control can make a helicopter fly upside down tumble do flips loops rows so we had very good human pilot help us fly the helicopter manually so what you can do is test whether or not the so this this thing here right that's just a payoff of the learned policy as measured on your reward function so check if the learn policy achieves a better or a worse payoff then a human pilot you can write and so that means you know go ahead and let the learn policy fly the helicopter and we get the humans up fi the helicopter and compute the summer rewards on the sequence of states that these two systems take the helicopter through and just see whether the human or the learn policy achieves a higher payoff achieves a higher summer rewards and if the payoff achieved by the learning algorithm is less than a payoff achieved by the human then this shows that the learned policy is not actually maximizing the summer rewards right because whatever human is doing you know he or she is doing a better job maximizing some rewards then the learn policy so this means that you should you know consider working on the reinforcement learning algorithm to try to make it do a better job maximizing the Sun removals and then on the flip side it is an equality goes the other way right so positive if the payoff or the rol is greater than the payoff of the human then what that means is that the ORR algorithm is actually doing a better job maximizing the summer rewards but they're still flying worse so what this tells you is that doing a really good job maximizing some rewards does not correspond to how you actually want the helicopter to fly and so that means that maybe you should work on improving the reward function that the reward function is not capturing what's actually most important to find helicopter well and then you multiply the reward function right so in a typical workflow I'm going to describe to you what what what it feels like to work on the machine learning project like this and it was a big multi or machine learning project but when you're working on a big complicated machine learning project like this the bottleneck moves around meaning that you build a helicopter get a human pilot fly it you're getting the world near on these Diagnostics and maybe the first time you do this you find wow the simulator is really inaccurate then you are going to work on improving the simulator for a couple months and then you know and every now and then you come back and rerun this diagnostic then maybe for the first two months the project you keep on saying yep soon this is not good enough so it's not good enough so as long enough after working on this simulator for a couple months you you may find that item one that's no longer the problem you might then find that item three is the problem the simulator is now good enough but when you run this diagnostic two months in the project you might say wow looks like you're our algorithm is maximally reward function but this is not good flying so now I think the biggest problem for the project or the biggest bottleneck with the project is that the reward function is not good enough and then you might spend you know another one or two or three or sometimes longer months working to try to improve the reward function and you might do that for a while and then when the reward function is good enough then that exposes the next problem your system which might be that the ROI algorithm is good enough and so the problem you should be working on actually moves around and it's different in different phases of the project and when you're working on this it feels like every time you solve the current problem that exposes the Nix most important to work on then you work on that and we solve that then this helps you identify an explosive next most important element work on and you kind of keep doing that or you keep iterating or keep solving problems until hopefully you get a helicopter that does what you wanted to but I think teams that have the discipline to prioritize according to Diagnostics like this tend to be much more efficient teams that kind of go by gut feeling in terms of selecting you know what to what to spend your time on all right any questions about this oh sorry so again yeah I kind of want to say yes let me think yeah I wouldn't usually check step one first and then if I think the simulator is okay then look at steps two and three maybe one of the thing about when you work on these projects there is some judgment involved so I think I'm presenting these things as those a rigid mathematical formula that's cut and dry this formula says now working on step one then this one says now work on step three there is there is more judgment involved because when you run these things I'll say if you might say well looks like the simulator is not that good but it's kind of good and there's a little bit ambiguous and oh looks like you know and so that's what it often feels like and so a team would get together look for the evidence from all three steps and then say you know well maybe the simulator is not that good but it's maybe good enough and but boy the reinforcement the reward function is really bad let's focus on that so there is some surrounding a hard and fast rule there there is some judgment needed to make these decisions but having a so when a reading machine there any teams often my teams will you know run D side.now sakes get together and look at the evidence and then discuss in debate what's the best way to move forward but I think the process in making sure you've that discussion to debate it's much better than the alternative which is you know someone just picked something very random and the team does that so just yeah maybe I had the laptop up you know a little bit of fun but a little bit because I'm to illustrate fitted value iteration let me just show another reinforcement learning video oh and by doing one of the I think if I look at the future of a I featured machine learning you know there's a lot of hype about reinforcement learning for game playing which is fine you know we all like we all love computers playing computer games like that's a great thing I think but but I think that some of the most exciting applications are reinforced with learning coming down the pipe I think will be robotics I don't know the next few years even though there are only a few success stories of reinforcement they applied to robotics there are more and more right now one of the trends I see you know we look at the academic publications and some of the things making their way into industrial environments is I think in the next several years just based on the stuff I see my friends in many different companies and many different entities are working on I think there will be a rise of reinforcement learning algorithms applied to robotics I think there will be one important area to watch out for right um but um so you know there's a now Stanford video this is again just using reinforcement learning to get a robot dog to climb over obstacles like these my friends that were less generous did not want to think of this as a robot dog they thought it was more like a robot cockroach but I think cockroaches done that for the x-ray coffee with six legs yeah but so how do you program a robot dog like this right to climb over terrain so one of the key components is work I am Zico Colter now a Connie Mellon professor another one of the machine learning greats is a key part of this was a valley function approximation where it dog sounds on the left and it goes get to the right then the approximate value function kind of I'm simplifying a little bit right but but the approximate value function tells it given the 3d shape of the terrain the middle plots is a height map where the different shades tell you how tall is the terrain but given the 3d shape the terrain the dog learns a value function that tells it what is the cost of putting his feet on different locations to the terrain and it learns among other things you know not to put his feet at the edge of a cliff because then it's likely to slip off the edge of a cliff and fall over right so but but hopefully this gives a visualization of whether learning value functions very very complicated functions I'll say and okay the states is very high dimension so this is all kind of project on so 2d space you can visualize it but but this is what the simplified value function looks like a robot like this okay all right so with that let me return to the so um there's just one class of algorithms I want to describe to you today which are called policy search algorithms and sometimes policy search is also called direct policy search and to explain what this means so far our approach to reinforcement learning has been to first learn or approximate the value function you know approximate v-star and then use that to learn or at least hopefully approximate PI star right so we had you saw a value iteration top reading Apollo through a chinois philosophy to reinforce the learning was to estimate the value function and then use that you know that equation with the arc max to figure out what is pi star so this is an indirect way of getting at a policy because we would first try to figure out was the value function in direct policy search we try to find a good policy directly right hence the term direct policy search because you don't you go straight for trying to find a good policy without the intermediate step of finding an approximation to the value function so um let's see I'm gonna use as the most Vida example the inverted pendulum great so that is that thing with the three things here and let's say your actions are to accelerate left or to a salary right right and then you could have and you can have stay still a cell it's from a cell rate that strong salary right you could more than two actions but let's just say you've an inverted pendulum with two actions so if you want to talk about pros and cons the direct policy search later but if you want to bypass direct policy search you're going to apply policy search the first step is to come up with the class of policies you are entertained or come up with the set of functions you use to approximate the policy so again to make an analogy when you saw logistic regression for the first time you know we kind of said that we would approximate Y as a hypothesis right whose form was governed by this sigmoid function and you remember in week 2 when I first described logistic regression I kind of pulled this out of a hat right and said oh yeah trust me let's use logistic function and and then later we saw there's a special case of the generalized linear model but you know we just had to write down some form for how we will predict Y as a function of X so in direct policy search we will have to come up with a form for pi right so right they just come up with a function for however is an H indirect policy search will have to come our way for how we approximate the policy pi right and so you know one thing we have to do is say well maybe the action will approximate with some policy PI may be parametrized by theta and it's now a function of the states and maybe it'll be 1 over 1 plus e to the negative theta transpose you know the state vector right where the state vector may be something like X dot and then the angle at the angle dot right if if just this fine maybe add an intercept okay and I switch this from theta to Phi to avoid conflict into notation okay um this isn't really the form of the policy were right so let me let me make one more definition and then I'll show you a form of a specific form of policy you can use but it's actually not quite this what we need to treat this a little bit so the direct policy search algorithm will use will use a stochastic policy so this is a new definition so sarcastic policy is a function so we're going to use for the direct policy search algorithm that you see today we're going to use the classic policies meaning that on every time step the policy will tell you what's the chance you want to accelerate left versus what's the chance you want to settle right and then you use a random number generator to select either left or right to accelerate on your inverted pendulum depending on the policies there depending on the probability is output by this policy okay and so here's one example let's see which is you can have [Applause] so you know continuing with the inverted pendulum here's one policy that might be reasonable where you say that let's see so you know in a state that's the chancy you take the salary right action is given by this sigmoid function and the chance that in the state that's you take the accellerate left action is given by that okay and here's one example for why this might be a reasonable policy so let's say the state vector s this one X X dot Phi Phi dot where you know this angle of the inverted pendulum is the angle Phi and let's say for the sake of arguments that we set the parameter of this policy Phi to be um zero zero zero one zero so in this case this is saying that let's see so theta transpose s is just equal to Phi right and so in this case right because you know theta transpose s just 1 times Phi everything else gets multiplied zero and so in this case the same that the chance to accelerate to the right is equal to one over one plus e to the negative how far is the PO tilted over to the right and so this policy gives you the effect that the further the PO is tilted to the right the more aggressively you want to accelerate to the right okay so this is very simple policy it's not a great policy but it's not a totally unreasonable policy which is well look at how far the post tilted so that for the right apply sigmoid function and then accelerate to the left or right you know depending on how far is tilted to the right now and and and because this is the right so this is really the chance of taking the ass a very right action as a function of the PO angle pi right now this is not the best policy because it ignores all the features other than Phi but if you were to set both theta equals you know 0 negative 0.5 0 1 0 then this policy the negative 0.5 now multiplies into the exposition right now this new policy if you have this value of theta it takes an account how far is your cards already to the right where I guess this is the X distance and the further your cart is already I guess if your cart is on the set of wheels right it's on the railway track and you don't want to fall off the rim and you want to keep the car kind of Center you don't want to fall off the end of your table but this now says the further this is to the right already your well the less likely you should be to accelerate to the right okay and so maybe this is suddenly better policy there were descending parameters and more generally what you would like is to come up with five numbers that tells you how to trade off how much you should aside to the right based on the position velocity angle and angular velocity of their current state of the car of the of the inverted pendulum and what a direct policy search Alber will do is help you come up with a set of numbers that results in hopefully a reasonable policy for controlling the inverted pendulum hope and a policy that hopefully results in a appropriate set of probabilities that cause it to accelerate to the right whenever's good to do so and Sarah to let you know more often when it's good to do so so so I'll go is to find the five parameters theta so that's when we execute PI of s a we maximize max of a theta the expected value of R of s 0 is 0 plus dot dot plus and so the reward function could be negative 1 whenever the inverted pendulum falls over and 0 whenever it stays up or whatever or something that measures how well you betcha Panem is doing but the goal of a direct policy search algorithm is to choose a set parameters theta so that we actually the policy you maximize your expected payoff and I'm gonna use to find a horizon setting for the album that was helpful today okay and then one one other difference between policy search compared to estimating the value function is that indirect policy search here as 0 is a fixed initial State it turns out that when we were estimating the value function V saw you found the best possible policy for starting from any state right and there's kind of no matter what state you start from is simultaneously the best possible policy for all states indirect policy search we assume that either there's a fixed start state fix initial state at 0 or there's a fixed distribution over initial States I'm going to try to maximize the expected reward or back to your initial state or respect to an initial priority distribution over what is the initial state okay so that's that's one other difference so all right so this right is out the go is a maximize overall theta the expected value of R of s 0 a 0 because R of s 1 a 1 plus dot dot dot up to R of s t-80 you know given pi theta and in order to simplify the math we'll write on this board today I'm just gonna set G equals 1 to simplify the math in order to not carry such a long summation but it turns out that so I'm just do like a 2 x mm DP just to simplify the derivation but everything works you know just with a longer some if you have a more general version of T and so this term here the expectation is equal to sum over all possible state action sequences right and again this way go up to St and 80 but you just said capital T equals 1 um what's the chance your MVP starts out and some state as 0 so this is your initial state distribution times the chance that in that state you take the first action a zero oh sorry just let me write this out right so the chance of your MVP going through the state action sequence times times that right so that's what it means to self compute the expected value of the payoff and so instead of writing all this sum I'm just going to call this the payoff and so this is equal to sum over s 0 a 0 s 1 a 1 of the Chauncey MTP starts in state 0 times the challenge that in state 0 you end up choosing the action a 0 times the chance governed by the state transition probabilities that you end up in state 1 state s 1 times the chance a state that's one you end up choosing so that's 1 and then times the payoff ok and so what we're going to be able to do is derive a gradient ascent algorithm actually so costly gradient ascent algorithm as a function of theta to maximize this thing to maximize the expected value of this thing and that and and this is a this is how we'll do direct policy search ok so let me just write out the algorithm and then we'll go through why the algorithm that I write down is maximizing this expected payoff so this algorithm is called the reinforced algorithm the option reinforced algorithm had a few other bells and whistles but explain the code the idea but they were enforcing that the reinforced algorithm does the following which is you're going to run your MDP right and just you know run it for a trajectory of tea time step so again you know I'm just gonna well right and and actually you would uh technically you would run it for tea time steps but you know let's just say for now well we'll do only the thing in blue we run it for one time set go to sleep capital T equal one and then you would compute the payoff right equals R of 0 plus R of s 1 and then in the more general case you know plus dot dot plus R of s T and then you perform the following update which is theta gets updated as theta plus the learning rate alpha times and then times the payoff and again I'm just setting capital t equals 1 if capital t was bigger you would just sum this all the way up to time T so that's the algorithm that's on every iteration through the reinforced algorithm and through the reinforced algorithm you will take your robot take your inverted pendulum run it through t time steps executing your current policy so choose actions randomly according to the current stochastic policy using current values of the parameters data compute the total sum rewards you receive let's call the payoff and then update theta using this funny formula right now on every iteration of this algorithm you're going to update theta and it turns out that grandpa's is a stochastic gradient ascent algorithm and you remember when we talked about linear regression right you saw me draw pictures like this if there's a global minimum then gradient descent would just you know take a straight path to the minimum but stochastic gradient descent would take a more random path right towards the minimum and it kind of also lays around there maybe it doesn't quite converge unless you slowly decrease the learning rate alpha so that's what we have for stochastic gradient descent for linear regression what we'll see in a minute is that reinforce is a stochastic gradient ascent algorithm meaning that each of these updates is random because it depends on what was this state action sequence that you just saw and what was the payoff the you just saw but what Willis show is that on expectation the the average update you know this this update to theta this thing you're having two theta that on average let's see so that on average this update here is exactly in the direction of the gradient so that on average yeah because uh every every loo every time through this loop you're making a random update to theta and this random and noisy because it depends on this random state sequence right then just a sequence is random because of the state transition probabilities and also because of the fact that you're choosing actions randomly but on but the expected value of this update you see in a little bit turns out to be exactly the direction of the gradient which is why this report algorithm is a gradient ascent algorithm so let's let's show that now so all right so what we want to do is maximize the expected payoff which is a formula we derive up there and so we're going to want to take derivatives with respect to theta of the expected payoff right I'm just gonna copy that for me there up there so that's a chance of that see going through that say action sequence time to pay off and so we want to take derivatives of this and you know so we can write go up hill using gradient ascent so we're going to do this in four steps now first want to remind you when you take the derivative of Smith of a product of three things right so let's say that you have three functions f of theta times G of theta times H of theta so by the product rule you know derivatives product grew from calculus the derivative of the product of three things is obtained by you know taking the derivatives of each of them one at a time right so this is f prime times G times H plus G prime here so the product rule from calculus is that if you want to take derivatives of a product of three things then you kind of take the derivatives one at a time you end up with three sums right and so we're going to apply the product rule to this where we have here we have two different terms that depend on theta and so when we take the derivative of this thing respect to theta we're gonna have of two terms that correspond to taking derivative this one is integral to doing that one's right and so this derivative is equal to so the first term is the sum over all the state action sequences you have s0 and then let's see so now we have PI of theta excuse me the derivative respect to pi theta as zero a zero and then plus Oh and then times that pay off right so the whole thing here is then multiplied by the payoff okay so we just applied the product rule for calculus where for the first term in the sum we kind of took the derivative of this first thing and then for the second term in the sum we took the derivative of the second thing and now I'm gonna make one more algebraic trick which is I'm going to multiply and divide by that same term and then most fine divided by the same thing here right so lots of multi but most times divided by the same thing right and then finally if you factor out so now the final step is I'm going to factor out these terms I'm underlining right because this terms I underlined this is just you know the probability or the whole state sequence right and again for the orange thing that this this orange thing right these two orange things multiplied together is equal to that for each thing in that box as well and so the final step is to factor out the orange box which is just P of s 0 a 0 s 1 a 1 right so that's the thing I boxed up in orange times then those two terms involving the derivatives okay and I think Oh right where I guess this term goes there and this term goes there and so this is just equal to well and if you look at the reinforced algorithm right that we wrote down this is just equal to sum over you know all the state action sequences times the probability of the gradient update because uh I guess I'm running out of colors but you know this is a gradient update and that's just right equal to this thing okay so what this shows is that even though on each iteration the direction of the gradient updates is random the the expected value of how you update the parameters is exactly equal to the derivative of your objective of your expected total payoff so we started saying that this formula is your expected total payoff so let's figure out what's the derivative your expected total payoff and we found that the expected the derivative your expected total payoff the derivative the thing you want to maximize is equal to the expected value of your gradient update and so this proves that on average you know if you have a very small learning rate you end up averaging over many steps right but on average the updates that reinforce is taking on every iteration is exactly in the direction of the derivative of the expected total payoff that you're trying to maximize any questions about this yeah oh is this impending the choice of the dysfunction this is true for any form of a stochastic policy where the definition is that you know pi theta as zero a zero has to be the chance of taking that action in that state but this could be any function you want it could be a soft massacree logistic function and many many different complicated features it could be has been continuous the has been differentiable function and actually one of the reasons we shifted to stochastic policies was because previously just had two actions is either left or right right and so you can't define a derivative over a discontinuous function like either left or right but now we have a probability that shifts slowly between what's the probability to go let's go right and by making this a continuous function of theta you can then take derivatives in five unison it doesn't really just a function so another way to train a helicopter controller is to use supervised learning where you have a human expert train you know so you can also actually have a human pilot demonstrate and just stay take this action right and then you supervise the running to just learn directly a mapping from the state into the action I think this I don't know this might be okay for low-speed helicopter flight I don't think it works super well I bet you could do this in not crash a helicopter but but to get the best results I wouldn't use this approach it turns out for some of the maneuvers where she fight better than human pilots as well oh and so for other types of policies messy [Applause] so direct policy search also works if you have continuous value actions and you don't want to discretize the action so maybe here's a simple example let's say a is a real number such as the magnitude that the force you apply to accelerating left or right it's around discretizing your inverted pendulum you want to output a continuous number how hard you're sorry to left or right or for self-driving car maybe theta is the steering angle which is a real value number so a simple policy would be a equals you know say the transpose s and then plus Gaussian noise and if just for the purpose of training you're willing to pretend that your policy is to apply the action theta transpose s and then a little bit of Gaussian noise to it then the whole framework for reinforce but this type of gradient descent also will also work and now I guess I reckon implementing this you pray turn off the Gaussian noise I know there are little tricks like that as well um so let's see some pros and cons of so when should you use direct policy search and when should you use value iteration or a value function based type of approach so it turns out this one setting actually there are two settings where a direct policy search works much better one is if you have a palm DP P Oh in this case that's a partially observable and that's it for example you know for the inverted pendulum that's a pro angle Phi you have the car and this is your position X and we've been saying that the state space is X X dot Phi Phi dot right but let's say that you have sensors on this inverted pendulum that allow you to Asia only the position and only the angle of the inverted pendulum so you might have an angle sensor you know down here and you might have a position sensor for your birthday pendulum but maybe you don't know the velocity and you don't know the angular velocity right so this is an example of a partially observable Markov decision process because and what this means is that on every step you do not get to see the whole state because you you don't have enough sensors to tell you exactly what is the state of the entire system so in a partially observable MDP at each step you get a partial and potentially noisy measurement of the state right and then have to take actions I have to choose an action a using these partial and potentially noisy measurements right which is uh maybe you only observe the position and the angle but your senses aren't even totally accurate so you get a slightly noisy you know estimate or the position you get a slightly noisy as for the angle but you just have to choose in action based on your noisy estimates of just two of the four state variables it turns out that there's been a lot of academic literature trying to generalize value function based approaches the pom DPS and they're very complicated algorithms in the literature on trying to apply value function based approaches upon GPS but those algorithms despite their very high level of complexity you know are not are not widely in production right but if you use the direct policy search algorithm then there's actually very little problem oh let me just write this down so let's say the observation is on every time step you observe y equals x phi plus noise right so you just don't know what is a state and in a pom DP you cannot approximate the value function or even if you knew what was V Star right you can't compute PI star because uh I mean maybe you know what is pi star best listen compute V Sarn pi saw but if you don't know what the state is you can't apply PI star to the state because it's in so how do you choose in action if you're using direct policy search then here's one thing you could do which is you can say that hi of given an observation the chance of going to the right given your parent observation is equal to 1 over 1 plus e to the negative theta transpose Y where I guess Y can be you know one read X plus noise v plus noise that's X plus noise and so you could run reinforce using just the observations you have to try to still classically try to randomly choose an action and nothing in the frame way we talked about provenza's album from working and so direct policy search just works very naturally even if you have only partial observations of the state and more generally instead of plugging the direct observations this can be any set of I just make a side comment for those who didn't know what common causes are don't have you don't but one common one common way of using the right policy search would be to use some estimate such as a common filter a proper grammar model or something to use your historical estimates look don't don't just look at your one set of measurements now but look at all your historical measurements and then their algorithms such as something called a common filter that lets you estimate whatever has the current state the full state vector you can plug that full state vector estimate into the features you used to choose a to choose an action that's a common design paradigm if you don't know what a common filter is don't worry about it but you take take one a steal invoice for something on them yeah but that's one common paradigm where you could use your partial observations that's been the full state and plug that as a features into the rent policy session okay so that's one setting where the right policy search works just just applies in a way that value function approximation is very difficult to even get to apply now one last thing is one last consideration for secure apply policy search algorithm or a value function transformation algorithm oh it turns out the reinforced algorithm is is actually very inefficient as in you end up you know when I when you look at research papers on the reinforced algorithm it's not unusual for people that run the reinforced algorithm for like a million iterations or ten million iterations so you just have to train it turns out the gradient estimates for the reinforced algorithm even though the expected values right there's actually very noisy and so if you train the reinforced algorithm you end up just running for a very very very long time right it does work was a pretty inefficient algorithm so that's one disadvantage of the reinforced algorithm is that the gradient estimates on expectation are exactly what you want it to be but there's a lot of variance in the gradient so you have to run it for a long time of a very small learning rate but one other reason to use the right policy search is it's kind of ask yourself do you think pi-star is simpler or is beast are simpler right and so um here's what I mean there are the in in in robotics there's sometimes what we call low level controls house and one way to think of low level controls house is flying a helicopter hovering the helicopter is an example of a low-level control toss and one way to inform me think of local control houses kind of really skilled human you know holding a joystick control this thing making see the depends decisions right so those are kind of almost instinctual in the tiny fraction of a second and almost by few you could control the thing those those are tend to be low level control Tasos either the parents holding a joystick a skill person because that inverted pendulum or you know steer helicopter those are low level control tasks in contrast playing chess is not a low-level control toss yeah because for the most part to be a very good chess player is not really a seat-of-the-pants you know take a bit make a decision in like in 0.1 seconds right you kind of have to think multiple steps ahead and in low-level control toss there's usually some control policy that is quite simple a very simple function mappings of states the actions that's pretty good and so that allows you to specify a relatively simple class of functions of PI star and direct policy search would be relatively promising for tasks like those whereas in contrast if you want to play chess okay go or do these things we have multiple steps of reasoning I think that if you're driving a car on a straight road that's a low-level control toss we just look at the road you just you know turn the steering or a little bit to stay on the road so that's a lot of control tasks but if you are planning how to you know overtake this car and avoid that other car whether it's a pedestrian and the bicycle is along the way then that's less of a low-level controlled house and that requires more multi-step reasoning right I guess depend how aggressive a driver you are right driving on the highway you know may require more or less multi-step reasoning where you want to overtake this car before the trucker comes in this Lane so that that type of thing is more multi-step reasoning and the person's like that tend to be difficult for a very simple like a linear function to be a good policy and for those things in playing chess playing go playing checkers a value function approximation approach may be more promising okay so any questions about the oh and so again a long helicopter flight actually my first attempts for flying helicopters were actually the right policy search because flying helicopters I should see the pants things but then when you try to find more complex maneuvers then you end up using something maybe closer to value function approximation that method so if you want to find very complicated maneuver so the video you saw just now the helicopter flying upside down the algorithm implemented on for that pickle video that was a different policy search algorithm right no not exactly this one a little bit different but that was a tear apart see so geography but if one helicopter fly very complicated maneuver then you need something maybe closer to the value from Shiprock Smith and Soda and there is exciting research on how to blend direct policy search approaches together with value function approximation book approaches so actually alphago one of the reasons alphago worked was sorry you know go claim program rate by deep I was there was a blend of ideas from both of these types of literature which enabled it to scale to a much bigger system to play go in a very very very impressive all right any questions about this alright um so just final application examples you know reinforcement learning today is making strong let's see so there's a lot of work on reinforce to learning for game playing checkers chess go that is exciting um reinforcement learning today is used in is using a growing number of robotics applications I think for controlling a lot of robots there is a honor if you go to robotics conferences if you look at some of the projects being done by some of the very large companies that make very large machines right I have many friends in multiple you know large companies making large machines that are increasingly using reinforcement if you control them there is fascinating work using reports of learning for optimizing anti factory deployments there's academic research we're still in researcher as far as I know I shouldn't mean maybe scientific deployed on using reinforcement learning to build chat BOTS and actually on using reinforcement learning to build a a I based guidance counselor for example right where the actions you take up what you say to students and then and then the reward is you know do you manage to help a student navigate their coursework or navigate their career there is uh and there's also starting to be applied to healthcare where one of the keys are reinforced with learning is is this a sequential decision making process right where do you have to take a sequence of decisions that may affect your reward over time and I think and in in healthcare there is work on medical planning where the goal is not you know send you to get a blood test and then we're done right in complicated medical procedures we might essentially get a blood test then based on the outcome of the blood test we might send you to get a biopsy or not all right ask you to take a drug and then come back in two weeks but is this very complicated sequential decision making process for treatment of complicated healthcare conditions and so this fascinating work on trying to apply reinforcement learning that instead of multi-step reasoning where it's not about what sense for treatment and then you never see you again for the rest of your life as well here's the first thing you do then come back let's see what stain you get to after taking this blood test so let's see what you can see you get to after trying a drug and then coming back on the week to see what has happened to symptoms but I think that these are all sectors where reinforcement learning is making inroads or even actually stock trading okay maybe not the most inspiring one but one of my friends on the East Coast was and then was a and just actually if you or your parents invest in mutual funds this may be being used to buy and sell shares for them today depending on what back they're investing I know what Bank is doing this but I won't say it out loud oh but but if you want to buy or sell you know say a million shares of stock a very large volume of stock you may not want to do it in a very public way because that will affect the price of the shares right so if everyone knows that a very large investors about to buy a million shares or buy ten million shares or whatever that will cause the price to increase and this this is disadvantage as a person wanting to buy shares but so there's been very interesting work on using reinforcement learning to decide how the sequence out you'll you'll buy how to buy the stock in small Lots in this trading market is called dark pools these are Google if you're curious as you don't bother to try to buy a very large lot of shares or so a very large lot of shares without affecting the market price too much because the way your effective market price always breaks against you know is always against you it's always bad right so this work laid out as well so anyway I think um many applications I pursue you think that one of the most exciting areas for reinforcement learning will be robotics but well we'll see what what happens over the next few years all right so let's see we're just five more minutes um and and just a wrap-up I think you know we've gone through quite a lot of stuff I guess from supervised learning to learning theory and advice or apply learning algorithms to unsupervised learning although it was it k-means pca EMA share gaussian factor analysis in Pentonville analysis to most recently reinforcement learning with Val function approaches fitted value iteration policy search so feels like we did feels like feels like I feels like you've seen a lot of learning algorithms um go ahead how does enforce learn compared to have a sarah learning I think of those as a pretty distinct logicians yeah yeah so I think and again actually I until I I know a lot of non publicly known facts about the machine there any world but uh one of the things that I actually happen to know is that some of the ideas our adversary learning you know so can you take a picture of ice you know very little bit by tweaking a bunch of pixel values they're not visible to human eye they're fools our learning algorithm into thinking that this picture is actually cat one's clean all the cattle whatever so I actually know that there are attackers out in the world today using techniques like that to attack you know websites to try to fool you know some of the websites down pretty sure you guys use in fooled there anti-spam anti-fraud anti undermining democracy types of algorithms into to make decisions so it's a it's exciting time doing machine learning right now that we get to fight battles like these okay and and I think you know I think we're really I think that what the things you guys have learned in machine learning I think all of you are now very knowledgeable right I think all of you are experts in all the ideas of core machine learning and I hope that um I think when we look around the world there's so many worthwhile projects you could do with machine learning and the number of you that know these techniques is so small that I hope that you take these skills oh and some of you will go you know build businesses and make a lot of money that's great some of you will take these ideas and help drive basic research at Stanford or at other institutions I think that's fantastic but I think whatever you're doing the number of worthwhile projects on the planet is so large and the number of you that actually know how to use these techniques is so small that I hope that you take these skills you're learning from this cause and go and do something meaningful and do something that helps other people I've even seen this looking valley that there are lot of ways you know to build very valuable businesses and some of you do that and that's great but I hope that you do it in a way that helps other people I think over the past few years we've seen I think that uh in Silicon Valley maybe ten years ago the contract we had with Society was that people would trust us with their data and then we'll use their data to help them but I think in the past year that contract feels like that has been broken and the world's faith in Silicon Valley has been shaken up but I think that places even more pressure on all of us on all of you to make sure that the work you go out into the world to do is work that action is respectful of individuals respectful individuals privacy is transparent open and that ultimately is helping drive forward humanity or helping people helping drive forward basic research or building products that actually help people rather than exploit their foibles for profit but to there I hope that all of you will take your superpowers that you now have an um go out to do to do meaningful work and let's see and I think oh end and lastly just I just don't personally I want to you know thank all of you on behalf of the TAS the ho teaching team and myself wants to thank all of you for your hard work sometimes they go with homework problems the good party also runs ago Wow that she got that problem I thought that was really hard or not project Muslims go hey that's really cool look forward to seeing your final project results at the final poster session so I know that all of you have worked really hard and if you didn't don't tell me that thing almost but I'm gonna make sure you know there's a I think it wasn't that long ago that I was a student you know working late at night on homework problems and and I know that many of you have been doing that for the homeworks standing for the midterm for work on your final term projects so want to make sure you know I'm very grateful for the hard work you put into this class and I hope that I hope that your your heart and skills will also reward you very well in the future and also help you do work that that you find this meaningful so thank you very much [Applause]