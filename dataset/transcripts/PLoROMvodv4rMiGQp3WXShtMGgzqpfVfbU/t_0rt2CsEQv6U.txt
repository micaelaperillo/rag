okay hey everyone so welcome to the final week of the class um what I want to do today is share with you a few generalizations of reinforcement learning and of mdps so you've learned about the basic MVP formula zone of states action stations info releases compactor and rewards the first thing you see today is to you know slight generalizations of this framework to state action rewards and to find the horizon MVPs that make it a little bit easier for you to model certain types of problems certain types of robots or certain types of factory automation problems will be easier to model with these two small generalizations so talk about those first and then second we'll talk about linear dynamical systems last Wednesday you saw a fitted value iteration which was a way to solve for an MDP even when the state space may be infinite even when the state space is several numbers was RN so it's an infinite list of states or contingency other states we use fitted value iteration in which we're to use a functional approximator right like linear regression to try to approximate the value function there's one very important special case of an MDP where even if the state space is infinite of continuous real numbers does that well there's one important special case we can still compute the value function exactly without needing to use you know like a linear function approximate or to use something like linear regression in the inner loop a fitted value iteration and so you also see that today and when you can take a robot or some factory automation tools or whatever problem and model within this framework it turns out to be incredibly efficient because you can fit a continuous for the value function as a function of the states without needing to approximate you can just compute the exact value function even though the state space is continuous so this is a framework that doesn't apply to all problems but when it does apply is incredibly convenient gruffly efficient so you see that in a second half of today oh yes a 1:1 tactical oh two two tactical things um let's see from the questions that we're getting from students um since they're asking us oh how is grading and CSU's you know and whatever I did well and does you know didn't do so on that um for people taking a class pass/fail c- or better as a passing great this is quite I think there's a standard at Stanford and I think sisters mignon has historically been one of the heavy workload classes we know that people taking sis you know I yeah I see a few has nothing people King sisters end up you know putting a lot of work on this class maybe frankly more than average for even Stanford courses and so we've usually been quite nice with respect to party and acknowledge that so I think yeah just for what as well so don't don't don't sweat too much do work hard for the finer projects don't don't sweat too much um oh and on Wednesday after Claus had a funny question after I talked about the fitted value iteration question the Sun came out to me and said hey Andrew um you know this algorithm you you just told us does it actually work it doesn't actually work on the Tongass helicopter and the answer is yes the algorithms are teaching you know if you do fits evaluation as you learned last week it will work on flying an autonomous helicopter at low speed so your fly very high speeds very dynamic maneuvers crazy bang flipping upside down you need a bit more than that but for flying a helicopter at low speeds the the exact algorithm that you learned last Wednesday as well as any of the algorithms you learned today including them lqr you know if you actually ever need to find autonomous helicopter forever all these albums were actually work decently well work quite well for flying helicopter at low speeds maybe not at very very high speeds and a crazy dynamic maneuvers but those speeds these algorithms pretty much as I'm presenting them won't work so okay so the first generalization to the MDP framework that I want to describe is state action rewards and so so far we've had the rewards be a function mapping from the states to the set of real numbers and we'll say action rewards this is a slight modification to the MDP formalism where now the reward function R as a function mapping from States and actions to D rewards and so you know in an MDP you stop the mistake s0 you take an action a zero then based on value gets s1 take an action a1 to state s to get to state s to take an action a 2 and so on and with a state action rewards the total payoff there's written like this and this is this this allows you to model that different actions may have different costs for example in the little robot wandering around the maze example maybe it's more costly for the robot to move than to stay still and so if you have an action for the robot to stay still the reward can be you know zero for staying slow and a slight negative reward for moving because you're burning or because because you're using electricity and so in that case Velma's equations becomes this v-star equals where now you still break down the value of a state as a sum of the immediate reward plus the you know expected future rewards but now the immediate or what you get depends on the action that you take in the current state right so this is a and so this is Bellman's equations and if and notice that previously you know we had the max kind of over here but now you need to choose the option a that maximizes you immediate reward plus your discounted future rewards which is why the max kind of moved right if your local equation you look at this equation I guess the Mac set to move outside because now the immediate reward you get depends on the action you choose at this step in time as well this models that different actions may have different costs yeah oh yes yes yes just max applies to the entire expression right yeah let's see so in this formulation ever wasn't deterministic based on the state and action yes that is correct so in this formulation the reward depends on the current state and the current action but all on the next date you get two okay oh and by the way there are multiple variations of formulations of MVPs but this is some one convenient one I guess the model that different costs and I think and and action and you're finding a helicopter a common formulation of this would be to say that yanking aggressively on the control statements should be assigned a higher cost because yanking the control stick aggressively causes a helicopter to jerk around more and so maybe you want to penalize that by setting reward function that you know penalizes very aggressive maneuvers so these are ways that this gives you the as a problem designer sort of more flexibility right and then and then finally so I'm gonna just write this on top in this formulation the optimal action so right so in order to compute the value function you can still use value iteration right which is snow you know V of s just updated as basically the right-hand side from pellman's equations so now the iteration works just fine for the state action reward formulation as well and if you apply value iteration until the convergence of esau then the optimal action is just the opposite right so soap I saw is just the odd max of this thing right now when you're given state you want to choose the action that maximizes your media reward plus your expected future rewards okay so I think just maybe another example if you want to use an MDP to planner shortest route for robot say drive from here in Stanford to drive up to San Francisco right then if it costs different amounts to drive on different Road segments because they're traffic or because of the speed limit on different roads then this allows you to say that while driving this distance on this road cost this much in terms of fuel consumption or in terms of time and so on so or in factory maintenance if you send in a team to maintain the machine that has a certain cost versus if you do nothing that has a different cost but then the machine breaks down it has yet another cost evaluations okay so that's the first generation the second generation is the finite horizon MDP and in a final horizon MDP we're going to replace the discount factor gamma with a horizon time she and and we'll just forget about the discount factor and in the final horizon MDP the MDP will run for a finite number of t step so you stopping to state a zero take an action a zero get to s1 take action a one get to state st take an action a T at time step T and in the world ends and it we're done right and so the payoff is this finite sum and and kinda it's just a full stop at the end of that um you can also apply discounting but usually when you have a finite horizon MDP maybe there's no need to apply discounting and so this model is a problem where there are you know T time steps and then the world ends after that right or what world end sounds a bit dire but you know if you find an airplane or if I hold copter and you know you only have fuel you know for 30 minutes right RC helicopter whatsoever have 20 30 minutes of fuel then you know that you're gonna run this thing for 30 minutes and then you're done and so the goal is to accumulate as many rewards as possible up until you you know run out of fuel and then you have laughs right so that be example of a finer horizon MDP now and and and ago is to maximize this payoff or the expected payoff over these tea time steps okay now one interesting property of a finite horizon of a fine horizon MDP is that the action you take may depend on what time it is on the clock right so there's a clock marching from you know x at 0 to x at t whereupon right the world ends the way whereupon that's all the rewards the MDP is trying to collect and one interesting effect of this is that this pendulum right is that the optimal action may depend on what what the time is on the clock so let's say your robot is running around this maze and there's a small plus one reward here and much larger +10 reward there and let's say your robots is here right then the optimal action for whether you go left or go right will depend on how much time you have left on the clock if you have only you know two or three times as left on the clock it's better to just rush and get the plus one but we still have you know 10 20 ticks left on the clock then you should just go and get the plus tender one and so in this example pi star of s it's not well-defined because well the the optimal action to take when you robot is here in this station you go left watch it you're right it actually depends on what time it is on the clock and so PI star in this example should be written instead PI star subscript T of s because the auto action depends on what time T it is the technical term for this is that this is a non-stationary non stationary policy and non stationary means it depends on the time actually changes over time right whereas in contrast up until now we've been saying you know PI star of s is the octal policy before we before this formalism right which is at PI star of s and that's was a stationary policy and stationary means or does not change over time okay so one one one thing that dumb I didn't quite prove but that was implicit was that the optimal action you take in the original formulation is the same action right no matter what time it is in the MDP so in the original formulation that you saw last week the octal policy was stationary meaning that the optimal policy is the same policy no matter what time it is it doesn't change over time whereas in fine horizon MDP setting the Austral policy you know the also action changes over time and so this is a non stationary policy so say XI versus on stage she just means does it change over time or does it not change over time okay and so if you're using a non stationary policy anyway you can also build an MDP with non stationary the transition probabilities one on stage three rewards actually so maybe here's an example um let's say you're driving from campus from Palo Alto to San Francisco and we know that rush R is that what like 5 p.m. or 6 p.m. or something right and maybe maybe the weather forecast even says it's gonna rain at 6 p.m. or something right but so you know that the dynamics of how you drive your car from here at the San Francisco will change over time as in the time it takes you know to drive on a certain segment of the road is a function of time and if you want to build an MDP to solve the best way to drive from here in San Francisco say then the state transitions so SC plus one is drawn from state transition probabilities indexed by the state at time T and the action at time T and if these state transition probabilities change over time then if you index it by the time T this would be an example of a non-stationary of a non stationary state transition probabilities okay or alternatively if you want non stationary rewards then you can have a superscript T of si is the reward you get for taking a certain action for being at a certain state at a certain time okay so all of these are different variations of MDPs and so maybe just a few examples of when you will want a final horizon MVP or use a non stationary state transitions so let's see if you are flying an airplane right for some airplanes something like for commercial very large commercial airplanes sometimes over a third of the weight of the airplane comes from the field right so actually if you take a large commercial airplane you know when you take off from SFO and you fly - oh no way you guys prefer to fly - I applied to London or something it's very direct flex appears in London by the time the plane lands and get much lighter airplane than when you took off because maybe sometimes maybe like a third of the weight disappear you know because of burning fuel and so then the dynamics there how an airplane feels between takeoff and landing is actually different because the weight is dramatically different and so this would be one example of where the state transition priorities changes and a pretty predictable way right or oh right already mentioned weather forecasts right where weather forecasts of traffic for cars would be driving here or drive yeah if you're driving over different types of terrain over time you know that's gonna rain tomorrow you're gonna know it's gonna rain tonight and the ground will turn muddy you know then all the traffic would turn bad and then on the industrial automation um I'll see how friends work on industrial automation and I think that may be one example if you run a factory 24 hours a day then the cost of labor you know getting people to come into the factory to do some work at noon is actually easier right and less costly than getting someone to show up in the factory to do some work at 3:00 a.m. right and so depending on really labor availability over time the cost of taking different actions and the cost of and and the likelihood of transitioning to different stations and priorities can vary over the 24 hour clock as well right so these are other examples of when you can have a non safety policy and non safety state transitions okay now um let's talk about how you would actually solve for a fine horizon MDP and I think for the sake of simplicity for the most part I'm going to not bother with non stations transition rewards so for the most part just focus on for the most part it's gonna forget about you know the fact that this could be beer being I mentioned it briefly but I want to focus on the finer horizon aspect so so let me define the autovalue function so this is the also value function for time T for starting a new state as so this is the expected total payoff starting in state s at time T and if you execute you know the best possible policy so now the optimal value function depends on what time it is because if you look at that example with the +1 reward on the left and the +10 reward on the right depending on how much time you have left on the clock the amount of rewards you can accumulate can be quite different if you have more time yet more than you know you can more time to get to the past and reward indeed and the +1 and +10 rewards that I drew example that you just now and so um in this example value iteration becomes the following it actually becomes a dynamic programming algorithm what you see in a second ok which is that all right which is that the star of T of S is equal to max' over a R of the s a plus and actually this is a question for you so this does this one missing thing here right so what's saying that the optimal value you can get when you start up in state as at time T is the max over all actions of the immediate reward plus sum of s prime state transmitter is s prime times V star of s prime and then what should go in that box okay cool awesome great right and then PI star of s is just you know off max of 80 right of the same thing of this whole expression up on top and so this formula defines VT as a function of V T plus 1 so this is like oh this is like the iterative step right given meet engine compute V now I'm given V now you can abbreviate given behavior music v7 and so to start this off there's just one last thing we need to define which is the capital T at the finite step at the final step when the clocks about to run out all you get to do is choose the action a that maximizes the immediate reward and then and then and then there's no sum after that right so if you start off at state as at the final time step T then you get to take an action and you get immediate reward and then there is no next day because the world just ends right after that step which is why the auto value at time T is just max over a at the immediate reward because what happens after that doesn't matter okay so this is a dynamic programming algorithm in which this algorithm does step on top defines you allows you to compute V saw of T and then the inductive step or the n plus 1 step I guess is if you then having computed V Star of T for every state s right so you know so you compute this for every state that's having done this you can then compute V star t minus 1 using this inductive step then it's not t minus 2 and so on down to V star of 0 so you compute this for every state and then based on these you can compute no sorry it's PI star of T right compute the auto policy the non stationary policy for every states as function of both the state ok and and I think again I don't want to draw on this but if you want to work with non stationary state transition probabilities or non stationary rewards then this algorithm hardly changes in that you can just add you know if your rewards the state transceiver is at index by time as well then this is just a very small modification to this algorithm and it turns out that once you're using a finite horizon MDP making the rewards and state transmitters non stationary it's just a small tweak right so okay okay this one Oh an on station so in the end you get a policy PI star subscript T of s I'm sorry oke this one oh this one OSE sure yes Oh a price on this is a non-stationary policy yes so that's Island yeah yeah so this the the auto policy will be an on station policy yes I think yes I think I was using PI started not not to denote that it has to be a fictional target yes that's an awesome awesome thank you right if you think big teeth and Finity can just become the usual value iteration so the everything so the two things with that so the two frameworks are closely related right you can consideration ship between the valuation um one problem with taking the strain were too big t to infinity is that the values become unbounded right as in yeah well and that's actually one of the reasons why we use a discount factor when you have an infinite horizon MVP when their interviews goes on forever one of the things that discount factor does is it make sure that the value function doesn't draw without bound right and in fact you know if the rewards are bounded by on right by some R max then when you use discounting then V you know is bounded by I guess R max over 1 minus gab there's someone with dramatic sequence Oh and so but but when you find her as entropy because you only add up tira was it it can't get bigger than T times R max oh let me think so I think they're dumb boy so I think you know what you find is that um let's see actually let me just draw a one Dedra just to make life simpler right so let's see there's a plus one reward there the plus one reward there if you look at the optimal value function depending on what time it is if you have two times and that's let's say the dynamics are deterministic right so there's no noise then if you're two time steps left then I guess V star would be you know ten ten ten one one one zero zero right and so depending on where you are I guess if you're yeah actually in fact I guess if you're here there's nothing you can do right this kind of gets either reward in time but depending on whether you're here or here or here at the auto action will will change you to computer this pi star this make sense okay yeah maybe do do cars here there if this yeah if you actually built a little you know grid simulator and use these equations to compute PI sine V star you will see that the Ottoman policy when you have lots of time will be this wherever you are go for the ten rewards but when the clock runs down if then the also policy will end up being a mix-up go left and go right all right cool all right so so the last thing I want to share of you today is the new quadratic regulation and as a saying at the start um lqr applies only in the relatively small set of problems but whenever the plies this is a great out room and I just you know use it whenever right it seems usable to apply because it is very efficient and sometimes gives very good control policies and let's see and so lqr applies in the following setting so let's see in order to specify an MDP we need to specify the state reactions the state transition or movies I'm going to use to find a horizon formulation so capital T and rewards this this also works with the discounted MVP formalism but this would be a little bit easier a little bit more convenient to develop we've defined a horizon setting so let me just use that today and lqr applies under a specific set of circumstances which is that this set of states there's an RN set of actions is in Rd and so to specify the state transition probabilities we need to tell you what's the distribution of the Knicks a given the previous state so to specify the state transition probabilities I'm going to say that the way st plus one evolves is going to be as a linear function some matrix a times s T plus some matrix B times a T plus some noise and sorry there's a little bit of notation over loading key and sorry about that a is both the set of actions as was this matrix a right so there's two separate things but same symbol I think I think that the field of all the ideas of lqr came from traditional control from what from mom I guess from EE a mechanical engineering a lot of their ideas of reinforcement learning came from computer science so these two literature's kind of evolved and then when the literature's merge you end up with clashing notation so CS people use a to denote the set of actions and the stuff you know mechanical engineering and EE people use a to denote this matrix and when we merge these two literature's the notation ends up being overloaded okay oh and then um it turns out one thing we'll see later is that this noise term it we'll see you later is actually not super important but for now let's just assume that the noise term is distributed Gaussian with some mean zero and some covariance Sigma subscript W okay but we'll see later that the noise will be less important than you think right and so this matrix a is going to you are n by n and this matrix B is going to be R and body D where N and D are respectively the dimensions of the state space in the dimension of the action space so for driving a car for example we saw last time that maybe the state space is six dimensional so if you're driving a car I mean the state space is XY theta x dot y dot beta dot and the action space is you know steering control so maybe a is two-dimensional right acceleration and steering okay so let's see so to specify an MDP we need to specify this five tuple right so we specify three of the elements the fourth one T is just some number right so that's easy and then the final assumption we need to apply out your arm is that the reward function has the following form the reward is negative of s transpose u s plus a transpose VA where you yes and by n V is d by D and UV are a positive semi-definite okay so these are matrices a and bringing the zeros or pauses immunity so the fact that U and V are positive semi-definite that implies that s T u s is greater than equal to 0 and s transpose u s sorry a transpose VA is also greater than or equal to 0 so here's one example if you want to fly an autonomous helicopter and if you want you know the state the state vector to be close to zero so the state vector captures a position orientation velocity angular velocity if you want a helicopter to just hover in place then maybe you want the state to be you know regulated or to be controlled near some zero position and so if you choose u equals the identity matrix and V also equal to the identity matrix this will be different dimensions right this would be an N by n identity matrix as you add D by D domain and D matrix then R of sa ends up equal to negative normal for s squared plus normal a squared and so this allows you to this allows you to specify reward function that penalizes you know with a quadratic cost function the state deviating from zero or if you want the actions deviating from zero that's penalizing very large jerky motions on the control sticks or we said V equal to zero then the second term goes away okay so these are some of the cost functions you can specify in terms of a quadratic cost function okay now again you know just so that you can see the generalization if you want non-stationary dynamics this model is quite simple to change where you can say the matrices a and B depends on the time T you can also say these mean you know the matrices u and V depends on the time T so if you have non stationary state transition probabilities or non stationary cost function that's how you would modify this but I won't I won't use this generalization for today now so the two key assumptions of the lqr framework are that first the state transition dynamics the way your sales change is as a linear function of the previous state and action plus some noise and second that the reward function is a quadratic cost function right so these are the two key assumptions and so first you know we're worth where do you get the matrices a and B one thing that we talked about on Wednesday already was so again this will actually work if you're trying to apply lqr to find the homicidal confidence this will work for a helicopter flying at low speeds which is if you find a helicopter around you know start to some state as zero take an action a zero get to state s1 do this until you get to st right and then this was the first trial and then you do this M times so we talked about this on Wednesday so fly the helicopter through M trajectory sort of T time steps each and then we know that we want st plus 1 is approximately ast plus PA t and so you can minimize right so we want the left in the right hand side to be close each other so you could you know minimize the squared difference between the left hand side and the right hand side in a procedure a lot like linear regression in order to fit matrices a and B so if you actually fly a helicopter around and collect this type of data and fit this model to it this will work you know this is actually a pretty reasonable model for the dynamics of a helicopter and those speeds okay so this is one way to do so let's see method one is to learn it right a second method is to linearize a nonlinear model so um let me just describe the ideas at a high level which is let's say that and I think for this it might be useful to think of the inverted pendulum right so that was uh you know say imagine you've uh yeah inverted pendulum that was that Rangi of a pole you're trying to your long vertical pole you try to keep it balanced so for an inverted pendulum like this if you download an open source physics simulator or if you have a friend you know from from the physics degree help you derived in Newtonian mechanics equations for this let's see I actually tried to work through the physics equations or inverted pendulum ones is pretty complicated but you might have a function that tells you that if the state is a certain position orientation of the pole lost the angular velocity and you was it apply a certain acceleration the options are certainly left for settling right then you know one tenth of a second later the state will get to this right so you you know your physics friend can help you derive this equation and and and then maybe plus noise right well no just ignore the noise for now and so what you have is a function right the maps from the state xx dot theta theta dot that's a position of the cart and the angle of the pole and the velocities and angular velocities they're maps from the current state at time t excuse me comma 80 right maps from the I guess current state vector to the next state vector as a function they comments a nuclear reaction okay so um yes what Vineyards asian means and i'm going to use a 1d example so because i can only draw on a flat board right i can't do because because of the two dimensional nature of the white board I'm just gonna use a let's let's suppose that do you have SC plus one equals f of s T and let me just forget let me just ignore the action for now so I have one input and one output so I can draw this more easy on the white board so if you have some function like this so the x-axis is s T and Y axis is SC plus one and this is the function f right well plug in back the action later what the linearization process does is you pick a point I'm gonna call this point st over bar and we're going to you know take the derivative of F and finish straight lines are you annoyed really not very well take the tangent straight line at this point s T bar and what and we're going to use this straight line and we're going to use the green straight line to approximate the function okay and so if you look at the equation for the green straight line the green straight line is a function mapping from st 2sc plus 1 and s bar is the point around which your linearizing the function so s bar is a constant and this function is actually defined by SC plus 1 is approximately the derivative of the function at s Bar times st minus s bar plus s Bar T ok and so so s bar T is a constant and this equation expresses s T plus 1 as a linear function of s T so think of it as volunteers a fixed number right it doesn't vary so given some fixed s bar this equation here this is actually the equation of the green straight line which is it says you know if you've used the green straight line to approximate the function f this tells you what is s T plus 1 as a function of s T and this is a linear and affine relationship between s T plus 1 ok so that's how you will linearize a function and and in a more general case where and in a more general case where SC plus 1 is actually a function of you know putting this back again right both st and a t the formula becomes well i'll write out the form in a second effect on in this example s bar T is usually chosen to be a typical value for s and so in particular if you expect your helicopter to be doing a pretty good job hovering near the stage 0 then it'd be pretty reasonable to choose s Bar T to be the vector of all zeros because if you look at how good is the green line as an approximation of the blue line right in a small region like this you know the green line is actually pretty close to the blue line and so if you choose as bar to be the place where you expect your helicopter to spend most of its time then the green line is not too bad an approximation so the true function to the physics OC you know if you expect for the inverted pendulum if you expect that your inverted pendulum will spend most of its time but the pole upright and the velocity not too large then you choose s bar to be maybe the zero vector and so long as your pendulum your inverted pendulum is spending most of its time kind of you know close to the zero state then the green lines not too bad an approximation for the blue line right so this is an approximation but you try to choose them because I mean in in this little region it's actually not that bad an approximation is only when you go really far away right that there's a huge gap between the linear approximation and the true function okay and so in the more general case where f is a function about the state and the action then what you have to do is the input now becomes st comma 80 because f maps from st comma 82 st plus 1 and then instead of choosing as bhatia choosing as bar t comma a bar T which is a typical state in action around which you linearize the function let me just write down the formula for that in which you would say if you linearize around the points given by s party a body under the typical values then the form that you have is st plus 1 is given by f of s bar ta bar t plus the gradient with respect to s so this is the generalization of the 1d function we measure just now we wrote down just now which says that you know the next day there's approximately this point around you which you linearize plus the gradient respect to s times how much the state differs from the linearization point plus the gradient respect the actions times how much the action vary from a bar and this kind of generalizes that equation you wrote so so this equation expresses as T plus 1 as a linear function or technically an affine function of the previous state and previous action right well some matrices in between and from this you know after some algebraic mundane you can re-express this as st plus 1 equals a st plus ba t and and just that there is just one other little detail which is you might need to redefine st to add an intercept term right and because this is a affine function with a intercept term rather than the linear function but so from this formula you know with a little bit of algebraic Montaigne you should really figure out whether the matrices a and B but you might need to add an instep term to the s but is just an affine function you can rewrite in terms of nature's the same ok um alright so I hope that makes sense right that this thing this linearization thing expresses St plus 1 as a linear function of s tnat right this is just a linear system the way st plus one varies you know it's just a matrix i'm st some matrix times 80 and that's why was someone jean you can get into this form there for some matrix a ok but because there are some constants floating around as well like this you might need an extra interceptor to multiply into a to give you that extra constant but where we are we now have that for these MVPs either by learning a linear model with the matrices a and B or by taking a nonlinear model and linearizing it like you just saw you can model hopefully more than MVP as a linear dynamical system meaning this you know st plus 1 is this linear function or the previous state in action as well as hopefully with a quadratic reward function or they really write in the form that we saw just now so let me just summarize the problem we want to solve AST sorry st plus 1 equals a st plus ba t + WT so this is a noise term and then our s a equals negative s transpose you have a transpose and this is a fine horizon MDP so the total payoff is a R of s 0 ok so let's take other dynamic programming algorithm for this the remarkable problem that the remarkable property of lqr and what makes this so useful is that if you are willing to model your MVP using those sets of equations then the value function is a quadratic function right and so let me show you what I mean and so if your if your model if your MVP can be modeled as this type of linear dynamical system with a quadratic cost function then it turns out that V Star is a quadratic function and so you can compute V star exactly right so let me show you what I mean we're going to develop a dynamic programming algorithm to compute the optimal value function V star similar to what we did you know below earlier today with the final horizon MDP with a finite set of states this starts with the final time step and it will work backwards so V star T of s T is equal to max' over 80 of R of s T 80 this is max over 80 of negative right but this is always greater than or equal to 0 because V is positive semi-definite and so the optimal action is actually they just choose the action zero and so the max over this is equal to negative st transpose u st right because uh because v is a positive semi definite matrix this thing is always greater than zero and then and so this tells us also that pi star the final action is the arc max so the auto action is to choose you know the vector of zero actions at the last time step okay so this is the base case for the dynamic programming step of value duration where the optimal value at the last time step is just choose the action that maximizes the immediate reward which means maximize this right and this is maximized by choosing the action zero at the last time step okay no now the key step to the dynamic program implementation is the following which is um suppose that V star T plus 1 st plus 1 is equal to a quadratic function so indeed yes it's true that this term is also driven to zero without the minus sign right what about the minus I that term is positive and so but you only get to maximize respect to eighty right so so the best you could do for this term the cell is zero thank you all right now for the inductive case we want to go from VT plus one we saw three plus one to computing beats 13 and the key observation that makes lqr work is let's suppose V star T plus one the auto value function at the next time step let's suppose it's a quadratic function so particularly suppose V star T plus one is you know this quadratic function parameterize by some matrix this capital Phi T plus one which is an enviable n matrix and some constant offsets I which is a real number um what we'll be able to show is that if you do one step of dynamic programming if this is true for V star T plus 1 that V T after one step as you go from V salty cylinder V T that the optimum value function VT is also going to be a quadratic function with a very similar form right look I guess T plus one replaced by T alright and so in the dynamic programming step we are going to update VT s T equals max of eighty R of s C comma 80 plus and then you know I think you remember when I previously writers previously we had some over s prime Oh actually St plus 1 I guess the s T 18 SC plus one B star c plus 1 St plus 1 so that's what we had previously when we had a discrete state space and was summing over it but now that we have a continuous state space this formula becomes expected value with respect to s T plus 1 drawn from the state transition probabilities B star pieces 1 so the optimal value when the clock is a time T is choose the action a the maximizes the immediate reward plus the expected value of you know your future rewards when the clock has now ticked from time T to time T plus 1 in your in state as t plus 1 at time t plus 1 so let's see so this is a pretty beefy piece of algebra to do I think I feel like showing this full result is oh no it's like at the love of complexity of a you know typical CS 229 homework problem which is quite hard but let me just show the outline of how you do this derivation and why you know why this inductive step works right but I think you but but if you want you could work through the algebra details yourself at home which is that so be sautee of s T is equal to max' over 80 of the immediate reward right so that's the immediate reward and then plus the expected value with respect to s T plus one is drawn from a Gaussian would mean a st plus ba t and covariance Sigma W so remember s T plus 1 is equal to ast plus ba t + WT where WT is Gaussian with mean 0 and covariance Sigma W right so if you choose an action a T then this is the distribution of the next state at time T plus 1 and then expected value of this quadratic term because this quadratic term here I kind of in the inductive case was what we showed was V star for the for the next time step right so it turns out that let's see so this is a quadratic function and this expectation is the expected value of a quadratic function with respect to s drawn from the Gaussian right well the certain mean a certain variance so it turns out that the expected value of this thing well this whole thing that I just circled this thing simplifies into a big quadratic function of the action 80 and then and so in order to you know derive the odd max of the derive V stylus you would derive this big quadratic function take derivatives with respect to a tea set to 0 right and solve for a tea and if you go through all that algebra then you actually then you end up with the formula for a tea as follows okay and I'm gonna use I'm gonna I'm gonna take that big matrix and don't know that Lt okay oh and so this shows also that PI star at time T of s T is equal to L T times s T so one the takeaway from this is that under the assumptions we have rather than the dynamical systems a quadratic cost function the octo action there's a linear function of the state s T right and this is not a claim that is made through function approximation what did I'm not saying that you can fit a straight line to the Osmo action and if you fit a straight line that you get this linear function right that's not what we're saying we're saying that of all the functions any way I could possibly come up with in the world linear or nonlinear the best function the best option is linear so there is no approximation here right so it's just that you know it's just a fact that if you have linear dynamical system the best possible action at any state is going to be a linear function of that state right so there's notice that we have an approximate or anything right let me let me let me write this here and then the other step is that if you take the autumn action and plug it into the definition of V star then by simplifying Michigan is quite a lot of algebra but after simplifying you end up with this equation where again I just write out the formulas okay all right so to summarize the whole algorithm right this let's put everything together oh and so sorry and so what these two equations do is they allow you to go from B star t plus 1 which is defined in terms of 5t plus 1 and side t plus 1 and it allows you to recursively go back to figure out what is V star T using these two equations right so Phi T depends on Phi T plus 1 sy T depends on Phi T plus 1 inside p plus 1 and this Sigma W this is the covariance of WT right there's the Sigma subscript of you this is not a summation over W is a Sigma matrix subscript 2 by W that was the covariance matrix for the noise terms we were adding on every step you know linear dynamical system okay and there's a trace operator some of the diagonals ok so just to summarize here's the algorithm you will initialize Phi T to be equal to negative u and side T equals 0 and so you know that's just taking this equation and map here there right so the final time step that those two oh sorry should be capital T so that those two equations to fire inside it defines V star of capital T and then you would you know recur circle let me calculate Phi T and sy t using Phi T plus 1 si T plus 1 so you go from you know but t equals t minus 1 t minus 2 and so on and go backward calm down from right t minus 1 t minus 2 and so on down to 0 calculate L T as above right then LT was a formula I guess we had over there saying how the optimal action is a function of the current state depending on the a and B and Phi and then finally PI star of s T equals L T of s T ok and this algorithm the remarkable thing one really cool thing about lqr is that there is no approximation anywhere right you you might need to make some approximation steps in order to approximate a helicopter as a linear dynamical system by you know fitting matrices a and B the data or by taking a nonlinear thing and linearizing it and you might need to just restrict constrict you knee restrict your choice that possibly reward functions reward function is called driving but once you've made those assumptions none of this is approximately everything is exactly yes that's right yep yeah so the approximation step neither are getting your MDP into the form of a linear dynamical system will record you have a reward so that is approximate but once you specify the MTP like that all of these calculations were exactly right so so we're not approximating the value function of a quadratic function is that the value function is a quadratic function and you're computing it exactly and the also policy is a linear function and you're just computing computing that exactly okay I want to mention before wrap i want to mention one one unusual fun facts about lqr and this is very specific to young and and and it's convenient but but it let me say well the fact is then just be careful that this doesn't give it a wrong intuition just it does imply to anything other than lqr which is that if you look at where so first if you look at the formula for L all right even though can the formula LT you need to compute I mean the you know you the go up doing all this work is to find the also policy right so you want to find LT so you can compute the auto policy you notice that LT this depends on Phi but not sigh right so you and and maybe it's kinda make sense you're going to when you take an action you get to some new stage and your future payoffs it's a quadratic function plus a constant it doesn't matter what that constant is right and so in order to compute the optimal action and all compute e you need to you need to know Phi or actually Phi T plus one but you don't need to know what is sigh t plus one know if you look at the way we do the dynamic programming the backwards recursion one of you can ferment a piece of code that doesn't bother to compute sigh right so these are the two equations you use update fire inside but whatever you know let's see you delete this line of code just don't bother the computer and just don't bother compute that and don't bother to compute that so you notice that Phi depends on Phi T plus one but it doesn't depend on side and so you can implement the whole thing and compute the octo policy and completely on also actions without ever computing sy right now the funny thing about this is that the only place that Sigma W appears is that it affects only citee right so you know if we do want to just cross out in our range and just don't bother the computes I T then the whole algorithm doesn't even use Sigma W so one very interesting property of the lqr you know if this formalism is that the optimal policy does not depend on Sigma W right and I think maybe this is a cell V star depends on Sigma W because if the noise is very large if they're huge does it wind blowing helicopter all over the place then the value would be worse but PI star and LT do not depend I'm sigma/w okay um so this is a property that's very specific to lqr don't don't don't over generalize it to other reinforcement learning algorithms but this I think the intuition to take from this is first if you're actually applying the system you know don't bother to don't don't like so don't don't try too hard to estimate Sigma W because you know actually you don't actually need to use it which is why when we're fitting a linear model I didn't talk too much about how you actually estimate Sigma W because in the lpr system it literally doesn't matter in a mathematical sense in terms of what is the optimal policy you compute and in a second there may be slightly useful intuition to take away from this is that for a lot of MVPs if you're building a robot you know remember to add some noise to your system but the exact noise you add doesn't matter as much as one might think so what I've seen and then working a lot of robots a lot of MVPs is you know do add some noise your system and make sure your learning algorithm Israel busting noise and the form of the noise you add it does matter I don't say it doesn't matter at all I mean now Kira doesn't matter all for other MGP as it does matter but I think the fact that you remember to add some noise is often in practice more important than the exact details of you know is the noise 10% higher distant noise stem cell or if the noise is a hundred percent higher lower that will often make the big difference but but but when I'm you know training all the were helicopter or something the noise is something that you know I pay a little bit attention to but I pay much more attention to making sure that the matrices a and B are accurate and then a little bit sloppiness in the actress even noise ball doing something that an MTP could probably survive then your policy for survive okay let's take one last question Oh V Bo OSE sorry yes see my nose V that was a this is a beat yes okay cool thanks everyone let's break and I will see you for the final lecture on Wednesday thanks everyone