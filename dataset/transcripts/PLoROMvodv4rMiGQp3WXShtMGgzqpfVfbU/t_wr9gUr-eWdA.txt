hello everyone so my name is Rafael Townsend I'm one of the head TA for this class this week Andrews travelling in my advisor is still dealing with medical issues so I'm gonna be giving today's lecture you heard from my wonderful Co head ta an on a couple weeks ago and so today we're gonna be going over decision trees in various ensemble methods so these might seem a bit like despair topics at first but really decision trees are sort of a classical example model class to use with various ensemble methods we're gonna get into a little bit why in a bit but just to give you guys an overview what the outlines can be refreshing to over decision trees then we're gonna go over general on something methods and then go specifically into bagging random forests and boosting okay so let's get started so first let's cover some decision trees okay so last week Andrew was covering SVM which are sort of one of the classical linear models and sort of brought to a close a lot of discussion of those linear models and so today we're gonna be getting to decision trees which is really one of our first examples of a non-linear model and so to motivate these guys let me give you guys an example okay so I'm Canadian I really like the ski so I'm gonna motivate it using that so pretend you have a classifier that given a time and a location tells you whether or not you can ski if it's a binary classifier saying yes or no and so you have you can imagine a graph like this and on the x-axis we're gonna have time in months so counting from the start so starting at 1 for January to 12 for December and then on the y-axis we're gonna use latitude in degrees okay and so for those of you who might have forgotten what latitude is it's basically at positive 90 degrees you're at the North Pole at negative 90 degrees you're at the South Pole so positive 90 negative zero being the equator it's sort of your location along the north-south axis okay so given this if you might recall the winter in the northern hemisphere generally happens in the early months of the year so you might see that you can ski in these early months over here and have some positive data points and then in the later months right and then in the middle you can't really ski verses in the southern hemisphere it's basically flipped where you can not ski in the early months you can ski during the name july/august time period and then you can't ski in the later months and then the equator in general is just not great for skiing there's a reason I don't live there and so you just have a bunch of negatives here okay and so when you look at a data set like this you've sort of got these separate regions that you're looking at right and you sort of want to isolate out those regions of positive examples if you had a linear classifier you'd sort of be hard-pressed to come up with any sort of decision boundary that would separate this reasonably now you could think okay maybe you have an SVM or something you come up with a kernel that could protect perhaps project this into a higher feature space that would make it linearly separable but it turns out that with decision trees you have a very natural way to do this so to sort of make clear exactly what we want to do with decision trees is we want to sort of partition the space into individual regions so we sort of want to isolate out things like positive examples for example and in general this problem is fairly intractable just coming up with the optimal regions but how we do it with decision trees is we do it in this basically greedy top-down recursive manner this would be recursive partitioning okay and so it's basically it's top down because we're starting with the overall region and we want to slowly partition it up okay and then it's greedy because at each step we want to pick the best partition possible okay so let's actually try and work out intuitively what a decision tree would do okay so what we do is we start with the overall space and the tree is basically going to play 20 questions with this space okay so like for example one question it might ask is if we have the data coming in like this is is the latitude greater than thirty degrees okay and that would involve sort of cutting the space like this for example okay and then we'd have a yes or a no and so starting from like the most general space now we have partitioned the overall space into two separate spaces using this this question okay and this is where the recursive part comes in now because now that you've sort of split this space into two you can then sort of treat each individual space as a new problem to ask a new question about and so for example now that you've asked this latitude greater than 30 question you could then ask something like month less than like March or something like that and that would give you a yes or no and what that works out to effectively is that now you've taken this upper space here and divided it up into these two separate regions like this and so you could imagine how through asking these recursive questions over and over again you could start splitting up the entire space into your individual regions like this okay and so to make this a little bit more formal what we're looking for is we're looking for sort of this split function okay so you can sort of define a region so you have a region and let's call that region R P in this case for our parent okay and we're looking for looking for a split s P such that you have an SP you can sort of write out this SP function as a function of J comma T okay where you so you were J is which feature number and T is the threshold you're using and so you can sort of write this out formally as sort of you're outputting a tuple where on the one hand you have a set X where you have the XJ the the J feature of X is less than the threshold and you have X's element of RP since we're only partitioning that parent region and then the second set is literally the same thing except it's just those that are greater than T and so we can refer to each one of these as r1 and r2 any questions so far now okay so we sort of define now how we would sort of do this we're trying to like greedily pick these picks that are partitioning our input space and the splits are sort of defined by which feature you're looking at and the threshold that you're applying to that feature a sort of a natural question to ask now is is how do you choose these splits all right and so I sort of give this intuitive explanation that really what you're trying to do is you're trying to isolate out the space of positives and negatives in this case and so what it's useful to define is a loss on a region okay so define your loss l on r and so for now let's define our loss as something fairly obvious is your miss classification loss it's how many examples in your region you get wrong and so assuming that you have given C classes total you can define P hat C to be the proportion of examples in our okay and so now that we've got this definition where we had this P hat C of telling us the proportion of examples that we've got in that case you can sort of define the loss of any region as loss let's call it in this classification it's just one - max over C of P hat C okay and so the reasoning behind this is basically you can say that for any region you've subdivided generally what you'll want to do is predict the most common class there which is just the maximum P hat C right and so then all the remaining probability just gets thrown onto miss classification errors okay and so then once we do this we want to basically pick now that we have a loss defined we want to pick a split that decreases the loss as much as possible so you recall I've defined this region our parent and then these two children regions r1 and r2 and you basically want to reduce that loss as much as possible so you want to basically minimize loss our parent - loss of r1 r2 and so this is sort of here parent loss this is your children loss okay and since you're picking and basically what you're minimizing over in some case is this J comma T that we defined over here since the split is really what is going to define our two children regions all right and what you'll notice is that the loss of the parent doesn't really matter in this case because that's already defined so really all you're trying to do is minimize this negative sum of losses of your children okay so let's move to this next board so I started to find this miss classification law so let's get a little bit into actually why miss classification loss isn't actually the right loss to use for this problem so okay and so for a simple example let's pretend so I've sort of drawn out a tree like this let's pretend that instead we have another set up here where we're coming into a decision node now at this point we have 900 positives and 100 negatives okay so this is sort of a Miss classification loss of 100 in this case because you'd predict the most common class and end up with 100 misclassified examples and so this would be your region RP right now all right and so then you can split it into these two other regions right say r1 and r2 and say that what you've achieved now is you have this 700 positive 100 negatives on this side versus 200 positives and zero negatives on this side okay now this seems like a pretty good split since you're getting out some more examples but what you can see is that if you just drew the same thing again right RP with 900 100 split split and say in this case instead you got 400 positives over here 100 negatives and 500 positives so most people would argue that this right decision boundary is better than the left one because you're basically isolating out even more positives in this case however if you're just looking at your Mis classification loss it turns out that on this left one here let's call this r1 r2 versus this right one let's call this r1 prime r2 prime okay so your loss of r1 plus r2 on this left case is just one hundred plus zero okay so just one hundred and then on the right side here it's actually still just the same alright and in fact if you look at the original loss of your parent it's also just a hundred right so you haven't really according to this lost metric changed anything at all and so that sort of brings up one problem with the Mis classification loss is that it's not really sensitive enough okay so like instead what we can do is we can define this cross-entropy loss okay which will define as L cross let me just write this out here and so really what you're doing is you're just summing over the classes and it's the probability the proportion of elements in that class times the log of the proportion in that class and how you can think of this is it's sort of a this concept that we borrow from information theory which is sort of like the number of bits you need to communicate to tell someone who already knows what the probabilities are what class you're looking at and so that sounds like a mouthful but really you can sort of think of it intuitively as if someone already knows the probabilities like say it's a hundred percent chance that it is of one class then you don't need to communicate anything to tell them exactly which classes because it's obvious that it's that one class versus if you have like a fairly even split then you need to communicate a lot more information to tell someone exactly what class you are in any questions so far yep the R 1 R 2 for the parent class for this case here yeah yeah so um for that case there so you see that I'll try and reach up there but so it's like say like RP was your start region right you could say it's the overall region right and then R 1 would be all the points above this latitude 30 line and our two would be all the points below the latitude 30 line yep yeah so the question is when you're trying to minimize this loss here is it the same as maximizing the the children loss and sir now let's see of maximizing the children loss and yeah it turns out it doesn't really matter which which way you put it it's just basically you're trying to either minimize the loss of the children or maximize the gain in information basically yeah let's see yeah you're right that should actually be a max let me fix that really quick because you start with your parent loss and then you're subtracting out your children's loss and so the amount left let's see the higher this loss is yeah so you really want to maximize this guy make sense everyone thanks for that okay so I've sort of given this late hand-wavy oh sure what's up so that would be log base two the question is for the cross-entropy loss is it log base 2 or log base C it's log base 2 okay yep or sorry I didn't quite hear that okay so the question is can what is the proportion that are correct versus incorrect for these two examples we've worked through here and so yeah basically what we're starting with is we're starting with we have nine hundred one hundred nine hundred faucets and one hundred negatives all right so you can imagine if you just stopped at this point right you would just classify everything that's positive right and so you get one hundred negatives incorrect that makes it because this is nine hundred positives and one hundred negatives so if you just stopped here and just tried to classify given this whole region or P you would end up getting 10% of your examples wrong right in this case we're sort of talking we're not talking about percentages we're talking about absolute number of examples that we've gotten wrong you can also definitely talk in terms of percentages instead and then down here once you've split it right now you've got these two sub regions and on this on this left one here you still have more positives than negatives right so you're still going to classify positive in this leaf all right and you're still gonna classify positive in this leaf too because they're both majority class or the positives are still the majority class there and in this case and you have zero negatives you're not gonna make any errors in your classification verses in this case you're still gonna make 100 errors and so what I'm saying is that at this level so if we just look above this line at our right you're making 100 mistakes and then below this line you're still making 100 mistakes so what I'm saying is that the loss in this case is not very informative so this so this P hat okay I'm being a little bit loose with the terminal with the notation here but the P hat in this case is a proportion okay but you can also easily basically it's like whether you're normalizing the whole thing or not okay so I've started given this a bit hand wavy explanation as to why miss classification loss versus cross-entropy loss might be better or worse we can actually get a fairly good intuition for why this is the case by looking at it from a sort of geometric perspective so pretend now that you have this this plot okay and what you're plotting here is pretend you have a binary classification problem okay so you have just is it positive class or negative class okay and so you can sort of represent say P hat as like the proportion of positives in your set okay and what you've got plotted up here is your loss okay for cross-entropy loss well your curve is gonna end up looking like is it's gonna end up looking like this strictly concave curve like this okay and what you can do is you can sort of look at where your children versus your parent would fall on this curve so say that you have two children okay you have one up here so the places call this lr1 and you have one down here lr2 okay and say that you have an equal number of examples in both r1 and r2 so they're equally weighted if you take when you're looking at the overall loss between the two right that's really just the average of the two so you can draw a line between these two and the midpoint turns out to be the average of your two losses so this is l r1 + l r2 divided by 2 that's what this guy's okay and what you can notice is that in fact the loss of the parent node is actually just this point projected upwards here so this would be your L R parent and this difference right here this difference is sort of your change in loss does this make sense any questions okay so we have this just to recap okay so we have say we have two children regions right and they have different probabilities of positive examples occurring all right they sort of would fall one would fall on this point on the curve and say the other one falls on this point on the curve then the average of the two loss is sort of falls on that midpoint between these two original losses and if you look at the parent it's really just halfway between on the x-axis and you can project upwards for that as well and you end up with the loss of our parent what's up okay so what we're looking at here is we're looking at the cross entropy law so you've got this function here this L cross entropy right and that's in terms of P hat C's right and in this case here we're just assuming that we have two classes okay and so what we're doing is we're just modifying the P hat see we're we're changing that on the x-axis and then we're looking at what the response of the overall loss function is on the y-axis and so what I just did here is for any this curve just represents for any P hat see what the cross entropy loss would look like okay and so we can come back to this for example right and if we look at this parent here right this guy has a 10% right it's sort of like P hat P hat for this guy is 0.1 it's 10% basically or or I guess no in this case would be 0.9 sorry and then vs. here in these two cases right your P hat in this case is one since you've got them all right all right and then in this case it's 0.8 all right and so you can sort of see since these are equal there's the same number of examples in both of these the P hat of the parent is just the average of the pH of the children okay and so that's how we can sort of take this LR parent this L R parent is just half way if we projected this down all right let me just erase this a little bit here if we projected this down like this we'd see that this is that this point here is the midpoint okay and but then when you're actually averaging the two losses after you've done the split then you can basically just you're just taking the average loss right you're just summing L R 1 plus L R 2 and if you're taking the average then you're dividing by 2 and what you can do is you can just draw the line and take the midpoint of this line instead yeah yeah exactly so yeah really any if there it's a good point the question was if you have an uneven split what would that look like on this curve right and so at this point I've been making the math easy by just saying there's an even split but really if there was a slightly uneven split you the average would just be any point along this line that you've drawn and as you can see the whole thing is strictly concave so any point along that line is going to lie below the original loss curve for the parent so you're basically as long as you're not picking the exact same points on the probability curve and not making any gain at all in your split you're gonna gain some amount of information through this split okay now this was the cross entropy loss right if instead we look at the miss classification loss over here let's draw this one instead well we can see in this case if you draw it is that it's in fact really this pyramid kind of shape we're just linear and then flips over once you start classifying the other side and if you did the same argument here where he had L R 1 and L R 2 and then you drew a line between them right that's basically just still the lost curve and so in this case like your midpoint would be the same point as your parent so your loss of our parent in this case would equal your loss of R 1 plus loss of R 2 divided by 2 all right and so in this case you can there's even though according to the cross-entropy formulation you do have a gain in information and intuitively we do see a gain in information over here for the misclassification law since it's not very sensitive if you end up with points on the same side of the curve then you actually don't see any sort of information game based on this kind of representation and so there's actually a couple I presented the cross entropy loss here there's also the Gini loss which is another one which people just write out as as the sum over your classes P hat C times 1 minus P hat C ok and it turns out that this curve also looks very similar to this original cross entropy curve and what you'll see is that actually most curves that are successful youth for decision splits look basically like this strictly concave function ok so that's what it covers a lot of the criteria we use for splits let's look at some extensions for a decision trees I'm gonna keep this guy okay so I so far I've been talking about decision trees for classification you could also imagine having decision trees for regression and people generally call these regression trees okay I'm so taking the ski example again let's pretend that instead of now predicting whether or not you can ski you're predicting the amount of snowfall you would expect in that area around that time and so like let's I'm just gonna say it's like inches of snowfall I guess or something per like day or something and just like maybe you have some values up here some high value because you're it's winter over there it's mostly zeros over here cuz you're summer and then you have some more high values over here and then you have zeros along the equator again zeros southern hemisphere over our winter like this and you can sort of see how you do just the exact same thing you still want to isolate out regions and sort of increase like the purity of those regions so you could still create like your trees like this split out like this for example and what you do when you get to one of your leaves is instead of just predicting a majority class what you can do is predict the mean of the values left so you're predicting predict Y hat wear well for RM so but then you have a region RM you're predicting Y hat of M which is the sum of all the indices in RM y I minus y hat M and you want the squared loss and then you skim sort of I guess in this case you want to normalize by the overall cardinality of RM or how many points you have and so in this case basically all you've done is you've switched your loss function or no sorry that's wrong this is actually I got a little bit ahead of myself this is actually just the mean value would just be this in this case right it's just you're summing all the values within your region so in this case seven nine eight ten and then just taking the average of that but so then what you do is what I was starting to write out there was actually really the the loss that you would use in this case right which is your squared loss okay so like we'll just call that l squared which in this case would be equal to Y minus y hat M squared over R M and that's what I started to write over there but in this case right you have your mean prediction and then your loss in this case is how far off your mean prediction is from the overall predictions in this case yep so that's a really good question the question was how do you actually search for you splits how do you actually solve the optimization problem of finding these splits and it turns out that you can actually basically brute force it very efficiently I'm going to get into sort of the details of how you do that shortly but it turns out that you can just go through everything fairly quickly I'll get into that I think that's in a couple sections from now any other questions okay so this is for regression trees right it turns out that another useful extension that that you don't really get for other learning algorithms is that you can also deal with categorical variables fairly easily and basically for this case you could imagine that instead of having your latitude in degrees you could just have three categories right you could have something like this is the northern hemisphere this is the equator and this is the southern hemisphere okay and then you could ask questions instead of the sort like that initial question we had before where was latitude greater than thirty your question could instead be is is I guess this would be is location in [Music] northern hemisphere right and so you could have basically any sort of subs that you could ask a question about any sort of subset of the categories you're looking at it's in this case northern you would still this question would still split out this top part from these bottom pieces here one thing to be careful about though is that if you have Q categories then you have I mean you basically are considering every single possible sub set of these categories so that's 2 to the Q possible splits and so in general you don't want to deal with too many categories because this will become quickly intractable to look through that many possible examples it turns out that in certain very specific cases you can still deal with a lot of categories one such case is for binary classification where then you can just the math it's a little bit complicated for this one but you can basically sort your categories by how many positive examples are in each category and then just take that as like a sorted order them and search through that linearly and it turns out that that yields you in optimal solutions so decision trees we can use them for regression we can also use them for categorical variables one thing that I've not gotten into is that you can imagine that in the limit if you grew your tree without ever stopping you could end up just having a separate region for every single data point that you have and so that's really you could consider that probably overfitting if you ran it all the way to that completion right so you can sort of see that decision trees are fairly high variance models and so one thing that we're interested in doing is regularizing these high variance models and generally how people have solved this problem is through a number of heuristics okay so one such heuristic is that if you hit a certain minimum leaf size you stop splitting that leaf okay so for example in this case if you've hit like you only have four examples left in this leaf then you just stop another one is you can enforce a maximum depth and sort of a related one in this case is a max number of nodes and then a fourth very tempting one I've got to say to use is you say a minimum decrease in loss right all right and I say this one's tempting because it's generally not actually a good idea to use this minimum decrease in wasallam and you can think about that I thinking that if you have any sort of higher-order interactions between your variables you might have to ask one question that is not very optimal or doesn't give you that much of an increase in loss and then your follow-up question combined with that first question might give you a much better increase and you can sort of see that in this case where initial latitude question doesn't really give us that much of a game we sort of split some positive negatives but the combination of the latitude question plus the time question really nails down what we want and if we were looking at it purely from the minimum decrease in lost perspective we might stomp too early and miss that entirely and so a better way to do this kind of loss decrease is instead you grow out your full tree and then you prune it backwards instead so you you grow out the whole thing and then you check which nodes to prune out pruning and how you generally do this is you you take it you have a validation set that you use this with and you evaluate what your miss classification error is on your validation set if for each example that you might remove for each leaf that you might remember so you would use miss classification in this case with the validation set any questions yep the minimum decrease in loss so yeah of course so you'll recall that before I was talking about sort of this RP this loss of our parent versus loss of our 1 plus loss of our two right so when we're or I had written out a maximization basically oh to be clear the question is can you explain a little bit more clearly what this minimum decrease in loss means and so you have your loss of r1 and r2 versus your loss of our parent right so the split before the split alright you have your loss before split yeah loss of our parent and then after split you have loss of r1 plus loss of r2 and if if this decrease between your loss of our parent to your loss of your children is not great enough you might be tempted to say okay that question didn't really gain us anything and so therefore we will not actually use that question but what I'm saying is that sometimes you have to ask multiple questions right yet to ask sort of suboptimal questions first to get to the really good questions especially if you have sort of interaction between your variables if there's some amount of correlation between your variables okay so we talked about regularization I said that we would get to run time let's actually just go up here again so let's cover that really quickly okay so it'll be useful to just find a couple numbers at this point so say you have n examples you have FB trace and finally you have D say the depth with your tree is D okay you've grunt you you have an examples that you trained on you with each has F features and your resulting tree is depth D so a test time your run time is basically just your depth of D it's just oh D all right which is your depth and typically though not all cases D and sort of about is less than the log of your number of examples and you can sort of think about this as if you have a fairly balanced tree right you'll end up sort of evenly splitting out all the examples in sort of recursively like doing these binary splits and so you'll be splitting it at the log of that end okay so at test time you've generally got it pretty quick at train time you have each point so if you return back to this example you'll see that each point right once you've done a split only belongs to the left or right of that split afterwards all right sort of like like this point right here once you've split here will only ever be part of this region will never be considered on the other side on the right-hand side of that split alright so if you're if your tree is of depth D each point each point is part of oh the nose okay and then at each node you can actually work out that the cost of evaluating that point for a train time is actually just proportional to the number of features F and I won't get too much into the details of why this is but you can consider that if you're doing binary features for example where each features just yes or no of some sort then you only have to consider if you have F features total you only have to consider F possible splits and so that's why the cost in that case would be F and then if it was instead a quantitative feature I mentioned briefly that you could sort the overall features and then scan through them linearly and that also ends up being asymptotically o of F to do that okay so each point is that most o of D nodes and then the cost of a point at each node is o of F and you have n points total so the total cost is really just is just oh of NFD like this and it turns out that this is actually surprisingly fast especially if you consider that n times F is just the size of your original design matrix right or your data matrix right your data matrix is of size and times F right and then your only your your run time is going through the data matrix that most depth times and since depth is log of n that turns out to be or generally bounded by log of n you have generally fairly fast training time as well any questions about run time okay so I've been talking a lot about the good sides of decision trees right they seem pretty nice so far however there are a number of downsides too and one big one is that it doesn't have additive structure to it and so let me explain a little bit what that means okay so let's say now we have an example and you have just two features again so X 1 and X 2 and you can say you define a line and just running through the middle defined by x1 equals x2 and all the points above this line are positive and all the points below it are negative now if you have a simple linear model like logistic regression to have no issue with this kind of setup but for a decision tree basically you'd have to ask a lot of questions that even somewhat approximate this line what you could try you're going to say okay let's split this way something like this and basically something like that right and even here you so you've asked a lot of questions and you've only gotten a very rough approximation of the actual line that you've drawn in this case and so decision trees do have a lot of issues with these kind of structures where this the features are interacting additively with one another ok so to recap so far since we've covered a number of different things about decision trees there's a number of pluses and minuses to decision trees ok so on the plus side there actually I think this is an important point is that they're actually pretty easy to explain right if you're explaining what a decision tree is to like a non-technical person it's fairly obvious you're like okay you have this tree you're just playing 20 questions with your data and letting it come up with one question at a time they're also interpret able you can just draw out the tree especially for shorter trees to see exactly what it's doing it can deal with categorical variables and it's generally pretty fast and however on the negative side one that I alluded to was that they're fairly high variance models and so are oftentimes prone to overfitting in your data they're bad at additive structure and then finally they have because in large part because of these bursts - they're generally have fairly low predictive accuracy I know what you guys are thinking I just spent all this time talking about decision trees and I tell you guys they actually sort of suck so why did I actually cover decision trees and the answer is that in fact you can make decision trees a lot better through ensemble and a lot of the methods for example the leading methods and kaggle these days are actually built on ensembles of decision trees and they really provide an ideal sort of a model framework to look at through which we can examine a lot of these different ensemble methods any questions about decision trees before I move on I I don't think that's strictly okay so the question is for the cross-entropy loss does the log need to be based - and the answer is I'm pretty sure that is not very relevant in this case I'm not a hundred percent sure about that but I'm pretty sure that the base is a lot good that makes it it's cross-entropy loss actually initially came out of like information there we have like computer bits and you're transmitting bits and so it's useful to think in terms of bits of information that you can transmit which is why I always came up as log base to initial formulation okay so now let's talk about ensemble okay so what I want is on Tom blowing help at some level you can sort of think back to your basic statistics so say you have you have excise excise which are random variables well sometimes right this is just our V that are independent identically distributed and so probably a lot of you are familiar with this already well you can call this iid okay now say that your variance of one of these variables is Sigma squared then what you can show is that the variance of the mean of many of these variables so let's of many of these random variables or written alternatively one over n sum over I of X I is equal to Sigma squared over N and so each independent variable you factor in is decreasing the variance of your model and so the thought is that if you can factor in a number of independent sources you can slowly decrease your variance okay so I saw that though this is a little bit simplistic of a way of looking at this because really all these different things are factoring together have some amount of correlation with each other and so this independent assumption is oftentimes not correct so if instead you drop the independence assumption so now your variables are just ID right okay and say we can characterize what the correlation between any two x i's is and we can write that down as rho so then you can actually write out the variance of your mean as Rho Sigma squared squared plus 1 minus Rho over m or no n Sigma squared okay and so you can sort of see that if your correlation if they're fully correlated then your this term will drop to zero and that you'll just have Sigma squared again because adding a bunch of fully correlated variables it's just going to give you the original variables variance versus if they're completely d correlated then this term drops to zero and you just end up with Sigma squared over n which gives you the initial independent identically distributed equation and so in this case really what you want to do the name of the game is you want to have as many different models that your factoring is possible to increase this n which drives this term down and then on the other hand you also want to make sure those models are as d correlated as possible so that your Rho goes down in this first term goes down as well okay and so this gives rise to a number of different ways to Ensemble and one way you could think about doing this is you just use different algorithms this is actually what a lot of people in cackled for example will do is they'll just take a core random forest and svm average of them all together and you know that actually works pretty well but then you sort of have to spend your time implementing all these separate algorithms which it's oftentimes not the most efficient use of your time another one that people would like to do is just use different training sets okay and again in this case like you probably spend a lot of effort collecting your initial training set you don't want your like machine learning person to just come and recommend to you that just go collect a whole second training set or something like that to improve your performance like that's generally not the most helpful recommendation okay and so then what we're gonna cover now are these two other methods that we use to do Ensemble II and one of them is called bagging which is sort of trying to approximate having different training sets I'll get into that quickly and then you also have boosting and just so that you had to have a little bit of context we're gonna be using decision trees to talk of a lot about these models and so bagging you might have heard of random force that's a variant of bagging for decision trees and then for boosting you might have heard of things like add a boost or XG boost which are variants of boosting for decision trees okay so that sort of covers that a high level would want to do these first two are very nice because they're sort of would give us a much more like independently correlated or less correlated variables but generally we're we end up doing these latter two because we don't want to collect new training sets or train entirely new algorithms okay so let's cover bagging first okay so bagging really stands for this thing it's called bootstrap aggregation okay and so first let's just break down this term so bootstrap what that is is this typically this method use and statistics to measure the uncertainty of your estimate okay and so what what is useful to define in this case for when you're talking about bagging is you can say that you have a true population P okay and your training set training set s is sampled from P you just are drawing a bunch of examples from P and that's what forms your training set and so ideally like for example this different training sets approach what you do with you just draw s1 s2 s3 s4 and then train your model and each one separately unfortunately you generally don't have the time to do that and so what that what bootstrapping does is you assume basically that your population is your training sample okay so you assume that your population is your training sample and so now that you have this s is approximating your P then you can draw new samples from your population by just drawing samples from s instead okay so you have bootstrap samples is what they're called z samples from s and so how that works is you basically just take your train your your training sample okay say it's of like cardinality n or something and you're just sample n times from s and this is important you do it with replacement because they're pretending that this is a population and so doing it with replacement sort of makes it of something hold that you're sampling from it as a population okay so that's bootstrapping so you generate all these different bootstrap samples Z on your from your training set and what you can do is you can take your model and train it on all these separate bootstrap samples and then you can sort of look at the variability in the predictions that your model ends up making based on these different bootstrap samples and that gives you sort of a measure of uncertainty I'm not going to go into too much detail that because that's not actually what we're gonna use bootstrapping for what we want to use bootstrapping force we want to aggregate basically bootstrap samples and so at a very high level what that means is we're gonna take a bunch of bootstrap samples train separate models on each and then average their outputs okay so let's make that a little bit more formal so you have bootstrap samples z1 through ZM say okay capital n let's just say how many bootstrap samples you're gonna take okay you train a model GM okay on Xia okay and then all you're doing is you're just defining this new sort of meta model I'm not putting a subscript on this one to show that it's the meta Model T of M which is just the sum of your predictions your individual models divided by the total number of models you have and this is just me writing out what I was sort of talking about right up there for bagging it's you're taking these bootstrap samples and then your training separate models and then you're just aggregating them all together to get this bagging approach and so if we just do a little bit of analysis from the bias-variance perspective on this we can sort of see why this kind of thing might work and so you recall we have this equation up here right there various variants of the mean is Rho Sigma squared plus 1 minus Rho over N Sigma squared so let me just write that out here and in this case our n is actually really just the number of bootstrap samples so we'll just use Big M in this case and what you're doing is by taking these bootstrap samples you're sort of D correlating your the models your training your bootstrapping is driving down well okay and so by driving this down you're sort of making this term get smaller and smaller and then your question might be okay what about this term here and it turns out that basically you can take as many bootstrap samples as you want and that'll slowly drive down it increases M and drive down this second term and it turns out that one nice thing about bootstrapping is that increasing the number of bootstrap models your training doesn't actually cause you to over fit any more than you were beforehand because all you're doing is you're driving down this term here so more M it's just less variance all you're doing is driving down the second term as much as possible when you're getting more and more bootstrap samples so generally only improves performance and so generally what people will do is they'll sample more and more models until they see that their error stops going down because that means I've basically eliminated this term over here so this seems kind of nice right you're decreasing the variance where's the trade-off coming in oh there's a question yeah there's definitely a bound right because I'm not gonna define one formally right now oh the question is can you define a bound on how much you decrease row by I'm not yeah so there's definitely a lower bound yeah lower bound and how far you can decrease row it basically comes down to your bootstrap samples are still fairly highly correlated with one another all right because they're still just drawing it from the same sample set s really your Z's gonna end up containing about two for each Izzy's gonna contain about two-thirds of s and so your Z's are still gonna be fairly highly correlated with each other and though I don't have a formal equation to write down as to exactly how much that decreases row by how much that balance row by you can sort of see intuitively that there is a bound there and then you can't just magically decrease row all the way down to zero and achieve zero variance so I was saying that you decrease variance this seems very nice one issue that comes up with with bootstrapping is that in fact you're actually slightly increasing the bias of your models when you're doing this and the reasoning for that is because of this subsampling that I was talking about here each one of your Z's is now about two-thirds of the original s so your training unless data and so your models are becoming slightly less you know complex and so that increases your bias in this case yes yeah for sure so the question is can you explain the difference between a random variable and an algorithm in this case right and so you can sort of at a very high level you can think of an algorithm as a classifier that as a function that's taking in some data and making a prediction right and if you sort of see those that whole set up as sort of like probably the algorithm is giving some sort of output in the problem holistic perspective you can sort of see the algorithm as like a random variable in a case in this case sort of like you're basically considering sort of the space of possible predictions that your algorithm can make and that you can sort of see as a distribution of possible predictions and that you can approximate that as a random variable I mean it is a random variable at some level because it's sort of like based on what training sample you end up with your predictions of your output model are going to change and so since you're sampling sort of these random samples from your population set you can consider your algorithm as sort of based on that random sample and therefore random variable itself okay so yeah your bicycle increased because of random subsampling but generally the decrease in variance that you get from doing this it's much larger than the slight increase in bias you get from from doing this random life subsampling so in a lot of cases bagging is quite nice okay so I've talked a bit about buying about bagging let's talk about decision trees plus bagging now okay so you recall that decision trees are high variance low bias and this right here sort of explains why they're pretty good fit for bagging okay because bagging what you're doing is you're decreasing the variance of your models for a slight increase in bias and since most of your error from your decision trees is coming from the high variance side of things by sort of driving down that variance you get a lot more benefit than for a model that would be on the reverse hybrid bias and low variance alright so so this makes this like an ideal fit for bagging okay so now this is sort of decision tree split bagging I said that random force or sort of a version of decision trees plus backing and so what I've described here is actually almost random for us at this point the one key point we're still missing is that random forest actually introduce even more randomization into each individual decision tree and the idea behind that is that as I had that question from before is this row you can only drive it down so far through just pure bootstrapping but if you can further D correlate your different random variables and you can drive down that variance even further okay and so the idea there is that basically for at each split four random forest at each split you consider only a fraction of your total features so it's sort of like for that ski example maybe like for the first plate I only let it look at latitude and then for the second split I only let it look at the time of the year and so this might seem a little bit unintuitive at first but you can sort of get the intuition for two ways one is that you're decreasing row and then the other one is you can think that say you have a classification example we have one very strong predictor that gets you very good performance on its own and regardless of what bootstrap sample you selects your models probably gonna use that predictor as its first split that's gonna cause all your models to be very highly correlated right at that first split for example and by instead forcing it to to sample from different features instead that's going to increase the or decrease the correlation between your models and so it's all about D correlating your models in this case okay and that sort of brings so close a lot of our discussion of bagging are there any questions regarding bagging okay now I've covered bagging let's get a little bit into boosting and I'll make this quick but basically whereas bagging we sort of saw in the intuition that we were decreasing variants boosting is sort of actually more of the opposite where you're decreasing the bias of your models okay and also it is basically more additive and how it's doing things so versus you'll recall that for bagging you were taking the average of a number of variables and boosting what happens you train one model and then you add that prediction into your ensemble and then when you turn a new model you just add that in as a prediction and so that's a little bit hand wavy right now so let me actually make that clear through an example so say you have a data set again x1 x2 x2 and you have some data points maybe some it's actually just called pluses and minuses say you have some more pluses here and then maybe a couple minuses and pluses here okay and what you say your training size one decision tree so decision stumps is what we call them it's you only get to ask one question at a time and the reason behind this just really quickly is that because you're decreasing bias by restricting your trees to be only depth one you basically are increasing their amount of bias and decreasing the amount of variance which makes them a better fit for boosting kind of methods and say that you come up with a decision boundary okay say this one here okay and what you're gonna do is on this side you predict positive right and on this side you predict negative it's like a reasonable like lying that you could draw here but it's not perfect right you've made some mistakes in fact what you can do is you can sort of identify these mistakes so if we draw this in red hey you've got made these guys as mistakes and what boosting does is basically it increases the weights of the mistakes you've made and then for the next decision stump that you train it's now trained on this modified set which I subscride over here and so now you these positives I'll just draw them much bigger you know you've got big positives here and some small negatives and some small positives some big negatives here and so now your model to try and get these right might pick a decision boundary like this and this is also basically recursive in that each step right you're going to be reading each of the examples based on how many of your previous ones have gotten it wrong or right in the past and so basically what you're doing is you can sort of weight each one of these classifiers you can determine for classifier GM a weight alpha M which is proportional to how many examples you got wrong or right so better classifier you want to give it more weight and a bad classifier you want to give it less right portion all and I think that the exact equation used in adaboost for example is just log of 1 minus the error of your n model divided lis basically log odds okay and then your total classifier is just f of a or let's just call it G of X again G of X it's just the sum over m of alpha and G of M and then each G of M is trained on a weighted on a reweighed it actually reweighed 'add and so i've glossed over a lot of the details here in interest of time but the specifics of an algorithm like this are will be in the lecture notes and this algorithm is actually known as adaboost and basically through similar techniques you can derive algorithms such as XG boost or gradient boosting machines that also allow you to basically re-weight the examples you're getting right or wrong in this sort of dynamic fashion and slowly adding them in is additive fashion to your composite model and that about finishes it for today thanks for coming great rest of your week