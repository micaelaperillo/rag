alright hey everyone welcome back um so let's continue our discussion today of reinforcement learning and mdps and specifically what I hope you learn from today is how to apply reinforcement learning um even to continuous states or infinite state MVPs so top out discretization model based RL talked about models the simulation and fitted value iteration is the main algorithm I want to lead up to for today just a recap because we're going to build on what we had learned in the last two lectures wanna make sure that you have the notation fresh in your mind MVP was state's actions transition probabilities discount back to reward that was an example V PI was the value function for a policy PI which is expected payoff if you execute that policy starting from a status and these star was the optimal value function and last time we figured out that if you know what is V star then Python the optimal policy or the optimal action for a given state can be computed as the augments of that and one one one thing though we'll come back to later is an equivalent way of writing that formula is that this is the expectation with respect to s Prime drawn from PS a B star best prime right so when we go to we've been we have been working with discrete state MVPs with the eleven state MVP so this is a sum over all the states s prime but when we have will go to continuous state MVPs the generalization of this or what this becomes this is the expected value with respect to s prime drawn from the state transition probabilities you know with index by s a current state currents that action of the value that you attain in the future so V star of s Prime and we saw the value iteration algorithm we're also so we talked about valuation policy iteration but today we'll build on value iteration but the value of the raishin algorithm uses Spellman's equations which says take the left hand side set it to the right hand side right and for V star if V was equal to V stop the left hand side it's equal to the right hand side that was um oh I'm sorry it's missing a max there right so if he was equal to V star then the left hand side and the right hand side would be equal to each other but what value iteration does is an algorithm that initializes V of s is 0 and repeatedly carries his update until the converges to V Star and after that you can then compute PI stock or fine for every state find the optimal action ok so um because we're going to build on this notation and this set of ideas today supposed to make sure all this makes sense right any questions ok all right so ok so everything we've done so far was built on the MDP having a finite set of states right so with 11 state MTPs was a discrete set of states um last time on Monday I think so much she asked well how do you do of continuous States so we'll work on that today but let's say you want to build a right let's say you want to build a car maybe a self-driving car right the state space of a car is let's see I'm gonna well instead of taking the my artistic side view of the car if you take a top-down view of a car alright so this is from the satellite imagery you know top down the other car with two wheels of call having this way how do you model the state of a car right well calm the way to model the state of a car that's driving around the planet earth is that you need to know the position and so that can be represented as X comma Y two numbers to represent you know really lots your lattice you or something right you probably want to know the orientation of the car right may be measured relative to north what's the orientation of the car and then it turns out if you're driving at very low speeds this is fine but if you're driving anything other than very low speeds then will often include in the state space also the velocities and angular velocity so X dot is the velocity in the X direction so X all this dx/dt right oh this this velocity X Direction Y dot it's lost in y direction and theta dot is the angular velocity the rate at which your car is turning okay and this sort of um up to you how you want to model the car is it important to model the current angle of the steering view is it important to model how worn down is your front left tire I supposed to have worn down is your beer right tire so depending on the application you are building is up to you to decide what is the state base state space you want to use the model this car and I guess it and if you're building a car to race on a racetrack maybe it is important to model what is the temperature of the engine and how you know one down this each of your four tires separately but for a lot of normal driving this would be you know sufficient level of detail to model the state space but so this is a six dimensional so this is a six dimensional state space representation oh and for those they work in robotics that would be called the kinematic mode of the car and that would be the dynamics model car right if you want to model the velocities as well um oh let's see how about the helicopter all right the States howdy marvelous taste is a helicopter helicopter flies around in 3d rather than drives around in 2d and so common way to model the statesman's helicopter would be to model it as having a position X Y Z and then also a 3d orientation of a helicopter is usually modeled with three numbers which we sometimes call the roll pitch and yaw right so you know if you ever an airplane row is that you roll into left or right pictures are you pitching up and down and you always know are you facing north south east or west so this is one way to turn the three-dimensional orientation of an object like an airplane or helicopter into CT numbers so so the details aren't important if you actually work a helicopter you can just figure this out but for today's purposes just right I guess the row picture but to represent the orientation of a three-dimensional object flying around is conventionally represented with three numbers such as a rotation jaw and then I thought why don't you see dot Phi dot theta dot side on the linear velocity and the angular velocity okay maybe just one last example so it turns out in enforcement learning maybe early early history of reinforcement learning one of the problems that a lot of people just happen to work on and and therefore you see in a lot of reinforcement learning textbooks there's something called the inverted pendulum problem but what that is is a little toy which is a little cot there's on wheels there's on a track and you have a little pole that is attached to this cot and there's a three swivel there and so this pole just flops over all this pose just swings freely and there's no motor there's no motor at this at this little hinge there and so the inverted pendulum problem is see that my prepared this right is if you have a if you have a free PO and if this is your carts moving left and right the inverted pendulum problem is you know can you with a swivel can you kind of balance that right well and so one of the common textbook examples of reinforced learning is can you choose actions over time to move this left and right so as to keep the pole oriented up with right and so for a problem like this if you have a linear wheel just a one-dimensional you know like a real aircraft that this cart is on the state space would be X which is the position of the cot theta which is the orientation of the pole as well X dot and theta dot right so this would be a four dimensional state space for the inverted pendulum if this like running left and right on a railway track and the one dimensional railway track right um so for all of these problems if you want to build you know a self-driving car and have it do something or build an autonomous helicopter and have it either harvest a bleed or flight trajectory or keep the pole upright and inverted pendulum these are examples of robotics problems where you would model the state space as a continuous state space so what I want to do today is focus on problems where the state space is our n so n dimensional set of real numbers and in these examples I guess n would be four or six or twelve right oh and again for the for the mathematicians in this class technically angles are not real numbers because they wrap around we go to 360 and then they wrap around to zero but I think for the purposes of today that's not important so we're just treatises RN so um serve the most straight straightforward way the most straightforward way to work with a continuous state space is discretization where you know you might have in this example a two dimensional state space maybe uh X and theta for the inverted pendulum and then you just lay down the set of grid values right and disparate eyes it back to a discrete state problem and so you know so you can give the state's a set of names one two three four whatever and anywhere within that little square you just pretend that you're MDP that you robot doesn't stay number one so this takes a Content stay problem and turns it back to a discrete state problem um this is such a simple straightforward way to do it this is actually reasonable to do for small problems and if you have a relatively small low dimensional state Series MVP like an inverted pendulum problem you're a four dimensional it's actually perfectly fine to discretize the state space and solve it this way let me describe some disadvantages of discretization first and then and then which a little bit about when you should just use discretization because even though it's not the best algorithm it works fine for smaller problems but for bigger problems we'll have to go to more sophisticated algorithms like fitted value iteration okay but um so what are the problems with discretization right well first this is a very this is kind of a naive representation for V star and PI star right which is you know remember the very first problem we talked about of predicting housing prices imagine if X was the size of a house and vertical axis was the price of a house and you had a data set that look like this discritization is that the discretization equivalent of trying to for the function of this data would be to look at the input feature and you know let's discretize it into five values and for each of these little buckets in each of these five intervals let's fit a constant function right something like that so this staircase would be how you know descritization will represent the price of a house as a function of the size and the analogy is that what we're doing in reinforcement learning is you want to approximate the value function and if you were to discretize it then on the x axis is maybe the state and now I'm down to one dimensional state right because that's what I can plot and you're saying that well let's approximate the value function you know as a as a staircase function as a function of the set of states right and you know and this is not terrible if you have a lot of data and very few input features you can get away with this this will work okay but this doesn't it doesn't seem to allow you to fit a smooth function right so that's one downside so it's not a very good representation and the second downside is the right someone fancifully named curse of dimensionality which is Richard bellman had given this name as a cool sounding name but what it means is that if the state spaces in RN and disparate eyes you know each dimension into K values then you get paid to the end discrete states so if this critize position and orientation into ten values which is quite small then you end up with you know ten to ten states which grows exponentially in dimensional state space n so this transition works fine if you have relatively low dimensional problems like two dimensions no problem four dimensions maybe it's okay but they were very high dimensional state spaces this is this is not a good this is not a good representation and it turns out the curse of dimensionality to take a slightly aside from continuous state spaces because the dimensionality also applies for very large discrete state MDPs so for example one of the places people have apply reinforcement learning is in factory optimization right so if you have a factory with a hundred machines in a factory and if every machine in the factory is doing something slightly different then if you have a hundred machines in the giant factory and each machine can be in K different states then the total number of states of your factory is K to the power of 100 right and so even if so so curse of dimensionality also applies to very large discrete state spaces such as if a factory over hundred machines and then your total state space becomes kids at 100 and it turns out that for this type of discrete state space fits a value iteration can be a much better album as well we'll get to Fitz evaluation a little bit okay so let's see so some practical so now despite all this criticism of digitalization if you have a small stage space is a simple method to try to apply you know and and if you're if you're very small so you say go ahead and discreet eyes they could be one of the quick things to try and just get something working so let me share have you some maybe guidelines this is this is how I do it I guess right if you have a you know two dimensional state space or three dimensional state space is no problem just discretized usually for a lot of problems it's just fine if you have maybe a four to six dimensional state space you know I would think about it and it will still often work so for the inverted pendulum which is four dimensional state space it works just fine I've had some friends work on trying to drive a bicycle right which you can model the six dimensional state space and you know disk realization it kind of works is that it works if you put some work into it one of the tricks you want to use as you approach to four to six dimensional state space range is choose your discretization more carefully so for example if the state s2 is really important so if you think the actions you need to take or the value of the performance is really sensitive to state as to and less institute to state as one then in this range people end up designing unequal discretization where you might discretize as too much more funny than s1 right and then the reason you do that is the number of states the number of discrete states is now blowing up exponentially something the power power for some of the politics and these tricks allow you to just reduce a little bit the number of discrete States you end up I think you know if you have a 7/8 dimensional problem that's that's pushing it that's when I would kind of be nervous and and you know be increasingly inclined to not use dissociation I personally rarely use this realization for problems that are 8 dimensional and then when your problem is that you even higher dimensional than this you know like 9 10 and higher then I would very seriously consider an algorithm that does not dispute eyes it's very very rare to use this code ization for 4 problems as high even 78 is quite rare I've seen it done in rare occasions but but and these things get worse exponentially right with the number of dimensions so maybe there's a set of guidelines for when to use the scores ation it when to seriously consider doing something else all right so um in the alternative approach that you see today what you'll be able to do is to approximate you start directly without resorting to descritization and there'll be an analogy that will make later just you know alluding to this plot again right to this analogy between linear regression when you're trying to approximate y is function of X and value iteration when you're trying to learn their approximate V as a function of s which is that in linear regression you say let's approximate X as a linear function of Y right or if you don't want to use the raw features Y what you can do is use you know theta transpose theta transpose v oh I'm sorry totally picks up right where Phi of X is the features of X so if right so this is what linear regression does where if X is your housing price then maybe Phi of X is equal to you know X 1 X 2 X 1 squared X 1 X 2 and so on right so that's how that's how you can use linear regression to approximate the price of a house either as a function of the raw features or as a function of some you know slightly more sophisticated study more complex of the features of the house and what we what you'll see in fitted value iteration is a model where we will approximate Basara ves as a linear function of features of the state ok so that's the algorithm wolf build up to and yeah we're going to try to use linear regression with a lot of modifications to approximate the value function okay and and and again enforce our learning in value iteration the your goal is to find a good approximation to the value function because once you have that you can then use you know the equation we had earlier to compute the optimal action for every state right so so we just focus on computing the value function now in order to derive the fitted value iteration algorithm it turns out that fits a value duration works best with a model or the simulator of D MVP so let me describe what that means and how you get a model and then we'll talk about how you can actually you'll implement the fitted value generation algorithm and have it work on these types of problems ok all right so um what a model or a simulator of your robot is is is just a function that takes as input a state takes as inputs in action and it outputs the next state s prime drawn from the state transition probabilities okay and the way that model is built is that the states and the actions are both and let's see and the way the model is built is the state is just a real value vector okay oh and um I think for simplicity but now let's assume that the action space is discrete it turns out that for a lot of em DPS the state space can be very high dimensional and the action space is much lower dimensional than the state space so for example for a car you know s is six dimensional but the space of actions is just two dimensionals right the steering and braking it turns out for a helicopter you know the state space is twelve dimensional and I guess you pray most of you I wouldn't expect you in their heart helicopter flies but it turns out that you have two four dimensional actions in the helicopter the way you find welcomes these are two control sticks so your left hand the right hand you know can move has two dimensions of control and for the inverted pendulum here's the state space is for D and the action space is just one D right you move left or right so you actually see in a lot of reinforcement learning problems that it's quite common for the state space to be much dimensional in the action space and so let's say for now that we do not want to discretize the state space because it's your high dimensional but just for the sake of simplicity let's say we discretize the action space for now right which is which is usually much easier to do but I think as we develop it evaluation as well well well you you might you get hints of when maybe you don't need to discretize the action space either but let's just say we have a dispute dispute action space so all right so how do you get a model right one way to build a model is to use a physics simulator so you know in the case of an inverted pendulum right it turns out that well if the action is what's the acceleration you apply to either a positive negative or to the to the X all right so therefore the right then it turns out that let's see so the state space is four-dimensional right and it turns out that if you sort of flip open the you know physics textbook using Newtonian mechanics if you know the weight of the card the way to the pole yeah I think that says actually you know the mass of the constant mass in the pole and the length of the pole it turns out you can derive equations about what is the velocity right so it's thought is equal you know don't don't worry about this think of the map as declaration other than something you need to learn where you know L was the length of the pole M is the mass of one of these things as you don't know m is the Hamas a is the force extender and so on and and and conventional physics textbook will kind of let you derive these equations or rather than trying to derive this yourself using you know either yourself using Newtonian mechanics or finding the help of the physicist friend there are also a lot of open source physics simulator software packages we can download open source simulator plug in the dimensions and mass and so on of your system and then they'll spit out the simulators and tells you how the state evolves from one time said to another times then right and so but so in this example the simulator will say that s prime is equal to S Plus you know delta T times I guess I times s dot where delta T could be lets say 0.1 seconds right so if you want to simulate this at 10 Hertz so that 10 10 10 updates per second so that the time difference between the current state in the next day there's one tenth of a second then you write a simulator like this okay and but and really the the most common way to do this is not to actually derive the physics update equations the most common way to do this is to just download one of the open source physics engines right so um so this will work okay for problems like the inverted pendulum I once use a physics engine to build a simulator for a four-legged robot and manager user enforcer learning together for the girl over to walk around right so it works the second way to get a model is to learn it from data right and I press they end up using this much more often so um here's what I mean let's say you want to build a controller for an autonomous helicopter right so so this is case study and what I'm describing is real like this will actually work right let's do you want to build up let's say you haven't let's say you have a helicopter and you want to build on songs controller for it what you can do is start your helicopter off in some state s0 right so with GPS accelerometers magnetic compass you can just measure the position and orientation of the helicopter and then have a human pilot fly the helicopter around so the human pilot you know using control sticks will move the helicopter they'll know their command the helicopter with some action a zero and then a tenth of a second later the helicopter will get to some slightly different position and orientation that's one and then the human pilot you know will just keep on moving the control sticks and so you record down what action they are taking a1 and based on that how copter will get to some new state s2 and then they will take some action a to or get to some state s3 and so on and let them just write this as capital T right so in other words what you do is a take the helicopter out to the field and hire a human pilot to fly this thing for a while and record the position of the helicopter ten times a second and also record all the actions that human pilot was taking on the control stick okay and then do this not just one time but to do this M time so let me use a superscript one what you get the idea to denote the first trajectory so you do this a second time and so on and maybe do this every time so there's just a lot of map of saying fly the helicopter around you know M times right and then recall everything that happened and now your goal is to apply supervised learning right to estimate s T plus 1 as a function of s T and a T so the job of the model the jobless simulator is to take as input the current state and the current option and tell you where the helicopters gonna go you know like a 0.1 seconds later and so given all this data what you can do is apply a supervised learning algorithm to predict well what is the next state s prime as a function of the current state in action right and the other notation is when I drew the boxless emulator above I was using s prime to denote s T plus 1 and s n right so that's the mapping between the notations and so if you use the linear regression version of this idea you will say this approximate s T plus 1 as a linear function of the previous state plus another linear function of the previous state and it turns out this actually works ok for helicopters flying at slow speeds this is actually not a terrible model about if your helicopter is moving slowly and and not flying upside down if you have a copters flying in the relatively level way and kind of at slow speeds this model is not too bad if you find your helicopter in the highly dynamic situations find very fast making a very fast aggressive turn this is not a great model but this is that you okay first little speed spiking um and so I guess a here will be a and by n matrix because the state space is n dimensional you know so a is a square matrix and B will usually be a tall skinny matrix I guess whereas the dimension of B is the dimensional state space by the dimension of the action space right and so in order to fit the parameters a and B you would minimize with respect to the parameters a and B this so you wanna approximate as cheapest one as a function of that and so you know pretty natural to fit the parameters of this linear model in a way that minimizes the squared difference between the left hand side the right hand side wait did I screw up yes okay oh sure what's the difference we find helicopter M times RS by helicopter once very very long in this example it makes no difference yeah this is fine either way unless some yeah for purposes doesn't matter sorry umm for the person since classes doesn't matter for practical purposes if you find helicopter M times it turns out the fuel burns down slowly and so the way to her coffee changes slowly and you've won an average over how much fuel do you have for winning conditions this is what actually it's done but for the purposes of understanding without room playing a single time for a long time you know well it's just fine as well okay um so this is the linear regression version of this and it and we actually talked about some other models later called lqr in lqg you you see this linear regression version of a model as well disree just a linear model the dynamics right well we'll come back to linear models dynamics later next week but it turns out that if you want to use a nonlinear model you know plug in a nonlinear you know if you you can also plug in write Phi of s you know it may be v prime of a as well if you want to have a low nonlinear model and this will work even better depending on your choice of features okay now um finally having run this your little linear regression thing where you were and this is not quite linear regression because a and B are matrices but but you can minimize this objective but it turns out - this turns out to be equivalent to running linear regression n times so s has 12 dimensions this turns out to equivalent to running linear regression n times to predict the first day second day third state to variable and so on right that that's this one what this is equivalent to but having done this you now have a choice of two possible models one model would be to just say my model will said st plus 1 as a st plus b 18 or another version would be to set st plus 1 equals a cos B T plus epsilon t where epsilon T is distributed maybe from from a Gaussian from a Gaussian density okay and so this first model would be a deterministic model and this model would be a stochastic model and if you use a stochastic model then that's saying that when you're running your simulator when you're running in the model every time you generate st plus 1 you would be something this epsilon from a Gaussian vector and adding it to the prediction of your linear model and and they've uses stochastic model what that means is that you know if you similar you have a calcifying around your simulator will generate random noise the add and subtract a little bit to the state space of the helicopter as if there were little wind gusts blowing it blowing the helicopter around okay and this is a so-so it and in in most cases when you're building reinforcement learning models oh and so the the approach we're taking here this is called model-based reinforcement learning when you're going to build a model of your robot and then let's train the reinforcement learning algorithm in the simulator and then take the policy learn and take the policy PI you learn in simulation and apply it back on your real robot alright so this is this dis approach we're taking is called model-based RL there is an alternative called model free RL which is you just run your enforcement learning algorithm on the robot directly and that the robot - the robot around and so on and then I learn I think that in terms of robotics applications I think model-based RL has been taking off faster a lot of the most promising approaches are model-based RL because of your physical robot you know you just can't afford to have a reinforcement learning algorithm - your robot around for too long or how many helicopters do you want to crash before you learn the armor things as well model free RL works fine if you want to play video games because if you're trying to get a computer or play chess or thell or go right because you have a perfect simulator for the video game which is a video game itself and so your your your ro algorithm you can on there blow up hundreds of millions of times in a video game and that's fine episode 4 playing video games were playing on like you know traditional games model free approaches can work fine but I most of the a lot of the successful applications of reinforced knowledge of robots have been model based although again the field is evolving quickly so there's there's very interesting work at the intersection of model-based in model free that gets more complicated but I would say if you want to use something tried-and-true you know for robotics problems seriously consider using model based RL because you can then fly a helicopter in simulation let me crash a million times right and no one's hurt there's no physical damage anywhere the world is just OK and and oh and just one last tip one things we learned building these reinforcer learning algorithms for a lot of robots is that you know have you run this model you might ask well how do I choose the distribution for this noise right how do you model the distribution for the noise once you could do is estimate it from data but as a practical matter what happens is so long as you remember to inject so let's see it turns out if you used to deterministic simulator a lot of reinforcement learning our and also learn a very brittle model that works in your simulator but doesn't actually work when you put it into your real robot and so if you actually look on YouTube or Twitter in the last year or two there been a lot of cool looking videos that people using reinforce learning to control various really configured robots a really good snake robot or some five ago thing or some whatever is this cool random is I I'm not gonna drink this but you know if you build a 5 bigger robot I didn't know what has five legs right how do you control that it turns out that if you have a deterministic simulator using these methods it's not that hard to generate a cool-looking video of your reinforcement learning algorithms supposedly controlling a 5 thing or some crazy you know a worm with two legs or something these crazy robots so you can build in simulator but it turns out that even those easy to well not easy even though you can generate those types of videos in the deterministic simulator if you use a deterministic model of a robot and you ever actually try to build a physical robot and you take that policy from your physics simulator to the real robot the odds of it work on the real robot are quite low if you use the deterministic simulator great because the problem with simulators is that your simulators never 100% accurate right yeah it's always just a little bit off and one of the lessons we learned the field the whole few learned applying RL so a lot of robots is that if you want your model-based are aware to work not just in simulation engineer cool video but you wanted to actually work on a physical robot like a physical helicopter that you own that is really important to add some noise to your simulator because if the policy you learn is robust to a slightly stochastic simulator then the all server generalizing you know to the to the real world to the physical real world it's much higher than if you had a completely deterministic simulator so I think whenever I'm building a robot right III pretty much yeah actually yeah I don't think I with one exception LKR LQG without ball next week well one with one very narrow exception I pretty much never use deterministic simulators when welcome to robotic control problems unless assuming assuming I wanted to work in the real world as well and and again you know tips and tricks so the most important thing is to add some noise and then sometimes the exact distribution of noise you know go ahead and try to pick something realistic but the exact distribution of noise actually matters less I want to say then just a faculty remembering to add some noise okay by the way you guys really don't know this but my PhD thesis was using reinforcement learning to fly helicopters so so I'm trying to oh no so you're talking to someone just crash a bunch of helicopters and that's model helicopters and has lived through the the pain and the joys are seeing this stuff work or not work all right so now that you have built a model build a simulator for your helicopter for your folding a robot or for your car how do you how do you approximate the value function right so um in order to apply fitted value iteration the first step is to choose features of the state s right and then we're approximately of s you know we're approximately saw using a function V of s which is going to be theta transpose Phi of s and so and so you know in the case of a in the case of a on inverted pendulum right then Phi of s maybe you have X x dot maybe one x squared or x times X dot or x times the pole orientation and so on so take take two states as and think of some nonlinear features that you think might be useful for representing the value and remember that what the value is the value of a state is your expected payoff from that state expect some discount or it was so the value function captures if your robot starts off in this state you know how well is it gonna do if it starts here so when you're designing features pick a bunch of features that you think help convey how well is your robot doing and so maybe for the inverted pendulum for example if the PO is way over to the right then maybe the pole will fall over or give it a reward of minus one when the pole falls over right but so sorry I'm overloading notation a bit theta is both the angle of the pole as was the parameters but but but if the PO is falling way over that looks like it's doing pretty badly unless X dot is very large and positive right and so maybe that's interaction between Phi and X dots you might say well let me have a new feature which is the anchor the PO multiplied by the velocity right because then because it seems like these two variables cannot depend on each other so so so just as when you are trying to predict the price of a house you would say well what are the most useful features for the price of a house you do something similar for fit evaluation and one nice thing about one nice thing about maldo based RL is that one small debasement folsom learning is that once you have built a model you see a little bit that you can collect an essentially infinite amount of data from your model right and so with a lot of data you can usually afford to choose a larger number of features because you can generate a ton of data with which to fit this linear fashion and so you know you are usually not super constrained in terms of needing to be really careful not to choose too many features because of fear of overfitting you could get so much data from a simulator that you know you could usually make up quite a lot of features and I saw the features and the not being useful is okay because you can get an update from running your simulator for the algorithm to store for their pretty good set of parameters data even if you have a lot of features because you can have a log that you can generate a lot of data to fit this function so um let's talk through the fitted value iteration algorithm alright you know what this is a long algorithm let me just use a fresh board for this alright so let me just write down the original value iteration algorithm to speed States so what we had previously was we would update BFS according to our FS plus gamma max over here right so this is what we had lost Monday and I said at the start of today's lecture that you can also write this as this so let's take that and generalize it to fit to value iteration all right um so first let's choose a set of States randomly unless initializer Prime's is equal to zero and what we're going to do is we're so so let's see in many regression and you learn the mapping from X to the Y and you have a discrete set of examples for X and you fit a function mapping from X to Y so in what we're going to do here we're going to learn a mapping from s to V of s and we are going to take a discrete set of examples for s and try to figure out what is V of s for them and then for the straight line you know to try to model this relationship right so so just as you had a finite set of examples a finite set of houses that you see a certain set of values of X in your training set for predicting housing prices we're going to see you know a certain set of states and then use that finite set of examples to use linear regression into 50 of s right so that's what this initial sample is meant to do and so this is the Ultimo's loop of value iteration a fit evaluation and then for I equals 1 through em let's see all right so what we're going to do is go over each of these M States for go over each of these M States and for each one of them we're going to and for each one of those days of each one of those actions we're going to take a sample of K things in order to estimate that expected value right and so this expectation is over s Prime drawn from the state-transition distribution it's saying you're from this state if you take this action where you get tunics and so these two loops this for I equals 1 through m and for each action a this is just looping over every state in every action and taking K samples that something K examples of where you get to if you take an action a in a certain status right and so and by taking that K examples and computing this average QA right is your estimate of that expectation okay so so all we've done so far is a take K samples you know from this distribution of with s prime is drawn and average V of s OS yeah oh I'm sorry and if I move our FS inside sorry then that's Q of a yeah sorry there's never just rewrite this to move our FS inside so this is written as gamma if you write this as Matt a little bit a of our s plus gamma yeah okay yes siree so move the Max and expectation out then this is this is Q of a next let's set Y I equals max over a of Q of a and so by taking the max over a of Q of a that's what Y is is your estimate at the right hand side of value iteration and so why is your estimate for for this quantity for the right-hand side of valuation now in the original value iteration algorithm I'm just using VI to approximate out to abbreviate value elevation in the original algorithm what we did was we set V of Si to be equal to Y I write it said you know in the original value iteration algorithm we would compute the right-hand side this purple thing and then said VFS equals to that he just said right-hand side equal to set the left-hand side equal the right-hand side but in fitted value iteration you know V of s is now approximated by a linear function so you can't just go into a linear function and set the value of points individually so what we're going to do instead is in fitted VI we're going to use linear regression to make V of si as close as possible to y:i but VF si is now represented as a linear function of the state so a linear function of the features of states so VF si is Theta transpose Phi of Si and you want that to be close to Y I and so the final step is run linear regression to choose the parameters theta that minimizes the squared error oh yes just make my curly braces match okay so that's fitted question oh this one oh this one oh no the M is used differently the so when we were learning a model M was just how many times do you fly the helicopter in order to build a model and the number of times you find the helicopter in order to build a physics model to build a model helicopter dynamics has it has nothing to do with this M which is the number of states you use in order to sort of anchor or in order to so I think I'm actually so the the way to think about this is um you want to learn a mapping from States to B of s and so the sample you know this M stays is we're gonna choose M States on the x-axis right so and that M is the number of points you choose on the x-axis and then in each iteration evaluation we're going to go through this procedure so you have sort of s 1 up to SM right and then for each of these you're going to compute some value Y I using this procedure and then you fill a straight line to this sample of Y eyes think of these think of the way you build a model and the way you apply fitted value duration as two completely separate operations so you can have one team of 10 engineers fly the helicopter around you know five helicopter around a thousand times build them although run linear regression and they have a model and then they could publish the model on the internet and a totally different team could download their model and do this and the second team does no need to talk the first team at all other than downloading the model off the internet oh yes a good question you mean there's something there's something K times right yep that's a great question yes that was a yes that was one of my next points which is the reason you sample from this distribution is because you are using so you should do this if you're using a stochastic simulator right and then and actually there's actually also ask you guys what should you do how can you simplify this algorithm if you use a deterministic simulator and service elastic simulator let's see so if you said determining if you said deterministic simulator then you know given a certain state kind of such an action it will always map to the exact same s Prime right so how can you simplify the yep yes so if your determines simulator you can set a equals one and set the sample only once because this distribution it always returns the same value so all of these case ampuls would be exactly the same so you might as well just do this once rather than K times this one oh no this is uh this is actually as square brackets the thing is we're trying to approximate this expectation and the way you're approximate the mean is you know sample K times if you take the average right right so so what we've done here is in alternate approximate dis expectation we're going to draw K samples and then sum over them and divide by K so you average over the case our polls let's see so how do you choose em and how do you test the overfitting so you know one once you have a model one of the nice things about model Bizzaro is let's say that Phi of s right let's say that Phi of s is 50 features so let's say you chose 50 features approximately the value function of your inverted pendulum system then we know that you know that you're going to be fitting linear regression right to this 50 dimensional state space I mean this step here this is really linear regression and so you can ask if you want to run linear regression with 50 parameters how many examples do you need to fit in linear regression and I would say you know if M was maybe 500 right maybe it'd be ok you have 5 10 examples to fit 50 parameters but if for computational reasons if it doesn't run too slowly to even set M equals 1000 or even 5000 then there's no harm to letting em be bigger so usually mu must all said to be as big as you feel like subject to the program not taking too long to run because it you know if you're if you're fitting it unlike supervised learning if you're fitting data to housing prices you need to go out and you know collect data right off Craigslist or was Zillow or Trulia or Redfern or whatever about prices of houses and so data is expensive to collect in the real world but once you have a model you could set m equals 5,000 or 10,000 or 100,000 and just and then your algorithm will run more slowly but but selassie algorithm doesn't run too slowly there's no harm to setting them to be bigger cool so so I know there's a lot going on to this algorithm but this is fitted value iteration and if you do this this skill you can get reasonable behavior on a lot of robots by choosing Casella features and learning the value functions are approximate the value of the really approximate expected payoff of a robot starting off in different states okay now just a few details to wrap up again some practical aspects of how you do this after you've learned all these parameters this you've now learned yeah OSE yes thank you yes so in this expression where do you get a V of s prime J from yes so you would get this from theta transpose Phi of s prime J using the parameters of theta from the last iteration of fitted value iteration just as in value iteration this is the values from the last iteration the you use update the new iteration so then you use the last value of theta is updated oh and one one one other thing you could do which is I talked about the linear regression version of this algorithm which is you know this hope that this whole exercise is about generating a sample of s and Y so you can apply linear regression to predict the value of y from the values of s right but there's nothing in this algorithm that says you have to use linear regression in order to now that you've generated this data set there's this box that happier this is linear regression right but you don't have to use linear regression in Mauldin you know deep reinforcement learning one of the ways well one of the ways to go from reinforce on a deeper enforcer learning is to just use the neural network for this step instead then you can call then then you call that deep reinforcement learning where no but hey it's legit you know but but you can also use locally weighted linear regression or whatever regression algorithm you want in order to estimate Y as a function of the state s yeah and I should have used a neural network it relieves the need to choose features PI as well you can feed in the raw features you know poor angle poor orientation and use a neural network to learn them having a supervisor alright so one last important I guess practical implementation of detail which is fitted VI right just approximation to V Star and this um implicitly defines PI star right because the definition for pi star is that so when you're running a robot you know you need to execute a policy prior given the stage music and actually given the stage Nipigon action and and having computed v-star it only implicitly defines the optimal policy PI staff and so if you're running a rover or if you're running a robot in real time then you know actually if you find a helicopter you might have to choose control actions at ten Hertz meaning ten times a second you given the state you have you choose in action if you're building a self-driving car again a ten Hertz controller would be pretty reasonable guys choose a new action there maybe ten times a second would be pretty reasonable but how do you compute this expectation and this maximization ten times for a second so in what we use for fitted value iteration we used sample of we use K samples to approximate the expectation but if you're running this in real time on a helicopter you know probably you don't want to at least I don't know for my robotics implementations I have been reluctant to use a random number generator right in the inner loop of how we control a helicopter it might work but I but I think you know it's approximately if you want to compute this arc Merricks it's approximate expectation and do you really want to be running a random number generator on a helicopter and if you're really unlucky and a random air engineer generator journey is an unlucky value.we helicopter to do something you know iiii would again just emotionally i don't feel very good you yourself driving car has a random number generator in a loop of house choosing to drive so just as a practical matter there are a couple of tricks that people often use which is the simulator is often of this form so most simulators of this form next state is equal to some function of the Peter previous state and action plus some noise and so one thing that is often done is for your deployment or for the you know for the for the actual policy you implement on the robot set epsilon T equals zero and set K equals one right and so so just this this is a reasonable way to make this policy run on a helicopter which is during training you do want to add noise to the simulator because it causes a policy you learn to be much more robust so little errors in the simulator your simulator is always going a little bit off you know maybe it didn't quite simulate wind gust or when you turn the helicopter does it back exactly right amount some of its as always in practice is always a little bit off so it's important to have noise in the simulator in model-based RL but when you're deploying this in a physical simulator one thing you could do to be very reasonable is just get rid of the noise and stay K equals one and so what you would do is let's see whenever you're in the state s pick the option a according to our masks over a of V s a so this F is this F from here so this is the simulator with the noise removed okay and so what you would do is actually and and you know computers are now fast enough you can you could do this ten times a second right if you want to control helicopters self-driving car ten Hertz you can actually easily do this you know ten times a second which is your car or your helicopters in some physical state in the world so you know what is s and so you can quickly for every possible action a that you could take use a simulator to simulate where your helicopter will go if you were to take that action so go ahead and run your simulator you know once for each possible action you could take right computer actually fast enough to do this in real time and then for each of the possible next actions you could get to compute V apply to that so this is really right as a prime drawn from PSA but with this term the six simulator right so every tenth of a second you could assimilate to try out every single possible action user simulator to figure out where you would go under each every single possible action and apply your value function to see of all of these possible actions which one gets my helicopter you know in the next one tenth of a second to the state that looks best according to the value function you've learned from fits evaluation and it turns out if you do this then you can this is how you actually implement something that runs in the whole time and oh and I just mentioned you know the the idea of a training was so costly simulator and then just setting the noise is zero it's one of those things there's not very rigorously justified but in practice this this works well oh yes so so um for purpose of this you can assume you have a discretized action space and it turns out that for a self-driving car is actually okay to this precise reaction space for a helicopter we tend not to disguise the action space but it turns out if F is a continuous function then you can use other methods as well right this is about optimizing over the I didn't mean to talk about this so sorry this getting a little bit deeper but even if a was a continuous thing you can actually use real time optimization algorithms to very quickly try to optimize this function even as a function of the concerns actually there's a literature on something called model predictive control which we can actually you can actually do these optimizations in real time and use final thoughts last question wait oh say that what's the question is oh I use an observation yeah yes yes so you take an action and then your helicopter do something there'll be some wind your model may be off and so you would then a tenth of a second later take another you know GPS reading accelerometer reading magnetic compass reading and use the whole copper sensor to tell you where you actually are no cool okay cool all right I hope yeah hopefully this was helpful I feel like you know the I think that's fascinating that the excitement that by myself driving cars and final hug calls and all that it gives both down to equations like these though I think that's not cool okay that's great thanks I'll see you guys next week