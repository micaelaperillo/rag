okay happy Halloween what I want to do today is share of you advice for applying machine learning and you probably allude to this before but um yeah I think over the last several weeks you've learned a lot about the mechanics of how to build different learning algorithms everything from the aggression which is aggression as VMS random forests is it neuro networks and what I want to do today is share of you some principles for helping you become efficient at how you apply all of these things to solve whatever application problem you might want to work on um and so a lot of today's materials I should not have mathematical but there's also some of the hardest material tests were in this class to understand um it turns out that when you give advice on how to apply a learning algorithm such as you know don't waste lots of time collecting data unless you you you have confidence is useful to actually spend all that time it turns out when I say things like that people you know this easy agree is that of course you shouldn't waste time collecting loss a day there unless you have some confidence it's actually good use your time that's a very easy thing to agree with but the hard thing is when you go home today and you're actually working on your class project right to apply the principles we talked about today when you're actually on the ground talking to your teammates saying alright do we collect more data for our class project now or not to make the right judgment call for that to map the concepts you learn today so when you're actually in the hot seat you know making a decision do we go and spend another two days scraping data off the internet or do goons tune this out tune this parameters algorithm to actually make those decisions it's actually it it often takes a lot of careful thinking to make the mapping from the principles we talked about today and they prepare all of you go yep that makes sense but they actually do that when you're in the hot seat making the decisions that that's something that we often take take some careful thought I guess and I think you know for a long time positive machine learning have been an art right we're you know we'll go through these people that have been doing it for 30 years and we say hey my learning algorithm doesn't work you know what do we do now and then they would have some judgment or you go people to ask me and for some reason because we've done it for a long time we'll say oh yeah get more dates or tune that parameter or try a new network of big hidden units and for some reason that work and what I hope to do today is turn that black magic that hot that that arts into much of a science so that you can much more systematic make these decisions yourself rather than talk to someone there's done this for 30 years then that for some reason is able to give you the good recommendations even if you know but turn it from more of a black art into a more of a systematic engineering discipline um and and just a one-note someone I wouldn't do today is not the best approach for developing novel machine or any research if you are if your main goes to write research papers some of what I'll say will apply some others say will not apply but I'll come back to that later but so most of today is focus on how the hell you build stuff that works right the build build applications that work so the three key ideas you see today are first is Diagnostics for debugging learning algorithms one thing you might not know or actually if you work on a class project maybe you know this already is that when you implant to learning Alvin for the first time it almost never works right that is not the first time and so what is it I still remember it was there was a weekend about a year ago where I implemented softmax regression on my laptop and it worked the first time and even to this day I still remember that feeling of surprise I know there's gotta be a bug and I went in to try to find a bug and there wasn't about it but it is so rare Travie Algren works the first time I still remember every year later and so longer the workflow of developing learning algorithms it actually feels like a debugging workflow right and so my hope you become systematic at that um and two key ideas here about our analysis innovative analysis so how to analyze the air as you're learning algorithm and also how to how to understand was not working with error analysis how long said what's working which is ablative analysis and then finally some philosophies and how the get started or the machine learning project such as your class project okay so let's starts without discussing debugging learning algorithms um so what happens all the time is you have an idea from machine learning application you implement something and then it won't work as well as you hoped and the key question is what do you do next right whenever I work on machine learning algorithm that's actually most of my workflow we usually have something unfermented it's just not working that well and your ability to decide what to do NYX has a huge impact on your efficiency I think when when when I was a when I was in undergrad at Carnegie Mellon University I had a friend that would debug their code by you know they write a piece of code and then as always we write Pisco initially always a bunch of syntax errors right and so their debugging strategy was to delete every single line of code that generates a syntax error because it was a good way to garrulous and so that wasn't a good strategy so in in in machine learning as well they're good and less good debugging strategies right um so let's not so motivating example uh let's say building an anti spam classifier and let's say you've carefully chosen a small set of hundred words to use as features so instead of all using you know ten thousand or fifty thousand words you've chosen a hundred words that you think could be most relevant to anti-spam and let's say you start off implementing logistic recognization I think when talked about this is also you know there's a frequencies in Beijing in school but you can think of basing logistic regression where you have the maximum likelihood term on the left and then that second term is the regularization term right so that's so that's Bayesian logistic regression if you are Bayesian or which is regression with regularization if you're you know using frequency statistics and let's say that they're just regression with regularization or based on Lynch's direction it gets twenty percent test error which is an unacceptably high making one in five mistakes on on your spam filter and so what do you do next um now for this scenario I want a and so um for when you implement an algorithm like this what many teams will do is try improving the algorithm in different ways so what many teams will do is say oh yeah I remember you know well we like big data more data always help so let's get some more data and hope that solves the problem so one some teams will say let's get more training examples good and it's actually true you know more data pretty much never hurts it almost always holds but but the key question is how much um or you could try using a smaller set of features behind your features Prarie somewhere in that relevance so let's get rid of some features or you could try having a larger cell features kinds of features that seem small resourceless add more features um or you might want other designs of the features you know instead of just using features in the email body you can use features from the email header the email header has on not just a from to subject but also routing information about what's a set of service of the internet that the email took to get to you or you could try running gradient descent for more iterations that that you know that never hurts right pretty usually in self grading descent let's switch to Newton's method or let's try a different value for lambda or always say you know forget about basically just regression or logistic regularization let's let's use two totally different algorithmic in his or network or something right so what happens in a lot of teams is someone will pick one of these ideas kind of a random it depends on you know what they happen to read the night before write about something or the experience on the last project and sometimes a project and sometimes your project leader will say you know we'll pick one of these and just say let's try that and it's been a spend a few days or a few weeks trying that and it may or may not be the best thing so I think that in in in my team's machine or any workflow so first if you actually you and a few others sit down and brainstorm a list of the things you could try you're actually already ahead of a lot of teams because lot of teams will kind of just by gut feeling right or the most opinionated person will pick one of these things at random and do that but you brainstorm a list of things and then and then try to evaluate the different options you're already ahead of many teams oh sorry and and and I think the yeah and I think right you know and then unless you analyze these different options it's hard to know which of these is actually the best options right so um the most common diagnostic I end up using in developing learning algorithms is a bias versus variance diagnostic right and I think talking about bias and variance already with a classifier is highly bias then it tends to under fit the data so high bias is well you guys remember this right if if you have a data set there's like this a highly biased crossbar and maybe much too simple and high variance classifies may be much too complex and some something in between you know with with trade-off bias and variance inappropriately right so those bodies invariance and so it turns out that one of the most common Diagnostics using in pretty much every single machine learning project is a bias versus fear instead gnostic to understand how much of your learning over this problem comes from bias and how much of it comes from variance and and you know i i've had i don't know they former PhD students right that learned about bias and variance when do the india PhD and then sometimes even a couple years after they graduated from Stanford and worked you know on more problems they actually tell me that their their understanding of bias and variances continue to deepen right for many years so there's one of those concepts this is um if you can system at the apply they're making much more efficient and and this is really the maybe the single most useful to our town understanding by the variants of debugging learning algorithms and so what I'm going to describe is a workflow where you would run some Diagnostics to figure out what is the problem and then try to fix whether problem is and so just to summarize this this example you know literature in Arizona have to be high and you want to and you suspect promise either high variance or high bias and so it turns out that there's a diagnostic that lets you look at your algorithms performance and try to figure out if how much the problem is variance and how much of the problem is biased oh and I'm gonna say test error but if you're developing should I really be doing this with a def said or a development set rather than a test set right but so let me let me explain this diagnostic in greater detail so turns out that um if you have a classifier with very high variance then the performance on the test set or actually be a better brother practice use the holdout cross-validation so that the developer said you see that the error that you classify has a much a much lower error on the training set than on the development set but in contrast if you have high bias then the training error and the test error on the deficit error will go behind so let me illustrate this with a picture so this is a learning curve and what that means is on the horizontal axis you're gonna vary the number of training examples right and when I talked about bison barians I had a plot where the horizontal axis was the degree of polynomial right you for the first order second order third order fourth order polynomial in this plots the horizontal axis is different this number of training examples and so it turns out that when you train a learning algorithm you know the more data you have usually the better your development set error your better your test set error right it's just error usually goes down when you increase the number of training examples the other thing the other and and and let's say that you're hoping to achieve a certain level of desired performance you know for business reasons you like your spam classifier to achieve a certain level of design performance and often sometimes design level performance is to do about as well as a human can there's a common business objective depending on your application but sometimes it could be different right so you have some your product manager you know tells you that all you if you're leading the brother you think that you need to hit a certain level target performance in order to be very useful spam filter so the other plot to add to this which will help you analyze bias versus variances support the training error now once you're happy of training error is that it increases as the training set size increases because if you have only one example right let's see building a spam classifier and you have only one training example then any algorithm you know can fit one training example perfectly and so if your training set size is very small the training set error is usually zero right because if you have a five training examples probably you can fit all five examples perfectly and there's only if you have a bigger training set that it becomes harder for the learning algorithm to fit your training data that well right oh well in the linear regression case you have you have one example yeah you can Phyllis straight nine two data if you have two examples you can fit any model pretty much to the data and have zero training around there's only if you have a very large training set that a classifier like which is regression or linear regression may have a harder time fitting all of your training examples so that's why training error or average training error averaged over your training set generally increases as you increase the training set size so um now there are two characteristics of this plot that suggest that if you plot the learning curves if you see this dis pattern to suggest that theorem has a large bias problem right and the two properties written in the bottom one the weaker signal the one that's harder to rely on is that the development set error or the test set error is still decreasing as you increase the training set size so the green curve is still you know still looks like it's going down and so this suggests that if you increase the training set size and extrapolate further to the right that the curve would keep on going down this turns out to be a weaker signal because sometimes we look at the curve like that is actually quite hard to tell you have to extrapolate to the right so if you double the training set size how much further would the green curve go down then she kind of hard to tell so I find this a useful signal but sometimes it's been hard to judge you know exactly where the curve will go of you extrapolate to the right um the stronger signal is actually the second one the fact that there's a huge gap between your training error and your test set error or your training or your jeff's that there would better thing to look at is actually a stronger signal that um this particular learning algorithm has has high variance right because as you increase the training set size you find that the gap between training test error usually closes usually reduces and so there's no a lot of room for making your test set error become closer to your training and so that's if you see a learning curve like there's a strong side that um you have a variance problem okay now let's look at what the curve what the learning curve will look like um if you have a bias problem so this is a typical learning curve for high bias which is that's your def set error or your development cell the holocrons values and say their test error and you're hoping to hit a level of performance like that and your training error looks like that and so one sign that you have a high bias problem is that this algorithm is not even doing that well on the training set right even on the training set you know you're not achieving your desired level of performance and it's like not learn it imagine you're looking out from you see it was like this algorithm has seen these examples and even for examples the scene is not doing this was you were hoping so clearly the algorithms not fitting the data well enough so this is a sign that you have a high bias problem not in the features you're learning or it was too simple and and the other signal is that there's a very small gap between the training under test error right and you can imagine when you see a plot like this no matter how much more data you get right go ahead and extrapolate to the right as far as you want you know no matter how much more data you get no matter how far you extrapolate to the right of this plot they read the blue curve the training error is never gonna come back down to hit the desired level of performance and because the test set error is you know generally higher than your training set error no matter how much more data you have no matter how far you extrapolate to the right the error is never going to come down to your design level performance so if you get a training error and test that error curve that looks like this you kind of know that you know while getting more training data may help right the green curve could come down like a little bit if you get more training data the act of getting more training data by itself will never get you to where you to go okay so let's work through this example so for each of the four bullets here each of the four first four ideas fixes either a high variance or a high bias problem right so let's let's go through them and and say an Oscar for the first one do you think do you think it helps you fix high bias or high variance cool all right high variance right Amy well I say it will say well right anyone I'll say why yes right yeah right I guess if you're feeling a very high order polynomial that we goes like this if you have more data it will make it anything up then the warm at least oscillate so crazy you feel a higher order polynomial right and um if you look at the high variance curve wow it's not latency in my that's all for some reason right so this is the high variance plot and and if you have a learning algorithm high variance you can hopefully you know if you extrapolate to the right there is some hope that the green curve will keep on coming down so so getting more training data if you have high variance which is if you're in this situation it looks like it could happen hope this is worth trying great can't guarantee your work with worth trying oh I see yes sorry other this is good so let's see um the curse will look like this assuming that your training data is iid right training and death in Texas are always drawn from the same distribution there is learning theory that suggests that in most cases the being curve should decay as 1 over square root of M that's the rate that we should decay until an answer because some Bayes error that's what the learning theory says that doesn't make sense and sometimes and learning algorithms errors don't always go to zero right because sometimes they're sometimes on the data is just ambiguous oh my god I guess yeah my PhD students are including on them we do a lot of work on health care and sometimes you look in an x-ray it's just blurry and you could try to make a diagnosis right is there is there or I show all on ins working on predicting patients mortality or what's the chance of someone dying in the next year or so and sometimes the local a patient's medical record you just can't tell right what is you know will they pass away in the next year or so or look like x-ray you just can't tell is there is there a tumor or not because it's just blurry so learning others error I don't always tk20 but the theory says that as M increases should decay roughly a rate of 1 over square root of them to what that baseline error which is which is called Bayes error which is the best that you could possibly hope anything could do given how blurry the images are giving home nobody see the day tourists right all right um oh sorry gave the eyes their way okay so try small Estella features that fixes a high variance problem right and one concrete example would be if you have this data set and you're fitting uh you know tenth order polynomial and the curve all sleeves off of the place that's high variance you could say well maybe I don't need a tenth order polynomial may be actually use you only Wow sorry what's going on okay right so maybe you say maybe I don't need my features to be all of these things ten for the pollen you you know maybe if this is too high bearings and get rid of a lot of features and just use you know much smaller number of features right so that fixes high variance and then if you use the largest set of features faces high bias right cool right so that's if you are setting a straight line to the data there's not doing that well in go G maybe actually add a quadratic term and just add more features right so that fixes values and having email header features yep January I would try this if they try to reduce bias right and so in the workflow of how you develop a learning algorithm I would recommend that you yes so so one of the things about building learning algorithms is that for a new application problem it's difficult to know in advance if you're gonna run into high bias or high variance problem right it is actually very difficult to know in advance what's gonna go wrong with your learning algorithm and so the advice I tend to give is if you work on the new app Kayson implements a quick and dirty learning algorithm it have a quick and dirty implementation of something so you can run your learning algorithm just you know sort of logistic regression it starts with something simple and then run this bias-variance type of analysis to see sort of what went wrong and then use that to decide what to do next you go to more complex algorithms you try any more theater the one exception to this is if you're working on a domain in which you have a lot of experience right and so for example you know I've done the long work on speech recognition so because I've done that work I kind of have a sense of how much they just need it for a new application then then I might just build something more complicated from the get-go over here doing all your working on say face recognition and because you've rid of all the research papers you have a sense of how much data sneed it then maybe it's worth trying something because you're building on the body of knowledge but but if you work on something on a brand new application that you and maybe you know no one in the published academic literature has worked on or you don't totally trust the published results to be representative of your problem then I would usually recommend that you implement a build a quick and dirty implementation look at the buyers and barons of the algorithm and then use that to better decide what to try next so I think bias and variance is I think as that is really like the single most powerful - I know you know for analyzing the performance of learning algorithms that do this pre-emergent every single machine or any application there's one other pattern that I see quite often which is which which edges the second set which is which is a which is your optimization algorithm working so so let me most let me explain this with most being example right so um it turns out that when you implement a learning algorithm you often have a few guesses for what's wrong and if you can systematically test if that hypothesis is right before you spend a lot of work to try to fix it then you can be much more efficient so let's explain that with a concrete example so you understand those words I just said maybe they're a little bit abstract which is let's say that you know you tune your logistic regression the algorithm for a while and let's say the suni Russian gets two percent error on spam email and 2% error on non step right and it's okay to have two percent error on spam email maybe right you know so you have to read a little bit of spam email it's like you that's okay but two percent error on non-stem it's just not really acceptable because you're losing one in fifty important emails and let's say that you know your teammate right also try chains in SVM and they find in an SVM using a linear kernel guess ten percent error on spam but 0.01 percent error on non-stem right maybe not great but for this purpose of illustration let's say this is susceptible um but because it turns out logistic regression is more computationally efficient and and and it may be easier to update right here you get more examples to run a few more iterations of gradient descent and let's say you want to ship a logistic regression implementation rather than SVM implementation so what do you do thinks it turns out that one common question you have when training your learning algorithm is you often wonder is your optimization algorithm converging right so you know it's great in a sense it is it converging and so one thing you might do is draw a plot of the training optimization objective of J of theta or whatever you're maximizing all along likelihood that J of theta or whatever versus number of iterations and often the plot will look like that right and you know the curve is kind of going up but not that fast and if you train it twice as long or even ten times as long will that help right and again training maybe the algorithm for more durations it you know pretty much never hurts if you regular eyes the algorithm properly trained me the algorithm longer you know although almost always helps right pretty much never hurts but is the right thing to do to go and burn another 48 hours of you know CPU or GPU cycles to just train this thing longer in the hoping works better right maybe maybe not so is there a is there a systematic way to tell is there a better way to tell if you should invest a lot more time in running the optimization algorithm sometimes it's just hard to tell right so um now the other question that you sometimes wonder so a lot of where a lot of this iteration of deeper learning algorithms is smoking which to learn I was doing and just asking yourself one of my guess is for what could be wrong and maybe one of your guesses is well maybe optimizing the wrong cost function right so so here's what I mean um what you care about is this weight and accuracy criteria you know we're sort of sum over your def set or test set of you know weights on different examples of whether it gets it right where the weights are higher for non-standard span because you really wanna make sure you label non-spam e-mail correctly right so so maybe that's the way to accuracy criteria you care about but for logistic regression you're maximizing this cost function right love likelihood - this regularization term so you're optimizing J of theta when what you actually care about is a of theta so maybe our optimizing the wrong cost function and then one way to change the cost function would be to fiddle with the parameter lambda right that's one way to change the definition of J of theta another way to change J of theta is to just totally change the cost function you're maximizing like change it to the SVM objective right oh and then then part of that also means choosing the appropriate value for see okay and so there's a second diagnostic which I end up using which is we shall help you tell is the problem your optimization algorithm in other words is gradient ascent not converging or is the problem that you're just optimizing the wrong function right and then we'll see two examples of this is the first example okay um and so here's the diagnostic that can help you figure that out so just to summarize this scenario this um this example this running example we're using the SVM Opera homes which is Russian but you want to deploy this regression let's let theta SVM but the parameters learned by an SVM and instead of writing the SVM parameters as W and B I'm just going to write the linear SVM as your linear kernel you know using the logistic regression parameter ization right so you have a linear set of parameters and that's the thing the prrb the parameters learned by the just aggression right so it's just yeah regularize which is russian or basically just in Russian so you care about weights and accuracy and and the SVM outperforms basing is just regression okay so this is one sly summary of where we are in this example so how can you tell if the problem is your optimization algorithm meaning that you need to run gradient descent longer to actually maximize J of theta or is sorry and then right and this is the what BRR tries to maximize right so so how these hell we were were two possible hypotheses we want to distinguish between one is that the learning algorithm is not actually finding the value of theta that maximizes J of theta or if for some reason great innocent is not converging so that would be a problem the optimization algorithm that J of theta that you know promptly opt for the problem to be what the optimization algorithm means that if only we could have algorithm that maximizes J of theta we would do great but for some reason gradient descent isn't doing well that's one hypothesis the second hypothesis is that J of theta is just a wrong function to be optimizing it's just a bad choice of cost function that J of theta is too different from a of theta the maximizing J of theta doesn't give you you know a classifier that does well on a of theta which is what you actually care about okay any quiz a problem set up I wanna make sure people understand this this is race raise your hand if this makes sense most people okay cool okay good any questions about this problem set up oh thank you why not mess myself theta directly because F theta is non differentiable so we don't actually have you know does this indicate a function so we actually don't we it turns out maximizing a of theta explicitly is np-hard but just we just don't have great algorithms to trying to do that okay so it turns out there's a diagnostic you could use to distinguish between D sub to these two different problems and here's the diagnostic which is check the cost function that logistic regression is trying to maximize so J and compute that cost function on the parameters found by the SVM and compute that cost function on the parameters found by based on logistic regression and just see which which value is higher okay so there are two cases either this is greater or this is less than equal to right there just two possible cases so what I'm going to do is go over case one and case two corresponding to this greater than or it's less than equal then and let's let's see what that implies so on the next slide I'm going to copy over this equation right that's that's just a fact that the SVM does better then based on logistic regression on our problem so on the next I'm going to copy over this first equation and then we're going to consider these two cases separately so great - that would be case one and less than equal to will be case - okay so let me copy over these two equations in the next slide right so that's the first equation that i just copied over here and that's this is the greater than this case one okay so let's see how to interpret this in case one J of theta SVM is greater than J of theta be our right meaning that whatever the SVM was doing it found a value for theta which we've written as theta SVM and theta as VM has a higher value on the cost function J than theta be wrong but base in logistic regression was trying to maximize J of theta right I mean basically just Russian it's just using gradient descent to try to maximize J of theta and so under case one this shows that whatever the SVM was doing whatever your buddy implementing SVM did they managed to find a value for theta that actually achieves a higher value of J of theta then your implementation of base in logistic regression so this means that theta prr fails to maximize the cost function J and and the problem is with the optimization algorithm okay so this case one case two again I'm just copying over the first equation right because this is just part of our analysis to spot the problem set up but in case two is now the second line is now a less than or equal sign okay so let's see how to interpret this so under if we look at the second equation right the less than equal to sign it looks like J do the better job than the SVM maximizing jr. excuse me it looks like basically just regression did a better job than GSB maximising junior theater right so you know you talk based on religious aggression to maximize jf Theta and by golly I found it I found the value of theta does that it found the value that achieves a higher value of G of theta than whatever your buddy did using an SVM implementation so it actually did a good job trying to find the value of theta that dries up J of theta as much as possible but if you look at these two equations in combination what we have is that the SVM does worse on the cost function J but it does better on the thing you actually care about a of theta so what these two equations in combination tell you is that having the best value the highest value for J of theta does not correspond to having the best possible value for a of theta so tells you that maximizing J of theta doesn't mean you're doing a good job on a of theta and therefore maybe J of theta is not such a good thing to be maximizing because maximizing it doesn't actually give you the result you also really care about so under case two you can be convinced that J of theta is just an is not the best function to maximizing because getting high value of J of theta it doesn't get your high value from what you actually care about and so the problem is with the objective function of the maximization problem and maybe we should just find a different function to maximize okay so um any questions about this yeah let me come back to that yeah it's a complicated answer yeah all right actually let's do this first um so all right for these four bullets does it fix the optimization algorithm or does it fix the optimization objective first one does it fix the optimization algorithm or does it fix the automation or objective cool second one oh I don't know what's wrong with this thing it's so strange okay right does it fix the optimization algorithm or fix also an objective positive right so Newton's method still looks at the same cost function J of theta but in some cases it just optimizes it much more efficiently um this is a funny one usually you fiddle with lambda to trade-off bias and variance things right this is one way to change the optimization objective although usually you change lambda so it just buys in their hands rather than this right and then trying to use an SVM right would be one way to totally change the optimization objective okay so to answer the question just now sometimes we find you have the wrong optimization objective is that there there isn't always an obvious thing to do sometimes you have to brainstorm a few ideas that there isn't always one obvious thing to try but at least it tells you that that category of things are trying out different optimization objectives it's working well right all right so let's go through a more complex example though they'll you know incorporate some of these what's wrong I spray my laptop and wonder if life was so strange this is what I can do all right oh all right let's go for more complex example that will illustrate some of these concepts that we've been going through and just let you see another example of these things oh and and I find that dumb one thing I've learned as a teacher you know one of the ways for you to become good at this right is to go you know working a good AI group five years right because when you work on a good AI group for some several years then you have seen you know ten projects and that lets you gain that experience but it turns out that it takes I don't know depending on what they are group you work on it it takes if you work on a different project every year then in five years I guess you're working five projects something I actually don't know or maybe ten projects or something but one of the reasons that song in the way I try to explain this you actually go give specific scenarios with you this so that you know my peer sisters and I we spend actually we spent like many years working with Stanford autonomous helicopter but I'm trying to distill the key lessons down for you so that you don't need to work on a project for you know feeis to gain this experience but to give you some approximation to this knowledge and maybe twenty minutes where is the twenty minutes won't give you the depth of the three years of experience but have each other summarize a key lesson so that so you can learn from experience that others took years to develop um all right so uh this helicopter he sits in my office but but if you go to my office and you know grab this helicopter and and we asked you to write a piece of code to make this fly by yourself use the learning algorithm to make this slide by yourself how do you go about doing so so it turns out a good way to make a helicopter fly by itself is to use is to do the following um step one is build a computer simulator for a helicopter so you know that's actually a simulator right like a video game simulator of a helicopter um the advantage of using you know say a video game simulator helicopter is you could travel all things trash a lot in simulation you know which is cheap whereas crashing a helicopter in real life is this is slightly dangerous and also more expensive but so step one build a similar helicopter except to choose the cost function and for today I'm just using a relatively simple cost function which is squared error so you want the helicopter to fly the position X desired and your hug copter is dead yeah wanders off to some other place X so let's use a squared error to penalize it right when we talk about reinforcement learning towards the end of this quarter well why should go through the same example again by using the reinforcement or any terminology you understand is that slightly in a slightly deeper level and we'll go over this exact same example after you learned about reinforcement learning but we'll just go over a slightly simplified very simplified version today and so running reinforce a learning algorithm and whatever enforcement learning algorithm does is it tries to minimize that cost function J of theta and so you know and so you learn some set of parameters theta subscript R L for controlling the helicopter right and we're talked about reinforcement learning you know well you see all this redone with proper reinforcement learning notation where J is a reward function theta R is a control policy and so on but don't worry about that for now um so let's say you do this and the resulting controller right the way you fly the helicopter it gives much worse performance than your human pilot you know so the helicopter wobbles off of the place and then doesn't quite stay where you were hoping it will so what do you do next right well here are some options corresponding to the three steps above you could work on improving your simulator it turns out even today you know we've had helicopters for what I don't know I think that having all commercial houses around 1950 zip or technical trunk also for many decades now but air flow around the helicopter is very complicated and even today they're actually some details of how air flows are after the aerodynamics textbook you know that even aero-astro people write the explicitly our answer cannot fully explain so helicopter is incredibly complicated and there's almost unlimited Headroom for building better and more accurate simulators our copter so maybe you want to do that or maybe you think the cost function is messed up you know maybe a square error isn't the best metric right and it turns out you know the way a helicopter helicopter has a tail rotor that blows went to one side right so yes because the the main rotor spins in one direction if it only had a main rotor then the body was spinning in the opposite direction kind of equal and opposite reaction but in 12 right so the main rotor spins in one direction if it only had the main rotor the rotor on top and it just spun that there's a body and her cotton spin in the opposite direction so that's why you need a tear altar to blow air down off to one side to not make it spin the opposite direction but because of that it turns out a helicopter staying in place is actually tilted slightly to a side because the tail rotor blows air in one direction so it's pushing you off to one side so you have to tell you how to caulked in the opposite direction so the main role said blows air to one side - terrible - blows air to the other side so you actually stay in place right so how often is actually asymmetric the definite right is not the same so so so because of this complication maybe squared error isn't the best error because you know you're your orientation your optimal orientation is actually not zero right so so so maybe you should multiply the cost function or maybe you want to modify the reinforcement learning algorithm because you secretly suspect that your algorithm is not doing a great job of minimizing that cost function great that is not actually finding the value of theta that absolutely minimizes J of theta so it turns out that each one of these topics can easily be a PhD thesis and you could definitely work for six years on anyone these topics and the problem is you know so actually I actually know someone that wrote a PhD thesis on write improving helicopter simulator right but the problem is maybe a helicopter simulator is good enough and you can spend six years improving your helicopter simulator but will that actually get you there is and you can write and you can write a PhD season together PhD doing that maybe but if you go is not just a very PhD thesis and actually make your helicopter fly better is that he's not not totally clear right if that's the key thing for you to spend time on um so what I'd like to do is describe to you a set of Diagnostics that allows you to use this sort of logical step-by-step reasoning to debug which of these three things is what you should actually be spending time on right so is it possible for us to come up with a debugging process to logically reason so to select one of these things to work on and have conviction and then be relatively confident that this is a useful thing to work all right so here's how we're going to do it so just to summarize a scenario right the controller given by theta RL the false Paulie right so this is how I would reason through a learning algorithm right so suppose suppose all of these things were true suppose that again corresponding to the three steps in the previous slide suppose the helicopter simulator was accurate and suppose you know the learning algorithm correctly you know minimizes the cost function and suppose J of theta is a good cost function right if all of these things were true then the learn parameters should fly well on the actual helicopter right but it doesn't fire on the helicopter so one of these three things is false and our job as a figure out is its identified at least one of these three statements one two or three that is false because that that lets you sink your teeth into something that to work on right and I think to make an analogy to more conventional software debugging of a big complicated program and for some reason your program crashes you're like the cool downs or whatever if you can isolate this big complicated program into one component that crashes then you can focus your attention on that component that you know crashes for some reason and try to find a bug there right and so instead of trying to look over a huge codebase if you could do binary search or try to isolate the problem in a smaller part of your codebase then you could focus your debugging efforts on that padukone page try to figure out why it crashes and then fix that at first and after you fix that they might still crash then there might be a second problem that we work on but at least you know that trying to fix the first bug seems like it seems like a worthwhile thing to do right so what we're going to do is come up with a server the gradient design come over set of Diagnostics to isolate the problem to one of these three components okay so the first step is let's look at how well the algorithm flies in simulation right so what I said just now was you ran the algorithm and it resulted in a set of parameters that doesn't do well on your actual helicopter so the first thing I would do is just check how well does this thing even do in simulation right and there are two possible cases if it flies well in simulation but doesn't do well in real life then means something's wrong with a simulator right and it means is actually worth working on the simulator because you know if it's already working well in the simulator I mean what else could you expect to learn the very force of learning algorithms right you know you you told the reporter learning algorithm to go and fly well in the simulator because it's just training simulation it's already doing well in the simulator so there's not much to improve on their release is hard to improve on that but but but if you found it learning out if you learn the I room just one simulator but not in real life then this means that the simulator isn't matching real life well and so dish that does strong evidence there's strong grounds for you to spend some time to improve your simulator yeah yeah right is that it just repeats another camera is it is ever the case that it flies values away to about one roll life I wish that happen very rarely I I think if that happens I would I would still work on improving the simulator so there's actually once an era where that happens it turns out that when we train this helicopter in the simulator or really any robot simulator we often add a lava noise to the simulator because one lessons of learn is that if your simulator is noisy customizers are always wrong right I mean any digital simulation is only an approximation in real world so we tend to have a lot of noise so all of our simulators because we think that the learning algorithm is robust so all this noise you've thrown at it in simulation then whatever noise the real world throws at it it has a bigger chance at being robust to as well and so we tend to throw a lot of noise into Intel simulators and so one case where that does happen is when we find we threw too much noise on it in simulation and then that might be a sign we should dial back the noise a bit yeah all right cool oh so yeah so this first I know see tells you should work on improving simulation but just I think if there's a big mismatch between simulation performance and real work performance that's a good sign that you know that improve insulation second um this is actually very similar to the diagnostic we use on the spam you know based on logistic regression as a SVM example so what we're going to do is we're going to measure this equation and this is this again this very similar to our previous equation which is take the cost function - similar to previous example take the cost function J that reinforcement learning is total minimize right it's JJ of theta was a squared error right so take the cost function that reinforcement learning was total minimize and see if the human achieves better squared error than the reinforcement learning algorithm and just see you know this human flies better so let's measure the human performance on this squared error cost function and see which one does better so they're two cases that equation will be either less than or they'll be greater than or equal to is there less law greater than equal to so case one is say two human is less than sees me J of theta human is less than J of theta RL that would be this case then that tells you that the problem is with the reinforcement learning algorithm right that somehow the human achieves a lower squared error and so the learning algorithm is not finding the best possible squared error that is some other controller as evidenced by whatever the human is doing that actually achieves a lower cost function right so in this case we think the learning algorithm or the harbor enforcement learning algorithm is not doing a good job minimizing that and what were Connie reinforcement or anyhow really the other case would be of the sine inequality is the other way around right now in this case you can infer that the problem is in the cost function because what happens here is the humanists line better than your enforcement learning algorithm but the human is achieving what looks like a worse cause that your enforcement learning algorithm so what this tells you is that minimizing J of theta does not correspond to flying well right your learning algorithm achieves a better value for J of theta you know J of theta are out is actually smaller than one of the human is doing so the reinforcement learning algorithm as far as it knows this doing a great job because it's finding a value of theta where J of theta is really really small but in this last case you know that finding such a small value of J of theta doesn't correspond to flying well off because a human doesn't achieve such a good value in the cost function but the helicopter actually just looks better was flying in a more satisfactory way and that tells you that the squared error cost function is not the right cost function for what flying after it events right and so um through this set of Diagnostics you could decide which one of these three things improving the simulator improving in our our algorithm before so learning algorithm or improving the cost function is the thing you should work on and what happens in actually in this particular project and what often happens in machine learning applications is you run the set of Diagnostics and this actually happened when we're working on this helicopter we've run the set of Diagnostics and then one week we were saying yep simulated score the problem let's work on that and it would improve the simulator improves the simulator and after a couple weeks of work we run these Diagnostics and say oh it looks like the simulator is not good enough and maybe there's a problem with the our our algorithm then we work on that work on that improve that and after that after a while I'll say oh they'll say that's also good enough and the problem is in the cost function and sometimes the the location of the most acute problem shifts right after you've cleared out one set of problems it might be the case that now the ball Oneg is the simulator right and so I often use this workflow to constantly drive prioritization for what to work on next and and to answer a question just now about how do you find a new cost function it turns out find me a new cost function it is actually not that easy so as you want one my own former PhD students Adam coats through this type of process realize that finding a good cost function is actually really difficult because if you want a helicopter to fly maneuver you're like fire speed and they make a bank turn right like how do you math Matthew define what is an accurate bank determines thank you really difficult to write down an equation to specify what is a good way for how to fly like that and then do a turn it's just how do you specify what is a good turn so he wound up writing a research paper one of the best application paper more than I see now on how to define a good cost function it's actually pretty complicated but the reason he did it and it was a good use of his time was running Diagnostics like these which gave us confidence that this was actually a worthwhile problem and then that results in you know making real progress now about it yeah okay um any questions about this all right cool actually I think of all right anyway all right fun how the to do so let's not show this is fine yeah you guys saw some of these earlier all right so um a long time all right let's go for this so um in addition to these specific diagnoses of bias versus variance and optimization of the results musician objective oh sorry and when we do our L I want to just go through that example one more time so you see everything we just saw again after you've learned about reinforcement learning they turn this course L okay now in addition to these type of Diagnostics how to debug learning algorithms there's one other set of tools you find very useful which is error analysis tools which lets you figure out which is another way for you to figure out what's working what's not working or really what's not working the learning algorithm so let's let's go through a multi-beam example um so let's say you're building a you know like a security system so when someone walks in front of a door you unlock the door not based on whether or not you know that person is authorized to enter right that please and so let's say that so they're longer machine learning applications where is not just one learning algorithm right but instead you have a pipeline that's trained together many different steps so how do you build a face recognition algorithm to decide if someone approaching your front doors authorized unlock the door right well here's something you could do which is you start with a camera image like this and then um you could do pre-processing to remove the background so all that complicated color background let's get rid of that and it turns out that um when you have a camera against a static background right you could actually do this you know what a little bit of noise be relatively easy because if you have a fixed camera that's just like mounted you know on your doorframe it always sees the same background and so you can just look at what pixels have changed and and just keep the pixels that have changed compared to I mean right because you know this camera always sees that gray background and some Brown bench in the back and so just look at what pixels have changed the login and does that's background remove all right so business this this is this is actually feasible back just looking at what pixels have changed and keeping the pixels they've changed relative today and so after getting to the background you could run the face detection algorithm and then after detecting the face it turns out actually you know I've actually worked a bunch of face detection told a bunch of face face recognition systems it turns out that for some of the leading face recognition systems someone depends on details with some of them it turns out that the appearance of the eyes is a very important cue for recognizing people this is why if you cover your eyes a much all the time recognizing people as eyes are very distinct were people just segment out the eyes segments the nose and not the same you send out the know it's Halloween and then and then feed these features into some other algorithm say which is Russian that then you know finally outputs a label that says is this the person right that that you know you're authorized to open the door for um so it so in many learning algorithms you have a complicated pipeline like this of different components that that have to be strung together and you know if you read the newspaper articles about or if you read research papers in machine learning often the research papers will say oh we build a machine translation system where you train down gazillion you know sentences found on the internet and it does great and a pure end-to-end system so there's like one learning algorithm that sucks in an input like suck on an English sentence and spit on the French sentence or something right so this that's like one learning algorithm it turns out that for a lot of practical applications if you don't have a gazillion examples you end up designing much more complex machine learning pipelines like this where it's not just one monolithic learning algorithm but instead there are many different smaller components and I think in I think that you know the I think that having a lot of data is great right I love having more data but big data has also been a little bit overhyped and a lot of things you could do with small data sets as well and in the teams I work with we find that if you have a relatively small D that's that often you can still get great results you know my team's often get great results with 100 images a hundred change examples something but when you're a small data it often takes more insightful design about machine learning pipelines like this know we have a machine learning pipeline like this the things you want to do one thing you want to do is so so you you build the pipeline like this and it doesn't work right there's this common workflow you build a pipe build something doesn't work so you want to debug it so in order to decide which part of the pipeline to work on is very useful if you can look at your the error of your system and try to attribute the error to the different components so you can decide which component to work on X right and and there's a here I'll tell you true story you're the pre process background removal step right since you're getting rid of the background it turns out that there are a lot of details of how to do background removal for example the simple way to do it is to look at every pixel and just see which pixels have changed oh but it turns out that if there's a tree in the background that you know waves a little bit because the wind moves the tree and blows the leaves and branches around a little bit then sometimes the background pixels do change a little bit and so they're actually really complicates a background removal algorithms they tried to model basically the trees and the bushes moving around a little bit in background so you know that even though the pixels of the tree roos around this part of the background is just still get rid of it so background removal there's simple versions where you just look at each pixel and see how much has changed and they're incredibly complicated versions so I actually know someone that was trying to work on a problem like this and they decided to improve background removal algorithm and they actually does this per person actually literally wrote a PhD theses on background removal and so I'm glad you got a PhD but it turned but you know when I look at the problem he was actually trying to solve I don't think it actually moved in you know right so so this one I suspected the American I so laws you know you can still publish a paper and and it was technically innovative I was thanking very good technical work but but but but if so there you go suppose you favor great do that but there goes to build a better face recognition system then I would carefully ask which components should you actually spend your time to work all right um so yes what you can do with error analysis which is say your overall system has eighty five percent accuracy it's what I would do I would go in and in your depth set and your development said to hold our cross-validation settle right go in and for every one of your examples in the DEF set I would plug in the ground truth for the background meaning that rather than using a some you know approximate heuristic algorithm for roughly cleaning out the background which may or may not were that well I would just use Photoshop and for every example in the Deaf said I would give it the perfect background removal right so imagine if instead of some noisy harbor I'm trying to remove the background this step of the algorithm was just had perfect performance right and then you could give it perfect performance on your depth set on your test set just by using Photoshop to just tell it this is a background this is the foreground right and let's say that when you plug in this perfect background remove all the accuracy improves to eighty five point one percent and then you can keep on going from left to right in this pat pipeline which is now instead of using some learning algorithm to do face detection this just go in and for the test set you know modify kind of have the face detection algorithm cheat right having just memorized it right location for the face and the test seven just give it a perfect result in the test set so when I shade in these things that means I'm giving a perfect result right so let's just go in and on the test set give it the perfect face detection for every single example and and then look at the final output and see how that changes the accuracy of the final output right and then same for these components I segmentation no segmentation most segmentation and then and you do this one at a time and then finally for let's just regression if you give it the perfect output you know your your accuracy should be a hundred percent right so now what you can do is look at the sequence of of steps and see which one gave you the biggest gain and it looks like um in this example it looks like when you gave it perfect face detection the accuracy improved from eighty five point one to ninety one percent so roughly a six percent improvement and that tells you that if only you can improve your face detection algorithm maybe your overall system could get better by as much as six percent so this gives you faith that you know maybe it's worth improving on your face detection component and in contrast just tells you that even if you had perfect background removal it's only 0.1 percent better so maybe don't don't don't spend too much time on that and it looks like that when you gave it perfect eye segmentation it went up another four percent so maybe that's another good project to prioritize right um and if you're in a team one common structure would be to do this type of analysis and then have some people work on face detection some people work on our segmentation you could usually do a few things in parallel if you have a large area team but at least this should give you a sense of the relative position of the different things this question Yeah right so if you just cumulatively sighs just give a perfect eye cementation then add on top perfect no segmentation or do you give a perfect eye segmentation and then take that away and then give it perfect no segmentation um the way I presented it here is done cumulatively and and it turns out that uh let's see if you give it once you give it perfect face uh once you give it you know perfect things in the later stages maybe the earliest stages doesn't matter that much anymore so that's one pattern but it turns out that you could do it either way right for the eyes nose mouth you could do it cumulatively or one at a time and you'll probably get relatively similar results no guarantee you might get different results in terms of conclusions but but I think to the extent that you are wondering if doing a cumulative Leivers is not a cure to you might give you different results I would just do it both ways and then and then and then I think these um error analysis is not hard mathematical rule if that makes sense it's not that you do this and then there's a form that that tells you okay work on face detection right I think that this should be married with judgments on you know how hard do you think it is to improve face detection versus my segmentation right but this at least gives you a sense of it gives you a sense of privatization and it's worth doing this in multiple ways if if you think that if you're concerned I'm a discrepancy in accumulative in documented versions um so when you have a complex machine learning pipeline this type of error analysis helps you break down the error to attribute the error to different components which lets you focus your attention on what to work on Oh ready yeah if your face insertion accuracy and then you're Eric jumps what is that anything um it's not impossible for that to happen it would be quite rare I will so at the high level what I would do is go in and try to figure out what's going on actually I wouldn't ignore that so this is another thing I see sometimes the team gets a discovers a weird phenomena like that and they just ignore and they move on I wouldn't do that I would is actually go whenever you find one of these weird things I wouldn't gloss over the lurid I would go in and figure out what's going on this make sense it's like debugging software you know if you are if you're a chunk debugger piece of software and if whenever you move your mouse over you know some button some random pixel color changes you go home that's weird and then some people just ignore it and say oh well the user won't see this so what you're saying is quite rare but not impossible but I would I would I don't have an easy solution for how to figure out what's going on but I would I would want to figure out what's going on all right so one last thing before we break so error analysis helps figure out the difference between where you are now 85% overall system accuracy and 100% right so it tries to explain difference between where you are and you know perfect performance there's a different type of analysis called ablative analysis which figures out the difference between where you are and something much worse so so here's what I mean um so let's say that you built on less heat build a good anti-spam crossfire by adding lots of clever features so this is a Russian right so you know spelling correction because families tried to misspell words to mess up the tokenizer to make words not you know spammy was not look like spam send the host features what machines email come from Eva Heather features you could have a parser from NLP pasta text use JavaScript pauses understand write or even couldn't fetch the webpages to know that email refers to and pause that and the question is how what the ditch these components really hope and it turns out if you're writing a research paper you know sometimes you rather use me to say hey look I build a great spam classifier and that's okay that's like a nice result to have but if you can explain to your reader either in a research paper or or in a class project report like a term project what what actually made a difference that conveys a lot of insight as well so um so say simple which is Russian whether all these clever features got ninety four percent performance and with all of your addition of all these clever features you've got ninety nine percent accuracy so in ablative analysis what you would do is remove the components one at a time to see how it breaks right so Justin so just now we were adding to the system by making components perfect with error analysis is how it improves here we're gonna remove things one at a time did not mean to remove that to figure out what's going on with PowerPoint all right remove things one at a time to see how it breaks so lets you remove spelling correction and as a set features the error goes down like that then let's remove the center host features room with email header features and so on until when you remove all of these features you end up there and again you could do this cumulative lee or the roof one and put it back we want to put back you know you could do it both ways and see if they give you slightly different insights and so the conclusion from this peculiar analysis is that the biggest gap is from the text positive features because when you remove that the error the accuracy went down by four percent so you know this is strong evidence oh you want to publish a paper you can say write text positive features sick of weekly improves spam filter accuracy and then that level of insight and then if you're working on the spam filter for many years right you know they're they're out there there are really important applications where sometimes the same team will work on for many years so this types of error analysis gives you intuition about what's important and what's not and helps you decide to maybe even double down on to expose the features or maybe ever or maybe a dissent the host features as to competition expensive to compute tells you maybe just get rid of that then without too much harm but and also if you're publishing paper or sending a report this gives much more insight into your replicates okay all right um so that's it for error analysis and if later analysis I hope this was useful real class projects as well take one last question oh right oh yeah how are you - zo there was no systematic way if you can have a systematic way you do that the other way to non-cumulative would remove one coming back when you might put it back so either way it works alright let's break and problem set who is due tonight a friendly reminder and prom said three will be posted in the next like several tens of minutes okay thanks everyone