welcome back everyone hope you had a good Thanksgiving these chairs here um by the way not sure thanks awning not sure you guys funny didn't use but in reinforcement learning which had a lot about robotics right then one of the you know cost a problem lot of people use reinforcement to solve is robotics and I think back in May the insight Mars Lander had launched from here in California and is about to make an attempt at landing on the planet Mars in the next two and a half hours or so so excited about that I think is actually one of the grandest applications of robotics because you know with what 20 minute life speed from Earth to Mars you know once it starts this landing there's nothing anyone on the earth can do and so I think is actually one the most exciting applications of a Tong's robotics but you launched this thing is now about 20 20 light minutes away from Planet Earth so you actually can't control it in real time and you just have to hope like crazy that your software works well enough but land on this planet you know and stuff well we'll find out a little bit afternoon if the landing have been successfully or longer as you know III think um sir I just get excited about stuff like this I hope you guys do too and but they'll see they're from California I mean take some pride that it launched from my home state of California and it's now nearing is uh landing on Mars alright so um what I want to do today is continue our discussion on reinforcement learning do a quick recap of the MDP or the Markov decision process framework and then we'll start to talk about algorithms for solving them DPS in particular need to define something called the value function which tells you how good it is to be in different states of the MDP and then we'll define the value function and then talk about an algorithm called Valley iteration for computing the value function and this will help us figure out how to actually find a good control or a finally good policy for them DP and it will wrap up with learning state transition probabilities and how to put alson together into an actual reinforcement learning algorithm that you can implement to recap our motivating example run the example from the last time from before Thanksgiving was this 11 state MVP and we said that an MDP comprises a five tuple list of five things with States so that example had 11 States actions and in this example the actions were the compass direction north south east and west we can try to go in each of the four compass directions the state transition probabilities and example if the robot attempts to go north it has 80% chance of heading north and 0.1% chance of viewing off to the left and the point one chance of veering off to the right gamma is a number slightly less than one usually say less than one there's a discount factor think of the 0.99 and R is the reward function that helps us specify where we want the robot to end up and so what we said last time was that the way an MDP works is you start up in some state as zero honestly better you choose an action a zero and as a result of that it transitions the new state s 1 which is drawn according to p FS 0 a 0 and then you choose a new action a 1 and as a result the MDP transition system new state PF s 1 a 1 and the total payoff is the sum of rewards and the goal is to come up with a way and formally that goes to come over policy pi which is a mapping from the states to the actions that will tell you how to choose actions from whatever state you're in so that the policy maximizes the expected value of the total payoff ok and so I think lost time I I kind of claimed that this is the optimal policy for this MVP and what this means for example is if you look at this state but this policy is telling you that fire 3 comma 1 equals West I guess oh you can write west or left or what do you call that left arrow right we're from the state from the state 3 1 you know the best action to take this to go left us to go west and so if you're executing this policy what that means is that on every step the action you choose would be you know PI right of the of the state that you're in ok so what I'd like to do is now define the value function so how did I come up with this right what I like to do is have you learn given an MDP given this five tuple how do you compute the octal policy and one of the challenges with finding the optimal policy is that you know there's a there's an exponentially large number of possible policies right if you have eleven states and four actions per state the number of possible policies is four to the power of 11 which is not that Bay because 11 is a small MDP right this is the number of policies possible policies for an MTP is combinatorially large is a number of actions the power a number of states so how do you find the best policy so what you learned today is how to compute the auto policy now in order to develop an algorithm for computing an auto policy we'll need to define three things so just as a roadmap what I'm about to do is define V PI V Star and PI star okay and based on these definitions will see that will come to definition derived that pi-star is the auto policy okay but so let's let's go through these few definitions first V PI so for a policy PI V PI is a function mapping from States to the rails is such that V PI of s is the expected total payoff for starting and state that's executing PI and so sometimes you write this as V PI of s is the expected well total payoff given that you execute the policy PI and the initial state as 0 is equal to s so the definition of a V PI this is called the value function for a policy this is called the value function for the policy PI ok and so what the value function for a policy PI denoted be pious is it tells you for any state you might start it there's a function mapping of states the rewards write for any say you might start saying what's the expected total payoff if you start off your robot in that state and if you execute the policy PI and XC the policy PI means take actions according to the policy PI right so here's a here's a specific example this policy so let's consider the following policy PI right so this is now the great policy right you're from some of these days it looks like is heading to the minus one reward oh sorry Segura the reward was plus one we get here and technically this called an absorbing state meaning that if you ever get to the plus one to the minus one then the world ends and then no more rewards or penalties after that right so but so there's actually not a very good policy so policy is any function mapping from the states to the actions so this is one policy that says are in this state you know this policy tells you in this state for one go north which is actually pretty bad thing to do right it's take you to the minus one reward so this is not a great policy but but just just a policy and V PI for this policy don't worry too much about the specific numbers but yo if you look at this policy you see that from this set of states it's pretty efficient at getting you to the really bad reward and from this set of states is pretty efficient at getting you to the good reward right what's some mixing because of their noise in the robotic veering off to the side and so you know these numbers are all negative and those numbers are at least somewhat positive right so but so V PI is just um if you start from say this state from the state 1 1 on expectation you're expected some of this counter Wars will be negative point-eight so that's what be pious no the following equation governs the value function it's called it's called a bellman equation and this is that your expected payoff at a given stage is the reward that you receive plus a discount factor times the future rewards so let me let me actually explain the intuition behind is right which is that let's say you start off at some state as 0 right so oh and again let's let's say s is equal to s 0 so V PI of s it is equal to well just for your robots waking up in that I'm gonna add to it in a second ok but just for the sake just for this for the fact that your robot woke up in this state s you get the immediate you get it reward RF as zero right away just as something's called this is also called the immediate reward because you know just for the for the good fortune of bad fortune of starting off in this state the robot gets a reward right away this is called the immediate reward and then it will take some action and get to some new stage s1 well receive you know gamma times the reward of s1 and then right and then I'll get some future reward at the next step and so on and just to flesh out the definition the value function V PI is really this given that you execute the policy PI and s0 equals s right and you start off in the same as zero now what I'm going to do is we write this part of the equation a little bit I'm going to factor out I'm just going to take the rest of this and factor out one factor of gamma so let me put parentheses around this right and just take out gamma there okay so I'm just you know taking this PVC this was gamma squared right but I think the parentheses here I'm just taking out one factor of gamma that multiplies in the restaurant equation okay does any sense no so as in gamma R of s 1 plus gamma squared R of s 2 equals gamma R times R of s 1 ok so that's that's what I did down there right just factor out 1 1 factor of gamma and so this is the the value of state s is the immediate reward plus gamma times the expected future rewards right so this the expected value of this is really V PI of s 1 right so this and so the second term here this is the expected future rewards so pelvis equation says that the value of a state the value the expected total payoff you get if your robot wakes up in the state s is the immediate reward plus gamma times the expected future rewards okay right and and this thing under you know above the curly braces is really asking if you rope out wakes up at the state s1 and excuse PI what is the expected total payoff right and this what if you robot wakes I'm gonna state s1 then you know take an action get us to take an actually get to s3 and this is the sum of this counter war sort of it starts off with the state s1 okay so this base on this you can write out well these justify Bellman's equations which is oh and and the mapping from this equation to this equation all right the mapping from the equation on top to the equation that bottom is that s maps to s 0 and s prime master s 1 right and and so if we have that be PI of s equals so the value of state s is our Vespas V PI of s prime where this is s 0 and this is s 1 and and in the notation of MDP if you want to write a long sequence of States we tend to use s 0 s 1 s 2 s 3 and s 4 and so on but if you have want to look at just the current state and the state you get 2 after 1 times that we tend to use s and s prime for that so that's why this is mapping between these two pieces notation so s prime is say you get two after one step well let's see one is s prime drawn from write this so does the state s prime or s 1 is the state you get to after 1 time step so what is what is the distribution to s prime is drawn from s prime is drawn from P of what okay PFS cool because in state s you will take action a equals PI of s right so we're executing the policy pi so that means that when you're in the state s you're going to take the action a given by PI of s goes PI of s tells you please take this action a when you're in state s and so s prime is drawn from P of s a where a is equal to PI of s right because the cause that's the action you took which is why s Prime the state you get to after one time step is drawn from the distribution s PI of s so putting all that together that's why well I just write the other game where belma's equations which is V PI of s equals R of s plus the discount factor times the expected value of V PI of s prime and so this term here is just some of the S prime be s PI of s be PI of s Prime okay so that underlying term I guess is this underlying term here um now notice that this gives you a linear system of equations for actually solving for the value function so let's say I give you a policy it could be a good policy could be a bad policy and you want to solve the PI of s what this does is if you think of the PI of s as the unknowns you're trying to solve for given PI write these equations these are the pelvis equations defines a linear system of equations in terms of the PI of s as the very values to be solved for so maybe here's a here's a specific example let's take the state v1 right so this is the state 3 1 what this what balance equation this tells us is the PI of the state 3 comma 1 is equal to the mediator what you get at the state 3 1 plus the discount factor times well some of s prime PS PI of s be PI of s prime right so oh and let's say that PI of 3 1 is no so let's see try to go north if you try to go north from the state then you have a 0.8 chance of getting to 3/2 plus a 0.1 chance of veering left plus 0.1 chance of veering right so that's what balance equation says about these values right and if your goal is to solve for the value function then these things I'm just circling in purple are the unknown variables and if you have eleven states like in our MDP then this gives you a system of eleven linear equations with eleven unknowns and so using server linear algebra solver you can solve explicitly for the value of these eleven unknowns so they way you it so let's say give you a policy PI you know any policy PI the way you can solve for the value function is create an eleven dimensional vector with V PI of you know one one V PI of 1 2 and so on down to the PI of whether is the last thing you have eleven state so V PI of easy or whatever for three right so if you want to solve for those eleven numbers I wrote up just in terms of defining V PI what you can do is I'll give you a policy PI you can then construct an eleven dimensional vector you know 11 dimensional vector of unknown values that you want to solve for and balanced equations for each of the eleven states for each of the eleven states you could plug in on the left hand side just gives you one equation for how one of the values is determined as a linear function of a few other of the values in this vector okay and so what this does is it sets up a linear system of equations with eleven variables in eleven unknowns right and using a linear algebra solver you you will be able to solve this linear system of equations does make sense okay all right and so this works so lousy about this piece yeah if you have eleven states you know it takes this takes almost it takes almost no time right and the computer to solve and then this is an eleven equation so that's how you would actually get those values if you have a called on to solve for V pi okay actually the there why just say make sense raise your hand if what I just explained made sense like cool awesome thing all right good so moving on our roadmap will define V PI let's now define V Star so so V Star is the optimal value function and we'll define it as V star of s equals max overall policies PI of V PI one of the slightly confusing things about reinforced with an in terminology is that there two types of value function there's value function for a given policy PI and that's the optimal value function V star so both of these are called value functions but one is a value function for a specific policy could be a great policy could be terrible policy could be also policy the other is V star which is the optimal optimal value function so V Star is defined as locally value for you know any look across all of the possible policies you could have all four to eleven where all the company totally large number of possible policy so there's MVP and these star affairs is well let's just take the max which is of all the possible of all the policies you know anyone could implement of all the possible policies let's take the value of the best possible policy for that state so that's V star okay that's the all Tolle also a value function and it turns out that there is a different version of bellman equations for this and again there's a balance equations for be pi/4 value of a policy and then there's a different version of bellman equations for the optimal value function right so just as the two versions of value functions there are two versions of balance equations but let me just write this out hopefully this will make sense actually let's think this through so let's say you start off your robot in a state s what is the best possible expected some of this counselor was what's the best possible payoff it again right well just for the privilege of waking up in state s the robot will receive an immediate what R of s and then it has to take some action and after taking some action it will get to some other state as a prime you know and after some other state s prime it will receive future expected rewards v-star best prime and we have to discount that by camera right so so well the state s prime was arrived at by you're taking some action a from the initial state and so whatever the action is you know but if you take action a so if you take an action a in the state s then your total payoff will be expected total payoff will be the immediate reward plus gamma times the expected value of the future payoff but what is the action a that we should plug in here right well the optimal action to take in the MDP is whatever action maximizes your expected total payoff maximize you expected some rewards which is why the action you want to plug in is just whatever action a maximizes that okay so this is um Domus equations for the optimal value function which says that the best possible expected total payoff you could receive starting from state s is the immediate reward R of s plus max over all possible actions of whatever action allows you to maximize you know your expected total payoff expect a future payoff okay so this is the expected future payoff expected future reward now based on the argument we just went through this allows us to figure out how to compute PI star of s as well right which is let's say let's say we have a way of computing V star of s right we don't yet but let's say I tell you what does V Sarvis and then I'll see you you know what is the action you should take in a given stage so remember PI spy star of PI star is going to auto policy and so what should PI star vests be right which is let's say let's say we're we're computing V Star and I now ask you hey my robots in state s what is the best action I should take from the state s right then how do I decide what action to take in the state yes well what would think is the best action to take from this state and the answer is almost given in the equation of oh yeah yeah cool awesome right so the best action to take and state us and best means maximizing expected total payoff but the option that maximizes your expenses total payoff is you know well whatever action we were choosing a up here and so it's just long max over a and because gamma is just a constant that doesn't affect the arcmap usually we just we just eliminate that this is just a positive number right so this gives us the strategy we will use for finding the also policy for an MVP which is we're going to find a way to compute V Star of S which we don't have a way of doing yet rightly star was defined as a max over combinatorially or exponentially large number policies so we don't have way of computing piece not yet but if we can find a way to compute B star then you know using this equation certainly just scratch themself using this equation gives you a way for every state of every state s pretty efficiently computes this augment and therefore figure out what is the optimal action for every state all right so all right so just practice with confusing notation all right let's see if you understand this equation I'm just claiming this I'm not proving this but for every state as V Star of s equals G of Pi star of s is greater than a PI of s all right for every policy Pyne every state s okay so I hope this equation makes sense this is what I'm claiming I didn't prove this one claiming is that the October value for state s is this is the optimal value function on the left this is the value function for pi star so this is this is about optimal value function this is the value function for a specific policy PI where the policy PI happens to be PI star and so what I'm claiming here is that what what I'm writing here is that the optimal value for state s is equal to the value function for PI star applied to the state s and this is great sin equal to V PI of s for any other policy by so the strategy you can use for finding for also policy is one v V star to you know use the R max equation to find pi star okay and so what we're going to do is well step to write we know how to do from the optimized equation so what we're gonna do is top an algorithm for actually computing visa because if you can compute V song then this equation helps allows you to pretty quickly find the optimal action for every state so um so value iteration is as an album you can use to to find V star so let me just write out the algorithm so in the value iteration algorithm you initialize the estimated value of every state to zero and then you update these estimated values using Bellman's equations and this is the optimal value function the V star version of Bellman's equations and so to be concrete about how you implement this you have um inferencing this right if you're implying didn't Python what you would do is create an 11 dimensional vector to store all the values of V of s so you create a you know 11 dimensional vector right that that represents V of 1 1 V of 1 2 you know down to V over 4 3 right so this is um 11 dimensional vector corresponding to the 11 states oh I'm sorry I should wait did I say 11 where 10 stays in the MTB don't we wait yes we have 10 sees I've been saying 11 all along sorry okay 10 oh yes you're right sorry yes okay sorry so 11 state MDP serie credit initial credit 11 dimensional vector and initialize all of these values to 0 and then you will repeatedly update the estimated value of every state according to balance equations right and so they're there they're actually two ways to interpret this and similar to some of the gradient descent right we've written out you know a gradient descent rule for updating the theta the the vector parameters theta and what you do is you know and you have and what you do is you update all of the components of theta simultaneously right and so that's called a synchronous update in gradient descent so one way to so the way you would update this equation in what's called a synchronous update will behave you compute the right hand side for all 11 states and then you simultaneously overwrite all 11 values at the same time and then you compute all 11 values for the right hand side and then you're simultaneously update all 11 values okay the alternative would be an asynchronous update and then a synchronous update what you do is you compute V f11 right and the value of V of 1 1 depends on some of the other values on the right hand side right but in a synchronous update you compute V of 1 1 and then you would overwrite this value first and then you use that equation to compute V of 1 2 and then you update this and then you update these one at a time and the difference between synchronous and asynchronous is you know if you're using asynchronous update by the time you're using V or 4/3 which depends on some of the earlier values you'd be using a new and refresh value of some of the earlier values on your list ok it turns out that value iteration works fine with either synchronous up these or asynchronous updates but further but because it vectorized is better because you can use more efficient matrix operations most people use the synchronous update but it turns out that the algorithm will work whether using is synchronous or asynchronous update sorry is unless unless otherwise you know stated you should usually assume that when I talk about validation I'm referring to synchronous update where you compute all the values all 11 values using the and then update all 11 values at the same time ok is there a question just now so my that yeah yeah yes so I think they're there yes so how do you represent the absorbing state the sink say we go to plus or minus one day the world ends in this framework one way to code that up would be to say that the state has inference from that to any other state is zero that's one way to that that would work another way would be less done less often maybe mathematical but clean up and not how people tend to do this it would be to take your let me say MDP and then create at all state and the tall state always goes back to itself with no further than what so do both both of these would give you the same result though mat batty is pretty more convenient to just set you know PFF say s prime equals 0 for all other states it's not quite safe hasn't already but that that will give you the ranch as well all right cool so just as a point of notation if you're using synchronous updates you can think of this as taking the old value function o estimate right and using it to compute the new estimates so this this you know assuming the synchronous update you have some previous 11 dimensional vector with your estimate of the value from the previous iteration and after doing one iteration of this you have a new set of estimate so one step of this algorithm is sometimes called via bellman back of operator and so where you update the equals b.o.b right where we're now he is a 11 dimensional vector so you have an O the leverage the original vector compute the bellmen backup operator was just that equation there and update the according to B and so one thing that you see in the problem set is is showing that this will make a BFS condors to be stock so it turns out that okay so it turns out that you can prove and you see more details that this is a problem set that by repeatedly enforcing Bellman's equations that this equate this this algorithm will cause your vector of eleven value so cause V to converge to the optimal value function V star okay and more details you see the homework Illumina lecture notes and it turns out this algorithm actually converges quite quickly right so to give you a flavor I think that uh with the discount factor if the discount factor is 0.99 it turns out that you can show that the error reduces your by a factor of point 99 on every iteration and so V actually converges quite quickly dramatically quickly if you are exponentially quickly to the October value function V Star and service you know the discount factor is 0.99 there was like a few where behind your iterations there are a few hundred iterations v p-- would be very close to be stock okay and and the discount factors point nine then with just you know ten or few dozens of innovations would be very close to be saw so this outer measure converges quite quickly to be stock so let's see [Applause] so just to put everything together if you if you run value iteration on debt MDP you end up with this so this is B star so solicited eleven numbers telling you what is the optimal expected payoff for starting off in into the eleven possible states and so I had previously said I think I said last week of the week before Thanksgiving that this is the optimal policy so you know let's just use as a case study how you compute the optimal action for that state given this v-star all right well what you do is you actually just use this equation and so if you were to go Wes then if you were to compute I guess this term sum of s prime Wetzel left I guess right P of si s prime B star of s prime is equal to if you were to go west you have a right so if you're in this state and if you attempt to go left then there's a point a chance you end up there with a visa 0.75 there's a point 1 chance you know if you try to go left this point one chance you veer off to the north and have a 0.69 and then there's a point 1 chance that you actually go south and bounce off the wall and end up with 0.71 and so do you expected future rewards expected future payoff given this equation is that if you tend to go Wes you end up with 0.7 for 0 as expected future rewards whereas you were to go north if you do a similar computation you know so 0.8 times point 6 9 plus point 1 times 175 plus point 1 times 24 9 the appropriate way to the average you find that is equal to 0.67 6 which is why the expected future rewards so if you go Wes it'll know left is 0.74 0 which is quite a bit higher than if you go north which is why we can conclude based on this low calculation that the also policy is to go left at that state and then really and technically you check north south east and west and make sure that going west gives a high reward and that's how you can conclude that going west is actually the better action at this state okay so that's value iteration and based on this if you are given an MDP you can implement this solve a V Star and be able to compute PI sock a few more things I'll go over but before I move on let me check are there any questions oh sure yep Islamic state is always finite so in what we're discussing so far yes but what we'll see on Wednesday is how to generalize this framework well looted this a little bit later but it turns out if you have a continuous state MDP one of the things that's often done i guess is to discretize into finite number of states but then there are also some other versions of you know value duration that applies directly to continuous states as well so what I described is an algorithm called value iteration the other on a common sort of textbook algorithm for solving for MVPs is called policy iteration and let me just well just write out what the algorithm is so here's the algorithm which is um you know initialize PI randomly okay so let's see what this algorithm does so let's talk about pros and cons evaluation versus policy aeration a little bit in policy iteration instead of solving for the optimal policy vsauce in that the iteration a focus of attention was v star right where you know you do a lot of work to try to find the value function and then once you solve for v song you then figure out the best policy in policy iteration the focus of attention is on the policy pi rather than the value function and so initialize pi randomly so that means for each of the 11 states pick a random action it's a random initial time and then we're going to repeatedly carry out these two steps the first step is solve for the value function for the policy pi right I remember for V PI this was a linear system of equations right with eleven variables with eleven unknowns in it was a linear system of eleven equations with eleven unknowns and so using a sort of linear algebra solver or a linear equation solver given a fixed policy PI you could just you know at the cost of inverting a matrix roughly right you can solve for you can solve for all of these eleven values and so in policy iteration you would you know use a linear solver to solve for the optimal value function for this policy pi that we just randomly initialized and then set V to be the value function for that policy okay and so this is done quite efficiently with a linear solver and then the second step of policy duration is pretend that V is the optimal value function and update PI of s you know using the balanced equations for the octal value function very updated as you saw right how do you update the higher best and then you iterate and then given a new policy you then solve that linear system equations for your new policy PI to get a new B PI and you keep on iterating these two steps until converges ok yeah yep yes that's right so in in value yeah yeah so in in value iteration in value iteration think evaluator HS waiting to the end to compute PI of s very soft wavy stop first and compute PI of s whereas in policy iteration we're coming up with a new policy on every single iteration okay so um pros and cons of poly and it turns out that this algorithm will also converge to the optimal policy pros and cons of policy iteration versus valuation policy duration requires solving this linear system of equations in order to get B PI and so it turns out that if you have a relatively small state space like if you have 11 states is really easy to solve a linear system of equations you know if 11 equations in order to get V PI and so if you're relatively small set of states like eleven states are really anything you know like a few hundred States policy raishin we're quite quickly but if you have a relatively large set of states you know like ten thousand stays or a million states then this step would be much slower at least if you do it right by solving the system of equations and then I would favor a value iteration over policy iterations so for larger problems usually value iteration will usually I would use value iteration because solving this linear system of equations you know this is pretty expensive if it's a good million by there's a million equations a million unknowns that's quite expensive but if in Lebanon stays in Lebanon knows there's very small system equations and then one one other pros and cons one of the difference that's maybe maybe more academic than practical but it turns out that if you use value iteration V will converge to what V Star but they won't ever get to exactly the star right so just as if you apply gradient descent for linear regression gradient descent gets closer and closer and closer to the global optimum but it never you know guess exactly the global optimum it just gets really really close really fast actually great in the sand actually turns out as an topically converges geometrically quickly really quickly right but but never quite gets you know definitively to the optimal to the one optimal value whereas you saw using normals equations it just jump straight to the optimal value and there's no you know converging slowly and so value duration converges to or V star but it doesn't ever end up at exactly the value V star this difference may be a bit epidemic because in practice it doesn't matter right but in policy iteration if you innovate this algorithm then after a finite number of iterations this album will stop changing meaning that after certain number of iterations PI of s would just not change anymore right so you find higher best update the value function and then after another iteration when you take these out maxes you end up with exactly the same policy and so this just salsa the also a value and the also policy and they just you know it doesn't converge it doesn't does converge to what the also value it just gets the optimal value when it when it converges okay so I think in practice I actually see value iteration use much more because solving this linear equations gets expensive you know if you have a large estate space but valuation it's usually policy I see valuation use much more but if you have a small problem you know I think you could also use policy iteration which might converse a little bit faster if you have a small problem so the last thing is kind of putting it together right and what if you don't know so it turns out that when you apply this to a practical problem you know in robotics right one common scenario you run into is if you do not know what is P of Si if you don't know the state transition probabilities right so when we built the MDP we said well let's say the robots if you go off you know has a point a chance a great knife and a point one chance of varying off so therefore rights if you actually the game this is a very simplified robot but if you build a actual robot to build a you know helicopter or whatever play play chess against an opponent the state-transition properties are often not known in advance and so in many MVP implementations you need to estimate this from data and so the workflow of many many reinforcement learning projects will be that you will have some policy and have the robot run around you know just have a robot run around a maze and counter of all the times you had to take the action north how often did it actually go know of and how often do they fear often left or right right so you use those statistics as state transition probabilities so let me just write this out so you estimate so after you know taking maybe a random policy to take some policy execute some policy in the MD for a while and then you would estimate this from data and so the obvious formula would be SVP of SAS prime to be number of times took action a and state s and got to s Prime and divide that by the number of times you took action that's right so TFSAs prime estimate there's actually a massive likely estimate when you look at the number of times you took action in state s and that was the fraction of times you got to this day that's prime right or 1 over s in a common you know heuristic is if you've never taken this action in just a before if you if the number of times you try action in state as a zero so you've never tried this action this state so you have no idea what's gonna do then just assume that the state transition probability is 1 over 11 right then you're randomly takes you to endlessly so this would be rather common heuristics that people use when implementing reports or learning algorithms and it turns out that you can use the paths moving for this if you wish but you don't have to because so you're in the past moving right Sofia you know adds one to the numerator and and 11 to the denominator would be if you were to use Laplace smoothing which a voice the problems of zero over zeroes as well but it turns out that unlike the naive Bayes algorithm these solvers MDPs are not that sensitive to 0 values so if if one your estimates are probably is zero you know unlike naive Bayes we're having a zero probability was very problematic for the classifications made by naive Bayes it turns out that MDP solvers including evaluation and policy duration they do not give sort of nonsensical / horrible results just because of a few probabilities are exactly zero and so in practice you know you can use the Laplace moving if you wish but because the reinforcement learning algorithms don't don't perform that badly of these estimates are often well below zero in practice the past moving is not commonly unison what I just wrote is it's more common so to put it together if I give you a robot and ask you to implement a MTP solver to find a good policy for this robot what you would do is the following take actions respect to some policy PI to get experience in the MDP so go ahead and let your robot loose and have it ask you some policy for a while and then update estimates of PFS a based on the observations where the robot goes when takes different states update update SMS app EFSA solve Velma's equation using value iteration to get V and then I'll update so this is the value generation way of putting it together if you want to plug in policy innovation instead and just that that's also okay but so if you actually get a robot you know right if you actually get a robot where you do not know in advance the state transition probabilities then this is what you would do in order to enter in a few times I guess right repeatedly finally find a final policy given your carbon estimate of the state transition probabilities get some experience update your S Pen is finally your policy and kind of repeat this process until hopefully converges to good policy now just to add more color more richness to this we usually think of we usually think of the reward function as being given right as part of the problem specification but sometimes you see that the reward function may be unknown and so for example if you're building a stock trading application and the reward is the returns on a certain day it may not be a function of the statement may be a little bit random or if you're robots is you know running around but depend on where it goes it may hit different bumps in the road and you want to give her the penalty every time it hits a bump build self-driving car right and every time it hits a bump hits a pothole you give the negative reward then sometimes the rewards are random function of the environment and so sometimes you can also estimate the expected value of a reward but but in some applications of the reward is the random function the state then this process allows you to also estimate the expected value the reward from every state and then running this more oq2 okay yeah yep cool great question so let me let me talk about exploration right so it turns out that um this one so it turns out this algorithm will work okay for some problems but there's one other again to add richness to this there's one other issue that this is not solving which is the exploration problem and possible earnings sometimes you hear the term exploration versus exploitation which is let me use a different MVP example which is um if your robot you know starts off here and if there is a plus-one reward here right and maybe a +10 the water here if just by chance doing the first time you run the robot it happens to find its way to the +1 then if you run this algorithm it may figure out that going to the +1 is a good way right over we're giving a discount factor does a feel so in charge of minus 0.02 on every step so if just by chance your robot happens to find this way to the +1 the first few times you run this algorithm then this algorithm is yourself locally greedy right it may figure out that this is a great way to get to +1 reward and in the world ends it stops getting these minus 0.02 surcharges but fuel and so this particular algorithm may converge to a bad you know kind of local optima where it's always heading to the +1 and as it has a +1 sometimes OVR randomly right and you look a little bit more experienced in the right half of the state space and end up with pretty good estimate of what happens in the right of the state space and and it may never find this hard to define +10 pilot goat over on the lower left okay so this problem is sometimes called actually wrong it is called the exploration versus exploitation problem which is when you're acting on an MDP you know how aggressively or how greedy should you be at just taking actions to maximize your rewards and so the average strive is relatively greedy meaning that is taking your best estimate of the state transition probabilities and rewards and it's just taking whatever actions and this is really saying you know pick the policy that maximizes your current estimate of the expected rewards and it's just acting greedily meaning on every step is just executing the policy that it thing's allows it to maximize the expected payoff and what this album does not do at all is explore which is the process of taking actions that may appear less optimal at the outset such as if the robot hasn't seen this +10 reward doesn't know how we get there maybe it should you know just try going left a couple times just for the heck of it right to see what happens because even if it seems less even if going left from the perspective of the current state of the knowledge robot maybe if it try some new things has never tried before maybe you'll find a new pot of gold okay so this is called the exploration versus exploitation trade-off oh and this is actually not just an academic problem it turns out that some of the large online web advertising platforms have the same problem as well again I I've mixed feelings about the advertising business it's very lucrative and it causes other problems as well but but it turns out that for something large online platforms you know when when an advertiser starts selling a new ad or you're posting a new ad on one of the large online ad platforms the ad platform does not know who is most likely to click on this ad and so pure explore in pure exploitation boy exploitation is such horrible conversation especially terrible on my lap that one's the technical term no there's no the social term when uses in the context but the pure you know reinforcement learning sense exploitation policy not not the other even more horrible sense of exploitation would be to always just show you show show users the ads that you know they are most likely to click on to drive short-term revenues who's gonna just show people the atom also you click on to actual turn revenue whereas an exploration for policy for a large you know something large online ad platforms it's a show people some ads that may not be what we think you're most likely to click on in this moment in time but by showing you that ad or by showing the pool of users an ad that you might be less like quick on maybe we'll learn more about your interests and that increases the effectiveness of these large of these ad platforms that finding more relevant ads for example I don't know purview no I I guess uh they're probably know a participants for maja slanders as I know but if the large online ad platforms don't know that I'm actually pretty interested in Mars Landers if it shows me an ad for Mars Lander which I don't think such a thing exists right and I click on it didn't learn that showing me as the Mars Landers great thing right or some other thing that I mean no no interesting so this is actually a real problem there are some of the large online ad platforms actually do explicitly consider exploration versus exploitation and make sure that sometimes it shows as that may not be the most likely you click on but you know allows you to gather information to then be better situated to figure out where the future rewards to be better position to learn how to mash it's not just that you but other users like you sorry okay but so in order to make sure they're reinforcement learning algorithm explores as exploits a common modification to this would be tick instead of taking actions respect to pie you may have a zero point nine chance respective high and 0pi one chance take an action randomly okay and so this particular exploration policy is called epsilon greedy we're on every time step and on every time step you toss a biased coin but on every time step let's say 90% of the chance you execute whatever you think is a current best policy and with 10% chance you just take a random action and this type of exploration policy increases the odds that you know every now and then maybe just by chance right it'll find this way to the plus 10 polyp goals and learning state transition probabilities and and and then eventually end up exploring the state space more thoroughly okay this is called epsilon greedy exploration and there's a little bit of a misnomer I think so in in in the way we think of epsilon we D epsilon is on say 0.1 is the chance of taking a random action instead of the greedy action this algorithm it's has always been a little bit strangely named because if 0.1 is actually the chance of you're acting randomly right so epsilon greedy it sounds like you're being greedy point one of the time but but you're actually taking actions randomly upon one at a time so epsilon greediest actually may be one minus Epsilon greedy so this name has always been a little bit off but that's what that's that's that's how people use this term epsilon greedy exploration means epsilon at the time which is a hyper parameter which is the parameter the algorithm you act randomly into instead of going to what you think is the best policy okay and it turns out that if you implement this algorithm with epsilon greedy exploration then this this algorithm will converge to the optimal policy for any discrete state MVP right sometimes didn't take a long time because you know if there's a if it takes a long time to randomly find plus ten it could take a long time before randomly stumbles upon 2 plus 10 particles but this algorithm women with an exploration policy will converge to the optimal what will converge the optimal policy for any MDP oh yeah yes sir right she always keep us on constant or she view dilation Epsilon so yes there are there are many heuristics for how to explore one reasonable thing to do would be start with a large value of epsilon and it's low e-string another common heuristic would be um there's a different type of exploration called Bo's from the exploration we can look up if you want which is uh if you think that the value of going off is you know 10 and the value of going solvus 1 then there's such a huge difference that you should bias your action to are going to the bigger result the bigger reward and then you could have the probability be e to the value basically right time divided times the scaling factor right so that's called bozeman exploration where instead of having a 10% chance of taking action completely at random you could just you know have a very strong bias to heading toward the higher values but also have some probability to go into lower values but where the exact probability depends on the different values so the another pair either I think I've saw greenie I feel like I see this use the most often for these types of MVPs and then Boltzmann exploration which is why just like this also use just two more questions you'd wrap up good oh yes you give her one for reaching say she has never seen before yes there's a fascinating line of research called intrinsic reinforcement learning I mean we started by sitting the same if you google for intrinsic intrinsic motivation you find some research papers on and then there was some recent fall on work I think by deep line or some of the groups but intrinsic motivation is to turn to Google where you reward reinforcement learning algorithm for finding new things about the world oh I see right how often should on the Asha issue you take before updating pi um there's no how to do it as frequently as possible in the if you're doing this with a real robot what you know I've seen is this is sometimes going to a physical robot and so you know I don't know one of five helicopters you've gone to the view for a day collect all data and then go back to the lab and evening and rerun the algorithms but if there's no barrier to running this all the time then it doesn't hurt the performance they're just running as beautiful as you can all right that's it for basis of MDP on Wednesday we'll continue with generalizing all these to continuous state ok let's break loose your Wednesday