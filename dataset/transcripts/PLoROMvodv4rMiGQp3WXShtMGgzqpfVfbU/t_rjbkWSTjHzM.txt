hey guys um let's get started so over the last several weeks you've learned a lot about many different learning algorithms from linear regression so this is regression to generalizing models Jennifer albums like GD n ie based most recently support vector machines what I'd like to do today is to start talking about advice for applying learning algorithms the fusional bottom of the theory behind how to make good decisions of what to do how to actually apply these algorithms and so today I want to discuss bias and variance and it turns out you know I've built quite a lot of machine learning systems and it turns out that bias variance there's one of those concepts this is sort of easy to understand but hard to master what is it lots of those was it all these board games or sometimes a smartphone game say he's to learn hard to master or something like that so Bosnians exactly one of those things where I've had some PhD students you know they're worked with me for several years and then graduated and worked in industry for a couple years after that and and they actually tell me that you know when they took machine learning at Stanford they learned bias and variance but as they progress for many years their understanding of bias and variance continues to deepen so I'm going to try to accelerate your learning of what bias and variance because I find that people that understand this concept are much more efficient in terms of how you develop learning algorithms and meet reverence work so let's talk about this today and to be a recurring theme that will come up with gain a few times in to make several weeks as well um and then we'll discuss regularization and talk about how to reduce variance in learning algorithms talk boy train dev test lists and then also talk about a few model selection and cross validation algorithms oh and the C reminders for today problem set one is due tonight 11:59 p.m. and if you are not yet ready to submit it today late submissions are accepted until Saturday evening Saturday 11:59 p.m. with the details of late submissions written according to the laid a policy written across websites if so so definitely purchase em your home on time today if for some reason you're not able to the late submission which we don't encourage anyone to take advantage of but it is written on the course website and problem set two will be released shortly actually I think I was already posted online and there's do two weeks from now right and so so oh and and and what I'm going to do today is talk about their conceptual aspects of this and if you want to see even more math between these sort of conceptual concepts at this Friday's discussion section will be covering some of the mathematical aspects of learning theories such as error decomposition uniform conversions and vc-dimension you know what one interesting thing I've learned really watching the evolution of machine learning over many years is that machine learning is a discipline as I become less mathematical over the years so I remember when you know machine learning people used to worry about computing the normal equations where X transpose X inverse equals X transpose Y how numerically stable is you a numerical solver for solving the normal equations of an inverting a matrix and solving linear equations but because numerical linear algebra has made tremendous strides you now we just call a linear you know linear algebra routine to invert the matrix of solving linear equations I got not worried about what is numerically stable or no but once upon the time lot of my friends in machine learning were reading text books on numerical optimization to figure out of your you know formula for inverting a matrix or really solving the system equations was numerically stable and so one of the trends have seen is that I think you know three or four years ago to understand violent variants there was a certain mathematical theory that was crucial to understand and so I used to teach that in CS 229 but we decided we're constantly trying to improve this class right but I decided that that mathematical theory is actually less crucial today if your main goal is to make these albums were so we still teach it but we're doing in the Friday discussion section and that means more time for the main lecture here to talk more about the conceptual things I think will help you build learning algorithms as well as for the newer topics like well we'll talk about the random forest decision trees around the forest and your network stinks so okay so the solver buys a variance um let's say you have this data set right I'm gonna draw the same data set three times okay so um let's say you have a housing price prediction problem where this is the size of the house and does the price of the house um you know it looks like if you fit a straight line to this data maybe it's not too bad right but it looks like if this data set seems to go up and then curve downward a little bit right and so maybe this is a slightly better model if you fit a whole series so this is if you fit a linear function theta 0 plus theta 1 X but if you fit a quadratic model maybe there's actually physically a little bit better or you could actually fit a high order polynomial this is one two three four five six examples serve you for the fifth order polynomial let's say the 5x to the fifth then you know you can actually fit a function that passes through all the points perfectly but that doesn't seem like a great model for this data and so um to name these phenomenon the function assuming you know the one in the middle is what we like fitting a quadratic function is maybe pretty grateful service called just right whoever as this example on the left it under fits the data as it's not capturing the trend that is maybe semi evidence in the data and we say this algorithm has high bias and the term buyers you know the term bias has has actually multiple meanings in the English language we as a society want to avoid you know racial bias and gender bias and discrimination against people's orientation and things like that so the term bias and machine learning has a completely separate meaning and it just means that and and and it just means that um this learning algorithm had very strong preconceptions that data could be fit by linear function this algorithm has a very strong bias a very strong preconception that the relationship between pricing and haut size of house is linear and this bias turns out not to be true okay so there's a different sense of bias then then the then the then the other type of undesirable bias where the boy this is society or which which interesting he comes up in machine learning as well in other contexts we want our learning out of what those that advises well there's different use of the term and in contrast in this curve on the right we say that this is a overfitting the data and this algorithm as high variance and the term high variance comes from this intuition that you happen to get these five examples but if you know a friend of yours was to collect data from series six six examples if a friend of yours was to collect you know a slightly different set of six examples right so four friend of yours with a rerun you collected slightly different set of housings how you know right then does our River will fit some totally other varying function on this and so the your predictions will have very high variance if you think of this as a version over different random draws of the data so so the variations if if a friend of yours does in the same experiment just slightly different data set just due to random noise then this algorithm fitting a fifth order polynomial results in a totally different result so that's so we say that this album has very high variance there's a lot of variability in the predictions this algorithm will make um so one of the things we'll need to do is identify if your learning algorithm so we clean the learning algorithm it almost never works the first time right and so when I'm developing learning algorithms my standard workflow is often to train an algorithm often trained on something quick and dirty and then try to understand if the algorithm has a problem of high bias or high variance if it's under fitting ozone Fillion data and I use that insight to decide how to improve the learning algorithm okay and I will say a lot more about how to improve the learning algorithm will have a menu of tools that we'll talk about in the next couple weeks about how to reduce bias or reduce variance of all of your learning algorithms um I just mentioned that the problems of bias variance also hold true for classification problems so so let's say that's a binary classification problem if you fit a logistic regression model to this you know straight line fit to the data maybe that's not great if you fitted logistic regression model with a few nonlinear features so yeah features x1 x2 if instead of using X 1 and X 2 as features you use additional features x1 squared x2 squared x1 times x2 x1 cube x2 it does a spy of X right and you can have a small set of features you choose by hand it's usually pretty more features than this or use an SVM kernel and use an SVM for this problem then if you let's see if you have too many features then you might actually have a learning algorithm that fits the decision boundary here that looks like that right and this learning algorithm actually gets perfect performance on the training set but this overfits excuse me I meant to make the colors consistent sorry I meant to use red thank you you get what I mean um and there's only if you choose somewhere in between you know there you get something that that seems to be a much better fit to the data the green line seems to be a pretty good way of separating positive and negative examples that they're sort of just right so similar to I guess I messed up the COS idea well kind of but similar to these colors here the blue line under fits because it's not capturing trends they're pretty apparent as data the the orange line over fits is just much too complicated hypotheses whereas the green line is just right so it turns out that in the area of you know GPU computing ability to train models of Lalla features one of the by building a big enough model so take a support vector machine if you add enough features to it if you're high enough you know dimensional feature space or if you take a linear regression model religious regression model and just add enough features to it you can often overfit the data and it turns out that one of the most effective ways to prevent overfitting is regularization so let me describe what that is and are you working today's lecture so this is and regularization is a it'll be one of those techniques that won't take that long to explain it'll sound deceptively simple but it's one of the techniques that I use most often I feel like that uses regularization in many many models so so just because it doesn't sound caught that comprise it or maybe won't even take that long to explain today don't underestimate how widely use it is this is using it's not using every single machine learning model but it's used very very often so here's the idea which is a stick linear regression right so that's the optimization objective for linear regression if you want to add regularization you just add one extra term here lambda times norm of theta squared right sometimes you write lambda over two to make some of the derivations come out easier and what this does is it takes your cost function for logistic regression which you try to minimize trying to minimize the squared error fit to the data and you are creating an incentive term for the algorithm to make the parameters theta is smaller okay so this is called the regularization term and it turns out that um let's take the linear regression overfitting example so you know if you said long D equals zero then it's just linear regression over the fifth order polynomial features it turns out that as you increase lambda or lambda to some intermediate value or depend on the scale of the data unless he said lambda equals one then when you solve for this minimization problem was this augment problem for the value of theta this term penalizes the parameters being to be and it turns out that you end up with a fit that looks a little bit better right there maybe looks like that okay and by preventing the parameters data from being too big you're making it harder for the learning algorithm to overfit the data it turns out fitting a very high order polynomial like that may result in value of stated is a very large right and and then if you sent lambda to be too large then you actually end up in an under fitting regime okay so they're usually be some optimal value of lambda where it long equals zero you're not using any regularization here so it may be overfitting if lambda is way too big then you're forcing all the parameters to be too close to zero in fact actually think about if mom it was equals you know 10 to the 100 or some ridiculously large number then you are really forcing all the Thetas to be 0 right if all the phases are 0 then you know then you're kind of fitting this straight line right so that's it mom V equals 10 to the 100 and so and this is a very simple function which is the function 0 right this function H of theta of x equals 0 right approximately 0 this is a very simple function which you get if you set down to be very large and by dialing lambda between you know a far too large value like 10 to the 100 compared to far too small value like angle 0 you use you smooth the interplay between this much to simple function of H equals 0 and a much to complex function okay so there is so that's pretty it it so that's pretty much it for regularization in terms of what you need to is meant but if you like your learning Outram may be overfitting add this to your model and solve this optimization problem and it will help relieve overfitting more generally if you are let's see more generally if you have a save logistic regression problem where this is your cost function then to add regularization I guess instead of min this is a max right if you're applying there just regression then this was the original cost function then you can have - long term or lambda over 2 there's just defensive scaling of lambda times the norm of theta squared and there's a minus here because village is regression we're maximizing around in minimizing what this could be our merits in any of the generalized linear model family as well but by subtracting lambda times the norm of theta squared this allows you to also regularize the classification algorithm such as logistic regression ok um it turns out that and and I want to make an analogy that where where all the math details are true but we don't want to talk about all the math details it turns out that one of the reasons the support vector machine doesn't over fit too badly even though it has you know you can be working infinite like a you know infinite dimensional feature space right so so why doesn't a support vector machine just over fit like crazy we showed on Monday that by using kernels is sort of using an infinite dimensional feature space right so why doesn't always fit these crazy complicated functions it just over today to say that crazy it turns out and the theory is complicated it turns out that you know the optimization objective of the support vector machine was to minimize the norm of W squared this turns out to correspond to maximizing the margin that you measured margin SVM and it's actually possible to prove that this has a similar effect as that right that this is why the support vector machine despite working infinite dimensional feature space sometimes by forcing the parameters to be small it's difficult for the support vector machine to overfit the data too much okay the theory to actually show this is quite complicated yeah there's actually very show that the class of classifiers where this is what normal W is small cannot be too complicating cannot over fit basically but that's why as you guys working you can work an infinite dimensional features basis yeah oh sure do you ever recognize per element appearances um not really and the problem with that is you know give one let me give one more specific example then come back to that right so it turns out that um so we talked about now you base as a text classification algorithm it turns out that tell me let's see if it's a classification algorithm problem yes classify spam and non-spam or classified it sentiment positive or negative sentiment of a tweet or something let's say you have a hundred examples but you have ten thousand dimensional features right so let's see your features are these you know take the dictionary a odd Bach and so on it's a one zero one right so let's you construct your feature vectors it turns out that if you fit the just regression to this type of data where you have 10,000 parameters and hundred examples this will badly this will probably overfit the data because you have but it turns out that if you use the just regress in with regularization this is actually a pretty good algorithm for text ossification and this will usually in terms of performance accuracy you know because this is Lee just regression you need to implement gradient descent or to solve the good value parameters but logistic regression with regularization for text classification will usually perform outperform naive Bayes on a classification accuracy you standpoint without regularization which is Russian will value over fit this data and and to to explain a bit more you know imagine that you have a three dimensional subspace where you have two examples then all you can do is fit a straight line right for the hyperplane to separate these two examples but so one rule of thumb for logistic regression is that if you do not use regularization it's nice if the number of examples is at least on the order of the number of parameters you want to fit right so this is if you're not using regularization it's nice if in fact I personally think that I tend to use the jurors and the only of the number of examples can be maybe 10x bigger than the number of examples because that's what you need to have enough information to fit good choices all these parameters but that's a good not using regularization but if you are using regularization then you can fit you know even 10,000 parameters right even with only 100 examples and this will be a pretty decent text classification out um the question you had just now why don't we regularize per parameter right so why don't we let's see so I guess instead of lambda you norm of theta squared it would be a sum over J lambda J you know and theta J squared right the reason we don't do this is because you then end up with if you have 10,000 parameters here you end up with another 10,000 parameters here and so choosing all this 10,000 lambdas is as difficult that's just choosing all these parameters in the first place so we don't have a good way to do this whereas when you talk about cross-validation multi-session a little bit we'll talk about how to choose maybe one parameter lambda but but those techniques won't work for choosing so 10,000 parameters you'll see right yes thank you um yes so in order to make sure that the different launchers on the similar scale a common pre-processing step we use in learning algorithms is take two different features so for text classification if all the features are zero one you can just leave the features alone but if housing classification if feature one is the size of house which I guess ranges from 100 how big are the biggest houses no no whatever let's see how's this girl from 500 square feet to 10,000 square feet 10,000 square foot really really big for housing yes but then um feature x2 is the number of bedrooms which proudly ranges from like oh no wonder I guess that some houses a ton of dangerous but I think most houses have been most 5 bedrooms right then these features are on very different scales and normalizing them to all be on a similar scale so subtract out the mean and divided by the standard deviation so scale all of these things to be between you know 0 1 or between minus 1 to 1 with would be a good pre-processing step before applying these methods it turns out that this will make gradient descent run faster as well as a comic pre-processing step to scale each individual feature to be on a similar range of values so it's actually both searches repeated why why don't support vector machines ever too badly is it because there's no small number of small vectors or is it because of minimizing the penalty w um I would say the formal argument relies more on the latter so it turns out that if you look at all the cost if you look at all the class of functions sector today to have a large margin that class has low complexity formalized by low VC dimension which you learned about in Friday's discussion section if you want to come to that and so it turns out that the cause of all functions that separate the data of a large margin is a relatively simple class of functions but and by simple cost functions I mean has low VC dimension which talked about this Friday and does any function within that class of functions is not too likely to overfit so it is convenient the support vector machine is relatively no number of support vectors but there you could imagine other algorithms of a very large number of support vectors that's all as the large margin is still a local Mexican so I would say the game oh sure yes so is it possible though so yes so one of the so yes so in general models that have high bias to enter under fit and models of high variance sent to overfit we use these terms overfilled high variance under fit hi buyers not quite and they have very similar meanings right their first frustration assume they didn't mean the same thing one thing we'll see later a two weeks from now is we'll talk about algorithms with high bias and high variance so this is a and actually one way to think of high bias in high variance was hopeful Dominator is your data set that looks like this and if somehow your classifier has very high complexity there's a very very complicated function but for some reason is still not fitting your data well right so that'd be one way to have high bias and high variance which does happen all right so to wrap up the discussion on regularization this one so mechanically the way you implement regularization is by adding that penalty on the norm of the parameters so that's what you actually implement it turns out that there's another way to think about regularization so you remember when we talked about the new linear regression we talked about minimizing squared error and then later on we saw that linear regression was maximum likelihood estimation on a certain journalize linear model using a using using using a Gaussian distribution as the expertise for the exponential family is a member of the extension family it turns out that does a similar point of view you can take on the regularization algorithm that we just saw which is let's say s is the training set so um given a training set you want to find the most likely value of theta right and so by Bayes rule P of theta given s is P R s given theta times P of theta divided by P of s and so if you want to pick the value of theta that's the most likely value of theta given the data you saw then because the denominator is just a constant this is our max over theta P of x given theta times P of theta and so if you're using logistic regression then the first term is this and in the second term is P of theta where this is the you know logistic regression models they write or any generalized linear model and it turns out that if you assume P of theta is Gaussian so if we assume your phases follow theta the probability on theta is Gaussian with mean zero and some variance tau squared I so in other words a p of theta is you know 1 over root 2 pi I guess this be determinant of tau squared I write e to the negative theta transpose tau squared i inverse so the Gaussian probability as follows it turns out that if this is your prior distribution for theta and you plug this in here and you take logs compute the max and so on then you end up with exactly the regularization technique that we found just now ok and so in everything we've been doing so far we've been taking a frequentist interpretation I guess the two main schools of statistics are there frequences school of statistic and the Bayesian school statistic and they used to be sort of Titanic academic debates about which is the right one but I think the statisticians have gotten together and and kind of made peace and and then it goes really between these two more and more these days maybe not not all the time later but the frequency school statistic we say that there is some data and we want to find the value of theta that makes the data as likely as possible and that's where we got maximum likelihood estimation all right and in the frequency school statistics we view there as being some true value of theta out in the world that is unknown and so there is some true value of theta that generated all these housing prices and our goal is to estimate this true parameter in the Bayesian school of statistics we say that theta is unknown but before you see even any data you already have some prior beliefs about how housing prices are generated out in the world and your prior beliefs are captured in a prior distribution denoted by P of theta so this is called a Gaussian prior and we say that and and and if you look at this Gaussian prior excuse me it's quite reasonable you're saying that before you seen any data on average I think the parent is of theta have mean zero because I don't know if each theta is positive negative so giving them a zero seems reasonable and most things in the world I thought since I just assumed that my prior on Thetas Gaussian so you know because we could debate if this is a the right assumption but it's not totally unreasonable right but they say well actually I think you know for the next linear regression problem I'm gonna work on next week and I have no idea what I'm gonna work on what I'm gonna apply linear regression in next week it was actually not too bad an assumption to say you know my priors is Gaussian and in the Bayesian view of the world our goal is to find the value of theta that is most likely after we have seen the data okay and so this is called map estimation we're sense for maximum a-posteriori estimation so this is actually the map estimator I get the odd max of this right there's the map or the maximum a posteriori estimates of theta which means look at the data compute the Bayesian posterior distribution on theta and pay the value of theta that's most likely okay and so one of the things you do in the problem set that was just released is no is actually show this equivalence as well as plug in a different prior States or other than the Gaussian prior you experiment about whether P of a is the Laplace prior and define and derive a different map okay wait sorry few said again yes OSE oh yes current difference we need these to be seen as recognize diversity is not right yes so so MOU here corresponds to you know the average salary without regularization and this procedure here corresponds to having regularization it turns out that free consistent decisions can also use regularization it's just that they don't try to justify it through amazing a prior they shouldn't say so if your frequency statistic if your frequentist statistics your job is a wake up and come up with an algorithm to estimate this you know true value of theta then because it's out in the world you can come of any procedure you want and then spa your procedure you can add a regularization term I think there's a lot of these debates in frequences and patients are more philosophical I think as a machine learning person as an engineer I don't really you know I think the philosophical debates are lovely but I just I just like my stuff to work so so I decided so we can say so frequencies can also in fin regularization it's just that they say this is part of the algorithm they invented rather than derived from a Bayesian prior alright cool so let's talk about so in in a discussion on regularization and choosing the degree of polynomial so let's see let's say I plot a chart where on the horizontal axis I plot model complexity so how complicated is your model so for example to the right of this curve could be a very high degree polynomial and what you find is that as you increase model complexity your training error if you do not regularize right so if you fit a linear function which I found any cubic function and so on you find that the higher the degree of a polynomial the bed to your training error because you know a fifth order polynomial always facilitate better than a fourth order polynomial if if you if you do not regular eyes but what we saw with the original picture was that the ability of the algorithm to generalize kind of goes down and then starts to go back up right and so if you were to have a separate test set and evaluate your classifier on a set of data that the algorithm hasn't seen so far so measure how well the album generalizes to a different novel set of data then if you fit a linear function then this under fits if you take the fifth order polynomial this over fits and this somewhere in between right that is just right okay and this curve is true for regularization as well so say you apply linear regression with 10000 features to a very small shiny example if launder was much too big then they will under fit if lambda was 0 so you're not recognizing at all then it will over fit and there will be some intermediate value of lambda that's not too big not too small that you know balances overfitting and underfitting okay so um what I like to do next is describe a mechanistic as you a few different mechanistic procedures for trying to find this point in the middle right and so um so given the data set what we'll often do is take your data set and split it into different subsets and a good hygiene is the ticket agent train to Train dev and test sets so if you have say 10,000 examples and you're trying to carry out this model selection problem so for example let's say you're trying to decide what order polynomial you want to fit right or you're trying to choose the value of lambda or you're trying to choose the value of tau there was the bandwidth parameter in a locally weighted regression that you saw on the problem set and if that we saw with a locally weighted regression all right so oh you're trying to choose a value C in a support vector machine so remember the SVM objective was actually this write what you know subject to some of the things but for the unknown soft margin they were southbound on Wednesday total on Monday you're trying to minimize the norm of W and then there was this additional parameter C that trades off how much you insist on classifying every training example perfectly so whether you're trying to make which of these decisions are trying to make how do you you know choose a polynomial size or choose lambda or choose tau or choose parameter C which also has this bias variance trade-off there will be some valleys that see the too large and some valleys of C that's you small so here's one thing you can do which is let's see so split your training data s into a subset which I'm gonna call the real training set as subscript train and in some subset which we call as subscript dev and F stands for development and then later we'll talk about a separate test set and so what you can do is train each model and by model I mean um option for the degree of polynomial on s train Soviet evaluating a menu of models right so let's say this is model 1 model 2 and so on up to model 5 up to some number they can train each of these models on the first subset of the data and then get some hypothesis that's called H I and then measure the error on s death which is the second subset of data called the development set and pick the one so rather than and and I want to contrast this with an alternative procedure right so the two cents of the day two substances they talk about tests and data training set and development sets and after training first of all the world second apollomon or third all polynomial on the training set evaluate all of these different models on the separate held up development sets and then pick the one with the lowest error on the development center okay but one thing to not do would be to evaluate all these algorithms instead on the training set and then pick the one with the lowest error on the training set right why not what what goes wrong when you do that Yeah right you just over fit I were you over it yeah yep cool right so if you use this procedure you always end up picking the fifth order polynomial right because the more complex our rhythm will always do better on the training set so if you do this this will always cause you to say let's use the fifth order polynomial or the highest possible order polynomial so this won't help you realize in the housing price prediction example the second order polynomial is a benefit to the data and that's why for this procedure if you evaluate your models error or the separate development set that the album did not see during training this allows you to hopefully pick a model that neither overfits no longer fits and in this example hopefully you find that there will be the second order polynomial right the one that's just right in between that actually does best on your development center okay now and then you know if you are if you are publishing an academic paper on machine learning then this procedure has looked at the training set as was the development set right so this this procedure this piece of code is you know is - in these decisions is tune the parameters the training set and is tuned the decision on the degree of polynomial to the DEF set and so if you want to know if you want to publish a paper to say oh my algorithm achieves 90% accuracy of this data set is not valid to report the result on the dev set because the algorithm has already been optimized to the data set in particular information about what's the most what's the best degree polynomial was derived from the DEF set from the development set and so if you're publishing a paper or you want to report an unbiased result evaluate the algorithm separate test set as a test and report that error okay and so if you're publishing your paper if you can say good hygiene to report the error on a completely separate test set that you did not in any way shape or form look at during the development of your model doing the training procedure or Devon test is there any different by Mike it depends on the jet size it depends on the size of the data set and so it turns out that actually let me let me give an example actually so let's say you're trying to fit a degree of polynomial and you want to choose right to death error so you can for the first second third fourth degree polynomial and so after fitting all of these let's say that the squared error right to just use round numbers is ten five point one five point zero nine seven ten okay just to just to use round numbers for illustrative purposes if you're using the def error to pick the best hypothesis to pick the best classifier you would say that using a fifth order polynomial against you for point nine squared error right but did you really earn that four point nine squared error or did you just get lucky because there is some noise and so maybe all of these actually have error that close to five point zero but some are just higher so much as lower and you just got a little bit lucky that on the DEF set this did better which is why if you look in your deaf set error your def set error is a biased estimate right and so where's there a very large test set there is a very large test said maybe the true numbers are 10 5 5 5 7 10 by your actual expected squared errors it's just that because of little bit of noise you got lucky and you reported 4.9 and so this would be a bad thing to do in an economic paper right because of what you earned was an error of 5.0 you didn't earn an error 4.9 that's just that because you're overfitting a little bit and the DEF set you chose the thing that looked best for the DEF set but your algorithm didn't actually keep that error it's just because of noise okay so so now in some cells consider good practice to report so reporting on the death error is in isn't isn't really a valid unbiased procedure and then question yeah so what one of the just a researcher said yes you're right one of the problems with some of the machine learning benchmarks that people worked on for a long time is does this unavoidable mental overfitting the people call them to use the Z's and everyone's worked my same trying to publish the best numbers the same test said so the academic committee on machine learning does have some amount of overfitting to the standard benchmarks that people worked on for a long time and this is an unfortunate result when the testing is very very large the amount of overfitting is probably smaller but when the test set is not big enough then the overfitting result can cause sometimes even research papers to publish results that are you're probably over fit to the data set and so I think there's actually one stand the academic benchmark because the dataset called seefox quite small so it's actually very same research paper analyzing results on C far arguing that some fraction of the progress that was made was actually perhaps researchers unintentionally overfitting to this dataset okay oh my the way um one thing I do when I'm building you know production machine learning systems so when I'm when I'm shipping a product right I just like build with speech recognition to them and just make it work I just wanna and not and if I'm not trying to publish a paper and not try and make some claim sometimes I don't bother to the test set right so and and that means I don't know the true error of the system sometimes but I'm very conscious of that if I don't allow data sometimes I'm gonna decide to just not have the test set and it means I just don't try to report the tested number I can report a Jeff said number which I know is bias and I just don't report a tested number I don't do this in your publishing your academia paper right this is not good if you're publishing a paper on making claims on the outside but already doing is building a product and not writing a paper distance this is actually okay yeah yeah okay good that's let me let me get to that good so um the next topic about some of the train dev test blitz is how do you decide how much data should go into you should these three subsets um so I can tell its owner just tell you the historical perspective and then a modern perspective um historically the rule of thumb was you take your training said right take your training set s and then you would send you know one rule of thumb that you see a lot of people refer to is 70% train right 30% tests this is one common rule of thumb that you just hear a lot or maybe you have if you if you don't have a test said if you're not doing model selection if you just if you already picked them although not realizing or maybe you have people use you know 60 percent range 20 percent 20 percent tests right so these are rules of thumb that people use to give and these are decent rules of thumb when you don't have a massive data set see if you have 100 tons of examples maybe of a thousand examples may be several thousand examples I think these rules of thumb are perfectly fine what I'm seeing is that as you move to machine learning problems with really really giant data sets the percentage of data you sent a dev and test shrinking and so here's what I mean let's say you have 10 million examples decent-size not giant but you can resize so let's take this is actually pretty good rule of thumb if you're a small desert if you have you know a 5 min examples this perfectly 5 rule of thumb to use but if you have 10 million examples then you know you have 6 million 2 million too many right trained deaf tests and the question is do you really need two million examples to estimate the performance of your final classifier sometimes you do if you're working on online advertising you know which I have done and you're trying to increase your ad click-through rate by 0.1 percent because it turns out increasing our click-through rates by 0.1 percent which I've done multiple times turns out to be very lucrative then you actually need a very large data set to measure these very very small improvements because to increase an ad click-through rate by 0.1 you might have llama projects you might have 10 projects each of which increases at click-through rate by 0.01% right and so to measure these very different small differences in algorithm one does 0.01 percent better than algorithm B but so you need a wall date so to tease out that very small difference so if you're in the business of teasing out these very small differences you actually need very large test sets but if you are comparing different algorithms and one algorithm is you know 2% better or even 1% better than the other algorithm then with a thousand examples maybe right a thousand examples may be enough for you to distinguish between these much larger differences so my recommendation for choosing the Jeff and Tess says is choose them to be big enough that you have been updated to make meaningful comparisons between different algorithms and if you suspect your algorithms will be Rhian performance by 0.01% you just need a law data to distinguish that right so if you have 100 examples then you know if one algorithm has 90% accuracy and one algorithm has 90 point zero one percent accuracy then unless you have at least a thousand examples and maybe ten thousand or more you just can't see this very small difference right if you're a hundred examples you just can't measure this very small difference so my advice is choose your dev and test sets to be big enough that you could see the differences in the performance of algorithms that you have yet that you roughly expected and then you don't need to make your jab in chest as much larger than that and I would usually then just put the data you don't need an indefinite essence back in the training set so when you're working a very large data set say you know million or ten million one hundred million examples what you see is that the percentage of data that goes into dev and test tends to be much smaller so it might be so you see for example maybe 90 percent train you know five percent dev and five percent test right or even smaller or even one percent one percent depending on how much states you really need to measure to the level of accuracy you need the differences that performance your algorithms okay all right um just to give this whole procedure a name what we just did here between the train and dev set this procedure that we have is called holdout cross-validation and sometimes to distinguish this from other cross validation procedures we'll talk about in a minute sometimes this is called simple hold our cross validation we'll talk about some other hold our cross validation procedures in a second and and the dev set is sometimes also called the our cross validation set right so sometimes you people use sometimes you hear people say you know we're going to use a cross-validation set that means roughly the same thing as a dev set okay so in the normal workflow of developing a learning algorithm when you're given the data said I was splitted into a training set and a dev set oh and I used to say cross-validation set but cross-validation is just a mouthful so I think just motivated by the reducing number of syllables because we're using the cause of so often more and more people just call the dev set but it means you're roughly the same thing right so so when I'm building a machine learning system I'll often take the days that splits into train and dev and if you need a test set and also a test set and then keep on fitting the parameters to the training set and evaluating the performance of your algorithm on the Jeff set and so of using that to come up with new features choose the model size choose the regularization parameter lambda really try out lots of different things and spend you know several days or weeks to optimize the performance on the DEF set and then when you want to know how well is your algorithm performing to then evaluate the model on the test set right and and the thing to be careful not to do is to make any decisions about your model using the test set because then then you're signed to fit the data to the test set there's no longer than bias estimate so and one thing that is actually okay to do is if you have a team that's working on a problem if every week they measure the performance on the test set and report out on the chart right you know the performance of the test set that's actually okay you can evaluate the model multiple times on the test set you can actually give out a weekly report saying this week for our online advertising system we have this result in a test set one week later with distres on test set won't be later this code says that it's actually okay to evaluate your algorithm repeatedly on the test set what's not okay is to use those evaluations to make any decisions about your learning algorithm so for example if one day you notice that your model is doing worse this weekend last week on the test set if you use that to revert back to an older model then you've just made a decision that's based on the test set and your test set is no longer bias but if all you do is report all the result but not make any decisions based on the test set performance such as whether to revert to an earlier model then you feel that it is actually legitimate it's actually okay to keep on you know user use the same test set to track your your team's performance over time okay all right good so when your very large data says this is a procedure of you know developing for defining the I trained F in test sets and this procedure can be used to choose the model of polynomial it can also be used to choose the regularization parameter lambda or the parent st or the parameter tau from a locally weighted regression now one of you have a very small data set so it turns out that so I'm going to leave out the test set for now let's just assume you're some circuit tester I'm not gonna worry about that for now um but let's say you have a 100 examples right if you're going to split this into you know 17 in the training set in s subscript train and 30 in s def then you train your algorithm on 70 examples instead of a hundred examples and so I've actually worked on a few healthcare problems okay most of my PhD students including Anand work doing a lot of work on machine learning advisor healthcare and so we're actually working a few data says in healthcare where you know every training example correspond to some patient that sometimes had a you know unfortunate disease or if ever you're working or if every example correspond it to injecting a patient with a drug and seeing what happened to the patient right sometimes there's literally lot of blood and pain that goes into collecting every example and if you have a hundred examples to hold out 30 of them for the purpose of model selection using only 70s 100 examples it seems like you're wasting a lot of data there was collected through a lot of you know literal pain right so is there a way to say do model selection so just choose the degree of polynomial without quote slightly wasting so much of the data there is a procedure that you should use only if you have a small dataset only if you're worried about the size oh and the other disadvantage of this is evaluate your model only on 30 examples and that seems really small right yeah okay can you just find more data to evaluate your models as well so there's a procedure that you should use only if you have a small data set called k-fold cross-validation or CAFO cv and this is in contrast to simple cross-validation but this is the idea which is let's say this your training set s you know x1 y1 down to x say 100 by 100 what we're going to do is take the training set and divide it into K pieces so for the purpose of illustration I'm going to use K equals 5 when I'm just to make the writing on the board same pen k equals 10 is typical but so what you do is um take your data set and divide it into five different subsets of in this example you would have 20 examples I'm doing 100 examples divided into 5 subsets is that 20 examples in each subset and what you do is from I equals 1 to K train I give fit parameters on K minus 1 pieces and then test on the remaining one piece and then your average right so in other words when K is equals five we're going to loop through five times in the first iteration we're going to Train on these and test on the last one fifth of the data so we'll hold out the last one fifth of the data trained on the rest and test on that and then in the second iteration through this for loop we'll train on pieces one two three and five and test on piece number four and we get the number and then you hold out this third piece trained on the others test on this and so on so you're doing five times we're on each time you leave out one fifth of the data trained on the remaining four fifths and you evaluate the model on that final one fifth okay and so if you're trying to choose the degree of polynomial what you would do is I guess for you know D equals one to five right so you do this procedure for a first-order polynomial fit you fit a linear regression model of five times each time on four fifths in the model and testin remaining one fifth and you repeat this whole procedure for the quadratic function repeat this whole procedure for the cubic function and so on and after doing this for every order polynomial from serve one two three four five you would then pick the degree of polynomial that oh and sorry and then for each of these models you then average the five SS you have for this error okay and then after doing this you will pick the degree of polynomial that did best according to this according to this metric right and then maybe you find it a second-order polynomial does this and now you actually end up with five classifiers right because you know at five classifies each one fits on four-fifths of the data and then and and there's a there's a final optional step which is the refit the model on all 100% of the data so if you want you could keep five classifiers around and output their predictions but then you're keeping five pass files around this may be a bit more common now that you've chosen to use a second-order polynomial to just refit them all the ones on all 100% of the data okay and so the advantage of k-fold cross-validation is that instead of leaving out 30% of your data for your dev set on each iteration you're only leaving out one over k of your data I use K equals five for illustration but in practice Kinkos ten is by far the most common choice that we use of sometimes seeing people use K equals 20 but quite rarely but if used K equals 10 then on each iteration you're leaving out just one tenth of the data 10% in later rather than 30% of the data and so this procedure compared to simple cross-validation it makes more efficient use of the data because you're holding out you know only 10% of the data on each generation the disadvantage of this is completely very expensive that you're now fitting each model ten times instead of just once okay but but then when you have a small data set this is actually a better procedure than simple cross-validation if you don't mind the competition expense of fitting each model ten times this does this actually lets you get away with holding on this data and then there's one even more extreme version of this which you should use if you have very very small data sets so sometimes you might have an even smaller if they set you know if you're doing a class project with twenty examples this that's that's small even by today's machine learning standards so this does an extreme version of k-fold cross-validation called leave one out cross validation which is if you say a equals m right so in other words here's your training set maybe twenty examples so you're gonna divide this into as many pieces as you have training examples and what you do is leave out one example trained on the other nineteen and test on the one example you held out and then leave out a second example trained on the other nineteen and tested one example you held out and do that twenty times and then you averaged this over the twenty outcomes evaluate how good different orders of polynomial the huge downside of this is this is completely very very expensive because now you need to train your algorithm m times so you you kind of never do this unless M is really small I personally have I pretty much never use this procedure unless Emma is a hundred or less that yes you you know if your model isn't too complicated you can afford to fill in linear regression model hundred times like it's not too bad right so so if if M is less than 100 you could consider this procedure but if if M is a thousand fifteen alidium although fitting them although a thousand times it seems to come out the world and usually use k-fold cross-validation instead but if you do have twenty examples then you know I would then if you have twenty examples I would probably use this procedure and somewhere between twenty and 50s maybe we're not switch over from leave one out okay for cross-validation oh yeah so right so since you have K estimates say ten tenesmus you're using 10-fold cross-validation can you measure the variance on those ten estimates it turns out that those ten estimates are correlated because each of the ten classifiers eight eight eight out of nine of the sense of data they trained on overlap so there was some very interesting theory result there's some research papers written by micro currents actually it was like a long time ago trying to understand how correlated are these ten estimates and from a theoretical point of view the we as far as I know the latest error result shows that this is not the worst estimate than training error but no but but maybe it's showing us in practice is not you could measure it but we don't really trust that estimate of variance because we think all 10s mr. Hardy Carlita Oh at least somewhat correlated where do I find using k-fold cross-validation debugging um if you have a very small training set then maybe yes but deep learning algorithms depend on the details right sometimes it takes so long to train that training training training on your network 20 times you know seems like a pain unless unless you have enough data unless uh unless you're near a network is quite small right so it's rarely done with a deep learning algorithm but if you're frankly if you have so little data if you're 20 training examples you know there are other techniques that you probably need to use the Bruce performance such as transfer learning or just more hand engineering of input features or something else sorry tell you oh sorry thank you for asking that this average step no I meant averaging the test errors so here you will have trained ten classifiers and you know when you evaluate it on the Left I won ten for the data you get it really you get a number right so you're looping ten times hold on one part trained on the others test on this part you left out and so that would give you a number like oh say oh when you test on this part you left out the squared error was 5.0 and then do it again square error was five point seven squared was two point eight so by average I meant average those numbers and the average of those numbers is your estimate of the error of a you know third order polynomial for this problem so this is an averaging instead of real numbers that you got from this is so so this loop gives you K real numbers and so this is averaging those cave numbers to estimate for this alternate how could a cost 5 with a degree polynomials okay well I should love questions there's one thing I like over go ahead but it's lost two good I see sure yes I'm sure in something other than f1 score with the doing other than average yes it would having f1 school is complicated yes I think I think we'll talk actually so this week Friday we'll talk about learning theory next week next Friday were talking about performance evaluation metrics so I'll talk about one school all right oh sure honey sample the data in these says so for the purpose of this cause assuming all your data comes to the same distribution oh I will use the randomly shuffle again in the era of machine learning and big data there's one other interesting trend is which which just wasn't true 10 years ago which is we're increasingly trying to train and test on different stats we're trying to you know train on data collected in one context and apply to totally different context suggests we're trying to you know train on speech collected on your cell phone because we've all that data and trying to apply it to a small speaker where it was collected on a different microphone in your cell phone or something so if you are doing that and the way you set your train to have test fit it's a bit more complicated um I wasn't going to talk about in this class you want to learn more I think that the sound of that cause I mentioned I was working on this book machine learning journey so that book is finished and if you go to this website you can get a copy of it for free that talks about that and I also talked about this more CS 2:30 English which goes more into the Big Data but you could you know go and learn machine you can also read all about it in machine learning learning if the training test different distribution yeah but random softly it would be a good default if you think your training test that's not too different all right just one last thing with a cover real quick which is a feature selection and so so them justice drag one so sometimes you have a lot of features so actually let's take text classification you might have ten thousand features because one in the ten thousand words but you might suspect that while the features are not important right you know there won't be whether the worthy is called a stop where whether the word D appears an email not doesn't really tell you this family of spam because of where the a of you know these are called stop words they don't tell you much about the content of the email but so if a lot of features sometimes one way to reduce overfitting is to try to find a small subset of the features that are most useful for your task right and so um this takes judgment there are some problems like computer vision where you have a lot of features Chris one into there being a lot of pixels in every image but probably every pixel is somewhat relevant so you don't want to select a subset of pixels for most configuration tasks but there are some other problems where you may have lot of features then you suspect the way to prevent overfitting is to find a small subset of the most relevant features for your task so feature selection is a special case of model selection that applies to when you suspect that even though you have ten thousand features maybe only 50 of them are highly relevant right and so one example if you are measuring a lot of things going on in a truck in order to figure out if the truck is about to break down right you might preventive maintenance you might measure hundreds of variables or many hundreds of variables but you might secretly suspect that there are only a few things that you know predict when the truck is about to go down so good preventive maintenance so if you suspect that's the case then feature selection will be a reasonable approach to try right and so here's the I'll just write out one algorithm which is start with this is script F equals the empty set of features and then you're repeatedly try adding each feature I so f and c which single feature addition most improves the DEF set performance and then step two is a go ahead and commit to add that feature so let me illustrate this with pictures so let's say you have all five features X 1 through X 5 and in practice are you more like X 1 through X 500 oh I went through 10,000 I'll just use 5 so start off with an empty set of features and you know train a linear classifier with no features so the model is H of X equals theta 0 right with no features so this won't be very good normal but see how well this does on your death set so the thinking were average the Y's right so it's not really normal NIC's so this is step one in the second iteration you would then take each of these features and add it to the empty set so you can try the empty set plus x1 and he said plus x2 and he said plus x5 and for each of these you infinite corresponding models so for this one you fit the picture of x equals theta 0 plus theta 1 x5 so let's try adding one feature to your model and see which model best improves your performance on the DEF set right and let's say you find it adding feature two is the best choice so now what we do is set the set of features to be x2 for the next step you would then consider starting of X 2 and adding X 1 or X 3 or X 4 or X 5 so if your model is already using the feature X 2 what's the other feature what additional feature most helps you algorithm and let's say this X 4 right fit for models see which one does best and now you would commit to using the features X 2 and X 4 and you kind of keep on doing this keep on adding features greedily keep on adding features one at a time to see which single feature addition helps improve your album demos and and and you can keep iterating until adding more features now hers performance and then pick what whichever feature subset allows you to have the best possible performance which I've said okay so this is a special case of model selection called forward search it's called forward search we're starting empty set of features and adding features one at a time there's a procedure called backward search which can read about that we start with all the features remove features one other time but this would be a reasonable feature section algorithm the disadvantage of this is is quite computationally expensive but this can help you select a decent set of features okay several running a little bit late let's break oh so I think I was meant to be on the road next week but because is still unable to teach I think we'll have Rafael teach decision trees next week and then also can talk about networks we okay so let's break for today and then maybe we'll see some of you at the Friday discussion