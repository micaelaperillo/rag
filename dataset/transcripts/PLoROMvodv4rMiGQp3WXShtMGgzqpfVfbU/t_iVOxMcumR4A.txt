okay welcome everyone so today we'll be going over learning theory this is this used to be taught in the main lectures and in previous offerings this year we're going to cover it as as a Friday section however some of the concepts here are we're going to be covering today are important in the sense that they kind of deepen your understanding of how machine learning kind of works and other covers what are the assumptions that you're making and you know why do things generalize and and so forth so here's the agenda for today so we're going to quickly start off with framing the learning problem and we'll go deep into bias-variance tradeoff will you go we'll spend some time over there and we look at some other ways where how you can kind of decompose the error as an approximation error an estimation error we will see what empirical risk minimization is and then we'll spend some time on uniform convergence and VC dimensions so let's jump right in right so d so the assumptions under which we're going to be operating for for this lecture and in fact for most of most of the algorithms that we'll be covering in this course is that there are two main assumptions one is that there exists a data distribution B from which X Y pairs are sampled so this is this makes sense in the supervised learning setting where you're expected to learn a mapping from X to Y but the assumption also actually holds more generally even in the unsupervised setting case the the main assumption is that there is a data generating distribution and the examples that we have in our training set and the ones we will be encountering when we tested are all coming from the same distribution right that's that's like the core assumption without this coming up with any theory is is it's going to be much harder so the Assumption here is that you know there is some kind of a data generating process and we have a few samples from that data generating process that becomes our training set and that's a finite number you can get an infinite number of samples from this data generating process and the examples that we're going to encounter at test time are also samples from the same process that's that's the assumption and there is a second assumption which is that all these samples are sample independently so with these two assumptions we can imagine learning the process of learning to look something like this so we have a set of XY pairs which we call this yes these are just x1 y1 X M Y M so we have M samples from from a sample from the data generating process and we feed this into a learning algorithm and the output of the learning algorithm is what we call as a hypothesis so hypothesis is is a function which accepts an input a new input X and makes a prediction about about Y for that X so this hypothesis is sometimes also in the form of theta hat so if we if we restrict ourselves to a class of hypotheses for example all possible logistic regression models of of dimension n for example then it's you know obtaining those parameters is equivalent to obtaining the hypothesis function itself so a key thing to note here is that this s is a random variable this is a random variable this is a deterministic function and what happens when you feed a random variable through a deterministic function you get a random variable exactly so so the hypothesis that we get is also a random variable right so all random variables have a distribution associated with them the distribution associated with the data is the distribution of capital D this is just enough ixed deterministic function and there is a distribution associated with the with the with the parameters that we obtain that has a certain distribution as well in in this in a more statistical setting we call this an estimator so if you take some advanced statistics courses you will call what you will come across as an estimator here we call it a learning algorithm right and the distribution of theta is also called the sampling distribution and the what's implied in this process is that there exists some theta star or in its H star however you want to view it which is in the sense or true parameter the true parameter that we wish to be the output of the learning algorithm but of course you know we never know we never know what theta star is and when what we get out of the learning algorithm is is going to be just a sample from a random a random variable now a thing to notice that this the theta star or H star is not random it's just an unknown constant not about when we say it's not random it means there is no probability distribution associated with it it's just a constant which we don't know that that's that's the assumption under which you operate right no let's see what let's see what what what are some properties about this theta theta hat so all the all all the entities that we estimate are generally decorated with a hat on top which which indicates that it's it's something that we estimated and anything with a star is like you know the true or the right answer which we don't have access to in general so any questions with this so far yeah yeah so this could be in case of like a linear or logistic regression linear regression generally happens to be a vector it could be a scalar it could be you know a matrix it could be anything right it's just an entity that we estimate and sometimes it's store can also be so generic that it need not even be parametrized will it's just some function that you estimate right so yeah so it could it could be a vector or a scalar or a matrix it could be anything right so let's see what happens when we so in the lecture we saw this diagram for India now when we are talking about bias variance so in case of regression and we saw that we saw this as the concepts of underfitting this is overfit and this is like just right alright so the concept of underfitting and overfitting are kind of closely related to bias and variance so this is how you would knew it from the data so this from the data view right because this is X this is y you know this is your data and if you look at you know look at it from a data point of view these are the kind of different algorithms that you might get right however or to get a more formal sense formal view into what's what's bias and variance it's more useful to seed from the parameter view so let's imagine we have four different learning algorithms and here this is the parameter space let's say theta 1 theta 2 let's imagine you know you have just two parameters that's easy to visualize theta 1 and theta 2 all right and this corresponds to algorithm a algorithm B C and D right there is there is a true theta star let's let's which is unknown right now let's imagine we run through this this process of sampling M examples running it through the algorithm obtain a theta hat right and then we start with a new sample sample from D running through the algorithm we get a different data hat right and the theta hat is going to be different for different learning algorithms so so let's imagine first we we we sample some data that's our training set run it through algorithm a and let's say this is the parameter we got and then we run it through algorithm B and let's say this is the parameter with art and through C here and through D over here and we're going to repeat this you know second one may be here and so on and you repeat this process over and over and over the the key is that the number of samples per input is M that is fixed right but we're going to repeat this process and over and over and for every time we repeat it we get a different point over here right so each point each dot corresponds to a sample of size M right the number of points is basically the number of times we repeated the experiment right and what we see is that these dots are basically samples from the sampling distribution right now the concept of of bias and variance is kind of visible over here so if we were to classify this now we would call this as bias and variance right so these two are algorithms that have low bias these two are have high variance these two have low where these are low bias high bias low variance high variance so what does this mean so bias is basically checking Rd is the sampling distribution kind of centered around the true parameter the true unknown parameter is it centered around the true parameter right and variance is is measuring basically how dispersed the the sampling distribution is right so so formally speaking this is bias and variance and it becomes you know pretty clear when we see it in the parameter view instead of in the the data view and essentially bias and variance are basically just properties of the first and second moments of your sampling distribution so you're asking the first moment that's the mean is it centered around the true parameter and you know the second moment that variance that's literally variance of the bias-variance tradeoff yeah yeah um so this is a diagram where I'm using only two Thetas just to fit you know right on a whiteboard so you you would imagine something that has high variance for example this one to probably be of a much much higher dimension not just two but it would still be spread out it would still have like high variance there would be points in a higher dimensional space you know but more spread out right so the question was the question was in over here we we actually had more number of Thetas but here with the higher variance plots we are having the same number of Thetas so yeah so you could imagine this to be higher dimensional and also different algorithms can have different bias and variance even though they have the same number of parameters for example if you had regularization the variance would come down for example we go over that a few observations that we want to make is that as we increase the size of the data every time we feed and so if this were to may be made bigger if we take a bigger sample for every every time we learn the variance of theta hat would become small right so if we repeat the same thing but with larger number of examples so this would be more all of these would be more tightly concentrated right so the spread is so the spread is a function of how many examples we have in each in each iteration right so as M tends to infinity right the variance tends to zero right if you were to collect an infinite number of samples run it through the algorithm you would get some particular theta hat and if you were to repeat that with an infinite number of examples you will always keep getting the same theta hat now the rate at which the variance goes goes to zero as you increase M is you can think of it as what's also called like the statistical efficiency it's basically a measure of how efficient your algorithm is in squeezing out information from a given amount of data and if theta hat tends to theta star as M tends to infinity you call such algorithms as consistent so consistent and if the expected value of your theta hat is equal to theta star for all M so no matter how big your sample sizes if you always end up with a sampling distribution that's centered around the true parameter then your estimator is called an unbiased estimator yes so efficiency is is basically the rate at which the variance drops to zero as M tends to zero so for example you may have one algorithm which which where the variance is a function of 1 over m square another algorithm where the variance is a function of e to the minus m you know you you can have the variance can drive down at different rates relative to M so that's kind of captures which what's efficiency here right yeah yeah so theta theta hat approaches so this is a random variable here so so here's one thing to be clear about here this is a number a constant and this is a constant but here this is a random variable right so what we are saying is that as M tends to infinity theta hat that is the distribution converges towards being a constant and that constant is going to be theta star which means at smaller values of M your algorithm might be centered elsewhere but as you get more and more data your sampling distribution variance reduces and also gets centered around the truth theta star eventually so informally speaking if your algorithm has high bias it essentially means no matter how much data or evidence you provided it kind of always keeps away from from theta star right you cannot change its mind no matter how much data you feed it it's never going to center itself around around theta star that's like a high bias toggle it's biased away from the true parameter and variance is you can think of it as your algorithm that's that's that's kind of highly distracted by the noise in the data and kind of easily get swayed away you know far away depending on the noise in your data so these algorithms you would call them as those having high variance because they can easily get swayed by noise in the data and as we are seeing here bias and variance are kind of independent of each other you can have algorithms that have you know an independent amount of bias and variance in them you know there is there is no correlation between bias and variance and one way so the how do we how do we kind of fight variants so first let's look at how we can address variants yes so bias and variance are properties of the algorithm at a given size M right so these plots were from well from a fixed size M and for that fixed size data this algorithm has high bias low variance this algorithm has high variance and high bias and so on yeah yeah you can you can you can think of it as yeah it you assume like a fixed data size right so fighting variance so one way to kind of address if you are in a high variance situation this will just increase the amount of data that you have and that would naturally just reduce the variance in your algorithm yes that is true so you don't know upfront what whether you're you're you know you know high bias or high variance scenario one way to kind of one way to kind of test that is by looking at your training performance versus test performance we go over that arm in fact we're going to go into you know much more detail in the main lectures of how do you identify bias and variance here we're just going over the concepts of what our bias and what our variance so one way to address variances you just get more data right as you get more data the your sampling distributions kind of tend to get more concentrated the other way is what's called as regularization so when you when you had regularization like l2 regularization or l1 regularization what we are effectively doing is let's say we have an algorithm with high variance maybe low bias no bias high variance and you add regularization right what you end up with is an algorithm that has maybe a small bias you increase the bias by adding regularization but low variance so if what you care about is your predictive accuracy you're probably better off training off high variance to some bias and getting down reducing your your variance to a large extent yeah yeah we'll we are going to look into that up next so in order to kind of get a better understanding of this let's imagine I think of this as the space of hypothesis space of right so let's assume there is a true there exists this hypothesis let's call it G right which is like the best possible hypothesis you can think of by best possible hypothesis I mean if you were to kind of take this take this hypothesis and take the expected value of the loss with respect to the data generating distribution of across an infinite amount of data you kind of have the lowest error with this so this is enough you know the best possible hypothesis and then there is this class of hypotheses let's call this classes H right so this for example can be the set of all logistic regression hypotheses or the set of all SVM's you know so this is a class of hypotheses and what we what we end up with when we take a finite amount of data is some member over here right so let me call this H star right there is also some hypothesis in this class let me call it kind of H star which is the best in class hypotheses so within the set of all logistic regression functions there exist some you know some model which would give you the lowest lowest error if you were to test it on the full data distribution right the best possible hypothesis may not be inside you're inside your hypothesis class it's just some you know some hypothesis that that's that's conceptually something outside the class right now G is the best possible hypothesis H star is best in class just in class h h ad is one you learnt from finite data so we also introduced some new notation so epsilon of H is you will call this the risk or generalization error right and it is defined to be equal to the expectation of X Y sample from D so you sample examples from the data generating process run it through the hypothesis check whether it matches with with your output and if it matches you get a 1 if it does if it if it doesn't match you get a 1 if it matches you get a 0 so on average this is you know roughly speaking the fraction of all examples on which you make a mistake and here we are kind of thinking about this from a classification point of view to check if you know the class that your output matches the true class or not but you can also extend this to the regression setting but that's a little harder to analyze but you know the generalization holds to the regression setting is but we'll stick to classification for now and we have an epsilon hat s of H and this is called the empirical risk is the empirical risk or empirical error and this over here is the difference here is that here this is like an infinite process you're you're sampling from D forever and calculating like the long-term average whereas this is you have a finite number that's given to you and what's the fraction of examples on which you make you make them an error right all right before we go further there was a question of how adding regularization reduces your variance so what you can see actually let me let me get back to that in a bit so you know G and this is called the Bayes error so this essentially means if you take the best ways possible hypothesis what's the fraction what's what's the rate at which you make errors you know and that can be nonzero right even if you take the best possible hypothesis ever and that can still still make some some mistakes and and this is also called irreducible error for example if your data generating process you know spits out examples where for the same X you have different Y's in two different examples then you know no no learning algorithm can you know do well in such cases that's just one one kind of irreducible error they can be other kinds of identity visible errors as well and epsilon of H star epsilon G is called the approximation error so this essentially means what is the price that we are paying for limiting ourselves to some class right so it's the it's the error between its it's the difference between the best possible error that you can get and the best possible error you can get from H star right so this is this is an attribute of the class so what's the cost you are paying for restricting yourself to a class and then you have epsilon of H and minus epsilon of H star and this you call it the estimation error the estimation error is given the data that we got you know the M examples that we got and we estimated you know using our estimator some H h H act what's the what's the what's the error due to estimation and this is like so this this the error on G is is the Bayes error the gap between this error and the best in class is the approximation error and the gap between the best in class and the hypothesis that you end up with is called the estimation error right and it's easy to see that H hat is actually equal to estimation error plus approximation error right it's pretty C's with you know if you just add them up all these cancel out and you're just left with epsilon of H hat so it's it's kind of useful to think about your generalization error as different components some error which you just cannot you know reduce it no matter what no matter what type odd says you pick no matter how much of training data you have there's no way you can get rid of the irreducible error and then you make some some decisions about that you're going to limit yourself to neural networks or logistic regression or whatever and thereby you're kind of defining a class of all possible models and that has a cost itself and that's your approximation error and then you are working with limited data and this is generally due to data right and with the limited data that you have and possibly due to some nuances of your algorithm you also have an estimation error I mean we can further see that the estimation error can be broken down into estimation variance and the estimation bias and you can all therefore write this as and what we commonly call as bias and variance are you know this we call it as variance and this we call it as bias and this is just irreducible so sometimes you see the bias-variance decomposition and sometimes you see the estimation approximation error decomposition they are somewhat related they're not exactly the same so the bias is based why is you know bias is basically trying to capture why is a chat far from a from G right why is it staying away from G you know why did our hypothesis stay away from the true hypothesis and that could be because your classes is kind of too small or it could be due to other reasons such as you know as we'll see maybe regularization that kind of keeps you away from a certain certain hypotheses right and the variance is generally due to it like it's almost always due to having small data it could be due to other reasons as well but these are like two different ways of of decomposing your your error so now if you have high bias how do you fight high bias fight.i bias so how would you fight high bias any guesses mm-hmm yeah exactly so one way is to just you know make your H bigger I'd make your H bigger and also you can you can try you know different algorithms after making your H bigger and what this generally means is what we saw there was regularization kind of you know reduces your your variance by paying a small cost in bias and over here you know so let's say your algorithm has some bias right so it has a high bias and some variance and you make H bigger your class bigger right and this generally results in something which reduces your bias but also increases your variance right so with this picture you can you can also see you know what's the effect of how does variance come into picture now just by having a bigger class there is a higher probability that the hypothesis that you estimate can vary a lot right if you reduce your the space of hypothesis you may be increasing your bias because you may be moving away from G but you're also effectively reducing your variance right so that's that's the one of the you know trade-off that you observe that any step you a step that you take for example in reducing bias by May get bigger also makes it possible for your H hat to land at much you know at a wider space and increases your variance and if you take a step to reducing your variance by maybe making your your class smaller you may end up making it smaller by being away from the end thereby increase your your increase your bias so when you when you add regularization you know the the question of somebody asks before of how does me in how does adding regularization decrease the variance by adding regularization you're effectively kind of shrinking the class of hypothesis that you have you start penalizing those hypotheses whose theta is very is very large and in a way you're kind of you know shrinking the class of hypotheses that you have so if you shrink the class of hypotheses your your variance is kind of reduced because you know there's much smaller wiggles room for your estimator to place your H hat and you know if you shrink it by going away from from from G you you also introduced bias that's like you know the bias-variance tradeoff any questions on the so far yeah you you you probably want to think of each of these you probably want to think of this as a generalized version of this right so here we have like fixed data on theta 2 but you know because you could parameterize them into a few parameters you can kind of plot it in a matrix space but that's like a more general like a bag of hypotheses and you know but in any case in both of both those diagrams a point here is one hypotheses a point there is one hypotheses here it's parameterize here it's not parametrized yes the thing is D for D so the question is how what if we we shrink it towards H star right the thing is we don't know where H star is right if we knew it we didn't even need to learn anything we could just go straight there right so yeah with regularization so the question is when you add regularization are we sure that the bias is going up no we don't know and and this is a common scenario what happens right you when you add regularization you you you reduce the variance for sure but you're very likely going to introduce some bias in that process so if you add regularization you're shrinking your hypothesis space in some way so you're kind of moving away from true G so you're kind of adding a little bit bias you're very likely to add some bias in that process yeah so it's so I I would encourage you to you know kind of after this lecture to think about this a little more slowly it's it's it's it takes a while to kind of internalize this the concept of bias and variance and and it's not very intuitive but but you know thinking about it more definitely helps all right any other questions before we move on so an example of a hypothesis class right so an example would be the set of all logistic regression models right and when you do gradient descent on your you know logistic regression class you are kind of implicitly restricting yourself to set up our possible logistic regression models that's kind of implicit so the H is the output of the learning algorithm all right so you feed an input to your algorithm this is not the model this is the learning algorithm like this is like gradient descent for example right and the output of that is the parameters that you learnt that converge to right so so yeah you probably don't want to think about this as the model that you learned but this as the like the training process and the output of the training process is a model that you'll know and that is a point in your in the class of hypotheses yes so you fix that the class of learning mods you say I'm going only going to learn logistic regression models right for different different samples of data that you feed it as your training set you're going to get learn a different theta hat yes but they have to be within the class of hypotheses all right so let's let's move on so next we come across this concept called empirical risk minimization this the empirical risk minimizer so so the empirical risk minimizer is a learning algorithm it is one of those kind of boxes that we drew it is you know so in the box that we drew earlier as learning algorithm right so the the diagram that we drew earlier based on which we reasoned everything so far didn't actually tell you what actually happens inside it could be doing gradient descent it could just do something else it couldn't be you know some some you know smart programmer who's written a whole bunch of if else and just returns a theta it could be anything right and no matter what kind of algorithm was used the bias-variance theory still holds right now we are going to look at a very specific type of learning algorithms called the empirical risk minimizer right so and this was eat into your algorithm and you get it star no H its add equal to so what does erm empirical risk minimization it's what we've been doing so far in the course we're we try to find a minimizer in a class of hypotheses that minimizes the average training error weight so for example this is trying to minimize the training error from a classification perspective this is kind of minimizing the or increase in their training accuracy which is different from what actually logistic regression did where we were doing the maximum likelihood or minimizing the negative log likelihood it can be shown that losses like the logistic loss are can be well approximated by by the ERM and and and this theory should should hold nonetheless right so if if we are limiting ourselves to do that class of algorithms which which work by minimizing the training loss right as opposed to something that say returns a constant all the time or does something else if we limit ourselves to empirical risk minimizer's then we can come up with more theoretical results for example uniform convergence which we are going to look at right now so so we are limiting ourselves to empirical risk minimizer's and starting off uniform convergence so there are two central questions that we are kind of interested in so one question is if we do empirical risk minimization that is if we just reduce the training loss right what what does that say about the generalization error of that so that is basically the height of H versus H so for you know consider some hypothesis right and that gives you some amount of training error right what does that say about its generalization error like that's one central question we want to consider and the second one is how does the generalization error of our learned hypothesis compared to the best possible generalization error in that class right note we're you know we're only talking about a star and not G yeah so it's star is is the best in class so these are these are two central questions that we want to we want to explore and for this we're going to use two tools right so one is called the Union bound right what's the Union bound if we have see different events - okay then this need not be independent then the probability of if this looks trivial it is trivial it's it's it's probably one of the axioms in in in your undergrad probability class the the probability of any one of these events happening is less than or equal to the sum of the probabilities of of each of them happening right and then we have a second tool is called the halflings inequality we're only going to state the inequality here there is a supplemental notes on the website that actually proves the tufting inequality you can go through that but here we are only going to state the result in fact throughout this session you only got a state result so you're not going to prove anything so let Z 1 Z 2 Z em be sampled from some Bernoulli distribution parameter fee and let's call we had to be average of them of Zi and let there be a gamma greater than zero which we call it as the margin so the huffing inequality basically says the probability that the absolute difference between the estimated fee parameter and the true fee parameter is greater than some margin can be bounded by two times the exponential of minus 2 gamma square em not very obvious but you know you can you can you can show this what what is basically saying is there is some there is some some parameter between 0 & 1 of a Bernoulli distribution the fact that it is between 0 & 1 means it's it's bounded and and that's a key requirement for the Hough dings inequality and now we take samples from this Bernoulli distribution and the estimator for this is basically you know and these are just zeros or ones Zi Zi each of the Zi is either a 0 or a 1 the sample a 0 or a 1 with probability P and the estimator is basically just the averages of your samples right and the absolute difference between the estimated value and the true value the probability that this difference becomes greater than some margin gamma is bounded by this expression so there are a lot of things happening here so probably one of slowly think through this so this is a margin right and this is like basically like the deviation or the error right it's the absolute value of how far away your estimated value is from from the true and you would like it to be closer so you you you probably want your your fee hat and fee to be not more than 0.001 right so in which case if the absolute value between the estimated and a true parameter is greater than 0.001 if that's the margin you're that you're interested in then this the huffing is inequality proves that if you were to repeat this process over and over and over the number of times fee hat is going to be great it's going to be farther than 0.001 from the true parameter it's going to be less than this expression which is a function of M right and that is you can kind of believe it because as M increases this becomes smaller which means the probability of your estimate deviating more than a certain margin only reduces as you increase M right so this is hoppings inequality and we're going to use this questions not so so the question is is hate star the limit of H at as M goes to infinity it is its star in in the limit as M goes to infinity if it is a consistent estimator right so we went over the concept of consistency I'd given infinite data will you eventually get to the right answer and if your estimator is not consistent then it will it need not be so in general H hat need not converge to H star as you get an infinite amount of data so now we want to use these tools tool 1 and tool to to answer our like the central questions any other questions yeah this is a more limited version of Hough Kings inequality and yes if we limit ourselves to a bernoulli variable which has some parameter fee and you take samples from it and you construct an estimator which is the average of the of the samples of the zeros and ones then this inequality holds that's the this inequality is called the Hough dings inequality yes so if you're in general that there are there there is this class of algorithms called maximum likelihood algorithms maximum likelihood estimators and a pure maximum likelihood estimator is generally consistent if you include regularization then it need not be it need not be consistent though I'm not very sure about that I'm not very sure about that so basically you know for the Mike what he responded was if you have an algorithm like a neural net which is which is non convex you may actually not end up with the same result even if you increase increase like the number of though I would probably call the fact I would probably think of the non convexity to be part of an estimation bias because you could in theory always find like the global minimum of a neural network is just that there's some bias in our estimator that we are using gradient descent and we cannot solve it okay so now let's let's use these two tools and for that we're going to start at this diagram so over here we have hypothesis here we have error and there's actually one one curve which I'm trying to make it thick and probably make it to look like multiple curves this just one curve and this we will call it s this is the generalization risk or the the generalization error of every possible hypothesis in our class right so pick one hypothesis that's going to be somewhere on this axis calculate the generalization error not the am particular the generalization error and you know that's the height of that curve right and we also have something like this right so this dotted line now corresponds to H of H now let's let's sample a set of M examples and calculate the empirical error of all our hypotheses in our class and plot that as a curve all right any questions on what these two are yeah it need not beats and I'm just in fact Li this is very likely not even a straight line you're just thinking of all possible hypotheses it need not be convex this is just to get some ideas you get better intuitions on some of these ideas yes so the black line the thick black line is the generalization error of all your hypotheses right and let's say you sample some some some data right let's call it s on that sample you have training error for all possible hypotheses right we haven't not learnt anything right it's it's this is the generalization error and this is the empirical error for the given s right now in order to apply halflings inequality here right so let's consider some H I right this is some hypotheses we don't know so we start with some random hypotheses right and so so by starting with some hypotheses like think of this as you start with some parameter right and so the height of this line up to the the thick black curve is basically the generalization error of H is the height to the thick black curve so let me call this epsilon of H I right and the height to the dotted curve until here and this is epsilon hat of H I I'm going to ignore the s for now right and this corresponds to like the the sample that we obtained now one thing you can you can you can check is that the expected value of where the expectation is with respect to the data the sample so what this means is that for one particular sample you this is the generalization error you got take another set of M samples that curve might look some you know some other way and you know the height of the dotted line would be there so in general on average if you Sam average across all possible training samples that you can get the the expected value of the height to the dotted line is going to be the height to the thick line right that's that's just a five now here if you apply halflings inequality you basically get probability of absolute difference between the empirical error versus the generalization error doing greater than gamma is less than equal to minus two square this is basically you know opting in equality we have right here except in place of fee and fee hat we have the true generalization error and the empirical error any questions on this so far so what we are saying is essentially the the gap between the generalization error and the empirical error right right the gap being greater than some margin gamma is going to be bounded by this expression so loosely speaking what this means is as we increase the size M if our trainings up if we plot the set of all dotted lines for a larger M they are going to be more concentrated around the black line does that make sense so so take a moment and think about it this dotted line correspond to s of some particular size M we could take another sample of you know a fixed set of examples and that might look something like this and take another sample of size M and that might look something like this now and now consider the set of all deviations from from the black line to every possible dotted line along the vertical line of H I right now this gap is greater than some margin gamma with probability less than this term over here right so so it essentially means that if you start plotting dotted lines with the bigger em right where the set of all those dotted lines are correspond to a bigger M they are going to be much more tightly concentrated around the true generalization of that of that H that make sense right you're basically applying tufting inequality to this gap over here instead of something that's basically what you're doing no that's good but but there's a problem here the problem here is that we started with some hypotheses and then averaged across all possible data that you could sample but in practice this is useless because in practice we start with some data and run the empirical risk minimizer to find the lowest H for that particular data right and when you when when which means that H and the data that you have are not really independent right you you chose the H to minimize minimize the risk for the empirical risk for the particular data that you are given in the first place right so to to fix this what we want to do is basically extend this result that we got to account for all H right now if we want to get a bound on the the gap between the a probabilistic bound and the gap between the generalization error and the empirical error for all age you know what's that bound going to look like right and this is basically called uniform uniform convergence this result is called uniform convergence because we are trying to we are trying to see how the risk curve converges uniformly to the generalization risk how the empirical risk curve uniformly converges to the generalization risk curve and and it's that that's called uniform convergence which you can apply to functions in general but here we are applying to the risk curves across our hypotheses and we can show I'm gonna just skip the math so this we showed using halflings inequality and you can apply the Union bound for unioning across all age except we can first we're going to limit ourselves to correct so let me start over so we got this bound for a fixed edge right but we are interested in getting the bound for any possible edge right so that's our next step right and the way we're going to going to extend this point wise result to across all of them is going to look different for two possible cases one is a case of finite hypothesis class and the other case is going to be the case for infinite hypothesis class so what does it look like so so let's first consider finite hypothesis classes so first we are going to assume that the class of H has a finite number of hypotheses the result by itself is not very useful but it's going to be like a building block for further for the other case so let's assume that the number of hypotheses in this class is some number K right we can show that I'm not going to go over the the derivation but I'm just going to write out the result it's it's pretty intuitive so basically what we do is we apply the Union bound for all K hypotheses and we end up just multiplying that by a factor of K all right so what we get is the probability that there exists some hypotheses in H such that the empirical error minus generalization error this is greater than gamma is less than equal to K times K times the probability of any one which is equal to K times 2 minus 2 gamma square M and this we flip it over we negate it and we get the probability that for all hypotheses in our class empirical risk - generalization risk is less than gamma this is going to be greater than equal to 1 minus 2 K so with probability at least 1 - you know this expression which we can call this Delta with probability at least so much for all hypotheses our margin is going to be less than some gamma right this is this is just hoppings inequality plus Union bound and just negate the two sides you get this and you can go with this slowly you know later from the notes the notes goes over this in more detail right now basically now what we have is no now let's let Delta K gamma square hmm so we basically now have a relation between Delta which is like the probability of error by here by error I mean that the empirical risk and the generalization risk are farther than some some margin and gamma is called the margin of error and M is your sample size so what so what this basically tells is if your algorithm is the empirical risk minimizer it could have been any kind of algorithm but if it is the kind that minimizes the training error then you can get by by just changing the sample size you can get a relation between the margin of error and the probability of error and related to the sample size right so what we can do with this relation is basically fix any two and solve for the third and that gives us you know some actionable results for example you can fix any two and solve for the third from this relationship right and what what what that could mean is for example so you you can choose any two and solve for the third am I'm only going to go over one one one of those so let's fix fix gamma and Delta to be greater than 0 and we solve for M and we get em to be a too many good one over to gamma square Delta so what this means is with probability at least 1 minus Delta which means probably at least 99% 99.9% for example the probability at least 1 minus Delta the margin of error between the empirical risk and the true generalization risk is going to be less than gamma as long as your training size is bigger than this expression all right that's something actionable for us right now theory can be useful so this is also called the sample complexity dessert right and basically what this means is as you increase em and you sample different sets of data sets your dotted lines are going to get closer and closer to to the thick line which means minimizing you're minimizing on the dotted line will also get you closer to the generalization error so this this is basically telling you how minimizing on on minimizing on the empirical risk gets you closer to generalization right okay so that so we started off with two questions relating the empirical risk to generalization risk now let's let's explore the second question what about the generalization error of our minimizer with the best possible in class so let's look at this diagram again let's say we started with this dotted curve right and the minimizer of that would be 8 star and this is 8 star sorry the diagram is a little so this is H hat sorry so this is H at and this has a particular generalization error right that is the point of let's assume we got this data set we ran the empirical risk minimizer and we obtained this hypothesis and when we deploy this in the world in the real world it's error is going to be so much right now how does this compare to the performance of the minimizer of the the best possible so this is H star best-in-class right now we want to get a relation between this error level and this irrelevant we got one bound that relates this to this and now we want something that relates this to this now how do we do that it's pretty straightforward so the generalization error of H hat that's this dot over here less than equal to empirical risk of H hat plus gamma so we got a result using a huffing and union-bound that the gap between the dotted line and the the thick black line is always less than gamma right and it's the absolute value so we can we can write it this way as well and this right so basically we we started from the thick black line dropped down to the dotted line and this is going to be less than the empirical error of H star plus gamma why is that because M empirical error the empirical error of H hat by definition is less than or equal to the empirical error on any other hypothesis including the best-in-class because this is the training error not not not the generalization error right so which means and and this is less than or equal to so we we dropped from the generalization to the test and we said this test is the this training error is always going to be less than the empirical error of the best-in-class you see that the best-in-class was higher for the trained particular and this again is now this gap is also bounded because we prove uniform convergence that the gap between the dotted line and thick line is bounded by gamma for any edge right and this is therefore H star plus 2 gamma because we added the extra margin so we wanted a relation between the the our our hypothesis generalization error to the generalization error of the best-in-class hypothesis so we dropped from the generalization error to the empirical error of our hypothesis related that to the empirical error of the best-in-class and again bounded by the gap between these two so we got a gap between the generalization bound the generalized error of our hypothesis to the best-in-class generalization any questions on this so the result basically says with probability 1 minus Delta and for training size M the generalization error of the hypothesis from the empirical risk minimizer is going to be within the best-in-class generalization error plus 2 times log Delta so this was basically so you can get this when you when you so in this expression if you set this equal to Delta and solve for gamma you will get this any questions I think we are already over time so the the case for infinite classes is an extension to this maybe I'll just write the result so there is a concept called VC dimension which is a pretty simple concept but you know we won't be going over it today VC dimension basically says what is the so VC dimension is you can think of it as trying to assign a size to an infinitely to it to an infinite size hypothesis class for a fixed size hypothesis class we had like you know care to me the size of the hypothesis so we see of some hypothesis class it's going to be some number right some number which which kind of which is like the size of that hypothesis it's basically telling you how how expressive it is and and on using using the the VC dimension there are very nice geometrical meanings of VC dimension you can you can get a bound similar bound but now it's not for high it's not for finite classes anymore some Big O right so in place of this margin we ended up with a different margin that is a function of the VC dimension and the the key takeaway from this is that the the number of data examples that the sample complexity that you want is generally you know an order of the VC dimension to get good results that's basically the domain result from that right from with that I guess will will will break for the day and we'll take more questions