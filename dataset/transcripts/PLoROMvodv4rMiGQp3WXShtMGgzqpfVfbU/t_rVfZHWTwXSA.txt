all right um let's get started so um let's see logistical reminder the class midterm is this Wednesday and it's for the eighth article midterm and the logistical details you can find at this Piazza Post right so the midterm will start Wednesday evening your $40.00 to do it and then submit it online through great scope and because of the midterm there won't be a section this Friday ok oh and the midterm will cover everything up to and including e/m which will spend most of today talking about Susie they don't look so stressed it'll be fun all right um so what I'd like to do today is start our foray into unsupervised learning so far spent a lot of time on supervised learning algorithms including advice and how-to applies to advise there any algorithms in which you'd have you know positive examples and negative examples and you run logistic regression or something or there's V M or something to find the line find the decision boundary between them in unsupervised learning you're given unlabeled data so rather than given data with x and y you're given only X and so your training set now looks like x1 x2 up through XM and you're asked to find something interesting about the data so the first unsupervised learning algorithm we'll talk about is clustering in which given a data set like this hopefully we can have an algorithm that can figure out that this data set has two separate clusters and so one of the most common uses of clustering is market segmentation if you have a website you know selling things online we have a huge database of many different users and brand clustering to decide what are the different market segments right so there may be you know people were certain age range of a certain gender are people different age range different of Education and people the East Coast versus West Coast versus also in the country but by clustering you can group people into different groups right so I want to show you an animation of really the most commonly used the clustering algorithm called k-means clustering and let me show you an animation of what k-means does and then we'll write right out the map and then how you can implement it so um let's you're given a set like this so all these are unlabeled examples so just exported here and we want an algorithm to try to find maybe the two clusters here the first step of k-means is to pick two points denoted by the two crop two crosses called cluster centroids and the cluster centroids are your best guess for where are the Centers of the two clusters you're trying to find and then k-means is an iterative algorithm and repeatedly you do two things the first thing is go through each of your training example oh I'm sorry oh okay thank you right let me know if it happens again okay right so right so right you have two cluster centroids so the first thing you do is go through each of your training examples the green dots and for each of them you color them either red or blue depending on which is the closer cluster centroid so here we've taken every daunting color that you know red or blue depending on which side it is which costs essential is close to two and then the second thing you do is look at all the blue dots and compute the average right just find the mean of all the blue dots and move the blue cluster centroid there and similarly look at all the red dots and look at only the red dots and find it mean finding oh what's wrong with this that's it oh this thing was very strange all right apparently if I keep moving my mouse it doesn't do that all right thank you and they find a mean of all the red dots and move your red cluster centroid there so let me do that right so the cluster centroids move as follows to the mean of the red and the blue dot since it's just a standard arithmetic gap and then you repeat again where you look at each of the dots and color in either red or blue depending on which cross the central is closer so when I recolor every point based on you know what's closer so that's the new set of colors and then the second part of the algorithm was again look at the blue dots find a mean look at the red cards find the mean and then move the cluster centroids over excuse me to that mean okay and so and it turns out if you keep running the algorithm nothing changes so the arm has converged so if you look at this picture and you repeatedly color each point red or blue depending on which cross the central discloser nothing changes and if you repeatedly look at each the two clusters of colored on same computer mean and move the cluster there nothing changes so this album has converged even if you keep on running these two steps okay so um let's see let's write down in math what we just did all right so this is um a clustering algorithm and specifically this is a k-means clustering algorithm so your data set now does not come with any labels and so in k-means step one is initialize the cluster centroids right I'm gonna call them mu1 up to the UK random V so this was a step where you plop down the Red Cross and the Blue Cross and when I did it on the powerpoints you know I did it as it will just choose these as random vectors in practice is a good way of it they're actually the most common way to select their brand-new initial cross the centroids isn't quite what I showed is to actually pick K examples out of your training set and just set the cluster centroids to be equal to K randomly chosen the examples right so in the low dimensional space like a 2d plot you know they can do on the diagram it doesn't really matter but when you work with very hard dimensional data says the more common way to initialize these two just pick you know K training examples and set the cluster centroids to be at exactly the location of those examples but then the all dimensional spaces you know it doesn't make a big difference and then next you repeat until convergence one is right so this is a I just write this down so ever since so the two steps you would alternate between the first one is set CI for every value of I so for every example set C are equal to you know either 1 or 2 depending on whether that example X is closer to cluster Center 1 no cluster centroid to right so it's as taking point of color either red or blue or and we represent that by setting CI equals 1 or 2 if you have two clusters if K is equal to 2 oh the note say L 1 R squared from this morning one else was sent out this morning oh that's weird it shouldn't be L 1 norm if it says that one norm that's a mistake serve all that but usually and and it turns out whether you use L 2 norm L 2 norm square they give you the same answer because the augment is the same either way but it's usually do a type one of those oh I see oh god oh oh oh okay looks like a nose he wrote that okay cool but by default when we write that norm we actually use we mean L 2 norm yeah by default this is the L 2 norm of X it is unspecified if it's L 1 norm we usually write this so L 2 norm is more common and with what without the square you get the same image okay thank you all right so that's colored adults pain each dot either red or blue and then for this this is you know some career examples and take all the examples assigned to certain cluster right assigned to cluster J and set new J to be average of all the points assigned to that cluster chain yeah oh you know I don't think I don't know whatever all right none of the black markers are working this better alright let me try to use this is that part of this is unclear if this part is you can't see oh I'll write it out more clearly oh sure I do a place in France got it let there be light all right awesome great that was the easy request isatis like okay let you look at it for another minute all right okay thank you go for it and this wasn't positive okay all right now I can move it up all right um so it turns out that this algorithm can be proven to converge that exactly why is written out in the lecture notes but it turns out if you write this as a cost function so the cost function for a certain set of assignments of points of examples to cross the centroids and for a certain set of positions of the cluster centroids so so see these are the assignments and these are the centroids right so so this cost here is some of your training set what's the squared distance between each point and the cluster centroid did this assigned to so it turns out I won't prove this a little bit more details legend elsewhere on truth is it turns out then on every iteration K means we'll drive this cost function down and so you know beyond a certain point this cost function can't go even you can't go any lower look just this can't go below zero right and so this shows that k-means must converge release this function must converge because there's a strictly non-negative function that's going down on every derivation so at some point that has to stop going down and then you could declare gave me a self converged in practice if you're running k-means and i'm very very large dataset then as you plot the number of iterations j may go down and you know and and just because of lack of compute or lack of patience you might just stop this running after a while it is going down too slowly so that's sort of k-means in practice but maybe hasn't totally conversions just cut it off and call it good enough now the most frequently asked question I get the k-means is how do you choose K it turns out that I when I use k-means i still usually choose K by hand and so and why which is in a supervised learning sometimes it's just ambiguous right how many clusters there are what this data said some of you will see two clusters and some of you will see full clusters and it's just inherently ambiguous what is the right number of clusters so there are some formulas you can find online with criteria like AIC and B I sieve automatically choosing the number of clusters in practice I tend not to use them because I usually look at the downstream application of what you actually want to use k-means for in order to make a decision on a number of classes so for example if you're doing a market segmentation here because you're marketers want to design different marketing campaigns right for different groups of users then your marketers might have the bandwidth to design for separate marketing campaigns but now the hundred marketing campaigns so they'll be good reason to choose for clusters rather than hundred clusters so as often if you look at the purpose of what you're doing this for I think in the program exercise in homework do you see a image compression exercise where you want to cluster colors into smaller number of clusters do you implement this it's actually one of the most fun exercises I think but but so there you you know be saying well how much do you want to compress the image to decide how many clusters to try to use okay so I usually pick the number of clusters you know either manually or looking at what you want to use Kanis cluster for are you trying to cluster news articles like the Google News example I think I showed in the first nature you say well how many clusters kind of make sense for news articles okay all right so oh sure welcome to yet second local minima oh yes kami intercepts of the local minima sometimes and so if you're worried about local minima or the thing you can do is run k-means say ten times or 100 times 1,000 times from different random initializations of the cluster centroids and then run it you know say a hundred times and then pick whichever run resulted in the lowest value for this cost function alright so you play up this more in in the program exercise know there's a there's a problem that seems so closely related but but there's actually quite different where he's arrived the algorithms which is density estimation so let me motivate this I actually about well right some time back has some friends working on a problem which I simplified little bits of you know if you have aircraft engines coming off the assembly line alright and every time an aircraft engine comes on the assembly line you measure some features of this engine so you measure some features about the vibration and you measure some features of all the heat that the aircraft engine is producing and let's say that you gathered a set and the anomaly detection problem is if you get a new aircraft engine that comes off the assembly line and if the vibration feature it takes on this value and the heat feature takes on this value is that aircraft engine an anomalous one this is your right and so the application of this is that as your aircraft engine comes off the assembly line if you see a very unusual signature in terms of the vibrations and heat the aircraft engine is generating then probably something's wrong with this aircraft engine if your people have you have your team inspected further or tested further before you should the airplane before you ship the engine tort or airplane may occur and then something goes around the air and there's a there's a major accident a major disaster right and so anomaly detection is most commonly done or one of the common ways to implement anomaly detection is the model P of X which is given all of these blue examples given all these thoughts can you model what is the density from which X was drawn so then if P of X is very small then you flag an anomaly meaning that gee I think something's funny here and maybe someone should inspect this aircraft engine a little bit further sonar detection is used for tasks like this for inspection tossed like this is used for many years ago as su work of some telecoms providers with you know helping out telecoms company e on anomaly detection to figure out if something's gone wrong with part of this cells her network right so if one day one of the South Tower starts throwing off network patterns that seem very unusual then maybe something's wrong with that cell tower like that something's gone wrong it sent out the technician to fix it it's also used a computer security of a computer save computer Stanford start sending are very strange you know network traffic there's very unusual relative their views on the four browser what was this is a very anomalous network traffic then maybe IT stops you have a look to see if that good computer has been hacked so these are some of the applications that were an all new section and what good way to do this is given the unlabeled data set model P of X and then if you have very low probability examples you flag that as a possible anomaly for further study now given this data sets how do you model this one distinct thing about this green dots is that neither the vibration no the heat signature is actually out of range right you know like there are a lot of aircraft engines with vibrations in that range they're long of aircraft engines with heat in that range so neither feature by itself is actually data unusual it's actually the combination of the two that is unusual and so that's less what I want to do is uh come up with an algorithm to model this and in fact welcome of an algorithm they can model you know maybe maybe your data density looks like this made more of an L shape like that but how do you model P of X with the data coming from an L shape and it turns out that there is no textbook distribution right you know there isn't you know if you look at this simple and there's no exponential family model the types of distributions there is no distribution for modeling very very complex distributions like this so what I'm going to talk about is the mixture of gaussians volatile which would look for data like this and say it looks like this data actually comes from two Gaussian there's one Gaussian maybe that's one type of aircraft engine that you know it's drawn from a Gaussian like the one below and a separate aircraft type of aircraft engine that's drawn from a Gaussian like that above and this is why there's a lot of probably Mars in just O'Shea region by very low probability outside that O'Shay region right oh and these ellipses I'm drawing other contours of these two gaussians right and so what I'd like to do next is develop the mixture of gaussians model which is useful for an audience section and and and then those this will lead us to our second unsupervised so in order to make the mixture of gaussians model a bit easier to develop let me just use a one-dimensional example where so let's see so let's say that we gather data set that looks like this so it's just one roll number searches online I've plotted a few dots um so looks like this day there maybe comes from two gaussians or it looks like you know there's some data from this Gaussian and there's some data from that Gaussian on the right um and it's and if only we knew right which example had come from which Gaussian if if we knew that these examples that come from Gaussian one we wanted to know with crosses and if only we knew what the actually this finally fell over if only we knew that these examples that come from Gaussian to which I'm willing to draw with oles then we just fake calcium 1/2 the crosses figure out into the O's and then we'd be pretty much done right oh and sorry and so these are the two gaussians and so the overall density would be something like this right that's the probability of all the party muscle left while probably must know very low less probably mass on so the overall density just told again would be no high no high something like that right but the reason and then if you actually had these labels if you knew that these examples came from gaussian one those examples come from gaussian two then you can actually use an algorithm very similar to GD a gaussian difference to fit this model the problem with this density estimation problem is you just see this data and maybe the data came from two different gaussians but you don't know which example actually came from which coliseum okay so the e/m algorithm or the expectation maximization algorithm will allow us to fit a model despite not knowing which Gaussian each example so let me first write down the young mixture of gaussians model and then we'll describe the EML room for this so let's imagine let's suppose that there's a so the term we sometimes use this latent but latent just means hidden observed so so let's imagine that there's some hidden random variable Z and the term latent just means hidden on observe it means that it exists but you don't get to see the value directly so I say later it just means hidden on observe so let's imagine that this hidden or latent random variable Z and Xin Z I had this joint distribution and this this this is very very similar to the model you saw in Gaussian destroyers but Zi is multinomial with some set of parameters Phi for a mixture of two gaussians this would just be Bernoulli with two values but if you're a mixture of K calcium's then Z you know can take on values from 1 through K and it was two gaussians it just before nearly and then once you know that one example comes from Gaussian number J then X condition that Zi is equal to J that is drawn from a Gaussian distribution with some mean and some coherence Sigma okay so the two unimportant ways this is different than GTA one well I set Z to be one of K values instead of one of two values and GDA god-centered from analysis we had Z know why the labels Y took on one of two values and then second is I have Sigma J instead of Sigma so by convention when we feed mixture of gaussians models we let each gaussian have his own covariance matrix Sigma we could actually force it to be the same way you want but these are the trivial differences the most significant difference is that in Gaussian districts I Y I whereas Y was observed and the main difference between this and Gaussian disappearing analysis is now we have replaced that with this latent or hidden random variables Z are they do not get to see in the training set okay so all right that was better all right so if we knew the sea-ice right then we can use maximum likelihood estimation right so if only we knew the value of the Z is which we don't but if only we did then we could use maximum likelihood estimation or mo e to estimate everything you know so we were right the log likelihood other parameters equals some log P of X our Zi you know given the parameters right and then you take the river to set the ders equal to zero and you guys did this in problem set one right and then you find that Phi J is equal to 1 over m okay so if only you knew the values of the sea-ice then you could use maximum likelihood estimates and this is what you get and this is pretty much the formulas actually these two are exactly the formulas we had for Gaussian Tuscon analysis except we'll replace Y with Z and then there's some other formula for Sigma just written in the lecture notes but I won't that one right down here okay um but the reason we can't use this use these formulas we don't actually know whether the values of Z so what we will do in the e/m algorithm is two steps in the first step we will guess the value of the Z's and in the second step we will use these equations using the values of disease we just guessed so let me so sometimes in machine learning something to call this a bootstrap procedure where you get something they run an algorithm you're using your guesses and then you update your guesses and then run the algorithm okay let me let me make that concrete by writing this down so the e/m algorithm has two steps the a step also called the expectation step is set w IJ so W IJ is going to be the probability that Zi is equal to J okay given all the parameters and and much as we did with generative learning algorithms right with generative learning algorithms we'll use Bayes rule to estimate the probability of Y given X and so to compute this you use a similar Bayes rule type of calculation and so disappear right where for example this term here P of X i given Z I equals J this would be a Gaussian density right this comes from a Gaussian density with mean mu J and covariance Sigma J right and so this term here would be a 1 over you know 2 pi it's an N over 2 Sigma J and then this term here I guess this would be a Phi J that's just a Bernoulli probability remember Z is multinomial right Suzy this multinomial we're parameters Phi so I guess the parameters v for multinomial distribution tell you what's the chance of Z B 1 2 3 4 and so on up to K so the chance of Zi being for the K is just this chance of Zi pee really Jane is just Phi J right it's just read it off one of the parameters and your multinomial probability but for the also CV different values okay and so and similarly the terms of denominator this term here is from Gaussian and that second term is from the multinomial probability that you have for Z and so that's how you plug in all of these numbers and use Bayes rule use this equation to compute given all given the position of all these gaussians what is the chance of W IJ taking on a certain value and and and so to to make this really concrete you remember how I guess ones and zeros are the other way if you were to look at these if you were to scan through right to left remember how you know you give the sigmoid function right the same point can be this way or this way or interval sign I guess these are positive examples these negatives you have a sigmoid function like this and so W IJ is just the height of this Sigma is just a chance you know each of these examples being coming from either the Z equals 1 is equal 0 and then you store all of these numbers in the variables W IJ okay so W IJ is just compute the posterior chance of discharge this example coming from the nest Gaussian present right now saying they just saw that W IJ so that's the e set and you compute the W IJ for every single training example I mix the m-step is sorry is this what oh this one you're sorry okay so in the so the e step tells us you know trying to guess the values of the Z's right we figure out what's the probability of Z being one two three four after Cain was stolen here and then in the m-step what we're going to do is use the formulas behalf for maximum likelihood estimation and I want you to compare these with the equations I had above okay see but so these equations are a lot like the equations above except that instead of indicator Z I equals J we replaced it with W IJ right which by the way is the expected value of this indicates a function because the expected value of an indicator function is just equal to the probability of that thing in the middle being true and then there's a formula for Sigma J as well that's all you can get from the lecture notes but I want I won't write down here okay so one intuition of this mixture of gaussians algorithm is that it's a little bit like k-means but with Sophos i'm in so in k-means in the first step we will take each point and just assign it to one of the clade k cluster centroids right and it was a little bit closer to the red cluster centroid than the blue cluster centroid we would just assign it to the red cross so even with just a little bit closer one closer than another k-means we just make what's called a hard assignment meaning you know whatever plus the centroid is closed assume we just assigned at a hundred percent to that say cluster centroid so yeah is you can think well yeah implements a softer way of assigning points to to the different cluster centroids because instead of just picking the one closest Gaussian Center and the signing of there it uses these probabilities and gives it a weighting in terms of how much the sign to calcium want versus gals into and then second obtains you know the means accordingly write sum over all the excise to the extent you're assigned to that cluster centroid divided by the number of examples assigned to a cluster centroid okay so so so that's one intuition behind between them and k-means and in a second but but when you run this algorithm it turns out that this algorithm will converge with some caveats I'll get to later and this will find a pretty decent estimate of the parameters you know say fitting a mixture of two gaussians model so this is some they owe and so if you are given the data set of say airplane engines you can run this algorithm for the mixture of two gaussians and then when a new airplane engine rolls off the assembly line you so after your fitting the k-means algorithm you now have a after 15 ml room you now have a joint density of a p of x comma Z and so the density for X is just sum over all the values of Z P of X comma Z and so and so a mixture of gaussians can fit distributions that look like this it can fit distributions that look like this right there's these up these are both mixtures of two Gaussian so this gives you a very rich family of models to fits very complicated distributions and now that right and you've also fit no know something like this so this is a mixture of two gaussians I guess one thin narrow Gaussian here and one much wider fatter Gaussian so mixture of two gaussians can't you fill them all the different things can fit a lot and the mixture of more than two gaussians can fit even richer models and so by doing this you can now model P of X for many complicated densities including this one this example I just now this will allow you to fit a priori density function that puts some all the promos on on a region that looks like this and so we have a new example you can evaluate P of X and a P of X is large then you can say you know this looks okay and the P of X is less than Epsilon you can find in an RV and say Oh take a look take another look at this airplane engine okay so um I kind of just wrote down this algorithm with a little bit of a hand wavy explanation at the house derive right so I said if only you knew the values of C and just used maximum likelihood estimation so let's guess the values of Z and then plug that into the formula so maximum IQ estimation it turns out that hand-wavy explanation works in the particular case of Yampa mixtures of gaussians but that there is a more formal way of deriving the EML rhythm that shows that this is a maximum likelihood estimation algorithm and then it converges at least the local optimum and in particular there what we'll do is show that if you go is given a model P of X Z prior tries by theta if you go this to maximize P of X right excuse me right so this is what maximum likely you're supposed to do that eeehm is exactly trying to do that okay so I'll go on in a minute present this more general derivation that the full morris derivation of the e/m algorithm that doesn't rely on this hand wavy argument of thus guesses ease and use master like you were to guess value so I'll do the rigorous derivation of VM in a minute but before I do that let me just pause and check if there are any questions maybe let's see maybe I'll help to not think of them as weights yeah I think this is actually there waiting you assigned to a certain Gaussian so that's one intuition and hence weights but so one way to think of this as W IJ is how much X I is assigned to you know to do so W IJ is a strength of how strongly you want to assign that training example X I to that cluster or to that to that particular Gaussian and so this is a number of G 0 and 1 right and the strength of all the assignments and every point is a sign with a total strength equal to 1 because all these properties must sum up to 1 and so when I take this point and assign it you know 0.82 more close gaussian and point to to a more distinct and this is our guest though you know well there's an 80% chance of him but that gal seen a 20% chance of camera a second girl seen this make sense oh I see so let's see um so when you're running the ml room you never know whether the true values of Z all right you're given the data set so you only told the excess and false we know these airplane engines were generated off you know two different gaussians maybe there are two separate assembly processes you know one from the one from plot number one one from plot number two and maybe they're actually operate a little bit differently but by the time they merge onto one but by the time the two supplies of aircraft engines get to you they've been mixed together and so you can't tell anymore which aircraft engine came from profit pond one and which profit aircraft engine came from plant - I don't even know there are two fonts you just see the stream of aircraft engines you're hypothesizing they're the two types and so in every iteration of PM you're taking each aircraft engine and guessing you know for this one I think does 80% chance that came for process one the 30% chance came for process - so that's the e step and then in the m-step you look at all the engines that you're kind of guessing were generated by process one and you update your Gaussian to be a better model for all of the things that were that you kind of think were generated by process one and if there's something that you're absolutely sure came from process one then it has a weight of one close to one and this do you think there was something that you know are the 10% chance come to process 1 then that example is given a lower weight and how you update the meaning for that all right so well I still remember when I was an undergrad doing a summer internship at AT&T Bell Labs and then someone the few offices down had learned about diem for the mixture of gaussians her first time was running on his computer and he's going around to every single office saying oh my god you gotta check this out this is unbelievable look at what this elephant can do Tiffany makes is a Gaussian so it shows you those other people I hang out with all right um so in order to derive you know so slightly hand wavy arguments that oh let's get to let's guess the values of the Z's let's just have these ways and plug them into maximum likelihood um what I like to do is give a more rigorous derivation for ye M algorithm is a reasonable algorithm and Y is a massive likely estimation algorithm and why we can expect it to converge and it turns out there rather than just proving you know that this is a sound algorithm what we'll see on Wednesday is that this view of p.m. allows us to derive em in a in a more correct way for other models as well they make sense of gaussians on Wednesday we'll talk about a model called factor analysis unless you model gaussians an extremely high dimensional spaces where if you have a thousand dimensional data but only thirty examples how do you for the girls into that so we talked about that on Wednesday and it turns out this derivation that yeah we're gonna go about through now is crucial for applying M accurately in problems like that so in order to lead up to that derivation let me describe Jensen's inequality so let F be a convex function to do yeah we're actually going to need concave functions so be all - of everything but what gets it done in a second but so a convex function means the second derivative is greater than 0 or in other words it looks like that right so that's a convex function that X be a random variable then F of the expected value of x is less than equal to the expected value of x maybe young here's an example right so here's a let's see that's the function f of X and let's say that these are the values 1 2 3 4 5 and suppose that X is equal to 1 with probability 1/2 is equal to 5 probably just an illustration then here is the F of 1 here is F of 5 here is f of 3 and F of 3 is f of the expected value of x right because so the expected value of x and sometimes I write 2 so called the square brackets it's the average of X is equal to 3 and so the expected value seems to be F of the expected value of x is equal to this value whereas the expected value of f of X is the mean of F of 1 and F of 5 right so the expected value of f of X f of X is a 50% chance of being F of 1 and a 50% chance of being a 4/5 and so the expected value of f of X is equal to this value in the middle let's really take these two take this value and this value and take the mean so it's this value up here and and this value expensive value and so in this example the expected value of f of X is greater than F of the expected value of x as predicted by Jensen's inequality I'm going to just draw one illustration that may or may not help is some of my friends like it I sometimes use it but it was confusing then don't worry about it but it turns out that if you draw a line that connects these two then the midpoint of this line is the height of F of expected value of x right so the height of this you know so given these two points this point in this point if you draw this line it's called a chord then the height of this point is expected value of f of X and this point is f of the expected value events and in any convex function you know really take any convex function that's also called back function if you draw any chords that mean point it's always higher right then that group Green Point which is Y which is another way of seeing Y Jensen's equality holds true okay if this visualization doesn't help don't worry about it but it's just so actually what a lot my friends do is we the cell you know we keep on forgetting which direction Jensen's equality goes that's not great that Sol all of my friends were don't remember we draw this picture and draw that chord and if we quickly figure out which we do equality girls all right so one addendum further it's strictly greater than zero and so if this is the case we say F is strictly convex so let's see a straight line is also convex function right so this is the convex function this congressional district on various ocean turns out a straight line that's also a convex function but so in this addendum is saying that if F is a strictly convex function meaning racing it's not a straight line right bit more than is not a straight line but if the curvature if it's always bending up then the only way for the left and right hand sides to be equal is an X is a constant meaning it's a random variable that always takes on the same value okay so Jensen's equality says that you know left hand sides got to be the same as right hand side sorry I think I reversed the order of these two for that equation that doesn't matter right so Jennison equality says left hand side is always less than equals to the right hand side and the only way is equal is if X you know is a random variable that always takes on the same value yeah so it turns out what if the other have one single that now the another feat it turns out does vary so let's see so one way that could happen would be if the function were like that and then if you take the drawing horde we take the meanest no higher then this Impala if you had a flat part here then the function is not strictly convex and so it's still less than equal to but it's not but it can't be equal to Y of X is random so um and and and we'll use this in a little bit well actually end up using this and again for the strict proper low states you know if those of you that don't know take classes in advanced probability the technical way of saying X is a constant is X's equal to DX we're probability one you know what I think for all practical human purposes you do not need to worry about this but if you think the cost in measure theory the professor in measure theory will be happy if you say this then you say X is a constant but maybe maybe none of you know okay this is don't worry about it oh yes okay now um just one one more addendum to this is that the form of Jensen's equality we're gonna use is actually a form for a concave function so instead of convex I'm gonna say concave and so you know a concave function is just a negative of a convex function right if you take a convex function and take negative of that it becomes concave and so the whole thing works with the with everything flipped around the other way okay so the phone with Jensen's inequality we're going to use it's actually the concave foam with Jensen's equality and we're actually going to apply it to the log function so the log function write log X looks like this and so that's a concave function and so the inequality will use to be in this direction to have an orange all right so just the density estimation problem meaning density estimation means you want to estimate P of X all right so we have a model of a P of X comma Z with parameters theta and so you know instead of writing out Mu Sigma Nu Sigma Phi like we did for the mixture of gaussians I'm just gonna capture all the parameters you have whatever your parameters are obviously capture them in one variable theta and you only observe thanks so your training set looks like that so the UM log likelihood of the parameters theta is equal to some of your training examples log hearings i franchised by theta and this in turn is log of sum over Z P of X I see I franchise by theta right because P of X you know is just taking the Joint Distribution and summary notes marginalizing out Zi and so what we want is maximum likelihood estimation which is to find the value of theta that maximizes is long likelihood and what well like what we'd like to do is derive in year now derive an algorithm which will turn out to be an e/m algorithm as an iterative algorithm for finding the mass of life for an estimate of the parameters theta so let me draw a picture that I could keep in mind as we go through the math which is you know the horizontal axis is a space of possible values of parameters theta and so there's some function o of theta then you try to maximize and so what yen does is lesson you initialize theta at some value may be randomly initialize so similar to the k-means clustering we just you know randomly initialize your muse for that ratio gaussians what the IAM algorithm does is in the east step we're going to construct a lower bound shown in green here for the log likelihood and this lower bound is being curve has two properties one is it is a lower bound so everywhere you look you know over all values of theta the green curve lies below the blue curve so this is a lower bound and the second property that the green curve has is that it is equal to the blue curve at the current value of theta so what the east step does which you will see later on and just keep this picture in mind as we go through the east of an e/m set is um it'll construct the lower bound it looks like this right oh and and also to foreshadow a part of the derivation right there was that addendum to Jensen's equality what we said well under these conditions it holds with equality right here if f of X equals F of G of X we said well the two things are equal with under certain conditions we want things to be equal we want the green curve to be equal to the blue curve at the old value of theta so what we'll use that addendum to just inequality when we do like that so that's estep is draw the green curve and then what the m-step does is it takes a green curve and it finds the maximum so what the em set does is it takes a green curve and it finds the maximum and one step of eeehm will then move theta from this green value to this red value okay so the e step constructs the green curve and the m-step finds the maximum of the green curve and this is one iteration of M the second iteration of M now that you're at this red thing is will construct a new lower bound again you know is it different though about everywhere the red curve is below the blue curve and the values are equal at this new value that's the e step and an M step will maximize this red curve and so on now you're here construct another thing do that and you can kind of tell that they keep running eeehm this is constantly trying to increase L of theta trying to increase the log likelihood until it converges to local optima they give Albert does converge only to local Optima so if you if there was another even bigger thing there that they may never find its way over to that other better optimum but the e/m algorithm by repeatedly doing this will hopefully converse to a pretty good local optimum all right so that's right to how we do that so I've already said that our goal is to find the Frances data then maximize this and so that equation we said or just now is some of our I log some of the Zi P X Y comma Z i given theta okay so this is just what we had written down I guess on the left what I'm going to do next is divided by I must find if I buy this where a Qi of Zi is a probability distribution ie some of us Zi Qi of Zi equals one so with the multiplying defined by some high resolution and we'll decide later how to come up with this probably this usually Qi right but you know I'm allowed to construct a prize distribution and multiply and divide by the same thing right now if you look at this all right let's put square brackets here if these Qi is this a probably distribution meaning that some of us Zi Qi Zi sums over from some some one then this thing inside is equal to sum of I of an expected value of Zi drawing from the Qi distribution we use colors to make this clearer right so the way you compute the expected value of you know some function of Z is you sum over all the possible values of Z I of the property of Zi times what if that function is so this equation is just the expected value who respect to Z I drawn from that Qi distribution of that thing in the square brackets in the purple square brackets now using the concave form of Jensen's inequality we have that this is greater than or equal to so this is a form of Jensen's equality where f of X is greater than or equal to X where here this is the logarithmic function so the log function is a concave function it looks like that and so using the I guess you use it using the form of Jensen's equality with the science reverse write f of e^x is great enclose in a of FX so you get log of expectation is pretty equal to expectation it along and then finally let me just take this expectation and unpack it one more time so this is now sum of I sum of Zi so I just took this expected value and turn the back to the sum over random variable probably times that thing okay so if you remember this picture from the middle what we wanted to do was to construct a function construct this green curve there's a lower bound for the blue curve and if you view this formula here as a function of theta right so your X X is just your data and Z is a variable you sum over so this whole thing is the function of theta or because X is FX Z is yourself you found some over so this whole formula here this is a function of the parameters theta and what we're showing is that this thing you know this formula here this is a lower bound for the log likelihood I thought for this thing I guess this is our theta so oh how we got to disagree sure sure so messy let's go let's say that Z takes on values from 135 right unless these details on Val's room one through ten zero attend sided guys and I want to compute you know the expected value of some function of some function G G of Z right then expected value G of Z is sum of all the possible values of C of the probability do you get that Z times G of Z right so that's that's what's the expected value of a function of a random variable right and and this is the expected value of Z is some of us Z P of Z times Z right that's the average of random variable and so in the notation that we have the probability of Z taking on different values is to note about a of Z which is why we wind up with that formula does make sense okay all right if one of these steps doesn't make sense then you know other questions okay all right now one of the things we want when constructing this green lower bound is we want that green lower bound to be equal to the blue function at this point right this is actually how you guarantee that when you optimize the green function by improving on the green function you're improving on the blue function so we want this lower bound to be tight right to meet the two functions being equal or tangent to each other so in other words we want this inequality to hold with equality so we want yeah so we want the left-hand side on the right-hand side to be equal for the current value of theta so on a given iteration with the current perhapses equal to theta we want we want I know this is a lot of math but you know we wanted the left and right hand sides to be equal to each other because that's what it means for almost for the lower bound to be tight for the green curve to be exactly touching the blue curve as we construct that know about and so for this to be true we need the random variable inside to be a constant so we need p of x i zi / qi of zi to be equal to constitutes a constant meaning that no matter what value of Zi you plug in this should evaluate to the same value you know in other words the ratio between the numerator and denominator must be the same and fortunately so far but not yet specified how will choose this distribution for zi right so so far the only constraint we have is that Qi has to be a probability density has their probability distribution over Zi we could choose whatever distribution you want for Zi and it turns out that we can set qi of zi to be proportional to p of x i zi parametrized by theta and this means that for any value of Z you know whether those e indicates is it from Gaussian one of Gaussian to right so this means that the chance of Gaussian one is proportional to the chance of Gaussian one versus goes into whether Zi takes on one or two is proportional to this and I don't want to prove it but one way to ensure this and this is proven in the lecture notes but it turns out that one way to ensure well so the Q is need to sum to one so one way to ensure that this is proportional to the right hand side is to just take the right hand side so one so let's see right so the cure eyes have to sum to one and so one way to ensure the proportionality is to just take the right hand side and normalize it it's something one and after after a couple steps that intellectually I don't want to do here you can show that this results in 7qi of zi to be equal to that that posterior probability okay and so sorry I skipped a couple of steps here you can get from the lecture notes but it turns out that if you want this to be constant meaning whether you plugged in CI equals 1 or Z equals 2 or whatever disavows the same constant the only way to do that is make sure the numerator and denominator are proportional to each other and because qi of zi is a density that must sound one one way to mr. Li proportional it was to just said this to be really right hand side but normalize the sum to one okay and then we derived this a little bit more carefully your lecture notes so just to summarize this gives us de em algorithm let's take all this everything we're just doing Rapids Indian algorithm and it Estep we're going to set Q I of Zi equal to that and previously this was the W IJ s right so incentive so previously restoring these probabilities and the variables you call WI J's and then in the M step we're going to take that know about that we constructed which is this function and maximize it with respect to theta okay and so remember in the M set we constructed this thing on the right hand side there's a lower bound for the log likelihood and so for the fixed value of Q you can maximize this respect to theta and that updates the theta you know maximizing the green lower boundary that's what the end step does and if you any rate these two steps then you find that this should converge to the whole optimal okay oh and there's just maybe that's the obvious question um why don't we try to maximize vary theta why we try to massage the log like indirectly it turns out that if you take the mixture of gaussians model try to take derivatives of this and set their 2 is equal to 0 there's no known way to solve for the value of theta the maximizing the log likelihood but you'll find that for the mixture of gaussians model and for many models including factor analysis we talked about on Wednesday if you actually plug in the Gaussian density if you actually plug in the mixture of gaussians model for p and take you know take take the riveter cetera t goes here and solve you will be able to find an analytic solution to maximize this M step and it'll be exactly what we have worked out ok but so this derivation shows that the yam algorithm you know is a maximum likelihood estimation algorithm with optimization solved by constructing little balance and optimizing those bounds ok all right that's it for today and only it's tough up to here right and so this stuff will be up to midterm but we're talking about on factor analysis we're not on my way okay so let's break for today and I'll see you guys on Wednesday