morning and welcome back so what we'll see today in class is the first in-depth discussion of a learning algorithm linear regression and in particular over the next one hour and a bit you see a linear regression batch and Tsukasa Granderson's is an algorithm for fitting linear aggression models and then the normal equations as a way of is a very efficient way to let you fit linear models and we're going to define notation and a few concepts today that will lay the foundation for a lot of the work that we'll see the rest of this quarter so to motivate linear regression scofield maybe the may be the simplest one of the simplest learning algorithms you remember the Alvin video the autonomous driving video that I had shown in class on Monday for the self-driving car video that was a supervised learning problem and this term supervised learning meant that you were given access which was a picture of what's in front of the car and the algorithm had to map that to an output Y which was the steering direction and that was a regression problem because the output Y that you want is continuous value right as opposed to a classification problem where Y is the speed and we'll talk about classification next Monday but supervised learning regression so I think the simplest maybe the simplest possible learning algorithm a supervised learning regression problem is linear regression and to motivate that rather than using a saw driving car example you know which is quite complicated it will build up a supervised learning algorithm using a simpler example so let's say you want to predict or what estimate prices of houses so the way you'd build a learning algorithm is start by collecting a data set of houses and their prices so this is a data set that we collected off Craigslist a little bit back this is data from Portland Oregon but so there's a size of house in square feet and that's the price of a house in thousands of dollars right so there's a house that is a 2100 full square feet who's asking price was $400,000 pulse with that size with that price and so on okay and maybe more conventionally if you plot this data with there's the size that's the price see some data set like that and what would end up doing today is fit a straight line to this data I didn't go through how to do that so in supervised learning um the process of supervised learning is that you have our training set such as the data set that I drew on the left and you feed this to learning algorithm and the job of the learning algorithm is to output a function to make predictions about housing prices and by convention I'm gonna call this function that it outputs a hypothesis and a job with the hypothesis is you know it will it can input the size of a new house the size of a different house that you haven't seen yet and will output the estimated price okay so the job of the learning algorithm is to input a training set and out for the hypothesis the job with hypothesis is to take as input any size of a house and try to tell you what if things should be the price of that house now when designing a learning algorithm and and you know even though linear regression right you may have seen it in a linear algebra class before in some class before the way you go about structuring a machine learning algorithm is important and design choices of you know what is the workflow what does the data say what is the hypothesis represents a hypothesis these are the key decisions you have to make in pretty much every supervised learning every machine learning algorithms design so as we go through the new regression I'll try to describe the concepts clearly as well because they'll lay the foundation for the rest of the algorithms sometimes much more completely I'll go as you see later this quarter so when designing a learning algorithm the first thing we'll need to ask is um how do you represent the hypothesis and in linear regression the for the purpose of this lecture we're going to say that the hypothesis is going to be input size X and output a number as a as a linear function of the size X okay and then the mathematicians in the room you say technique doesn't have fine function it was a linear function there's no theta zero technically you know they've been in machine learning is sometimes just causes a linear function but technically is an affine function it does it doesn't matter so more generally in this example we have just one input feature X more generally if you have multiple input features so if you have more data more information about these houses such as number of bedrooms excuse me mom handwriting's okay that's the word bedrooms then I guess my father-in-law lives a little bit outside Portland and he's actually really into real estate so this is that your real data set in Portland so more generally if you know the size as was the number of bedrooms in these houses then you may have a two input features where x1 is the size and x2 is the number of bedrooms I'm using the pound sign bedrooms to denote number of bedrooms and you might say that you estimate the size of a house as H of X equals theta 0 or Stata 1x 1 plus theta 2 x2 where x1 is the size of the house and x2 is is the number of bedrooms okay so in order to so in order to simplify the notation in order to make that notation a little bit more compact I'm also going to introduce this other notation where we want to write the hypothesis as sum from J equals 0 to 2 of theta J XJ so the summation where for conciseness we define X 0 to be equal to 1 ok see we define if you define X 0 to be a dummy feature that always takes on the value of 1 then you can write the hypothesis H of X this way sum from J equals 0 to 2 or just theta J XJ it's the same with that equation that you saw to the upper right and so here theta becomes a three dimensional parameter theta 0 theta 1 theta 2 this index starting from 0 and the features become a 3 dimensional feature vector X 0 X 1 X 2 where X 0 is always 1 X 1 is the size of the house and X 2 is the number of bedrooms of the house so to introduce a bit more terminology theta is called the parameters of the learning algorithm and the job of the learning algorithm is to choose parameters theta that allows you to make good predictions about your prices of houses right and just to lay out some more notation that we're going to use throughout this quarter I'm going to use a standard that M will define as the number of training examples so M is going to be the number of rows right in the table above where you know each house you have your training said just one training example you've already seen me use X to denote the inputs and often the inputs I'll call features you know I think as a as an emerging discipline grows up right notation kind of emerges depending on what different scientists use for the first time when you write a paper so I think that you know I think that the fact that we call these things hypotheses frankly I don't think that's a great name but but I think someone many decades ago wrote a few papers calling a hypothesis and then others follow and we kind of stuck with some of this terminology but X is what's called input features a sentence input attributes and Y is the output right and sometimes we call this a target variable and so X comma Y is one training example and I'm going to use this notation X superscript I comma Y superscript I in parentheses to denote the training example okay so the superscript parentheses I that's not exponentiation I think that as we build this is this notation X I comma Y I this is just a way of writing an index into the table of training examples above so so maybe for example if the first training example is the size house of science to 104 so X 1 1 would be equal to 2104 right because this is the size of the first house in the training example and I guess X the second example feature one would be one four one six with our example though so the super strip in parentheses is just some it's just the index into the different training examples where I superscript I here we're running from one through m1 through the number of training examples you have and then one last bit of notation I'm going to use n to denote the number of features you have for the supervised learning problem so in this example n is equal to 2 right because we have two features which is the size the house and the number of bedrooms so two features which is why you can take this write and write this as a sum from J equals 0 to n and so here X and theta are n plus 1 dimensional because we added the extra X 0 and theta 0 ok so so if you have two features then these are three dimensional vectors and more generally if you have n features you end up with X and theta being n plus 1 dimensional features all right and you know you see this notation multiple times in multiple algorithms throughout this quarter so if you you know don't manage to memorize all these symbols right now don't worry about it you see them over and over and over come familiar alright so um given the data set and given that this is the way you define the hypothesis how do you choose the parameters right so you're the learning algorithms job is to choose values for parameters theta so that it can output hypotheses so how do you choose parameters theta well what we'll do is let's choose theta such that H of X is close to Y for the training examples so and I think the final bit of notation I've been writing H of X as a function of the features of the house as a function of the size and number of bedrooms the house sometimes to emphasize that H depends both on the parameters theta and on the input features X I'm going to use H subscript theta X to emphasize that the hypothesis depends both on the parameters and on the you know input features X right but sometimes for notational convenience I just write this as H of X sometimes I include the theta there and they mean the same thing it's just maybe a abbreviation in notation but so in order to learn set of parameters what we'll want to do is choose a parameters theta so that at least for the houses whose prices you know that you know the learning algorithm outputs prices that are close to what you know were the correct prices for that set of houses with their compare asking prices for those houses and so more formally in the linear regression algorithm also called ordinary least-squares with a linear regression we will want to minimize I'm going to build out this equation one piece of the time okay minimize the squared difference between what the hypothesis outputs H subscript theta of X minus y squared right so let's say we want to minimize the squared difference between the prediction which is H of x and y which is a correct price and so what we want to do is choose values of theta that minimizes that to fill this out you know you have M training examples so I'm going to sum from I equals 1 through m of that squared difference so this is sum over I equals 1 through all say 50 examples you have right the squared difference between what your algorithm predicts and what the true price of the house is and then finally by convention we put up one-half there it's put a 1/2 constant there because when we take derivatives to minimize this later putting 1/2 there will make some of the math a little bit simpler so you know changing 1 adding a 1/2 minimizing that formula should give you the same ran says minimize a 1/2 of that but we often put a 1/2 there since I make the math a little bit simpler later ok and so in linear regression I'm gonna define the cost function J of theta to be equal to that and we'll find parameters theta that minimizes the cost function J of theta ok and questions often gotten is you know why squared error why not absolute error or this error to the power of 4 we'll talk more about that when we talk about when we talk about a generalization of linear regression when we talk about generalized linear models we should do next week you see that linear regression is a special case of a bigger family of algorithms called generalizing the models and using squared error corresponds to a Gaussian but justified may be a little bit more Y squared error rather than absolute error or error to the power 4 next week so um let me just check see if any questions okay cool alright so um so let's Knicks let's see how you can implement an algorithm to find a value of theta that minimizes J of theta that minimizes the cost function J of theta we're going to use an algorithm called gradient descent and you know there's our first loss seeking this Austrian so trying to figure out what just takes like this all right and so with gradient descent we are going to start with some value of theta and it could be you know theta equals the vector of all zeros would be a reasonable default we could initialize a random you can't doesn't really matter but theta is this three dimensional vector and I'm writing zero with an arrow on top to denote the vector of all zero so zero with an arrow on top does it vector there's a zero zero zero everywhere right so so stop to some you know initial value of theta and we're going to keep changing theta to reduce J of theta okay so let me show you a but let me show you a visualization of gradient descent and and it will write all the math so alright let's say you want to minimize some function J of theta and is importantly get the axis right in this diagram right so in this diagram the horizontal axes are theta 0 and theta 1 and what you want to do is find values for theta 0 and theta 1 in our in our examples as you say the zero theta 1 theta 2 cos theta 3 dimentional I can't plot that so I'm just using theta 0 and theta 1 but what you want to do is find values for theta 0 and theta 1 right that's the right you want to find values of theta zero and theta one that minimizes the height to the surface J of theta so maybe this this looks like a pretty good point or something okay and so in green descent you you know start off at some point on this surface and you do that by initializing theta 0 and theta 1 either randomly or to the value of all zeros or something doesn't doesn't matter too much and what you do is imagine you are standing on this little hill right standing at the point at that little extra that little cross what you're doing great in the sentence is turn on turn around all 360 degrees and look around you and see if you were to take a tiny little step you know take a tiny little baby set in what direction should you take a little step to go downhill as fast as possible because you're trying to go downhill which is go to the lowest possible elevation go to the lowest possible point of J of theta so what you're in descent will do is a stand at that point look around look all around you and say well what direction should I take a little step in to go down so as quickly as possible because you want to minimize J of theta you want to reduce the value of J of theta you want to go to the lowest possible elevation on this pillow and so Grint descent will take that little baby step right and then and then repeat now you're a little bit lower on the surface so you can take a look all around you and say oh looks like that he'll let that little direction this cheapest direction of the steepest gradient downhill so you take another little step take another set another step and so on until until you until you get to a whole via local optima now one property or green descent is that um depending where you initialize parameters you can't get to local different points right so previously we had started it at that little point X but imagine if you had started it you know just a few steps over to the right right that new axis of the one on the Left if you are on green descent from that new point then that wouldn't be the first step there on the second step and so on and you would have gotten to a different local optimum your different level okay it turns out that when you run gradient descent on linear regression it turns out that there will not be local optimum the world we'll talk about that a little bit okay so let's formalize D gradient descent algorithm [Music] in gradient descent each step of gradient descent is implemented as follows so so remember in this example the training set is fixed right you you know you've collected the data set of housing prices from Portland Oregon so you just have to add so in your computer memory and so the data centers takes the cost function J is a fixed function this function parameters theta and the only thing you're gonna do is tweak or modify the parameters theta one step of gradient descent it can be implemented as follows or just say that J gets updated as say that J - I'll just write this out so bit more notation I'm gonna use : equals and let me use this notation to denote assignment so what this means is we're going to take the value on the right and assign it to theta on the left right and so so in other words in the notation will use this quarter you know a colon equals a plus one this means increment the value of a by one whereas you know a equals B if I write a equals B I'm asserting a statement of fact I'm searching that the value of a is equal to the value of B and hopefully I won't ever write a equals a plus one right because because that is really true alright so in each step of gradient descent you're going to for each value of J so you're gonna do this for J equals 0 1 2 or 0 1 up to n where n is the number of features for each value of J takes a DJ and update it according to theta J minus alpha which is called the learning rate alpha the learning rate times this formula and this formula is the partial derivative of the cost function J of theta with respect to the parameter theta J okay and then this is partial derivative notation for those of you that I know haven't seen calculus for a while or haven't seen you know some of their prerequisite or a while we'll go over some more of this in a little bit greater detail in discussion section but I'll do this quickly now but I know if you take a look calculus class a while back you may remember that the derivative of a function is you know defines the direction of steepest descent so it defines the direction that allows you to go downhill as steeply as possible on the hill and dance question oh how do you determine learning rate let me get back to that it's a good question for now you know there's a theory there's a practice in practice you said to 0.01 let me say a bit more about that later if you actually if you scale all the features between zero and one you know minus one and plus one or something like that and then try you could try a few values and see what lets you minimize the function best but if the features are scale to plus minus one I usually start with 0.01 and try increasing and diffusing it say say a little more about it all right cool so um let's see I'm just quickly show how the derivative calculation is done and you know I'm gonna do a few more equations in this lecture and then and then over time I think all of this all of these definitions and derivations are written out in full detail in the lecture notes posted on the course website so sometimes I'll do more math in class when we want you to see the steps of the derivation and sometimes the save time and cost will gloss over the mathematical details that leave you the read over the full details in the lecture notes under sisters you know course website so partially whatever strategy of theta that's the partial er of respect to that 1/2 H of theta of X minus y squared and so I'm gonna do a slightly simpler version assuming we have just one training example right the actual derivate definition of J of theta has a sum over I from 1 to M over all the training examples so I'm just forgetting that some for now so if you have only one training example and so from calculus if you take the derivative of a square you know the two comes down and so that cancels out with the hall so two times one half times the thing inside right and then by the chain rule of derivatives that's times the partial derivative of theta J minus y so if you take the derivative of a square the two comes down and then you take the derivative of what's inside and multiply that right and so the 200 one half cancel out so this leaves you with minus y times partial derivative respect to theta J of say the zero X zero plus theta 1 x1 plus dot plus theta n X and minus y I where I just took the definition of H of X and expanded it out to that to that sum right because H of X is just equal to that so if you look at the partial derivative of each of these terms with respect to theta J the partial derivative of every one of these terms respect to theta J is going to be 0 except for the term corresponding to J because you know if if J was equal to one say right then this term doesn't depend on theta 1 this term this term all of them do not even depend on theta 1 the only term that depends on theta 1 is this term over there and the partial derivative of this term respect to theta 1 would be just X 1 and so when you take the partial derivative of this big some with respect to say the J intercept just J equals 1 and respect to theta J in general then the only term that even depends on theta J is the term theta J XJ and so the partial derivative of all the other terms end up being zero and partial observer this term respect to theta J it is equal to XJ okay and so this ends up being a theta X minus y times XJ okay and again if you haven't you haven't played with calculus for a while if you you know don't quite remember what positive is or don't quite get what I just said don't worry too much about it go over a bit more in section and we and then also read through the lecture notes which kind of goes over this in in in more detail and more slowly than then we might do in class so um so plugging this let's see so we've just calculated that this partial derivative is equal to this and so plugging it back into that formula one step of gradient descent is is the following which is that we will let theta J be updated according to u theta J minus the learning rate and times H of X minus y times X change ok now I'm gonna just add a few more things to this equation so I did this for one training example but this was I kind of used definition of the cost function J of theta defined using just one single training example but you actually have M training examples and so the the correct formula for the derivative is actually if you take this thing and sum it over all M training examples the derivative of the derivative sum is the sum of the derivatives right so so you actually if you redo this derivation you know summing with the correct definition of J of theta which sums over all M training examples if you just redo that the derivation you end up with some equals I threw em that right where remember X I is the I've training examples input features y I is the target Abel is the price in the life training example and so this is the actual correct formula for the partial derivative respect to that the cost function J of theta when it's defined using all of the what is defined using all of the training examples and so the gradient descent algorithm is to repeat until convergence carry out this update and in each iteration of gradient descent you do this update before J equals 0 1 up to n where n is the number of features so n was 2 in our example ok and if you do this then you know then what will happen is show you the animation yes you fit hopefully you find a pretty good value of the parameters theta so it turns out that when you plot the cost function J of theta for a linear regression model it turns out that unlike the earlier diagram I have shown which has local Optima it turns out that if J of theta is defined a way that you know we just defined it for linear regression there's this sum of squared terms then J of theta turns out to be a quadratic function where it's the sum of these squares of terms and so J of theta will always look like look like a big bowl like this another way to look at this oh and so and so J of theta does not have local Optima or the only local Optima is also the global optimum the other way to look at the function like this is to look at the contours of this plot right so you pull the contours by looking at the big bowl and taking horizontal slices and plotting where they're where the curves where where the edges of the horizontal slices so the contours of a big bowl or I guess a formal is a bigger and of this quadratic function will be ellipses like these or these ovals or these ellipses like this and so if you run gradient descent on this algorithm let's say I initialize my parameters at that little X shown over here and usually you initialize say to the ruler zero but but you know but it doesn't matter too much so let's to initialize over there then with one step of gradient descent the algorithm will take that step downhill and then where the second step will take that step downhill oh and by the way fun fact if you if you think about the contours of a function it turns out that the direction of steepest descent is always at ninety degrees is always orthogonal to the contour direction right so no seem to remember that file my high school something I think it's alright and so as you as you take steps downhill because there's only one global minimum this algorithm will eventually converge to the and so the question just now about the choice of the learning rate alpha if you set alpha to be very very large to be too large then it can overshoot right the steps you take can be too large and you can run past the minimum if you said to be too small then you need a lot of iterations and yah will be slow and so what happens in practice is usually you try a few values and and and see what value of the learning rate allows you to most efficiently you know drive down the value of J of theta and if you see J of theta increasing rather than decreasing you see the cost function increasing rather than decreasing then there's a very strong sign that the learning rate is too large and so actually what I often do is actually try out multiple values of the learning rate alpha and and and and usually try them on exponential scale so try 0.01 0.02 to 0.04 0.08 kind of a doubling scale or doubling scale or tripling scale and try a few values and see what value allows you to drive down the learning rate that's this I'm just so I just want to visualize this in one other way which is with the data so this is this is the actual dataset there they're actually 49 points in the dataset so M the number of training examples is 49 and so if you initialize the parameters to 0 that means initializing your hypothesis or initializing your straight line fit to the data to be that horizontal line right so if you initialize theta 0 equals 0 theta 1 equals 0 then your hypothesis is you know for any input size of house of a price the estimated price is 0 and so your hypothesis starts off with a horizontal line there just whatever the input X the output Y is 0 and what you're doing as you run gradient descent is you're changing the parameters theta right so the parameters went from this value to this value to this value to this value and so on and so the other way of visualizing gradient descent is if gradient descent starts off with this hypothesis with each iteration of gradient descent you are trying to find different values of the parameters theta that allows the straight line to fit the data better so after one iteration of gradient descent this is the new hypothesis you now have different values of theta zero and theta one that fits the gate a little bit better after two iterations you end up with that hypothesis and with each iteration of grading descent is trying to minimize J of theta is trying to minimize one half of the sum of squares errors of the hypothesis or predictions on the different examples right well three iterations of green descent for their Asians and so on and then and then the bunch more durations and eventually converges to that hypothesis which is pretty pretty decent straight line fit to the data ok so question oh sure me just repeat question why is the y-you subtracting alpha times the gradient rather than adding alpha times the gradient um let me suggest let me raise the screen so let me suggest you work through one example it turns out that if you add a multiple times the gradient you'll be going uphill rather than going down low and maybe one way to see that would be um you know take a quadratic function right if you're here the gradient is a positive direction and you want to reduce so this would be theta and just me jr. yes so you want thither to decrease so the green is positive you want to decrease say there is you want to subtract in multiple times a gradient um I think maybe the best way to see that would be to work through an example yourself said J of theta equals theta squared and sin theta equals one so here at the quadratic function the differs is equal to one so you want to subtract the value from say a rotten egg all right great so you've now seen your first learning algorithm and you know green descent and linear aggression is Daphne's still one of the most widely used learning algorithms in the world today and if you implement this review if you implement this today right you could use this but some actually pretty pretty decent purposes right now I want to give this algorithm one other name so our gradient descent algorithm here calculates this derivative by summing over your entire training set M and so sometimes this version of gradient descent has another name which is Bosch gradient descent and the term batch you know and again I think a machine learning a whole committee we just make up names of stuff and sometimes the names aren't great but the the term Bactrian descent refers to that you look at the entire training set all 49 examples in the example I just had on PowerPoint you know you think of all 49 examples it's one batch of data and we're gonna process all the data as a batch so hence the name batch gradient descent do you disadvantage a bachelor in descent is that if you have a giant data set if you have and and you're in era of big data we're really moving to large and larger data set there and serve use you know train machine learning models of like hundreds of millions of examples and and if you are trying to if you have if you download the US Census database if your data on the United States Census that's a very large data set and you want to predict housing prices from across the United States that that that may have a data set with many many millions of examples and the disadvantage a batch gradient descent is that in order to make one update to your parameters in order to take even a single step of gradient descent you need to calculate this sum and if M is say a million or ten million or 100 million you need to scan through your entire database scan your entire dataset and calculate this for you know 100 million examples and sum it up and so every single step of gradient descent becomes very slow because you're scanning over you're reading over right like 100 million training examples and before you can even you know make one tiny little step of gradient descent okay by the way I think I don't know I feel like in today's ever a big day there people start to lose intuitions about what's the baby that's and I think even by today's standards like a hundred many examples is still very big I rarely only rarely use dungeon examples although maybe in a few years we'll look back on Hydra examples and say that was really small but at least today so the main disadvantage of battery and descends is every single step of your in descent requires that you read through you know your entire data set may be terabytes of data sets maybe maybe maybe tens of hundreds of terabytes of data before you can even update the parameters just once and if gradient descent needs you know hundreds of iterations to converge then you be scanning through your entire data set hundreds of times oh and sometimes we train our algorithms so thousands of tens of thousands of iterations and so so this this gets expensive so there's an alternative to bash gradient descent and let me just write out the algorithm here then we can talk about it which is going to repeatedly do this so this algorithm which is called stochastic gradient descent instead of scanning through all million examples before you update the parameters theta even a little bit in stochastic grain descent instead in the inner loop of the algorithm you loop through J equals 1 through m of ticking a gradient descent step using the derivative of just one single example of just that one example oh excuse me I write so let I go from 1 to M and update theta J for every J so you update this for J equals 1 through n update theta J using this derivative but now this derivative is taken just with respect to one training example the example I just I guess you update this for every J and so let me just draw a picture of what this algorithm is doing if this is the contour like the one you saw just now so the axes are theta 0 and theta 1 and the height of the surface right to know the contours J of theta with stochastic render sense what you do is you initialize the parameters somewhere and then you will look at your first training example hey let's just look at one house and see if we can predict that houses better and you modify the parameters to increase the accuracy where you predict the price of that one house and because you for the innovator just the one house you know maybe you end up improving the parameters a little bit but not quite going in the most direct direction downhill and you're going look at the second house and say hey let's try to fit that house better and then update the parameters and look at third house a house and so as you run so costly gradient descent it takes a slightly noisy slightly random path but on average is headed to what the global minimum okay so as you run stochastic current descent so her great in the sense will actually never quite converge in backstreet understand it kind of went to the global minimum and stopped right but so classroom in this end even as you won't run it the parameters oscillate and won't ever quite converge because you're always running around looking at different houses and trying to do better on just that one hold on that one house on that one house but when you have a very large data set stochastic gradient descent allows your implementation allows you algorithm to make much faster progress and so and and so when you have very large data sets the casa grande descent is use much more practice than - brilliant you know is it possible stop so customers and and it's such over the battery understand yes it is so boy something wasn't a tough one in this class solvency a suitor these mini battery in the sense where you don't when you use say 100 examples this time rather than one example of the time and so that's another algorithm that I should use more often in practice I think people rarely actually so in practice you know when your data set is large we rarely ever switch to batch gradient descent because battery in the sin is just so slow right so I don't know I'm thinking through concrete examples of problems that worked on and I think that what may actually may I think that dump right for a lot of for modern machine learning where you have if you have very very large datasets right so you know whatever if you're building a speech recognition system you might have like a terabyte of data right and so it's so expensive to scan through a terabyte of day they're just reading it from disk right it's so expensive that you would probably never even run one iteration about you in the sense and it turns out the the the this one one huge saving graces to consecrate understands is let's say runs the costly grained descent right and you know you end up with this parameter and that's the parameter you use for your machine learning system rather than the global optimum it turns out that parameter is actually not that bad right you you probably make perfectly fine predictions even if you don't get to like the global global minimum so what you said I think is a fine thing to do no harm trying it although in practice in practice we don't bother I think in practice we use the customer in the same the thing that actually is more common is to slowly decrease the learning rate so just keep using so-called green descent but reduce the learning rate time so it takes smaller and smaller steps so if you do that then what happens is the size of the oscillations of decrease and so you end up oscillating or bouncing around the smaller regions so wherever you end up may not be the global global minimum but at least it'll be it'll be closer to the yeah so the appeasing learning rate is used much more often cool oh sure when do you start with certain ranges and plot to J of theta over a time so J of theta is the cost function that you're trying to drive down so monitor J of theta as you know it's going down over time and then if it looks like this stop going down then you can say oh it looks like a spout going down when it's not raining oh and you know one nice thing about linear regression is it has no local optimum and so if you run into these conversions debugging in terms of issues less often when you're training highly nonlinear things like neural networks which talk about later in CST tonight as well oh these issues become more acute okay great so um Oh which I learn here if you want to rent times linear expressions and not really it's usually much bigger than that yeah yeah because if your learning rate was 1 over n times that of Y should use the fashion in descent then it ended up being as slow as factual innocence so there's usually much bigger okay so um so that's the classic greater sin oh and and so I'll tell you what I do if you have a relatively small dataset you know if you have if yep I don't know like a hundreds of examples maybe a thousands of examples where it's computationally efficient to do batch gradient descent if battery and descent doesn't cost too much I would almost always just use battery and descent because it's one less thing to fiddle with right it's just one less thing to have to worry about the parameters oscillating but your data said is too large that battery understand becomes prohibitively slow then almost everyone would use you know so costly Granger sentence there right Oh however more like so cost for any sense all right so gradient descents both master and descent as so costly green descent is an iterative algorithm meaning that you have to take multiple steps to get to you know get near hopefully the global optimum it turns out this is another algorithm oh and and for many other algorithms we'll talk about in this class including general linear models and neural networks and a few other algorithms you will have to use gradient descent and so and so we'll see gradient descent you know as we develop multiple different algorithms later this quarter it turns out that for the special case of linear regression and I mean linear regression but not the other and we'll talk about next Monday not the r1 with help on the expensive but if the algorithm you're using is linear regression of exactly linear regression it turns out that there's a way to solve for the optimal value of the parameters theta to just jump in one step to the global optimum without needing to use an iterative algorithm right and this this one I'm gonna present makes is called the normal equation it works only for linear regression doesn't work for any of the other arrows talked about me to the school sir but but let me quickly show you the derivation of that and what I want to do is give you a flavor of how to derive the normal equation and where you end up with is you know what what I hope to do is end up with a formula that lets you say theta equals some stuff where you just set theta equals to that and in one step with a few matrix multiplications you end up with the optimal value of theta that lands you right at the global optimum right now just like that just in one step okay um and if you've taken you know advanced linear algebra classes before something you may have seen in this formula for linear regression what what what the longer than Yashiro clauses do is what some of the natural classes do is cover the board with you know pages and pages and matrix derivatives what I want to do is describe to you a matrix derivative notation that allows you to derive the normal equation in roughly four lines of linear algebra rather than so pages and pages in linear algebra and in the work I've done in machine learning you know sometimes notation really matters right if you're the right notation you can solve some problems much more easily and what I want to do is define this matrix linear algebra notation and then I don't want to do all the steps of the derivation I'm gonna give you a give you a sense of the flavor of what it looks like and then I'll ask you to get a lot of details yourself in the in the lecture notes will work out everything in more detail than I want to do algebra in class oh and um in problem set one you get to practice using this yourself - you know derive some additional things that I found this notation really convenient right for deriving learning algorithms okay so I'm going to use the following notation so J right there's a function mapping from parameters to the real numbers so I'm going to define this this is the derivative of J of theta with respect to theta where remember a theta is a three-dimensional vector so it's r3 Rashi's are n plus 1 right if you have two features of the house if N equals 2 then theta is three-dimensional n plus 1 dimensional so it's a vector and so I'm going to define the derivative with respect to theta of J of theta as follows this is going to be yourself 3 by 1 vector so hope this notation is clear so this is a three-dimensional vector with three components all right so that's why so that's the first component is vector that's the second and the third okay it's a partial derivative J respecting each of the three elements and more generally in the notation we'll use maybe an example um let's say that E is a matrix so let's say that's a a is the 2 by 2 matrix then you can have a function right so let's say a is you know a11 a12 a21 a.22 right so a is a 2 by 2 matrix then you might have some function of a matrix a right then that's a real number so I'm gonna be F max from a 2 by 2 - excuse me are 2 by 2 it's a real number so and so for example if F of a equals a 1 1 plus a 1 2 squared then f of you know 5 6 7 8 would be equal to I guess 5 plus 6 squared right so as we derived this will be working a little bit with functions that map from matrices to real numbers and this is just one made-up example of a function that impose a matrix and maps the matrix massive values of a matrix serial number and when you have a matrix function like this I'm going to define the derivative we respected a of F of a to be equal to itself a matrix where the derivative of f of a with respect to the matrix a this itself will be a matrix with the same dimension of a and the elements of this are the derivative with respect to the individual elements I'm just ready to like this okay so if a was a 2x2 matrix then the derivative of F of a respect to a is itself a two by two matrix and you compute this two-by-two matrix just by looking at F and taking derivatives with respect to the different elements and plugging them into the different the different elements of this matrix okay and so in this example I guess the derivative respect to any of F of a this would be right it would be over that and I got these four numbers by taking the definition of F and taking the derivative with respect to a 1 1 and plugging that here taking the respect to a 1 2 and plugging that here and taking the derivative respect to the remaining elements and plugging them here so that's the definition of a matrix they remember - yeah oh yes we do same definition for a vector and by 1 or n by 1 matrix yes and in fact that definition and dis definition for the the reserving J respective thing so these are consistent so if you apply that definition to a column vector treating a column vector as an N by 1 matrix or input and it has to be n plus 1 by 1 matrix then that that specializes to what we described here all right so let's see so um I want to leave the details to lecture notes because there is more lines of algebra but I want to but he'll give you an overview of what the derivation of the normal equation looks like so arms of this definition of a derivative of a matrix the broad outline that what we're going to do is we're going to take J of theta alright that's the cost function take the derivative with respect to theta right since theta is a vector so you want to take derivatives with respect to theta and you know well how do you minimize the function you take the Reuters respective theta and set it equal to zero and then you solve for the value of theta so that the derivative is zero right the minimum you know the maximum minimum function is whether there is equal to zero so so how you derive the normal equation is take this vector so J of theta maps from a vector to a real number so we'll take derivatives with respect to theta set that there is equal zero and solve for theta and then we end up with a formula for theta that lets you just you know immediately go to the global minimum of the cost function J of theta and and and all of the build up and all of this notation is you know is there what does this mean and is there an easy way to compute the derivative of J of theta okay so to help you understand the lecture notes when hopefully you take a look at them just a couple other derivation if a is a square matrix so let's say a is a an N by n matrix so number of rows equals number of columns I'm going to denote the trace of a to be equal to the sum of the diagonal entries so some of our III and this is pronounced the trace of a and and and you know you can you can also write this as trace operator like the trace function applied to a but by convention we often write trace of a without the parentheses and so this is called a trace so trace just means sum of diagonal entries and some facts about the trace of a matrix you know trace of a is equal to the trace of a transpose because if you transpose the matrix right you're just flipping along the 45 degree axis and so the diagonal entries actually stay the same when you transpose the matrix so that trace of a is equal to trace of a transpose and then there there there are some other useful properties of the trace operator here's one that I don't want to prove but that you could go home and prove yourself with a few some little bit of work maybe not not too much which is if you define F of a equals trace of a times B so here it B is some fixed matrix right and what F of a does is it multiplies a and B and then it takes to sum of diagonal entries then it turns out that the derivative with respect to a of F of a is equal to B transpose and this is you could prove this yourself for any matrix B if F of a is defined this way the derivative is equal to B transpose the trace function or the trace operator has other interesting properties the trace of a B is equal to the trace of B a you could prove this from principle it's a little bit of work to prove that you if you expand out the definitions a and B sure that and the tracer a times B times C is equal to you the trace of C times a times B this is a cyclic permutation property if you ever multiple you know multiply several matrices together you can always take one from the end and move it to the front and the trace will remain the same and another one that is a little bit harder to prove is that the trace excuse me derivative of eight runs a transpose C is okay so I think just as just as for your ordinary calculus we know the derivative of x squared is 2x right and so we all figured out that grew and we just use it too much without without having to read arrive every time this is a little bit like that the trace of a squared C is you know two times C a right it's an open like that with matrix notation that's there so think of this as analogous to DDA of a squared C equals to AC but this is like the matrix version of that all right so finally what I'd like to do is take J of theta and express it in this you know matrix vector notation so we can take the Reuters respect to theta and set those equal to zero and just solve for the value of theta right and so let me just write out the definition of J of theta so J of theta it was 1/2 something I equals 1 cm squared and it turns out that it turns out that some if you did if you define the matrix capital X as follows which is I'm going to take the matrix capital X and take the training examples we have you know and stack them up in rows so we have M training examples right so so the XS will call them vectors so I'm taking transpose you just stack up to K examples in n rows here so let me call this the design matrix but couple X color design matrix and it turns out that if you define X this way then x times theta is this thing times theta and the way of matrix vector multiplication works is your theta is now a column vector right so theta is you know theta 0 theta 1 theta 2 so the way that matrix vector multiplication works is you multiply this column vector with each of these in intern and so this ends up being X 1 transpose theta X 2 transpose theta down to X M transpose theta which is of course just a vector of all of the predictions of the algorithm and so if now let me also define a vector Y to be taking all of the labels from your training example and stacking them up into a big column vector right let me define Y that way it turns out that J of theta can then be written as 1/2 X theta minus y transpose X theta minus y ok and let me just outline the proof but I won't do this in great detail so X theta minus y is going to be right so this is X later this is y so you know X theta minus y it's going to be this vector of H of X 1 minus y 1 down to H of X M minus y M right this is just all the errors your learning algorithm is making on the examples the difference between predictions and the actual labels and if you remember so Z transpose Z is equal to sum over I Z squared like a vector transpose itself is the sum of squares of elements and so this vector transpose itself is the sum of squares of the elements right so so which is y so the cost function J of theta is computed by taking the sum of squares of all of these elements of all of these errors and the way you do that is to take this vector you know X a the minus y transpose itself is the sum of squares of the which is exactly the era so that's why you end up with this is the sum of squares of the those error terms okay and um if some of the steps don't quite make sense really don't worry about it all this is written out more slowly and carefully in the lecture notes but I wanted you to have a sense of the brought off of the of the big picture of the derivation before you go through them yourself and great disease on the lecture notes elsewhere so finally what we want to do is take the derivative respect the theta of jr. theta and set that to zero and so this is going to be equal to the derivative of 1/2 X theta minus y transpose X Y and so I'm gonna I'm gonna do the steps really quickly right so the steps require some of the little properties of traces and matrix derivatives that wrote down briefly just now but so I'm gonna do these very quickly without going into the details better so this is equal to 1/2 there's our theta of so take transposes of these things so this becomes theta transpose X transpose minus y transpose and then kind of like expanding out a quadratic function right this is you know a minus B times C minus D as you can this is AC minus 80 and so on since so I just write this out and so what I just did here this is similar to how you know ax minus B times ax minus B is equal to a squared x squared minus ax b minus b ax ax plus b squared it's kind of it's just expanding out the quadratic function and then the final step is is that right oh yes thank you um and then the final step is you know for each of these four terms first second third and fourth terms to take the derivative with respect to theta and if you use some of the formulas I was alluding to over there you find that the derivative which which I don't want to show the derivation out but it turns out that the derivative is X transpose X theta plus X transpose X theta minus X transpose Y minus X transpose Y and we're going to set this derivative and so the simplifies to X transpose X theta minus X transpose Y and so as described where they are gonna set this derivative to zero but how they go from this step to that step is using the matrix derivatives explained in more detail in the lecture notes and so the final step is you know having said this a zero this implies that X transpose X theta equals X transpose Y so this is called the normal equations and the optimum value for theta is theta equals x transpose x inverse X transpose Y okay and if you implement this then you know you can in basically one step get the value of theta that corresponds the global minimum and and and again common question I get is - well what if x is non-invertible what that usually means is you have redundant features that your features are linearly dependent but if you use something called the pseudo-inverse you you kind of get the right answer if that's the case although I think that even more right answers if you have linear dependent features probably means you have the same feature repeated twice and I would usually go and figure out what features actually repeated leading to this problem okay all right any last questions before so that so that's a normal equations hope you read through the detailed derivations in lecture notes any last questions going great oh yeah how do you choose salon area it's this is quite empirical I think so most people would try different values and just pick one all right I think let's let's break if people have more questions where the tears come up we can acute Aten questions were less grateful today thanks everyone