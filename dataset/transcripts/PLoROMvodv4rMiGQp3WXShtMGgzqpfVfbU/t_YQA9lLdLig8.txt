hey everyone let's get started so um let's see plan for today is we'll go over the rest of ICA independent component analysis and in particular talk about CDF's cumulative distribution functions and then um all right so plan is a we'll go over the rest of ICA independent components analysis and we'll talk a bit about CDF cumulative distribution functions and then derive the ISEE model and in the second half of today we'll start on the final of the four major topics of the cost which is reinforcement learning we talk about MDPs or Marvel teacher processes so to recap briefly we had you remember the overlapping voices demo so we said that in the I see a problem dependent components now this problem we're seeing we have sources s which are are n if you have n speakers so for example if this is speaker ones audio then at time T s you know superscript parenthesis T subscript 1 is the sound emitted by speaker 1 at time T as I've seen all right make that go a little bit and we're using sometimes I to index training examples and so the training examples sweep over time and sometimes usually I use I sometimes I use T I guess in the case where the different examples come from different points in time in your recording and what your microphones record is X I equals a of s I so just for now let's say you have two speakers and two microphones in which case a will be a 2 by 2 matrix and home or problem your face because in five microphones in which case a will be a five by five matrix what's helped later about what happens is the numbers because in microphones is not the same and the goal is to find a matrix W which should hopefully be a inverse so that si is w times X recover the original sources and we're going to use these W 1 up to WN represent the rows of this matrix w oh yes you're right thank you so last time we had all right just remember this is a picture the cocktail party problem and last time I showed these pictures about you know why why is ICA even possible right given two overlapping voices how is even possible to separate them out how is there enough information to know you know what are the two overlapping voices and so one picture we saw was this one where if s1 and s2 are uniform between minus 1 and plus 1 then the distribution of data will look like this if you pass this data through the mixing matrix a then your observations now the axes have changed X 1 and X 2 may look like this and your job is a finding unmixing matrix W that map's this data back to the square ok now this example is possible because the examples because the sources s-one and s-two were distributed uniformly between minus 1 and plus 1 it turns out human voices you know the recordings per moment in time I not distributed uniform between minus 1 and plus 1 and it turns out that dumped if the data was Gaussian then ICA is actually not possible here's what I mean let's say that so the uniform distribution is a highly non Gaussian distribution right uniformly mine's one plus one you know this is not Gaussian and that that makes I see possible what if s1 and s2 came from Gaussian densities right if that were the case then this distribution s1 and s2 would be rotationally symmetric and so there'd be a rotational ambiguity right any axis could be s 1 and s 2 you can't map you know this type of parallelogram back to this square right so so you can't so if I think in this parallelogram you can sort of read off you know there may be one axis should look like that sorry I'm joining with Mouse not doing very well well second axis should maybe look like that right and by by inverting that you can get the data back to the square but in the case of the data look like this then you actually don't know because maybe this should be s 1 and that should be s 2 right but so there's this rotational ambiguity because the Gaussian distribution is rotationally symmetric of s 1 and s 2 are standard Gaussian then then this distribution is rotation symmetric and you don't have enough information to recover the directions that correspond to the original sources ok so it turns out that there is some ambiguity and the output of ICA in particular last time we talked about two sources of ambiguity you don't know which is speaker 1 at which the speaker 2 right you don't know which one to number speaker 1 which on the numbers because you and you might take this data and flip it horizontally reflect this you know on the name s 1 goes to negative s 1 or reflect this on the vertical axis we don't know this positive s 2 and negative s 2 and in the case of this example where s 1 s 2 a uniform minus 1 plus 1 those are the only sources of ambiguity but the data was Gaussian that the additional rotation on vacuity which actually in part which actually makes it impossible to separate out the sources ok so it turns out that so it turns out that the Gaussian density is the only distribution that is rotationally symmetric if s1 and s2 are independent and the distribution is rotationally symmetric meaning that the distribution has sort of circular contours then it then it then it must be a Gaussian density and so there is a theorem which just stated formally that I see is possible only if your data is not Gaussian right but but so once your data is not Gaussian then it is possible to recover the independent sources okay I'm just taking that informally so let's see so what I'd like to do is develop the ICA algorithm assuming that the data is non Gaussian okay now in order to divert the ICA model we need to figure out what is the density of s right and I'm going to use P subscript s you know of the of the random variable s to represent the density of s an equivalent way to represent the probability of the density of continuous random variables virus CDF which stands for cumulative distribution functions and the cumulative distribution function of a random variable f of s in probability is defined as the chance that the random variable is less than that value so I guess notation has been inconsistent sorry but this is capital S I'm using to denote the random variable and this is some constant right and it's that same constants is that lowercase s okay and so for example if this is the PDF of random variable s may be of a Gaussian right the CDF is a function that increases from 0 to 1 where the height of a CDF at a certain point is the probability so if you take the curves the same point so the height of a CDF at a certain point lowercase s is the probability that the random variable takes on the value equal to this value or lower which means that the height of this function is equal to you know the probability mass the area under the curve of your PDF over to the left at that point okay so that's a know some sometimes this some problem in statistics courses teach this concept in some Jones I guess but there so there's a mapping between the PDS and the CDF of a function of a continuous random variable and the relation between the PDF and the CDF is that the density is equal to the first derivative right F prime so if you take the derivative of the CDF then you should recover the PDF ok but so I think in order to specify you know some random variable we could either specify the PDF right the probably density function or you could specify the CDF which is just no less tell me what's the chance of the random variable taking on any value less than any particular value s and by taking the derivative this you can always recover the PDF and by integrating this you can always go to the senior ok and so what we're going to do in ICA is instead of specifying a PDF for how speakers voices sound we're instead going to specify a CDF and we have to choose as India that is not the Gaussian density CDF because we have assumed that the data is non Gaussian and and the CDF you know is a function that always goes from right zero to one okay so all right so we'll specify so in a little bit we'll specify some CDF for the density of the sources of what human voices sound like let's say and if you differentiate this you will get the PDF or the density is equal to that now um we're going to derive a massive likely estimation algorithm in a minute but our model is that X is equal to a s which is equal to I guess W inverse of s and s is equal to WX right so that that's that's the model and in order to derive a maximum likelihood estimate for the parameters when you have so this is going to be the density X so this is a relationship between this is relationship between X and s X is equal to a s equals W inverse s and has to equal to WX right so this is a model and what I'd like to do is let's say you know what's the density of s what is the density of X if X is computed as the matrix a times s so one step that's tempting to take is to just say well s is equal to W times X so the probability of X is just equal to the probability of s taking on the certain value right so so I mean this is s and so the probability of seeing a certain value of x is equal to the probability of s taking on that corresponding value because assuming W is an invertible matrix is one-to-one mapping between X and s so to find it probably FX just find a pair of s and compute a corresponding probability it turns out this is this is incorrect and this works were probably mask functions but discreet party distributions that take on discrete values but this is actually incorrect for continuous probability densities so let me let me um show an illustration and go back to derive what is a correct way of computing the density of X oh and we'll want a density of X because when you get the training set you only get to observe X and so for finding a master of like ledesma parameters you need to know what's the density of X you come and you know choose the parameters choose the parameters W the maximizing likelihood so that's what we want to compute the density of X but um let's let's use a simple example let's say the density of s is indicator SS between 0 and 1 okay so this is SS distribution uniform from 0 to 1 and let's say X is equal to 2 times s sitting on notation is equal to 2 W is equal to 1/2 this is a N equals 1 one dimensional example so this is a density of s right uniform distribution from 0 to 1 and if X is equal to 2 times s then this seems like X should be equal X is distributed uniformly from 0 to 2 right Chris if s is uniform from 0 to 1 you multiply by the 2 X a certain uniformly from 0 to 2 and so the density for X is equal to this and it's now half as tall because probably density functions need to integrate to one right so this is a uniform from zero to two probably density function and so the correct formula is P of X x equals 1/2 times indicator is realistic of the X let's say equal to 2 okay and more generally the correct formula for this is actually this x this is the determinant of the matrix W and in the case of a real number the determine if I want real number is just this absolute value which is why we have the density of x equals 1/2 you know that's the absolute value of the determinant of W x times y times indicator where there are two times s is within 0 0 to 1 ok right so I guess this oh this is indicator zero less than 1/2 okay so this is an illustration showing why this is the right way with the determinant of W most value here as the as a way to compute its identity of X and don't you familiar with determinants and determinants is the function you can call your numpy to compute but also the intuition of deterministic measures how much it stretches out a local whopping and so you need to sort of divide by the determinant of a or multiply by determinant of W in order to make sure these distributions don't normalizes the one right so that's where that comes from so we're nearly done just one more decision and then we can derive a maximum likelihood estimation to derive a mess with likely estimate of this of the parameters the last thing we need to do is choose the density of what you know speakers voices sound like and as I said just now what we are going to do is choose a non Gaussian distribution right and so well f of S is equal to the chance of this person's voice right random variable s being less than certain value and we need a smooth function that goes between you know 0 and 1 right we need a smooth function that has Davey that shake and so well what functions we know that they be that shape let's take the sigmoid function and it turns out this world this will work ok there are many choices that actually work fine it turns out that if you choose the sigmoid function to be the CDF then if you look at the PDF this induces if you take the derivative this right so take P of x equals the derivative CEDIA it turns out that if this is the Gaussian then the PDF that this choice induces is something with fatter tails by which I mean that it goes to zero you know so Gaussian density goes to zero very quickly right it's like e to the negative x squared the Gaussian is a square in the exponent of the density and it turns out that this because the density taken by compute derivative a sigmoid it goes to zero more slowly and this captures human voice and many natural phenomena better than the Gaussian density because there are larger number of extreme outliers there are more than one or two standard deviations away but they're actually multiple distributions that work you could have used a double double exponential distribution so this is an exponential distribution exponential then see if you take a symmetric go to side the explanation that's the PIA best they'll also work quite well for ICA but I think early history of ICA you know researchers I think of them might have been Terry Sadowski download the softest if you just needed a function with these properties and you picked the sigmoid and plugged it in and it works just fine it's been a good enough default that tom is still it's still widely use right but but but but they've used this double side the exponential sometimes also called the laplacian distribution this works fine as well as a choice of P of s so the final step the density of s is equal to the product of the lessee soap rather from I equals 1 through your n sources of the probability of each of the speakers emitting that sound right because the N speakers are speaking independently right wait say that again oh yes you're right sorry about that yes this should be sorry yes this should have been up here right go from a CD f sub-p di by taking derivatives oh cool so um s is the vector of all you know two speakers are all five speakers voices at one moment in time so the density of s right s is in RN is the product of the individual speakers probabilities and this is the key assumption of ICA that you know your two speakers or your five speakers are having independent conversations and so at every moment in time they choose independently of each other what sound teammate and so using the formulas you worked out just now the density of X is equal to well as we did the density of W x times the determinant of W so and this is equal to Oh in this notation WI transpose X this is um right because WI is the I've row of the matrix W and so you know I guess s SJ is equal to W J transpose X right so you take a corresponding row and multiply it by X to get the corresponding source actually sorry I think that's right yeah let me use J there okay and so um this writes out so this shows what is the density of X expressed as a function of P of s which have assumed which affects as a CDF of the sigmoid as a as the derivative of the sigmoid and as a function of the parameter W right so this is a model that given a setting of the parameters W which square matrix allows us to write down what's the density of banks so the final step is we could use maximum likelihood estimation to estimate the parameters W so the log likelihood of W is equal to sum over the training examples of log and you can use the cost agree in the sense take the derivative of W respective along likelihood and it turns out this is derived a lecture notes I'll just write it out here I hope I got that right yeah okay right and it turns out that if you use this formula don't don't worry about the form for derivatives the full derivations give a legend else but it turns out that if you use the derivative of the log likelihood with respect to parameter matrix W and use stochastic gradient a sense to maximize the log likelihood run this for a while then you can get ICA to find they're pretty good matrix W for unmixing the sources okay so just to recap the whole algorithm right you would have a training set of x1 up through XM where each of your training examples is the microphone recordings at one moment in time and so the time goes from 1 through m what you do is initialize the matrix W say randomly and use gradient descent with this formula for the derivative in order to maximize the log likelihood of the data and after a gradient ascent converges you then have a matrix W and you can then recover the sources as s equals W of X and then now we have the sources you can take say s1 1 through s 1m and play that through your you know laptop speaker in order to see what source 1 sounds like so that's how you would take you know overlapping voices and try to unmixed up a wise choice the save point now rotation America boy how to visualize that try plotting it in numpy matplotlib I guess if you plot the contours of the day so it turns out that if this is s 1 and s 2 what you do not want is a density whose contours look like that haven't done this for a while I believe if you take this distribution the contours will look like that it's been a while since I hope that this but I think it'll look like that so this is not rotations in magic do you know this laplacian yeah ok yeah oh yes the pasta he looks like that I think sigmoid looks a bit like that too yeah talk to this even right Paul's on Piazza if one of you positive because you can see I haven't done ever louder oh why don't you interpret differently along that actually yes the law should be like this I think oh sorry G is the sigmoid function yes so Jia Zi sure what's the yeah what's the closest nonlinear extension of this I don't we don't have great answer to that right now frankly so a bunch of people including you know my former students and me have done research to try to extend this to nonlinear versions and there's some stuff that kind of works but I don't think there's like a tried-and-true algorithm that I'm ready to say this is the right way to do it yeah actually maybe I should I can say a little bit more about other people interesting well yeah yeah yeah let me let me try there all right let's see so so for several several years ago and and so kind of ongoing there's been research some done by my collaboration me some time my others aren't trying to build nonlinear versions of ICA and so some of you might have seen this slightly infamous Google Katz result right so it doesn't want to leave the Google brain project one of the first parts if you did this a few years ago now where we trained in your network on was it many many hours of YouTube videos and eventually it learned to detect cats because apparently there are a lot of cats and YouTube videos and so it turns out that the algorithm we used was a was sparse coding which is actually very closely related to ICA and so this rough algorithm was attempting to build a nonlinear version of ICA where you train one version once trained train one layer of sparse coding let's say to extract low-level features and then recursively apply this on top to learn not just edge detectors but object part detectors and then eventually you know the somewhat infamous two somewhat infamous google cat but I think that this is actually still ongoing research I think the most interesting research some of the most interesting research has been on hierarchical versions of sparse coding in sparse coding it's a different algorithm that turns out to be very closely related to ICA and then you can show that they're optimizing for very similar things so if I say sparse coding is very similar ICA but there are hierarchical versions of this they tried to turn this as a multi-layer neural network and it kind of works wherever that show can learn there's new features but what happened was a supervised learning there and really took off in the whole world shifted Walters attention to supervised learning and building deep supervised learning in your own networks and so the hierarchal sparse coding running I see over and over to learn nonlinear versions there's there's pretty less attention from research on that on that topic then it then it really deserves so maybe you maybe someone in a costly go back and do more research on that I still think is a promising area all right um so let me wrap up with some ICA examples so there's actually a former ta from the class Katie Chang and so it turns out that ICS routinely used to clean up EEG data today so what's an EEG right place many electrodes on your scalp to measure little electrical recordings on the surface of your scalp so you know what does human brain do right human brain your neurons in your brain right now fire generated little pulses of electricity and if you place electrode on your scalp you can get a very weak measurement of the of the voltage of the electrical activity in a you know at a certain point in your scalp so the analogy to oh excuse me what's wrong alright so the analogy to the cocktail of Hardy problem the overlapping speakers voices is that you know your your brain does a lot of things at the same time right your brain helps regulate your heartbeat part of your brain does that and now the part of your brain you know makes your eyes blink every now and then another part of your brain probably brain is also responsible making sure that you breathe and then part of your brain is responsible thinking about machine learning and stuff like that right so so your brain actually handles memories didn't ask at the same time and as your brain sorry else not sure what's wrong with this okay and as your brain carries out these different tasks in parallel different parts of your brain generate different electrical impulses so I think of there as imagine that you have a you know cocktail party in your head right so many overlapping voices so this is now voices in your head bad but one one one part of your brain is saying alright hot go and be hot go and beat harder and beaten and not my brains I hate breathe in and breathe out breathe in and breathe out now if I were in a zoo you know what's wrong with this PowerPoint right um and what's each electrode on the surface of your scalp does is it measures an overlapping combination of all of these voices because the different positive brain are sending these electric impulses they add up and so any one point on the surface of your brain reflects a sum or a mixture really a sum of these different voices of these different things your brain is doing and so if you just just zooming into the EEG plot each line is the voltage measured at a single electrode right on say your scalp and these signals are quite correlated you see that when there's a massive voice in your brain shouting you know like right beat your heart or blink your eyes that signal can go through all of the different electrodes which is why you can see these artifacts are affected in all of these electrodes all right turns out a pretty good way to clean up this data is to take all of these time series pretty-pretty exactly as we learned about it with the ISEE algorithm and separate out into the independent components and so it turns out in this example there are two components corresponding to driving the heartbeat that's actually the eye blink component and so one way to clean up this data sorry I should really wonder what's wrong with this all right let me try something all right if you write says hi Peters I blink and alright and if you run I see a and then remove out I have a person say oh that's happy that's I blink and remove subtract out those components then you can end up with a much more cleaned up eg signal which you can then use for downstream processing sorry overpass there is a lot of research on your chicken eg reading to try to guess at the high level what you're thinking right it turns out that if you train a trainer trainer you know supervised learning algorithm to try to decide are you thinking of a noun or a verb or you thinking of something edible or are you thinking of something any other boat there's been very interesting research trying to use EEG to figure out just in a very coarse level no not quite my reading every thought you are thinking but that that can we categorize very coarse level thoughts like are you thinking of a person or you think of an object then you can actually do that to some extent using EQ meeting it's been cleaning up the data to get really I blink the heartbeat artifacts is a very useful pre-processing step to get cleaner data to feed into the learning algorithm to try to figure out try to categorize you know some coarse cavity of what you're thinking okay and then more research it turns out that what kind of I mentioned that Google can't thing just now it turns out that if you train I see a font is messed up if you train I see a on natural images I see a will say that the natural independent components of natural images are these edges and as in that you know when you see a little image patch in the world we see you know look somewhere in there one looked just a tiny little piece of the image right like 10 pixels by 10 pixels and if you take that data and model as ICA I say we'll say that the world is made up of edges or made up of patches like these and that the way you end up with images in the world is by each of these patches you know independently saying is there reservations or horizontal insurers is there this type of light on the left dark on the right is that this type of lighter on top doctor the bottom and so on and it's by adding all of these voices there you get a typical image passionate world so there are there interesting theories in neuroscience about whether this is how you know the human brain learns to see as well so so very very same work on them I see and sparse coding to try to use these mechanisms to explain how you know the human brain tries to explain it tries to learn to perceive images for example okay so all right so that's it for um the algorithms of ICA justify no comments I think on Mondays someone asks do the number of speakers the number of microphones need to be equal so it turns out that if the number of microphones is larger than the number of speakers that's actually fine right if you're the number of microphones large number of speakers then if you run ICA or a slightly modified version of it you find that some of the speakers are just silent speakers and so you know if you have ten microphones and five speakers if you run this algorithm on ten microphones you can find that well maybe five of the sources are just silent or there ways to just now model those five sources as well right if you think that they're just some sources of silence so so this so slightly modified version of this works quite well if the number of speakers is larger than the number of microphones if the excuse me the number of microphones is lodged in the Armagh speakers this works quite well if the number of microphones is smaller than the number of speakers then that's still very much a cutting-edge research problem so so for example if you have two speakers and one microphone it turns out that if you have one male and one female speaker so one relatively high patient one much lower pitch then you can sometimes have some algorithms that separate out two voices with one microphone but it doesn't work that reliably is a little bit finicky but there have been research papers published showing that you know you could make a reasonable attempt at separating out two voices with my one microphone though the pitches are quite different such as is one male one female voice but separating out two male voices or two female voices is still very hard and then there's ongoing research in in those settings so that's ICA and I guess you get to play more of it in your homework problem as well okay any last questions about ICA oh wait sorry it would be Jose yeah so um I think actually go through a lot of math it just breaks down I think because there you can have two independent sources but W is now no longer a square matrix right it'll be uh what is it so I write so is that X is equal to a s right and so if X is a real number and s was two-dimensional so I guess this would be um a would be two by one s would be a would be 2 by 1 SOT - Suzi a would be 1 by 2 and s would be a 2 by 1 and this is 1 by 1 then you know ain't inverse kind of doesn't exist right so you need to come over way to form the mass molecular model and where you have one microphone it's just how do you separate out to overlapping voices so it takes much higher level knowledge yeah to separate out two voices oh I see right let's see so right so if you don't know how many speakers there are you have all these microphones where you about the number of electrodes you have is fixed so that's just a data set and it turns out that if you run ICA where the large numbers speakers you find them in the speakers are silent there are also some versions of ICA that you so if you think that there are let's see why no smells worse on this but it turns out that um if you think that there is a relatively small number of speakers then you don't need to explicitly model all the speakers instead what you would model so again suppose sense of Max or likely estimation problem let's say that's X is in our 10 right Co 10 recordings but you suspect that you're near 5 speakers then in this case I guess the matrix a would be um what is it was it be 10 by 5 is it right to mix the five sources into 10 speakers and you could for me the maximum likelihood estimation problem assuming the existence of only 5 speakers without modeling a lot of speakers and then finding later that they're all silent so if your formula so if you parameterize model like this using a instead of W then can form their maximum likelihood estimation problem where you just assume that they're 5 speakers and s is generated by five speakers mixing through a linear thing plus the noise oh I see sure right how do you know if you have how do you know how this because you have so I think it's one of those things a little bit like k-means I guess where you try it and see what works and if you find that the first view you know speakers will capture mostly variance you find the digital speakers are quite silent and they're quite small that you could just cut off at that time I don't want to go too much into the different numbers of speakers and and and microphones I see a verbose let me just take a couple of questions only one question yeah is it oh do you ever see my parent of you um I'm sure you can is not usually done in this version of the algorithm but I would not be surprised if there are some other versions where you do I've not seen that about myself actually all right cool good um so all right all right so that wraps up our chapter on unsupervised learning right so you learned about yes k-means clustering the Yemm algorithm for mixture of gaussians really makes your gaseous model factor analysis model and also PCA and then you know today the ica independent components analysis algorithm and all of these were the algorithms that could take as input an unlabeled training set just the excise and no labels and we find various interesting structures in the data such as clusters or subspaces or in the case of ICA the voices of you and the speakers and you implement ICA and play about yourself in the homework problem well you get to separate out many five overlapping voices the loss of the four major topics well cover in this verse we Thomas two eyes learning kind of advice machine learning on two eyes learning and the fourth and the final major topics we cover in this class will be a reinforcement learning so to motivate reinforcement learning let's say you want to have a computer learn to fly a helicopter right I think I showed some of the videos that are in the first lecture and so I'll just skip that here but it turns out that if you are at every point in time given the position of a helicopter call the state of a helicopter and you also take an action on how to move the control sticks you know to make the helicopter fly in a certian trajectory it turns out that it's very difficult to know what's the one right answer for how to move the control sticks of a helicopter right so if you don't have a mapping from x to y because you can't quite specify the one true way to fly a helicopter it's hard to use supervised learning and what the enforcement learning does is is is it is an algorithm that doesn't ask you to tell it the right answer at every step it doesn't ask you to tell it exactly what's the one true way to move the controls of a helicopter at any moment in time instead your responsibility as a designer a machine or an engineer or an engineer is to specify reward function that just tells the helicopter when it's flying well and when it's lying poorly so your job as a designer is to write a cost function or reward function that gives the helicopter a high reward whenever it's doing well flying accurately find reject you want sue and gives the helicopter a large negative reward whenever it crashes with or something bad then I think I think you know think of this like training a doll right when do you say good dog when you say bad dog and the dog figures out when to do more the good dog things and your job is not to tell the dog you know well you can't actually talk to the dog and tell what to do I guess that doesn't work but you can tell a good dog and bad dog and hopefully their instruments positive negative was how to do more of the good things another example let's say you want to write their program to play chess or I guess most know somewhat famously and arguably somewhat slightly over height go alpha go right so it's very difficult to know in given a certain chess board position or checkers or go for position what is the one true move what's the one best move so it's very difficult to formulate you know playing chess as a supervised learning problem and instead the mechanisms used to play chess are much more like reinforcement learning where you can let your program play chess or go or whatever and whenever it wins you go oh good computer and when it loses you go oh bad computer so that's a reward function and learning algorithms job is to figure out by itself how to get more of the positive rewards right and actually common rewards for learning to play chess or checkers or fellow go is a plus reward of plus one for win - one for Luzon zero for a time say ready a chest pain program this be a common choice reward where R is the reward function and s is the state okay and I will go into the notation in a little bit and so as you can imagine giving only this type of information to their chest pain program it places much more burden on the program to figure out what to do in fact one of the challenges of reinforcement learning is so just call the reward that's called the state and the state means on the status of the chess board where are the pieces on a chess board or the status of the helicopter where exactly is a helicopter and are you the right side up or upside down and where are you right and it turns out one of the challenges one of the things that makes them reinforce the learning Hart is the credit assignment problem and that means that if your program is playing a game of chess and let's say it loses on move 50 you know so plays a game and then I'll move 50 right it's checkmate and then loses his opponent so gets a reward a negative one but how can the program actually figure out what it did well and what it did poorly right if you lose a game and move 50 it might be that the program made a really bad move made a blunder and move 20 and then you know but they just hope another 30 moves before his fates were sealed right so in the game of chess we made a bad mistake early on you can still take many many games there are many many moves in the game of chess before before the final outcome of losing or winning or losing this reached or in a and a another it turns out that so if you are trying to build a self-driving car if ever car crashes rain chances are the thing the car was doing right before it crashes was break but it's not breaking that causes a crash it's pretty something else I called it many many seconds ago then - the bad outcome so there's a bad outcome how does the algorithm know of all the things that did before how does it know whether it did well which you should do more of and one it did poorly which you should do less of and conversely if that's a good outcome you're like a wins a game of chess well how do you know what you did well right so that's called the credit assignment problem which is when your algorithm gets some reward how do you actually figure out what you did what you did poorly so you know what to do more of and what to do less up right so as we develop reinforcement learning algorithms will see that the algorithms we use have to at least indirectly try to solve the credit assignment problem okay so um reinforcement learning problems like play chess of AI helicopters or you know building these there's robots is modeled using the MDP or the Markov decision process and this is a way this is a notation in the formulism but modeling how the world works and then reinforcement learning algorithms will solve problems using this formula zhim so what is an MDP sin MDP is a five tuple and let me explain what each of these are so s there's a set of states so for example in chess this would be the set of all possible chess positions or in flying a helicopter this would be the set of all the possible positions and orientations and velocities of your helicopter a is the set of actions where in the helicopter this would be all the positions you could move your control sticks or in Chester's be all the moves you could make you know in a in a game of chess P subscript s a is is a state transition probabilities and so we'll see later this state transition probably is tell you if you take a certain action a and a certain state s once the chance of you ending up at a particular different state s prime our gamma is the discount factor as number between 0 and 1 don't worry about this for now we'll come back to this in a minute and R is that all-important reward function so in order to develop a reinforcement learning algorithm I'm going to use as a running example a simplified MDP that we can draw on the whiteboard right so helicopters in chess and go and so on they're really complicated MDP so just to illustrate the algorithms I'm going to use a simpler MVP and this is an example we drawn from the textbook Russell and Norvig then we'll use imply MVP in which you have a robot navigating this simple maze and there's an obstacle so this is a grid world icy robot you know and it's navigating this very simple maze and this is a pillar or this is a wall so you can't walk into that wall and let me just use indexing on the states as follows so this MDP let's let's go through the five tempo and talk about what the the each of the five things are so this MVP has eleven states corresponding to the eleven possible positions that the robot could be in right each of these banks square so the eleven possible states and the actions are north south east and west right you can come on your robot to move in any of these directions and I don't know if you're working robots before you know that um when you come on the robot you know head straight it doesn't always go exactly straight sometimes the wheel slip it veers of a slight angle and so in this simplified example we're going to model it as that if you command the robot to go north from a certain state that there's a 0.8 percent chance they'll successfully go where you told it to and there's zero point one chance that they'll accidentally if you're off to the left for a student if you're off to the right okay if you are working on row robots right what's a lot of robots it is actually important to model the noisy dynamics of a robot real slipping so your orientation being slightly off now in a real robot you'd have a much bigger stage space than the eleven states right so so this is simplified so this is not a realistic model for how robots actually slip but because we're using such a small state space I think just for illustration purposes as well well we'll use this and so for example the state transition probably so specified is you say that every under state 3 1 so the state 3 comma 1 and you command it to go north that the chance of getting to the state 3 2 is 0.8 and the chance are getting to the States for 10.1 Charles again 2 to 1 is 0.1 and the chance of getting to other states is like 3 3 and other states is equal to 0 ok so the state transition probabilities would capture that if you here in surgical north as the whole point a chance of getting here 0.1 chance again here 0.1 chance of getting here and you know point Oh a chance of right hopping to steps oh it's implement a PE example we'll just assume that the robot you know hits a wall it just bounces off the wall and stays where it is so if you told us to go 'yes it slips off it just bounced off the wall and stay exactly where this now let's specify the reward function we'll come back to discount factor later but let's say you want the robot to navigate to this cell in the upper right hand corner and so to incentivize the reward incentivize the robot to get to this square you know that's the prize Saguna knees let's put a +1 reward there and let's say you really don't want the robot to go to this cell they could put a negative one or what there right so the way you specify the toss for a robot to do is in designing the reward function so in our example I'm just copied out of the game plus one minus one we have that the reward at the cell for three is plus one and the reward at the cell for 2 is minus one and then you know if you want the robot to get to the plus one rewards cell as quickly as possible then again there there are many ways of designing reward functions but one common choice would be to put a negative penalty a very small negative penalty right such as a set of rewards a negative 0.02 for all other states and the effect of a small negative reward like this is to charge it right every every step it is just loitering around so charge a little bit for using electricity and wandering around because this incentivizes a robot to hurry up and get to the plus one reward right so you give a small penalty you know loitering and wasting electricity so this is how an MDP works your robot wakes up at some state as zero at time zero because you turn on the robot and the robot says oh I'm at this state and based on what state it is in it will get to choose some action a zero so decides I want to go north south east or west let's choose some action based on the action the consequence of the choice is it will get to some state s1 to stay that the next time step which is distributed according to the state transition probability is governed by the previous state and the action and chose so develop what actually chose is there's different chances of moving north south east or west now that is an s-1 it then has to choose a new action a1 and as a consequence of the action a1 it will get to some new state s2 which is governed by the state transition probabilities you know s 1 a1 and so on okay and then the robot just keeps on running and so the robot will go through a sequence of states s0 s1 s2 and so on depending on the choices it receives defend the actions it chooses and the total payoff is written as follows with one more detail is that term gamma so think of gamma as a number like 0.99 so gamma is usually chosen to be just slightly less than one and what the so the total payoff is the sum of rewards or more technically as a sum of discounted rewards and what this does is it adds up all the rewards that the robot receives over time but the further reward is into the future you know the smaller the gammas ^ time that that reward is x okay so anyway what'd you get this time one you get all of that every one you get at time - its x point 99 Roy gets thanks that this one point 99 squared or not a cube and so on and so what the discount factor does is it has the effect of giving a smaller way to rewards in the distant future and this means that this encourages the robot to also get the positive rewards faster or postpone the negative rewards right and so in financial applications the discount factor has one as has a natural interpretation as the time value of money because if you have a dollar today you know you're better off having a dollar today they're having a year $1 year from now right because you put the dollar in the bank and interests for a year on your dollar and so dollars they're strictly rather than thought in the future and conversely having to pay $100 or having to pay one dollar a year from now is also better than having to pay a dollar today right because if you could you know save your money and earn inches and then issue a payment to someone else a year from now rather than now then you're actually slightly wealthier and so and so the gamma and financial applications as an interpretation as the time value of money oh it's the interest rate I guess and but but but more generally even for non-financial applications most of our most there are some financial applications our enforcement but lots of non fan traffic as well this mechanism of using a discount factor has the effect of encouraging the system to get to the positive was as quickly as possible but then also conversely to try to push the negative rewards as founds in the future as possible right oh and I think to be pragmatic there are two reasons why people use gamma the story I just told time value of money your friends'll sponsor was postponed that was that's that's the story you tend to people you tend to hear people say in terms of why we have a discount factor the other reason where the discount factor is actually much more pragmatic one which is that lovely reinforcement learning algorithms you see they converge much faster or they work much better if you're willing to have a discount factor right so it turns out that if gamma is is equal to 1 if gamma is not strictly less than 1 it's much harder or they're there many ripples to learning algorithms that may not converge you as much how the croutha conversions of no may not converse which isn't a pragmatic thing this makes the job much easier for your algorithms now I see sorry you're shaking your heads in this disapproval all right yeah yeah yes yeah that's a good point yes so one of the things if there's no camera is that the reward summer was you know could be can increase or decrease of our balance so by having gammer discount easier the total payoff is a finite value whereas the boundary value so that that's one of the parts they go into some of the proofs or something reasons behind why rate for so many avenues conversion yeah okay so the go of reinforcement learning is to choose actions over time to maximize the expected total payoff and in particular what most reinforcement learning algorithms will come up with is a policy that maps from States to actions right so the output of most reinforcement learning algorithms will be a policy or controller in the our world we tend to use the term policy but policy just means controller there maps of states actions so it turns out that for the MDP that we have right it turns out that this is the optimal policy so for example I want you to take this example just this cell here to sell over here this policy is saying PI apply to the state 3 1 as equal to West and that so so it separately worked out what is the optimal policy and this turns out to be also a policy in the sense that if you we say execute this policy so the executors policy means that whenever you in the state s take the action given by PI of s so that's what it means to execute a certain policy and it turns out that this policy was I worked out separately right offline you know in my laptop that this is the optimal policy for this MVP and it turns out that if you execute this policy meaning whenever a certain state you know take the action indicated by the arrow that this is the policy that will maximize the expected total payoff okay and the problem in reinforcement learning is given a definition for an MDP or given a problem suppose the problem is an MDP figure out what's the set of states with set of actions one of the state transition probabilities specify a discount factor a specified reward function and then to a reinforcer learning algorithm find the policy PI that maximizes expected payoff and then when you want your robot to act or when you want your chess playing program to act whenever you're in something s take the action given by PI of s and hopefully this will result in a robot that you know efficiently navigates to the +1 state so turns out that MVPs are quite good at making fine distinction so one example is actually not totally obvious whether here you're better off going off or going west right and it turns out that there is a trade off if you go Wes here then you know you're gonna take a longer route to get to the plus one so you take longer the plus one is discounted more heavily you're taking these penalties along the way excuse me but on the flip side if you were to try to go north you could try to get there faster but on this that there's a 0.1% chance that you accidentally slip off to the minus one state so so what is the optimal action right it's actually quite hard to just look at it with your eyes and make a decision but it turns out that if you solve for the optimal set of actions and does MDP you in this example is they just take longer and safer route the sense of why cycles and policies so if the optimal set of actions is the cycle around then it should find out I mean for example if they're only penalties everywhere and she's just go and run in a circle you know then then the algorithm watch she choose to do that but in this case you want to get to the plus one as quickly as possible and so what we'll see is one question wait so alright sure sorry so testing checkers and go and so on they're a little more complication is you take a move so actually to refine the description of chess what happens in playing chess is the state status your board right says your move so you see a board that's the state and so you make a move and then the opponent makes a move and then that's the new state so the state is when you and your opponent both make take turns then it's cut back to you right and because you don't know exactly what your opponent will do there is a probably distribution over if I make a move or what's the other person gonna do yeah right oh they're probably sighs I'm very no two point eight point one point one where does that come from so we'll talk about that later in some applications does this learn so if you build a robot you might not know is it point eight point one point one or you know point seven point one five point one five so it's quite common to use data to learn those state transition probabilities as well well we'll see a specific example that into it okay so alright so where we are just to summarize this is how you formulate a problem as an MDP and then the the job reinforcing learning algorithm is ready to go from there MDP to telling you what is a good policy okay so let's break and then Oh have a good Thanksgiving everyone won't see you for like a week and half enjoy yourselves and we'll reconvene after Thanksgiving with