all right good morning let's get started so today you see the support vector machine algorithm and this is one of my favorite algorithms because it's very turnkey classification problem so in particular talk a bit more about the optimization problem you have to solve in support vector machine then talk about something called the representor theorem and this would be a key idea to how will work in potentially very high dimensional like 100,000 dimensional or a moving dimensional or 100 billion dimensional or even infinite dimensional feature spaces and just to teach you how to represent feature vectors and how to represent parameters that may be you know hundred billion dimensional or 100 trillion dimensional or infinite dimensional and based on this we derived kernels which is the mechanism for working these incredibly high dimensional feature spaces and then hopefully time permitting wrap up with a few examples of compute implementations of these ideas so to recap on last Wednesday we started to talk about the optimal margin the classifier which said that even the data set that looks like this then you want to find the decision boundary with the greatest possible geometric margin right so the geometric margin can be calculated by this formula and this is just the derivations in lecture notes just you know measuring the distance to the nearest point and for now let's assume the data can be separated by a straight line and so gamma I is this is sort of geometry I guess derivation the lecture notes this is the formula for computing the distance from the example X I Y I to the decision boundary governed by the parameters W and B and gamma is the worst case geometric margin right you will make so of all of your M training examples which one has the resource possible geometric margin and the support that the optimal margin classifier will try to make this as big as possible and by the way what will what you see later on is that optimum Archana classifiers BC this algorithm and okto margin classifier plus kernels meaning AC take this idea of the pie in a hundred billion dimensional feature space that's the support vector machine ok um so I saw one thing I didn't have time to talk about on Wednesday was the derivation of this classification problem so whether this optimization objective come from so let me let me just go over that very briefly so the way motivated these definitions was said that given training set you want to find the decision boundary parametrized by W and B that maximizes the geometric margin right and so again as we can your classifier output G equals W transpose X plus B and so you want to find parameters W be they'll define the decision boundary when your classification switch from positive and negative that maximizes the geometric margin and so one way to pose this as an optimization problem is let's see is to try to find the biggest possible value of gamma subject to that subject to that the geometric margin must be greater than equal to gamma right so so in this optimization problem the parameters you get to fiddle with our gamma W and B and if you solve this optimization problem then you are finding the values of W and B that defines a straight line that defines the decision boundary so that so this constraint says that every example right so this constraint says every example has Joe mentioned margin greater than equal to gamma this is this is what is saying and you want to set gamma as big as possible which means that you're maximizing the worst-case geometric logic this make sense so so if I so the only way to make gamma say 17 or 20 or whatever is if every training example has geometric margin bigger than 17 right and so this optimization problem is trying to find Delvian be to drive up gamma as big as possible and have every example have geometric margin even bigger than camera so this optimization problem maximizes the it causes your causes your defined Delta NP with as big a geometric margin as possible eyes bigger the worst case your magic margin as possible okay and so does this defense actually yeah right okay actually raise your hand if this makes sense oh okay well many of you all right I mean this even a slightly different way um so let's see of a few training examples you know in the training examples geometric margins are 17 2 & 5 right then the Jamaican margin in this case is the worst case value 2 right and so if you are solving an optimization problem where I want every example where I wanted the where I want a min over I of gamma I to be as big as possible one way to enforce this is to say they can I must be bigger than equal to camera for every possible value of I and then I'm gonna lift camera up as much as possible right because the only way to live camera up subject to this is it every verb value of gamma is bigger than that and so lifting gamma up maximizing gamma as effective maximizing the worst case examples geometric margin which is which is which is how we've defined this optimization okay and then the last one step to turn this problem into this one on the left is this interesting observation that you might remember when we talked about the functional margin which is the numerator here that you know the function margin you can scale W and B by any number and the decision boundary stays the same and so you know if if your classifier is y so this is G of W transpose X plus B right so if if W was the vector 2 1 let's say that's the classifier right then you can take W and B and multiply it by any number you want I can multiply this by 10 and this the point it's the same straight line right so if I take a look I think let's see with this 2 1 X this actually defines the decision boundary that looks like that if this is X 1 and this is X 2 then this is the equation of the straight line where the V transpose X plus B equals 0 right that's 1 & 2 you could verify for yourself you plug in this point then W transpose X plus B equals 0 we plug at this point there because it was x equals 0 and so that's the decision boundary where the SVM will predict positive everywhere here and predict negative everywhere to the lower left and this straight line you know stays the same you very much supply these parameters by any constant okay and so to simplify this notice that you could choose anything you want for the normal W right just by scaling this by a factor of ten you can increase it or scale me by a factor one over ten you can decrease it but you have to flexibility to scale the parameters W and B you know up or down by any fixed constant without changing the decision boundary and so the trick to simplify this equation under that one is if you choose to scale the normal W to be equal to 1 over gamma because if you do that then this optimization objective becomes maximize one of the norm of W subject to right if you substitute normal W equals one of Yama and so that cancels out and so you end up with this optimization problem instead of maximizing one of a norm W to minimize one half the normal W squared subject to this okay and so that's a rough I know I did this relatively quickly again as usual the full derivation is redundant lecture notes but hopefully this gives you a flavor for why if you solve this optimization problem and you're minimizing over W and B that you are solving for the parameters WMV that give you the optimal margin classifier okay now Delta margin classifier we've been deriving this algorithm as if you know the features X I let's see we're from deriving these algorithm is if the features X I are some reasonable dimensional feature X equals r2 it's a 100 or something um what we will talk about later is a case where the features X I become you know 100 trillion dimensional right or infinite dimensional and what what we will assume is that W can be represented as a sum as a linear combination of the training examples so um in order to the writers for vector machine we're going to make an additional restriction that the parameters W can be expressed as a linear combination of the training examples right so and it turns out that when X I is you know 100 trillion dimensional doing this will let us derive algorithms that work even in these hundred trillion or in these infinite dimensional feature spaces now I'm describing this just as an assumption it turns out that there's a theorem called the represented theorem that shows that you can make this assumption without losing any performance the proof the represented theorem is quite complicated I don't want to do this in this class there's actually written out the proof but why you can make this assumption is also written election else is pretty long and involve proof involving a primal dual optimization I don't to present the whole proof here but there give you a flavor for why this is a reasonable assumption to make okay oh and just just to make things complicated later on we actually do this right so why I is always plus minus one so so we're actually by convention we're going to assume that WI can be written all right so in this example this is plus minus 1 right so this makes some of the math a little bit down so you come out easier but this is still saying that W is can be represented as a linear combination of the training examples ok so um let me just describe less formally why this is a reasonable assumption but it's actually not an assumption that represents a theorem proves that you know this is just true at the optimal value of W but let me convey a couple ways why this is a reasonable thing to do I see yes so um maybe his intuition number one and I'm going to refer to logistic regression right we're suppose that you run logistic regression with gradient descent say so cost appear in descent then you will initialize the parameters to be equal to zero at first and then for each iteration of stochastic gradient descent right you would update theta gets updated as theta minus a learning rate times you know times X okay and so sorry here alpha is a learning rate nothing does overload the notation this alpha is nothing to do that alpha but so they're saying that on every iteration you're updating the parameters theta as a by adding or subtracting some constant times some training example and so kind of proof by induction right if they tostop zero and on every iteration a great descent you're adding a multiple of some training example then no matter how many iterations you run gradient descent theta is still a linear combination of your training examples okay and again identical stay there that it was really theta 0 theta 1 up to theta n right whereas here we have a B and then w1 down to W I know this pens really bad if you like alright yeah throw these away so they don't keep haunting us in the future ok right so but if you but so I did those a theta rather than chop you but it turns out if you work through the algebra this is a little fruit by induction that you know as you run logistic regression after every iteration the parameters theta of the parameters W are always a linear combination of the training samples and this is also true of you use batch gradient descent if you use factory in the Senate then the up a roux is this and so it turns out you can derive gradient descent for the support vector machine learning algorithm as well you can derive your descent authorized W subject to this and you could have a proof by induction you know that no matter how many iterations you run through in descent it will always be a linear combination of the training examples so that's one intuition for how you might see that assuming W is a linear combination of the training examples you know is a reasonable assumption I want to present a second set of intuitions and this one would be easier if you're good at visualizing high dimensional spaces I guess but let me just give intuition number two which is um let's see so so first of all let's take our example just now right let's say that the classifier uses this 2 1 X minus 2 right so this is W and this is um then it turns out that the decision boundary is this where this is 1 and this is 2 and it turns out that the vector W is always at 90 degrees to the decision boundary right this is a effective I guess geometry or something well then the algebra right where it's the vector W to 1 so the vector W you know is sort of two to the right and then one up is always that well all right the vector W is always 90 degrees to the decision boundary and it doesn't bounce separates where you predict positive from where you predict negative okay and so it it turns out that if you have to take a simple example let's say you have two training examples a positive example and a negative example right then is next to the linear algebra way of saying this is that the vector W lies in the span of the training examples oh and and and the way to picture this is that W sets the direction of the decision boundary and as you vary B then the position so the relative position you know setting different values of B will move the decision boundary back and forth like this and W pins the direction and just one last example for for why this might be true is so we're gonna be working in very very high dimensional feature spaces for this example let's say you have three features x1 x2 x3 right and then later we'll get to where this is like a hundred trillion right and let's say for the sake of illustration that all of your examples line the plane of x1 and x2 so let's say X 3 is equal to 0 ok so let's say from all of your training examples X 3 equals 0 then the decision boundary you know will be will be some sort of vertical plane that looks like this right so this is going to be the plane specifying W transpose X plus B equals 0 where now W and X are three dimensional and so the vector W will have a should have W 3 equals 0 right if one of the features is always 0 was always fixed then you know W 3 should be equal to 0 and that's another way of saying that the vector W you know should be represented as an in the span of just two features and clinics span of the training examples okay all right I'm not sure if either intuition one or intuition to convince this you I think hopefully that's good enough but this the second intuition will be easier if you're used to thinking about baptism high damaged all feature spaces and again the formal proof of this result which is called the represent two theorem is given in the lecture notes but is a very possessive well no it was actually there's actually on the most complicated is well it's definitely the high end in terms of complexity of the of the full derivation of the formal derivation is this result so so let's assume that W can be written as follows so optimization problem was this you want to solve for W and B so that the norm of W squared is as small as possible and so that do this is the one for every value of I so let's see normal W squared this is just equal to W transpose W and so if you plug in this definition of W you know into these equations you have as the optimization objective min of 1/2 sum from I equals 1 through m so this is W transpose W which is equal to I guess some of I some of AJ of my ELF ej y iy j and then X I transpose XJ and I'm going to take this so this is an inner product between X I XJ and I'm gonna use I'm just only write to this this right X I did this notation so X comma Z equals X transpose Z is the inner product between two vectors this is maybe another alternative notation for writing inner products and when we derived kernels you see that expressing your algorithm in terms of inner products between features X this is the key map practical step needed to derive kernels and we'll use this slightly different so open angle bracket closing angle brackets notation to denote the end the inner product between two different feature vectors so that is the optimization objective oh and then this constraint it becomes something else I guess this becomes a what is it why I times W which is transpose X plus B is greater than one and again this simplifies or if you just multiply this out so just to make sure the mapping is clear all these pens are like dying all right so that becomes this and this becomes that okay and the key property we're going to use is that if you look at these two equations in terms of how we pose the optimization problem the only place that the feature vectors appears is in this inner product and it turns out when we talked about the kernel trick when we talked about the application of kernels it turns out that if you can compute this very efficiently that's when you can get away with manipulating even infinite dimensional feature vectors well we'll get to this in a second but the reason we want to write the whole algorithm in terms of inner products is there'll be important cases where the feature vectors are hundred trillion dimensional but you can compute it or even infinite dimensional but you can compute the inner product very efficiently without needing to loop over you know the 100 trillion elements in an array right and we'll see exactly how to do that later in very shortly so all right now it turns out that we've now expressed the whole optimization algorithm in terms of these parameters alpha right defined here and B so now the parameters theta well now the parameters need to optimize for our alpha it turns out that by convention in a way that you see support vector machines refer to you know in research papers or in text books it turns out there's a further simplification of that optimization problem which is that you can simplify to this and the derivation to go from that to this is again relatively complicated but it turns out you can further simplify the authorization problem I wrote there to this and again you can copy this down if you want but it's also written the lecture notes and by convention this slightly simplified version optimization problem is called the dual optimization problem the way to simplify that authorisations problem to this one that's actually done by using convex optimization theory and and again the derivation is written in lecture notes but I don't want to do that here if you want think of it as doing a bunch more algebra to simplify that problem to this one and cause density you can still be along the way there's a little more complicated than that but but write full derivation is given in the lecture notes and so finally you know the way you train for the way you make a prediction right as you solve the Alpha rice and maybe for B right so you solve this optimization problem or that optimization problem for the Alpha rice and then to make a prediction you need to compute H of W B of X for a new test example which is G of W transpose X plus B right but because of the definition of W dub this is equal to G of that's W transpose X plus B because this is W and so that's equal to G of sum over I of I in a product between X and X plus B and so once again you know once you have stored the alphas in your computer memory you can make predictions using just inner products again right so the entire algorithm both the optimization objective you need to do during training as was how you make predictions is is expressed only in terms of inner products so we're now ready to apply kernels and sometimes in machine learning people sometimes we call this a kernel trick and then we just the other recipe for what this means step one is write your algorithm in terms of X I XJ in terms of inner products and instead of carrying the superscript you know X I XJ I'm sometimes gonna write in the product between X and Z right where X and z are supposed to be proxy is for two different training examples exam thanks Jay but it simplifies a notation or write a little bit to let there be some mapping from your original input features X to some higher damage though set of features Phi and so one example would be lets you try to predict housing prices a particular house will be sold in the next month so maybe X in this case is the size of the house or maybe is size in yeah right maybe X is the size of a house and so you could take this 1d feature and expand it to a high dimensional feature vector with X x squared X cubed x to the 4th right so this would be one way of defining a high dimensional future laughing well another one could be if you have two features x1 and x2 corresponding to the size of house in the number of bedrooms and you can map this to different 5x which may be x1 x2 x1 times x2 x1 squared x2 x1 x2 squared and so on it kind of a polynomial set of features or maybe other set of features as well ok and what we'll be able to do is work with feature mappings Phi of X where the original input X may be 1d or 2d o or whatever and Phi of X could be you know a hundred thousand dimensional or infinite dimensional but we'll be able to do this very efficiently right oh you an infinite dimensional okay so guess we'll get some concrete examples of this later but I want to give you the overall recipe and then what we're going to do is a final way to compute ok of X comma Z equals Phi of X transpose Phi of Z so this is called a kernel function and what we're going to do is we'll see that there are clever tricks so that you can compute the inner product between X and Z even when Phi of X and Phi of Z are incredibly high dimensional right we'll see an example of this enough in very very soon and in step four is replace XZ in algorithm with K of X Z okay because if you could do this then what you're doing is you're running the whole learning algorithm on this high dimensional set of features and and the problem with you know swapping out X for Phi of X right is that it can be very computationally expensive if you're working over a hundred thousand dimensional feature vectors right I mean even by today's standards you know a hundred thousand yes yeah it's not the biggest I've seen I've seen actually because I've seen that here are the convenient features but you've been by today's standards hundred thousand features is actually quite a lot and if you're not King said just a thousand this is a lot and large number of features I guess and the problem of using business is quite computation expensive to carry around these hundred thousand or image the 100 million dimensional future vectors or whatever but that's what you would do if you were to swap in Phi of X you know in the naive straightforward way for X but what we'll see is that if you can compute K of X Z then you could because you've written your whole algorithm just in terms of inner products then you don't ever need to explicitly compute Phi of X so you can always just compute these kernels then get to that later I won't go for some kernels I was talked about by his Darian spray on Wednesday yeah I think the no free lunch theorem is a fascinating theory or concept but I think that it's been I don't know it's been less useful actually because I think we have inductive biases that turn out to be useful there's a there's a famous theorem in learning theory called no free lunch was like 20 years ago dad basically says that in the worst case learning algorithms do not work for any learning algorithm I can come up with some data distribution so that your learning algorithm stops that that's roughly the no free lunch to ever improved about like 20 years ago but it turns out most of the world most the time the universe is not that hostile to all that so so yeah that's the learning I was turned out okay all right let's go through one example of kernels so for this example let's say that your original input features was three dimensional X 1 X 2 X 3 and let's say I'm gonna choose the feature mapping Phi of X to be all so pairwise monomial terms so I'm gonna choose X 1 times X 1 X 1 X 2 X 1 X 3 X 2 X 1 okay and there are a couple duplicates that X 1 X 3 is equal to X 3 X 1 but agenda writes it out this way and so notice that if you have if X is in RN right then Phi of X is in R and squared right so the three-dimensional features two non dimensional and I'm using small numbers for illustration in practice think of X as a thousand dimensional and so this is now a million well think of this as maybe ten thousand and this is now like a hundred million okay so and squid features this much bigger and then similarly Phi of Z is going to be Z 1 z 1 z 1 z 2 some have gone from n features like 10,000 features to n square features HP in this case hundred million features um so because there are n squared elements right you would need order N squared time to compute Phi of X or to compute Phi of X transpose Phi of Z explicitly right say we want to compute the inner product between 5s and Phi of Z and they do it explicitly in the obvious way they'll take N squared time to just compute all of these in their products and then do the right and then compute this uh compute this right and there's actually n squared over 2 because a lot of these things are duplicated but that's the order N squared but let's see you can find a better way to do that so what we want is to write out the kernel of X comma Z so this Phi of X transpose Phi of Z right and what I'm gonna prove is that this can be computed as X transpose Z squared and the cool thing is that remember X is n dimensional Z is n dimensional so X transpose Z squared this is an order n time computation right because taking X transpose Z you know that's just in a product of two n dimensional vectors and then you take that number you a transpose Z is a real number and you just square that number so that's the order n time computation and so let me just prove that X transpose Z is equal to well let me let me let me prove this step right and so X transpose e squared that's equal to right so this is X transpose Z right and then times this is also X transpose e so this formula is X transpose e squared is X transpose e times itself and then if I rearrange sums this is equal to sum from I equals 1 through n sum from J equals 1 through n X i zi x j zj and this in turn is you know some of i sum over J of X I XJ times Zi zj and so what this is doing is this marching through all possible pairs of inj and multiplying X I XJ with the corresponding Zi Zi J and having that up but of course if you were to compute Phi of X transpose Phi of Z what you do is you take this and multiply it with that and then add it to the sum then take this and multiply with that and add it to some and so on until you end up taking this and multiplying that and having it to your son right so that's why so that's why this formula is just you know marching down these two lists and multiplying multiplying multiply and add it up which is exactly Phi transpose which is exactly the Phi of X transpose Phi of Z okay so this proves that you've turned what was previously an order N squared time calculation and turn order n time calculation which means that if n was 10,000 instead of needing to manipulate a hundred thousand dimensional vectors to come up with these URLs as my phone buzzing axes really loud okay instead of needing to manipulate so 100,000 dimensional vectors you could do so in the plating only 10,000 initial vectors now a few other examples of kernels it turns out that if you choose this current so let's see we had K of X comma Z equals X transpose e squared and we now add a plus C there where C is a constant so C is just some fixed real number that corresponds to modifying your pictures as follows where instead of justice you know binomial terms pairs of these things if you add plus C there it corresponds to adding x1 x2 x3 to this to your set of features technically there's actually waiting on this and actually root to see look to see root 2 C and then as a constant C here as well you can prove this yourself and it turns out that if this is your new definition for Phi of X and make the same change to Phi of Z you know it's a root 2 C Z 1 and so on then if you take the inner product of these then it can be computed as this right and so that's and so the row of the constant C it trades off the relative weighting between the binomial terms though you know xixj compared to the to the single to the first degree terms like x1 or x2 or x3 other examples if you choose this ^ d notice that this still is in order n time computation right x transpose e takes all the end time you add a number to it then you take this the power of G so you can compute this in all the end time but this corresponds to now apply of X as all the number of terms turns out to be M plus D choose D but it doesn't matter it turns out this contains all features of monomials up to order d so by which I mean it if let's say D is equal to five right then this contains then Phi of X contains all the features of the form X 1 X 2 X 5 X 17 X 29 right this is a fifth-degree thing or X or X 1 X 2 squared X 3 X you know 18 this is also fifth order polynomial fit for the monomials called and so if you choose this as your kernel this corresponds to constructing Phi of X to contain all of these features and there are exponentially many of them right there all of these features any older although these are called monomials basically all the polynomial terms all the more narrow terms up to a fifth degree polynomial up to a fifth order monomial term so and there are turns out there are n plus d choose DS which is a roughly M plus D to the power of the very roughly so this is a very very large number of features but your computation doesn't blow up exponentially even as D increases so what the support vector machine is is taking the optimal margin classifier that we derived earlier and applying the kernel trick to it in which we already had D so well so okto margin classifier plus the kernel trick right that is the support vector machine and so if you choose some of these kernels for example then you could run an SVM in these very very high dimensional feature spaces in these you know hundred trillion dimensional feature spaces but your computational timescales only linearly as all their n as the number as the dimension of your input features X rather than as a function of this hundred trillion dimensional feature space you're actually building a linear classifier so um why is this a good idea let me just not show a quick video to give you intuition for what this is doing let's see okay I think projects it takes a while to warm up does it any questions while we're yes so this kind of function appears applies only to this vision happy so each kernel function of yes up to trivial differences right if you have a feature mapping where the features that come you are permuted or something then the kernel function stays the same so there are trivial function transformations like that but if you have a totally different feature mapping you would expect to need a totally different kernel function so I wanted to let's see oh cool so I want to give you a visual picture I've wiped this all right this is a YouTube video that can cotton faroush would teach us to that cs2 there the environment such as IU so I don't know who Rudy are erroneous but there's a nice visualization of what a support vector machine is doing so let's say you have a learning algorithm where you're trying to separate the blue dots from the red dots right so the blue and the red dots can't be separated by a straight line but you put them on the plane and you use a feature mapping Phi to throw these points into much high dimensional space so it's now throwing these points in the 3-dimensional space in this three dimensional space you can then find W so W is now three dimensional because apply the optimal margin classifier in this three dimensional space that separates the blue dots and the red dots and if you now you know examine what this is doing back in the original space then your linear classifier actually defines that elliptical decision boundary variance right so you're taking the data all right so taking the data mapping it's a much higher dimensional feature space three dimension in this visualization but in practice can be 100 trillion dimensions and then finding a linear decision boundary in that hundred trillion dimensional space which is going to be a hyperplane like a like a straight you know like a plane or a straight line or a plane and then when you look at what you just did in the original feature space you found it very non linear this is your value so this is y and again you know you can only visualize relatively low dimensional future spaces even even on a display like that but you find that if you use a SVM kernel all right you can learn very nonlinear decision boundaries like that but that is a linear decision boundary in a very high dimensional space but when you project it back down to you know 2d you end up with a very dominant is about okay all right oh sure yes so in this high dimensional space represented by the feature mapping 5x so this data always have to be linearly separable so far been pretending that it does our coming back and fix that assumption later today so um no how do you make kernels right so just here's something so here's some intuition you might have about kernels if X and z are similar you know if - if - infinite ampuls X is here close to each other or summit each other then chain of X Z which is the inner product between X and Z right presumably this should be large and conversely if X and z are dissimilar then KF x z you know this maybe should be smaller right because uh the inner product of two very similar vectors that are pointing in the same direction should be large and the inner product of two dissimilar vectors should be small right so this is one guiding principle behind you know what you see in the law of kernels just see if if this is Phi of X and this is Phi of Z the inner product is large but then they kind of point off in random directions the inner product will be small right that's how vector inner product works and so well what have you just poor functionality straight out of the air which is K of X Z equals e to the negative X is minus Z squared over 2 Sigma squared right so this is one example of a similar if you think of kernels there's a similarity measure of function just you know let's just make up another similarity measure of function and this does have the property that if x and z are very close to each other then this would be e to the 0 which is about 1 but if x and z are very far apart then this would be small right so this function it it actually satisfies this criteria satisfies those criteria and the question is is it okay to use this as a kernel function so it turns out that a function like that K of X Z you can use it as a kernel function only if there exists some Phi such that K of X Z equals Phi of X transpose Phi Z right so we derived the whole algorithm assuming this to be true and it turns out if you plug in the kernel function for which you know this isn't true then all of the derivation we wrote down breaks down and the optimization problem you know can have very strange solutions right that don't correspond to good classification that could cost firewalls and so this puts some constraints on white kernel functions we could choose for example one thing it must satisfy is K of X X which is 5 X transpose Phi of Z this had better be greater than or equal to 0 right sorry because in a proper weight vector with itself had better be non-negative so if K of X X is ever 0 or less than 0 then this is the only valid kernel function okay um more generally there's a theorem that proves when is something a valid kernel somebody just outlined that that proof very briefly which is let's set X 1 up to X D you know be any D points and that's net ok sorry about overloading of notation this is a so K represents a kernel function and I'm gonna use K to represent a kernel matrix as well sometimes it's also called the grand matrix but it's called the kernel matrix so the K IJ is equal to the kernel function applied to two of those points xixj right so you have D points so just apply the kernel function to every pair of those points and put them in a matrix and a Big D by D matrix like that so it turns out that given any vector Z I think you've seen something similar to this in problem set one but given any vector Z Z transpose AZ which is sum of I sum over j zi k IJ zj right if k is a valid kernel function so if there is some feature mapping Phi then this should equal to sum of i sum of j zi v of x i transpose Phi of Z XJ times EJ and by a couple of other steps let's see this v of x AI transpose 5xj I'm going to expand out that inner product so sum of a K Phi of X I element of K times Phi of XJ other than K times Z J and then rearranging sums is Sun some of our K actually sorry I'm running out of white board and it's still an exploit several range sums some okay sum of i sum of a j zi v x phi subscript K times Phi of X J subscript K times Z J which is sum of a K squared and therefore this must be greater than or equal to 0 and so this proves that the matrix K the kernel matrix K is positive semi-definite okay and so more generally it turns out that this is also a sufficient condition for a kernel function since our function K to be a valid kernel function so I'm just write this out this is called a vs. theorem so k is a valid kernel so K is a valid kernel function ie there exists v such that K of X Z equals Phi of X transpose Phi of Z if and only if for any D points you know x1 up to X D the corresponding kernel matrix is a positive semi-definite so let's write this K trading equals zero and I proved just one dimension of one one direction of this implication right this proof outline here shows that if it is a valid kernel function then does this positive semi-definite this Allah did improve the opposite direction it's as if an earlier right shows both direction so this algebra we did just now proves that dimension of the proof I did improve the reverse dimension but this turns out to be a if and only if condition and so this gives maybe one test for whether or not something is a valid kernel function okay and it turns out that the kernel I wrote up there that one K of X C it turns out this is a valid kernel this is called the Gaussian kernel this is a probably the most widely use kernel well actually did what well actually the most widely-used kernels is maybe the linear kernel which just uses K of X Z equals X transpose Z and so this is using you know Phi of x equals x right so no no no high dimensional features so sometimes you call it the linear kernel it just means you're not using a high dimensional feature mapping or the future mapping is just equal to the original features this is this is actually pretty commonly used kernel function you're not taking advantage of kernels in other words but after the linear kernel the Gaussian kernel is probably the most widely used kernel or the one I wrote out there and this corresponds to a future dimensional space that is infinite dimensional right and this is actually the difficult kernel function corresponds to using all monomial features so if you have you know X 1 and also X 1 X 2 and X 1 squared X 2 then X 1 squared X 5 to the 10 and so on up to you know X 1 to the 10,000 and X 2 to the 17 right whatever so this the kernel corresponds to using all these polynomial features without end going to arbitrarily high dimensional but giving a smaller waiting to the very very high dimensional ones which is why it's why [Music] great so the furniture and then toward the end I'll give some other examples of kernels so it turns out that the kernel trick is more general than the support vector machine it was really popularized by this vortex machine where you know researchers I guess volume of Athens and Corinth and Cortez found that applying this kernel tricks the support vector machine makes for a very effective learning algorithm but the kernel trick is actually more general and if you're any learning algorithm that you can write in terms of products like this then you can apply the kernel trick to it and so you you play of this for a different learning algorithm in the in the program assignments as well and the way to apply the coho Sheikh is take a learning algorithm write the whole thing in terms of inner products and then replace it with okay z it was some appropriately chosen kernel function k of XE and all of the discriminative learning algorithms would learn so far it can be written in this way so they can apply the kernel trick so linear regression which is super aggression everything that generalizes any model family the perceptron algorithm all of the all of those algorithms you can actually apply the kernel trick to which means that you can apply linear regression in an infinite dimensional feature space if you wish and later in this class we'll talk about principal components analysis we should heard of but when I talk about principal components analysis turns out that's yet another algorithm that can be written only in terms of inner products and so there's an everyone called kernel pca kernel principal component analysis if you don't know how pcs don't worry about it we'll get to later but lot of algorithms can be married with the kernel trick to implicitly apply the algorithm and even an infinite dimensional feature space but about needing your computer to for infinite amounts of memory or using an infinite amount of computation but this actually the single place is most powerfully apart is that it's the support vector machine in practice I guess in practice the kernel trick is apply all the time in support vector machines and that often in other algorithms all right I shoot any questions all right so last two things I want to do today um one is fixed the assumption that we had made that the data is linearly separable so you know sometimes you don't want your learning algorithm to have zero errors on the training set right you know so when you take this low dimensional data and map it's a very high dimensional feature space the data does become much more separable but it turns out that if your data centers will be noisy right if your data looks like this you maybe wanted to find a decision boundary like that and you don't want it to try so hard to separate every little example right as to find it really complicated decision boundary like that right so sometimes either the low dimensional space one the high dimensional space file you don't actually want the algorithm to separate out your data perfectly and then sometimes even in high dimensional feature space your data may not be linearly separable you don't want to algorithm to you know have zero error on the training set and so um there's an Aron County or one norm soft margin SVM which is a modification to the basic algorithm so the basic algorithm was win over this subject to and so what the l1 norm soft margin does is the following it says you know previously this is saying that so remember this is the geometric margin right if you normalize this by the normal W becomes excuse me this is the function of margin if you divide this by the normal W becomes the geometric margin so this optimization problem was saying let's make sure each example has functional margin created equal to one and in the unknown soft margin SVM we're going to relax this we're gonna say that this needs to be bigger than 1 - see there's a Greek alphabet see and then we're gonna modify the cost function as follows where these C eyes are greater than or equal to 0 so remember if the function margin is creating equal zero it means the algorithm is closed find that example correctly right if so long as this thing is getting a little zero then you know why and this thing will have the same sign either both positive or both negative that's what it means by product of two things to be greater than zero both things have to at the same sign right and so if this is if so so as this is bigger than zero it means it's closed find that example correctly and the SVM is asking for it to not just classify correctly but classify correctly with a with a functional margin at least one and if you allow CI to be positive then that's relaxing that constraint okay but you don't want the sea-ice to be too big which is why you add to the optimization cost function a cost for making see I - great and so you optimize this as function of W and these are the alphabets and if if you draw a picture it turns out that in this example with that being the optimal decision boundary it turns out that these examples these three examples will be equidistant from this straight line right because if they were then you can fiddle the straight line to improve the margin you've a little bit more it turns out that these three examples have function margin exactly equal to one at this example over there we have functional margin equal to two and the further away examples that even bigger functional margins and what this optimization objective is saying that this is okay if you have an example here with functional margin so everything right - the everything here as function margin one if an example here I have functional margin little bit less than 1 and this by having by setting CI to 0.5 say is letting me get away with having function logic loading less than 1 and one other reason why you might want to use the unknown soft margin SVM is the following which is that's it you have a data set that looks like this you know seems like it seems like that would be a pretty good decision boundary but if we add just a lot of examples there's a lot of evidence but if you have just one outlier say over here then technically the data set is still linearly separable right if you really want to separate this data set sorry that seemed to be cooling these pens myself as well if you want to separate out this data set you can actually you know choose that decision boundary but the basic optimization classifier will allow the presence of one training example to cause you to have this dramatic swing in the position of the decision boundary so the R because the original auto margin classifier it optimizes for the worst case margin the concept of optimizing for the worst case margin allows one example by being the worst case training examples have a huge impact on your decision boundary and so the l1 non-stop margin SVM allows the SVM to still keep the decision boundary closer to the blue line even when this one outlier and it makes it so much more robust to outliers and then if you go through the represents a theorem derivation you know represent w is a function of the alphas and so on it turns out that the problem then simplifies to the following so this is I'm just right after some some after you know the home represent account the whole represented calculation the derivation this is just what we had previously I've not changed anything so far right this is just exactly what we had and it turns out that the only change to this is we end up with an additional condition on the Alpha rise so if you go for that simplification now that you've changed the algorithms of this extra term then the new form this is called the dual form where they also say no problem the only change is that you end up with this additional condition the constraints between alpha are between 0 and C and it turns out that today there are very good you know packages software packages for just solving that for you III think once upon a time we were doing machine learning you need to worry about whether your k√∂ppen inverting matrices was good enough right and when when Kofa inverting matrices will as mature there's just one thing yet to think about but today linear algebra you know packages have gotten good enough that when you invert the matrix it just invert the matrix you doesn't worry too much when we're solving you have to worry too much about it so in the early days of SVM solving this problem was really hard you don't worry of your optimization packages you optimizing it though I think today they're very good numeric often and packages they just solved this problem for you and you can just call it with all worrying about the details that much all right so this l1 no soft margin SVM and oh and so and so this parameter C is something you need to choose we'll talk on Wednesday about how to choose this parameter but it trades off how much you want to insist on getting the training examples right versus you know saying it's okay if you label if you can example as well well we'll discuss on Wednesday we'll discuss PI's and variants how to choose the parameter like C all right so the last thing I want to lastly would like you to see today there's really just a few examples of SVM kernels let me just give all right so it turns out the SVM with polynomial kernel works quite well so this is a you know K of X Z equals X transpose e to the T this is no that's called a polynomial kernel and this is called a Gaussian kernel with this video the most widely used one is a Gaussian kernel right and turns out that I guess early days of SVM's you know one of the proof points as yes was few the machine learning was doing a lot of work on handwritten digit classification so that's a so digit is a matrix of pixels with values that are you know 0 or 1 or maybe grayscale values right and say you take the list of pixel intensity values and list them so there's 0 0 0 1 1 0 0 0 0 1 0 and just this sound all the pixel intensity values then this can be your future x and they feed it to an SVM using either of these kernels it'll do not too badly as a pen written digit classification right so does the classic data set called m-miss which is a classic benchmark in computing in history of machine learning and it was a very surprising result many years ago that you know support vector machine with a kernel this does very well on hermit integer classification in the past several years we found that deep learning algorithms must be convolutional neural networks do even better than the SVM but for some time lesbians were the best algorithm and and they're very easy to use in turnkey there aren't a lot of parameters if they don't work so that's the one very nice property about them um but more generally a lot of the most innovative work in SVM's has been into design of kernels so here's one example let's say you want a protein sequence classifier so protein sequences are made up of amino acids so because a lot of our bodies are made of proteins and proteins are just sequences of amino acids and there are 20 amino acids but in order to simplify the description and really not worry too much apology and hope the biologists don't get mad at me I'm gonna pretend that 26 amino acids even though they're because they're 26 alphabets so I'm gonna use the alphabets A through Z to denote amino acids even though I know this suppose me only 20 but this is easier to talk with with 26 alphabets and so protein is a sequence of alphabets right because the protein in your body is the sequence is made up of the sequence of amino acids and amino acids can be very variable 9 something very very long so if you're very short so the question is how do you represent the feature X so it turns out and so the goal is to be the input X and make a prediction about this particular protein like what is the function of this protein right and so well here's one way to design a feature vector which is uh I'm going to list out all combinations of four amino acids you can tell this will take a while right go down to a a a Z and then a a B a and so on and eventually you know there'll be a be a JT TST a down to zzzzz right and then I'm going to construct five x according to the number of times I see the sequence in the amino acid so for example being a JT appears twice so I'm gonna put two there you know TST a whatever right a PS ones so I'm for the one there and there are no a is no ABS no you see okay so this is a 20 to the for you know 26 to 420 is a four dimensional feature vector so this is a very very high dimensional feature vector and it turns out that using some statistical 22 for is 160,000 it's pretty high dimensional quite expensive to compute and it turns out that using dynamic programming given to amino acid sequences you can compute 5x transyl Phi of Z as K of X Z and there's a there's a there's a dynamic programming algorithm for doing this the details aren't important for personal siestas you know if any of you have taken in advanced years algorithm schools and learned about the knuth-morris-pratt algorithm this is quite similar to that so don new right with Stanford Stanford professor emeritus professor here so the DPRK's question was in ads and using this is actually quite this is that here pretty decent algorithm for sequence of say amino acids and training a supervised learning algorithm to make a clock binary classification on University premises so as your PI support vector machines one of the things you see is that depending on the input data you have there can be innovative kernels to use in order to measure the similarity of two amino acid sequences or the similarity of two of whatever else and then to use that to buy the classifier even on very strange shaped object which you know do not come as a feature okay so and I think actually another example or if the input X is a histogram you know maybe you have two different countries your histograms of people's demographic because it turns out that there is a kernel that taking the min of the two histograms and then summing up to compute a kernel function that inputs two histograms it measures how similar they are so there many different kernel functions for many different unique types of inputs you might want to possible okay so that's of SVM's a very useful algorithm and what we'll do on Wednesday is continue with more advice on now do you know all of these learning algorithms we'll talk about bias and variance to give you more advice on how to actually apply them so that's great and then I look hard to see you on Wednesday