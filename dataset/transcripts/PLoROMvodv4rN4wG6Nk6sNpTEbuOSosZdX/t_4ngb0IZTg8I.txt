hi everybody welcome back we're going to be um talking more about policy gradient methods today and then starting to talk about imitation learning but we'll do a quick refresh your understanding to start for I think everyone agrees that it will not necessarily converge to a global Optima so that's great um this uh some different opinions about some of the other ones so maybe turn to a neighbor and talk about whether a baseline term can help to reduce the variance and whether or not after one step of policy gradient the resulting policy can get worse the you're my seems not okay great all right so I think a lot of people um remembered from last time that in general a baseline term does help with the variance and that's one of the reasons we are adding it um you can initialize it with a sub uh you can't initialize it with a deterministic policy does somebody want to say why it's the problem with deterministic policies yes and my uh potentially there's like an action that the policy will never take so it's it's not able to reach a low minimum or Optima right yeah so if you're EXA said so if you are only taking actions deterministically you're never going to know about what other actions are in that state so if you're current policy is suboptimal you won't get to The Optimist table and then the last one is something we're going to talk more about today uh so in general it can get worse this is true um in general we're not guaranteed to have monotonic Improvement we would like to have monotonic Improvement but um in general policy gradient doesn't guarantee that but last term uh with po we saw things that are trying to get more towards that kind of monotonic improvement okay great so what we're going to be doing today is is we're going to talk a little bit more about Po and some of the sort of theoretical underpinnings of it as well as another feature about it that we didn't talk about last time and then we're going to start talking about imitation learning okay so first and and all of you guys are going to be implementing po as part of your homework as well as reinforce so you'll get a chance to practice with this um we're first going to talk about generalized Advantage estimation so first let's just refresh our memory of um sort of one some of the ch Alles with policy gradients that motivated Po and a whole bunch of other research on sort of better policy gradient methods Beyond reinforce um so in general we're we're using data to prorize the policy space and we're just going to do stochastic gradient descent to try to get to a good value um a good a policy with a good value the challenges is that in general when we did reinforce the sample efficiency was poor we had to run get data from one policy take a single gradient step and then get more data from the new policy and as we were just discussing I think was mentioning this or maybe um that the distance in the parameter space is generally not equal to the distance in the action space um so sort of the policy space so when you make a small change in the parameters it might really change the type of actions you take so in proximal policy optimization we saw two different ways to try to make it so that we could essentially take bigger steps steps in between each run of when we um execute a policy uh but do so in a way that would try to encourage monotonic Improvement and so we saw this bound we're going to come back to that very shortly which looked at sort of how we could try to approximate um the performance of a new policy under only using the data that we have right now so that's an instance of sort of off policy estimation and the bound showed that this relates to the kale Divergence between the actual action taken under the new policy versus the old policy and in po that you could either do so this adaptive kale penalty which sort of says don't go too far from your previous policy in terms of the actual actions it takes um or a clipped objective which is going to do something similar right so one thing that you probably noticed in particularly if you started implementing this already is that we talked like last time a lot about using the advantage function so we talked about how we're going to be doing and in general for sort of um policy gradient we're often going to want an advantage function and you might wonder what we're going to plug in for that there are a lot of different choices for what the advantage function could be so what we're going to talk about today is a particular choice um that was used in Po and that can be pretty powerful so let's go back to last lecture before we introduce Po and talk about the endstep estimators so in general in class sort of since the first probably since the second lecture or so we've been talking uh second or third I guess probably third lecture three we've talking about this trade-off between methods that bootstrap and use the Markoff property such as temporal difference learning and methods that don't leverage anything about the markof property like Monte Carlo so in particular we've talked about cases where this is sort of a temporal difference estimate where you just have the immediate reward plus gamma times the you immediately bootstrap you plug in your estimate of the value of the next State versus ones like a Infinity here where basically we have a Monte Carlo estimate obviously you can't actually go out to Infinity you'd have to have like um episodic cases but we always we we're generally have been focusing on episodic cases so far in the class minus the value so the the blue there is just us subtracting off our current value so we have this Advantage estimate so we talked before about sort of the trade-offs between these different estimates and how some of them would have higher bias and some of them would have higher variance and what I'm going to talk about now is sort of a way to use the to try to get to a new form of Advantage estimation um and it also involves a technique that comes up a lot in reinforcement learning so it's a useful thing to be aware of so what we're going to do is we're going to define something called Delta VT let me just highlight this and what this is is it's just essentially here a TD backup so this is just what we've often seen where we have our immediate reward plus gamma * V of the next state so notice here that we've got a Time step t so that's going to be important and then this is just the value our current estimate of the value of the state so this should look like normal this is sort of the same Advantage as up here but note I could plug in different T's here okay so we've defined this new thing called Delta so when we use this new Delta then we would say that our advantage with the advantage function we've seen before we use a TDS is just exactly equal to Delta VT why is it V well V is specifying here what value function we're going to plug in and this is just exactly equal to this so same as what we saw before now the next thing that we can say is well actually what is the advantage for if we use this two-step estimate that is this okay so we've got this expression and that is exactly equal to Delta VT Plus Delta v t + one so I'm just going to write that out so we can see it for for one second for why that's true okay so we had RT + gamma V of St + 1 minus V of St that's this term plus gamma R of t + 1 because notice here this is t + 1 okay plus gamma V V of s + 2 minus V of s + 1 so when we do this in this way what we'll end up canceling here is the V of St + 1 let me just so not we have a gamma here in the front so we have this term cancels with this term oop sorry this term this term let me just be a little careful here we're going to have this term cancels with this term good we're going to get the two rewards RT and RT + one second one so that's here and here and then we have gamma 2 * V of St + 2 okay so I just wrote out exactly what the definition was of Delta and Delta t + 1 and essentially the important thing to see here is that one of the terms canel okay okay and so that's why we ended up getting exactly the same expression for a 2 as we had before and you can repeat this and what will happen is kind of all of those intermediate terms these things where you were bootstrapping will cancel along the way so this is why it's called a telescoping sum because here we're adding something that at the next round we're going to subtract from this the next one and so those are going to cancel and so you just get end up getting all the discounted sum of rewards plus the last term who's seen telescoping sums before okay Sobe about half people here okay so for somebody this is really familiar for some this might be new it's a useful technique to see to know about because it comes up in a lot of the reinforcement learning proofs yeah just in comparing this to the equation for a hat sub T2 above yeah are we basically saying that gamma s the St + 2 is equal to gamma the S + one no we're just we're just actually literally canceling it good question so when we write out this expression there's a gamma in front and because there's a t + 1 here this will become s of t + 2 this will become s of t + 1 and there's a gamma so this will be a gamma gamma time a minus B of St + 1 and that will cancel with the V of St plus one in the previous one yeah yeah like on top and on the bottom are they equivalent then they're identical identical okay yeah yeah that was a good question so yeah I've written this in this notation now with the like Delta notation but this is exactly equal to this which is the same so these are I mean these are identical o uh yeah so thanks so there's a typo there that might be the question yeah just highlight that so this should be T yeah in general we're always bootstrapping with the final time step thanks for cing that yeah so these all end up being exactly equivalent we've just Rewritten it in terms of this Delta notation using AOS okay so these are just different endep estimators we haven't done anything new yet in terms of I we Rewritten things but we haven't introduced any new type of estimator these are just different Advantage functions and as you might imagine you know the first one is going to be uh low variance High bias the last one is going to be low bias High variance so generalized Advantage estimation involves taking a weighted combination of kstep estimators so we had here this was just lots of different estimators and you might say well how do I pick among them I'm not sure I'm going to pick among them I'm just going to take a weighted combination of all of them and in particular you could just take um an average weighted combination so let's just step through this a little bit just to see how we do this so what we what this is saying here is I'm going to take the one step estimate of um Advantage estimator plus Lambda I've introduced a new parameter here Lambda * the two-step 1 * Lambda S Plus the 3ep 1 I'm just saying like Okay well why don't I use all of my estimators and I'm going to weigh my different estimators so now what I'm going to do is I'm going to I've next just written this in this the Delta notation that we saw on the previous slide okay and now what I'm going to see is that some of these terms appear a lot of times so there's this this term appears in all of the terms in all of the advantages the second one only appears in the second to the last one Etc so I'm going to collect terms so I'm going to write this as follows and this was introduced in a previous paper to PO and then po bills on it okay I just noticed what I've did there I've noticed that I had this term me so I'm just taking all of those terms and I'm noticing how many lambdas I had in front of them okay and then I'm going to have Delta t + 1 V time Lambda * 1 + I'll write it differently so this term is going to start with Lambda plus Lambda s because it's in the second through the all the rest of the terms plus all right so I'm just rearranging the sums and now when I look at this I realize that I've got a geometric Series so this is just going to be cool to 1 - Lambda * Lambda TV / 1 - Lambda plus let me just make sure got there was I'll put on the next page just to make sure I made a there should have been a gamma here let me just put gamma right cleanly in the next slide so this will be clearer okay so we have we also had a gamma here from before okay so gamma squar and then what you do is you realize this is a geometric series that goes to 1 over 1us Lambda and then this is gamma * Lambda 1 1us lamb and this is gamma s * Lambda s I'm just using the fact that this is a geometric series it's fine if you haven't seen this if you've done real analysis you've seen this before and that means that the term below just looks like the following and this was introduced by a previous paper and the idea there was to say well why don't we just take kind of a weighted average over all these different terms that have different biases and variances and then we can reexpress it compactly we don't actually have to compute all of the advantages separately and track them we just are going to keep track of these deltas and these Deltas are pretty easy to keep track of because those are just like these one-step differences between so just remind ourself what what the Deltas look like the Deltas are pretty easy to keep track of because they're just the difference between your previous imate and your new reward plus gamma BST plus one so you can kind of just keep track of those over time and then you're just weigh in them okay and our derivation just followed and so then you just sum these up and you essentially have different weights okay all right so let's think about what this means in terms of uh bias and variance as we often like to in terms of the estimators we're using so this is trying to be an estimate of the advantage and we'll do a check your understanding now about about how different choices so this is a a discount Factor so this is the discount Factor comma your choice of Lambda which is how much you're waiting earlier ones versus later ones so G is generally a function of these two hyperparameters and let's think for a second about what this does for bias and variance and how it relates to TV for can you like b or no you can okay good see otherwise the te is helping make these just check with them and feel free to go to the previous slide look at the definitions and the reason these all are really important is because if you get better estimates of the advantage you're going to get better estimates of the gradient if you get better estimates of the gradient can hopefully use less data to get to that really good policy so that's why people meant quite a lot of effort thinking about with the with sort of using deep neural networks both either for the advantage or for the policy how can we really quickly get good estimates is gay like Z even def there's a z to Z there there is this yeah because in the first term there will be 0 to zero Z to the zero it shouldn't be to the Expo Oh you mean here um you in that case you would just plug zero in up here okay and then that would disappear and you would just get this all right why you turn turn nebor and see if you got the same answer use the defition [Music] would you not look at um Z it would just be well I guess okay so thanks for the good question I'll make sure to clarify like the notation if Lambda is equal to oopsie I'll make sure to clarify um that let's just say if Lambda is equal to equal to zero look at first line okay so I'm I'll make sure to clarify that in next year's slides so I in that case everything drops off so if Lambda is equal to zero all the other estimators go away basically you have no um no no weight on all of the advantage estimators that are two or more and so it just becomes the first term and the first term is the TD estimator yesal one shouldn't the entire thing become zero if if what if Lambda is one if Lambda is one yeah so if the Lambda is one well then you're also kind of you're suing from an infinite number of terms too um but yes NE well so this is this is this is true so B is true and the second one we'll see on the next slide I mean this is a it's a little weird to write down in this fractal infinite Horizon because you can't ever do multicar returns with infinite Horizon so but it's a good you guys have good questions I'll make sure to like clarify what happens in like sort of the H equals infinity one and Lambda equals z cases just so that like the the infinities are clear um but this is certainly false because this is not tv0 because we'd have a whole bunch of terms here and there'd be this weird waiting in that case Okay um and then the cuz then you sort of have to weigh how much is this term versus the Infinity of the other like the Zero versus the Infinity of the other term so I'll make sure to clarify that um D is also true because once this is a TD estimate then we generally know TD estimates have higher bi have higher bias and lower variance okay so are true now note in general you would think therefore you want to put um Lambda somewhere in the middle because it will be balancing between bias and variant but what they do in PO is a little bit different but it's related to this so this is what the generalized Advantage estimation is um we kind of do this like exponential waiting over lots of different Advantage estimators but without actually having to have separate copies in memory of all the advantage estimators so that's why this is nice um so what we're going to do now is see what we actually they actually did in po which is instead of doing all of these we're just going going to do a finite number so what they're going to do is a truncated version where they use this but they only go up to a certain point and so they're not going to go up for to Forever there's multiple benefits to this including the fact that they're going to be in episodic domains and what this means is that let's say Your Horizon is very long but not Infinity so your horizon might be something like 2,000 steps for your Mount car or something like that um you might pick t equal to be 200 and what that would mean is so remember the benefit one of the benefits of temporal difference learning compared to Monte Carlo is that you can update your estimate after every step um the problem with the advantage estimator that is defined here is you still have to wait till the very end to update your EST like to update your estimator because you need your advantage with near infinity and then you're going to weigh all of them so you don't actually want to do that in practice so um one thing that PO proposes is to say well why don't we just do a truncated version and that means every T steps like Big T steps so let's say t is 200 every 200 time steps you can compute this you compute your your new sort of weighted um average EST like your advantage estimator and then update so you can think of sort of the the Big T here is determining how long you have to go before you can make an update so that's what they do in po um they use this truncated generalized advantage estimation um in order to get better estimators okay anybody have any questions about that before we move on to going back to this question of monotonic improvement okay so now let's go on to another important feature of Po which is it's really sort of in some ways going backwards but I wanted to make sure to go through the algorithm for last time so that you guys could start working on implementation but if I think uh and as in many papers the theory is a little bit decoupled from what's actually done um but it's sort of serves as motivation so I think it's useful to go back to the bound that was proposed there that helped Inspire their algorithm and think about what it actually um implies about what happens when we do updates okay so remember that um and as you're proving right now for homework 2 uh remember that what we do in um what they were thinking of doing in po was to say we want to be able to use our old data from a policy pi to estimate the performance of policy Pi Prime but the problem is is that in general that's going to induce a different state distribution and so we played this EST approximation and said let's just ignore the difference in the state distributions and that's great because now we can use our old data to estimate the value of our new policy only our old data um because we always know what the actual policy parameter is but we don't actually have to gather new data from it and and we called this sort of L Pi Pi Prime because Pi Prime is here but everything else is being used by pi and what was proven was that if your two policies have a close scale Divergence in terms of the actual actions they take then um you get this bound on performance okay so it said um this approximation is not too bad and in particular we get this thing of sort of this monotonic Improvement Theory saying that the value here so I'll just write down that J of pial to B of Pi some people use J some people use V we mostly use V in the class okay so that the value of your new policy Pi Prime minus the value of the old policy is greater than or equal to this term that we had on the previous slide so this L term this whole thing minus this sort of um error that we get from the fact that we are approximating the state distribution by something that's not true okay so we have this nice bound and now what we're going to go through now is to show why if we maximize with respect to the right hand side that we are guaranteed to improve over Pi that shouldn't necessarily um well guess I'll ask for so who is seeing the sort of majorized maximize out them before I wouldn't expect you to but I'm sorry so this kind of goes back a I think we've seen ideas related to this in um uh policy improvement from the very beginning uh but this is different because we've got sort of these bounds up so what this is saying is this is a lower bound right like this says that the difference between these two policies is at least as big as this term minus this term but it shouldn't and what we're actually going to propose to do is to say all right well if we try to pick a pi Prime that um maximizes this lower bound is that actually mean that we're going to be guaranteed to improve over pi and it shouldn't necessarily be immediately obvious that that would TR be true okay but it's going to turn out that that's the case so let's just go through the proof for that which is pretty cool all right so we're going to prove that if you do this if what you try to do is pick a policy Pi k + 1 which is the argmax of this lower bound that you will in fact get a new policy that's either the local Optima or is actually better than your previous policy so that's that's the idea of what we're trying to do okay so note a few things okay so Pi so we're going to assume that we have some Pi K that's our previous policy and that it was feasible okay so like it's it's like a it's a well- defined policy sums to one you know it satisfies all of those constraints okay so now let's in terms of okay so now recall that let's just do something a little silly but it's going to be useful okay so we're going to look at what L Pi of Pi K of Pi K is okay that's this term let's just see what that is if we just plug in if we try to evaluate what that term what that sort of expression is when we plug in the same policy as what we actually use to gather our data all right so remember that would just be equal to 1 over 1us gamma expected value over s according to D Pi K just writing down the definition of what L is okay and this is going to be Pi K of a given s / p k of a given s * a of p k all right well this is just one right so this this cancels but the important thing to remember here is that the advantage function of a policy with respect to itself is zero so if I take actions according to to the current policy and compare what the value is to taking actions according to that current policy and then acting according to the current policy minus just first taking Pol actions according to the current policy that is difference is zero okay so that's the can just write that out too in case right so just remember like what we have here is we're going to have q Pi K of S A minus V Pi K of s okay but notice what we have here is that what are we taking you know what's the distribution we're taking these actions it's exactly Pi K so Q Pi K if you first follow like I can just write that out just in case it's helpful um is so this is like sum over a pi K of a s q k s which is just equal to V K it's like if you start taking this action and you follow the policy and then you follow the policy from all future time steps versus if you just follow the poy from now till forever that's exactly the same so that means that this is zero that's good that means that like because if we think back to what this looks like that says that the difference between the value of the policy and the policy itself is zero so this bound is tight if your value in it with respect to itself there's no difference between the value of the policy and the policy itself because the D oh I'll say the next thing so then because also D KL of Pi K Pi K is equal to zero there is no K the K Divergence between a um a distribution and itself is zero okay all right so now let me just label these two so let's call this term one and this term two okay so what we have here is we have that term one is zero for pi K and term two is zero for pi K all right so that means 1 - 2 has to be at least as great as zero does I want to say why that is I made an that's not immediately obvious from these steps yet you have to make one more step and it has to do with the argmax let me see why that is I want to sure why is 1 minus 2 always have to be greater than or equal to zero given the arcmax achieve zero for exactly said yeah so um Pi K is an ex distance proof that there exists at least one policy for which the right hand side is zero we're taking an argmax over the whole policy space that means the argmax has to have value at least zero hopefully better then and so um that is exactly why so because R Max is at least as good as but because we're trying to maximize that okay so what that means then is that so remember all of this term here on the right hand side was what we had here okay so this whole so we had J of Pi k + 1 minus J Pi K is greater than equal to term oneus term two which we just showed is greater than equal to zero so what we just proved is that by maximizing with respect to our lower bound we got a new policy that was at least as good as the old policy which is really cool so that means that using a lower bound on the gap between the um the performance of the policies is sufficient to allow us to make monotonic Improvement is that super elegant so now we could have something if we actually did this most policies do not do this and we'll talk about that in a second but um if you actually did this you would get monotonic Improvement and there's certainly like like a number of domains would be really cool to get monotonic Improvement so I think I've mentioned education before but you could imagine Healthcare as well like a lot of cases if you're doing stuff in the Intensive Care Unit Etc you might people might be kind of worried about doing random exploration or Epsilon greedy but if you could say we're only going to improve when we know that we the new policy is at least as good as the old policy that's likely to be a scenario that's much more palatable okay all right sorry about this out a little bit more here and one of the elegant things about this is that um we can restrict ourselves to parameterized policies this doesn't mean we have to have um you know completely we can think about any sort of policy class and as long as we initialize or initial policies in that class it could be a gan it could be a deep neural network you know um uh then you will and then you keep doing argmax over your policy class you'll get this monotonic Improvement so it's it's really nice it's really um elegant that you could sort that you can do it in this case but unfortunately like many beautiful Theory things it has some limitations um so if you look at the actual so C is a constant and we haven't went through where the constant is in class but you're welcome to look it up in the paper when gamma's near one and what gamma near one means is that we care almost as much about long Horizon rewards as we do about immedia rewards when is close to one gamma is pretty large and so what what that means is that in general that second term um can make you be very conservative so why is that well that means you've got if C is really large that means that if your Pol your new policy takes actions that are quite different than your old policy you're going to have a really big penalty so what that basically does is it shrinks your step size it says um uh this is going to be a term that is weigh a lot and unless you only make very small changes you could get a big penalty essentially because you're saying I'm really not sure it might be that when I change my policy I end up with very different state distributions and I don't know whether the rewards would be there so what that means is that in practice if you actually try to use this equation directly like just straight from the theory the step sizes are too small now when people say they're too small that doesn't mean that like there's anything wrong with them it just means it's going to take way too long it just means that people are impatient um but either impatient or you know we're being very sample and efficient so it means that this is reasonable it will hold you will get monotonic Improvement it's just going to take a really really long time and it's not going to be feasible for a lot of things or it's not going to be practical and so that is what sort of helped motivate why you might want to tune the KL penalty which we saw last time where you sort of like increase or decrease how much you care about this penalty um or use a trust regen or use the clipping and so that's why we see sort of a difference between what's formally guaranteed by if you were to just directly use this lower bound versus what's actually done done in practice but I think in terms of kind of the the the take homes from this part on policy gradient and and mppo is that it's really useful to know that you don't just have to take one gradient step you can be much more data efficient you can play this trick of pretending there's no change in the state action dist or state distribution in order to take several gradient steps and that you can do that while still trying to maybe approximately get monotonic Improvement poo does not guarante monotonic Improvement but it can be pretty close um by thinking explicitly about um these lower bounds and how much uh your performance might change and how much essentially your state distribution might change so that when you're not confident in these approximations it also uses um generalized uh Advantage estimation which can be helpful and as I mentioned before it's extremely popular you can use it in many many places um in part because also you don't need your reward function to be differentiable so people have used it in lots of domain right and the other thing that I think is just useful to remember when we think about policy gradients is that you can also use them with actor critic methods so you can have um you know deep neural networks to approximate your value function and then use that for your um Advantage estimation and combine them and so that's what most people do is that they have some sort of U critic AK your value function estimate and a policy and these are um only two you know reinforce P are of course not the only policy gradient algorithms but they are the backbone to all they're still used empirically a lot and then also um they're the backbone to many of the other ones so if you read other papers they'll be really useful baselines that you often see or that people are building on all right we're now going to go into imitation learning but does anybody have any questions before we start there yeah on on slide 22 uh just a general question I guess sorry the one before this yeah so this um does that mean when the policy is more myopic and like H is near zero then um um your step size will be like like you'll be able to improve more to a greater extent yeah that's a great question so yeah you're like is the is the converse good so if C if if um gamma is near zero is this practical I don't actually know off the top of my head what C looks like for gamma equals z um or not zero but near zero so I don't think anybody uses this in practice um I think they always use like the clipping or like the kust region so my guess is that it's still not practical often times the the C constants will often be a function of Vmax like your maximum value often scaled by um 1 over 1us gamma so it can really be quite enormous in many cases um uh so it might be that here it was particularly they might be interested in cases where your Horizon's pretty large or where you I think one thing here too is that if we're in the episodic case there's not really a good reason to think that the discount Factor shouldn't be near one because you probably actually do just care about all the rewards so they're probably mostly interested in domains where they didn't think it was reasonable but yeah it's a good question all right let's talk about imitation learning okay so as we've said before in general in computer science we like to try to reduce things if we can we like to reduce them to other problems that we know how to solve and so imitation learning is going to be our attempt to try to do that at least in certain ways for the all of reinforcement learning okay and some of these slides come from some of my colleagues at Berkeley and at uh C so in general we're going to now be thinking about the case where we're not going to be gathering online so we saw in po that we tried to reuse our data a little bit more to take bigger steps but one thing you might wonder is well why do I need any more data at all couldn't I just gather some data and then just use that and maybe I don't need to gather any new online data and we'll see more ideas about that shortly um H but one case where you might think that would be reasonable is what if you have great demonstrations so you know you have instances of doctors making really good decisions in the Intensive Care Unit or you have people people flying planes or you have people driving cars why couldn't we just use those examples to directly learn decision policies okay um and so the hope would be there is like if we just have those recordings you know anytime someone's driving like a Tesla or anyone's someone's driving an airplane could we just get those um sort of State action um Pairs and tles and use that information to try to learn a policy directly now one thing you could do instead is to say like you'd have a human in the loop but that's going to be pretty expensive and so the hope would be that um instead we could just use the demonstrations people are already doing and that might be much more reasonable too in terms of people's time so one thing in this case would be all right now maybe we're going to try to just look directly at demonstrations and that means we're not going to get anybody need to have anybody to label things this is an example from sort of um trying to understand what the reward function might be for driving so I guess I just say in addition to the fact that we often have data about people doing these sorts of complex tasks that we'd like to imitate it also might be in those tasks that it's really hard for someone to write down a reward function like maybe in this sorts of setting like you want to avoid the water unless it's really really steep or really grally in which case maybe your your truck or train can go into the water or maybe like in general you want to avoid trees but again you know if it's really slippy and muddy it's actually better and so it might just be that it's really hard for people to write write down a reward function in this case but they could drive it and sort of indicate that implicit reward function and so again that might be easier to gather this comes up in a lot of different cases and people have thought about it certainly a lot for kind of um manipulating heavy machinery or manipulating cars or things like that um but you know for things like driving and and uh and uh parking and stuff those are a lot of cases where people provide those sorts of demonstrations where it might be hard to specify that reward function so the idea from learning from demonstrations um is that you're going to get a number of expert demonstrations so experts will demonstrate things whether flying a helicopter or manipulating you know something with a a robotic arm or stuff like through T operation dorsa SES group does a lot of this and it will give you a sequence of states and actions not rewards so you just are going to have like trajectories of State action S Prime okay so we're not going to have any rewards anymore there's everything's just going to be implicit in this case and now we're going to assume that it's easier for people to do this so they're just going to hopefully be able to provide these demonstrations or they maybe already have so what's the setup for to for the rest of today the setup is that we still have a state space and an action space we're going to assume this some transition model and and there's a reward function but we don't know it so or there might be a reward function but we don't know it so there's nothing explicit here there's no explicit rewards and that we have these set of demonstrations in Behavior cloning what we're going to do is just reduce this to supervised learning and try to learn a mapping from States actions we're just going to try to clone the behavior and then we're going to also see some about can we actually recover the reward function that people might be using to generate their behavior and then if we have that can we actually try to get a new good decision policy right but the first one is just kind of to try to directly learn a policy so this is called Behavior cloning and essentially once you decide to do this this is just offthe shelf supervised learning so now you treat it is as you have a sequence of states and actions from your expert trajectories and you can use whatever tools and supervised learning you want so just anything can be done there I just to reduce it's strictly not made it into a supervised learning problem and there were some really early successes so like Alvin from a very long time ago um and then 1982 learning to fly in a flight simulator really early on in sort of the history of reinforcement learning or like kind of the modern history of reinforcement learning people thought like could we just reduce this problem and we'll see in a second what's one of the challenges that comes up when we do this but it certainly can be really helpful so it's kind of fun to look at Alvin this was you know yeah late ' 80s so um but I think it's must have been kind of amazing they were already thinking about cards then they're already thinking about not so deep neural networks but they were thinking about neural networks I think this came out of CMU if I remember right and they had this like tiny you know 30X 32 video input and they use this Rangefinder um and so they were trying to you know use not so deep neural networks to do Behavior cloning for driving in the late ' 80s which is pretty awesome okay so so it can be can be done pretty well um in reality this is something that still people try a lot it's a really good Baseline to try if you have good data um and I'll talk about some of the challenges with join Behavior cloning but I think one thing now is like if you have a lot of data like a lot a lot a lot of demonstrations like imagine you have all the data from all the pilots um like you have their actual what they're doing all of the different sort of input actions they're doing and you have that from I don't know all of United or something like that so if you have an enormous amount of data and you have a pretty sophisticated um uh supervised learning technique it can work really well particularly if you use Behavior cloning with an RNN or something that takes track of the history so while what I wrote here involved just States and then like the last state like a Markoff assumption like the state in the action you don't have to do that right you could also say I could have my state action State and then go from there to A1 or like State action State action state so you in general could use something that um is like a recurrent neural network or anything that like keeps track of long-term histories it does not have to be a Markoff representation and that often can work very well again it depends a lot on your domain I think that um there's a nice paper a few years ago in Coral which is one of the robotics conferences where they looked at sort of what were some of the important factors when you're doing offline learning and um uh from for robot applications so it doesn't always work well um but it can work really well particularly if youth history um what cuz like imagine like if you're think you're flying or driving really what matters just the current moments so like when when is that actually help I actually would debate that so I think even it's a great question um I I think maybe partly depends on how you're thinking of the state space but I think if your state say for let's say I'm driving if my state is just my immediate like position that's probably not enough I probably need at least my last few to get to like velocity and acceleration so you might already be thinking oh in my state I already have those if that's the case if your state order incorporates like something about like you know the first or second order derivatives that's probably okay in some cases um but in other cases if it's just your immediate then um immediate sensors then you want the the longer history to come capture that and save for planes and stuff yeah it's a good question so this is always just a really good thing to try it's a really natural Baseline it's generally really easy to do people often report it in offline RL um it's extensively used does not always work let's see why it might not work and it I think one of the themes that you're seeing now with like the um policy gradient work and now is this challenge of what are the states you reach and how when you use different policies you're going to end up at different States in general that's sort of the definition if your policies don't ever reach any different states and they never take different actions they're the same policy they generate the same trajectories so dagger was a paper 2008 I'm trying to remember um I it came out in the um in the first decade of the 2000s to try to address some of the challenges with behavior cloning okay and I think what they were noticing is this challenge of if you do your cloning sometimes things go badly and essentially that's because the decisions that you make over time can have sort of cascading effects so let's see what that might look like so if you do something like supervised learning you so and this is what we do when we would be reducing our problem to like imitation Behavior cloning we just reduce it gr learning in general we assume that our um our pairs our data points like our XY Pairs and supervised learning are IID so they're independent and identically distributed and they're going to you know they're ignoring temporal structure because they just assume they're totally independent but in our case they're absolutely related in fact if you assume a Markoff structure then what happens is you have s0 a0 S1 and so whatever you did here exactly helps determine what is the next state you do so they're not states are definitely not independent um all of your all of your different time points so one of the challenges with that is that if you have independent in time errors and generally that's not too bad like and that's what most of our supervised learning guarantees are for is that you assume your data is all IID and then you can think about how much um sort of error in your estimates um what what sort of error you get so in general if you have an error at time t with probability less than equal to Epsilon and you have t decisions so let's ass we have t decisions then your expected number of total errors if all of your decisions are independent is just Epsilon * t plus you because they're all IID okay and that's not too terrible but that's not what we normally have in oh I see what happened there okay okay let's think about something else I'll add a different picture later let's think of a race track all right so in this case you have a RAC trck and you're car is driving except for your supervised learning thing isn't perfect and so it makes a small error so what you actually do you should maybe maybe you actually should have went this way but you went to the black part okay and now you again make a little bit of an error and now you're off the track and now this is really tricky because you may have almost no data in the part of because your humans never decided to drop dri drive off the track and so now you're in of the region where you have very little data and very little coverage and you're even more likely to make mistakes and so what you can see in this case is that like if you make small mistakes early on those can compound and get you into parts of the stat space where you have even less coverage and you generally have even less accuracy and so in general you can actually do much worse and this is because you have a data distribution mismatch if the policy that you compute gives you a different distribution between train and test then you don't necessarily have the same guar okay and we're going to get a different distribution here because the policy we were using um to gather the data is not exactly the same as the policy you get now and we saw that in P2 that when the policy changed we're going to get to different states and actions what's causing our policy to change here is the fact that we can't perfectly imitate the expert okay so let's just see what that looks like so in this case we had in our training set we had pistar which we assume to be our expert and we're generating States from piar in our test set we have learned a policy by trying to match the state action pairs we saw in our trading set and we're going get a different distribution of States so in general this is going to be different and we're going to get worse errors in this case sorry about the let see what happened in this case so I'll just draw it so what can happen in this case is let's say this is the error you make now and then you can make another error and it keeps compounding so if you make an error at time step t with probability e essentially what can happen there is that you may then make errors on the remaining time steps so it make so cause you to get into parts of the state action space for which you make lots of errors and then you sort of incur lots of regret or cost through the end so in general and I am not going to step through all of the proof today the error can actually comound instead of linearly with the number of time steps it can comound quadratically which means that essentially your performance is much worse than supervised learning would predict supervised learning said oh I've got an Epsilon optimal or Epsilon accurate um policy great and what this says is because all of those decisions are being made across an entire trajectory you can actually end up with Epsilon * t^ 2 errors instead of Epsilon * t so this is what motivated dager dger said okay what's the problem the problem that's happening here that we'd like to address is whenever we make mistakes we go into a different part of the state space once we're there we maybe have very little guarantees that we're going to do anything reasonable so essentially what we want to try to do is kind of figure out how we might correct or adjust to those states that we reached that weren't in our original training set so the idea in this case is that your Pol this is going to be an iterative approach so you get a data set where you take a current policy and you execute it in the environment so it's like you know you drive your race car around a track and hopefully you know you you know it's similar to what the expert would have done but probably not perfect and then what you do is you go to your expert and you say okay this is what I did when I went around that track what should I have done they're like a coach and so then what the coach does or the expert is they say ah in each of those States this is what you should have done okay so it would say if you went like this and then you did this and did all these other crazy things after that it would have said okay no first of all here you should have gone here and then once you reached here you should have went down to try to get back onto the road so essentially what you're having a human do is you're having them label it every time point at every state in that trajectory what they would have done and when you do that that gives you a new set of data to learn from so it's like you know your export pilot gives you feedback on every place you made a mistake um when you just did your last flight run and then you integrate that you're like oh okay when I'm like you know filling this form of lift next time I got to do this so it gives you a whole bunch more data and then we aggregate that data that's why it's called dagger so we're aggregating the data sets of the old data we had and the new data that we just got from our expert we then do Behavior cloning again on our new data set which now includes more of the states in the environment and then we repeat and I think part of the motivation for this and this is why I said Behavior cloting can work really well when you have enough data is that the problem that's happening here is that um we're assuming we don't kind of have full coverage over the whole domain of what the expert would do at any place inside of the save race car track and what this is allowing us to do is to better figure out over the whole Space what the what the expert would do and make better decisions and correct in case we end up in those so in dagger we do this over and over and over again and there's some nice theoretical guarantees of what you'll converge to when you do this and what they did is to show this for things like driving driving in like a simulated domain like a Mario Kart or a video game um and show that they could learn quickly how to sort of get a very good policy that didn't suffer from these kind of compounding errors can I think of what a limitation might be of doing this over Behavior CL cloning yeah exactly it's super expensive yeah so you have to um basically it's like you have to have that coach or your teacher or your expert with you the whole learning so the nice thing about Behavior Clon is you get data once the data might already be available and then you can just learn from it here you have to have constant supervision now in some cases that might be reasonable but in most settings that's going to be um really impasible so this this is very human in the loop human has to supervise and so I think for those reasons that's one of the reasons that um you know in robotics and some other areas you'll certainly have built a lot on dagger but I don't think it's as popular as Behavior clothing because it really does require a lot more work of the human all right so a second thing you might want to do is learn a reward so you might say all right there's I'd like to actually figure out what the reward is you might want this for several reasons you might want to learn the reward because you want to understand something about human decision making like you might say all right I want to understand how surgeons are making trade-offs when they're dealing with really complicated situations of like how do I trade off time or risk or things like that and maybe it's really hard or just you know their time is really valuable to ask them lots of questions but you really would like to understand kind of that preference structure so that's one goal and another is that you might want to use that then to learn a policy you might so like if I can extract that from the data then I can learn a policy from that and you'll see that in homework three because we're going to be doing r jef as part of that we're going to try to learn from preferences so there's lots of reasons you might want to be able to learn reward function so in this case we're going to be in a similar setting we're going to still have a state space and action space and a transition model still no reward function still going to have some expert demonstrations and what we want to do is infer the reward function the expert was using implicitly to make their decisions and what we're going to assume for now is that the teacher's policy is optimal so like or you can call it expert the teacher or the expert's policy is optimal so let's think about what we can infer from that so if you see someone's demonstrations and you know that they're optimal so teacher I'll use teacher equals to expert for this thing if you if you see this you know it's optimal is there a single unique r that makes teacher policy optimal are there many does it depend on the markup decision process you're not sure and remember we know that the actual policy is optimal and if you think there are many I'd like you to give me an sample simple one which would make things optimal I mean not the thing but I'll ask in a second all right why don't we just do a quick check um and talk to a neighbor and see what you find t s I don't really understand repres okay good so almost everybody said the answer is B which is true there is many does anybody want to tell me um kind of a silly one that any policy is optimal under yeah two are like just scale by constant fact yeah that's great and I was hearing that too over there so if you scale if you take a word function and you multiply it by a positive constant like then that can't change the policy um zero works too so you can just use zero and any policy is optimal if you never get reward so I bring this up um not to trip ASE it but just to highlight that this is a huge identifiable Pro identifiability problem there is not a single R even if you know that the demonstrations are expert um there's not a single reward function that's compatible with them and so that's a problem and that's something to keep in mind when we start getting into uh rhf and DPO shortly um that this is either you need to be making other sorts of assumptions to constrain your reward function or or in general we're going to have to make additional Cho choices or constraints because otherwise this a is a not identifiable problem great so one thing some people do to try to think about how we might do this is to think about um so I think I know what happened I was editing two sets of slides and I think the other one is now well updated but this one must not unfortunately um in any case we did um we talked briefly about uh value function approximation through deep Q learning deep Q learning naturally implies that um we would use a deep neural network but you could use a linear value function just like you know a very shallow network uh the idea here and this is all the this work predated deep Q learning is to think about generally where your reward is linear over the features so your reward of s so here we're just doing reward respect to States is w w is just going to be a feature Vector W is just going to be a vector X of s and X of s here is just a feature representation so this is just features are X so that for example could be like if I'm a robot if this is my current location what's the distance to that wall what's the distance to that wall that wall and this wall that would be a set of features and then I could have a weighted combination of those to give me the reward of me standing here and the goal is to identify the weight vector you given a set of demonstrations okay so in that case you can also Express the resulting value function for a policy as a combination of these weighted features okay and so just write out sort of particularly because we didn't do it in class um very much going to write out sort of what that looks like so it's the states we reach under this policy this time STS T equals 0 to Infinity gamma t of our weight Vector it's unknown times our feature representation for that time step given we start in s0 but note here that W um is always the same so we can just take this out okay we have WT the expected value s pi and they should start to look somewhat familiar because it's going to look like these weird discounted features that we've seen sort of before so we can also call this w T new of Pi for this is the state distribution under discounted State distribution under P okay we've seen this before we go sort of back and forth between thinking of there being time steps and thinking of us is sort of saying well over all time how much time do we spend in each of the different states so in particular here I've defined mu to just be the discounted weighted frequency of State features starting in a particular state so why have I done this well I've done this to say we can relate what the value is to just a linear combination under this linear reward function a linear combination of my weight feature which I don't know times my feature distribution okay and that's good because I have access to Features I have access to trajectories that were demonstrated by my experts and I can use that to extract the features of those stes and compute something like new okay all right so but we don't know what w is yet so let's think of um what we could do so the goal here is that um we want to identify the weight Vector W given a set of demonstrations we've just seen that we can rewrite the value of a policy Pi if these rewards are linear as WT mu of Pi where it's the um discounted State frequency all right so what we know is that V for the optimal policy is greater than or equal to V Pi for any other policy and that means that W Pi of mu Pi star has to be greater than equal to W Pi of mu Pi for all pi where this is observed so to um exper so what it means is that if I pick any other policy and I generate what are the state features you'd get under running that policy in the world that distribution of features has to have lower reward than the features I've actually observed in my data because I've assumed my expert is optimal so my experts demonstrated things it's optimal and when they demonstrated things like let's say they're um you know controlling a robot and the robot spends all this time over in this part of the room and if they spend time over this part of the room then all my features are going to come from over here and that means that any other policy that I use its features have to have a lower weight if they don't match what the features are of the expert okay regardless of what w is right because this has to hold okay so this is for the W that we pick this has to be true so we can rewrite that as saying that the value of V St has to be greater than V which means that the value under um this is yeah the the we can just write it down in terms of this and the resulting frequencies so therefore the experts demonstrations are from the policy to identify W it's sufficient to find a w star such that this holds so we know this has to be true under the true expert that under the true W it has to be that features we get under the expert policy have to be have a higher reward than the features we get under any other policy so this gives us a constraint that says when we are searching for what w is because remember W determines our reward function this has to hold okay this constraint um and then what we can do is so it's sufficient to say well what would be one thing we could do to be optimal if we want to get to a policy well we just need to match the match the features of the expert so we need a policy that induces the same distribution of States as the expert so in general if you have a policy such the features you generate under that policy are really close to the features you get under Pi star then for all W with W Infinity less than equal to one this is using holders inequality you're guaranteed that the reward of this policy is very close to the reward of the optimal policy or your expert policy and all of this is just to say you can reduce the problem of sort of uh reward learning and policy learning in this case to feature matching that's kind of the high level idea is to say in the case where you don't observe the reward directly but you have access to Optimal demonstrations by an expert all you need to do is to find a policy and or reward function that um that allows you to match those features because those are the features that we know have higher reward all right now as we've already talked about there is still an infinite number of reward functions with the same optimal policy so even when we think about this mapping features it doesn't solve the issue we just identified um and and there are many stochastic policies that can match the feature counts so I haven't told you anything yet to solve that big problem I've just told you sort of another way to think about it and so there's this question of like how do we pick among all these different options so there's a number of different ways to do this some of the largest and most influential ideas are are these two maximum entropy inverse reinforcement learning and Gale and what we'll do next time is to talk about maximum entropy imerse reinforcement learning which has been very very influential so this is in 2008 so we'll pick up on that On th on Wednesday thanks