hi everybody we're going to go ahead and get started because we're going to be having a guest lecture today which will start at 1:45 um so welcome back we um just in terms of where we are a few different quick Logistics things the midterm as everybody probably knows is on Wednesday it'll be in class you're allowed to have one side of a normal sheet of paper um in terms of your sheet of notes all the material through today is going to be eligible for the exam um that was also in the Ed poost and you can see the Ed post for any additional information around um midterms and uh PRI exams because homework 2 was only due on Friday and a lot of people use late days through yesterday we're not releasing we won't be able to grade it in time for the midterm but we will release Solutions so those will be available by the end of today all right so let's start with a quick refresh your understanding this is on the polls and then I'll do a quick um recap of rhf before we dive into our guest lecture this be a good reminder of some of the ideas that will be relevant to today's lecture as well for all right we have pretty good consensus on the first one that this is true the Bradley Terry model expresses the probability that someone will select one option over another option so this is true and we have pretty good consensus the last one is false in rhf we do not update the model after each po roll out um there's a little bit of disagreement particularly about these two so why don't you turn to a neighbor and quickly see if you can resolve is f as a hint it's useful to think about whether things can change based on whether or not it's positive or negative all right I hope everyone got a chance to think about that for a second so the second one is true the third one is also true somebody want to say why the fourth one is false false fourth one is false You by a negative Conant yeah exactly so remember your name so that is exactly right so if you multiply by negative of course that's exactly flipping all the rewards um and so in general that will not preserve preferences um you can shift it by any constant and if you go through the math you can see that the exponentials will all cancel so um that part is true okay great so what we talked about last time was maximum entropy inverse reinforcement learning and we started talking about rhf um including how you could use the Bradley Terry model for Markoff decision processes I'm going to do a really quick discussion of rhf with respect to large language models before we get into our guest lecture today and then on Wednesday is the midterm so as we talked about last week while you could um do imitation learning where you get sort of full trajectories and you want to imitate those that is um uh less information than you might be able to get if you got par wise preferences and we talked about how par wise preferences might be an interesting intermediary point between humans having to label like they do and Dagger at every step of what they someone should do or provide really dense rewards um versus just providing demonstrations and so this was sort of has motivated a long line of work including um preference learning recently we saw how you could learn the parameters of a Bradley Terry model as we saw just now uh these are not unique in general um you can do translations of the rewards and you will preserve um the resulting preferences can maximize this with cross entropy and last time we saw how you could do this for trajectories as well as for Bandit like problems where you only have a finite set of actions um in homework 3 you're going to be implementing both DPO and rhf for markof decision processes so get a chance to play with this um where you're using roll outs from mu joku like problems okay but before we go on to our guest lecture I wanted to just briefly um go through how you go from doing this sort of um uh approach to learning uh reward models all the way to chat GPT and so for this I'm going to draw upon some of Tatsu Hashimoto's really nice lecture notes from an NLP class so recall from the start of uh start of the reinforcement learning course we looked at this sort of pipeline from chat GPT and here we had the demonstration data collecting the comparison data and then optimizing a policy so now we've seen sort of how those last two steps happen so in particular you can generate pair wise preferences or in fact you can generate full rankings um and then use that to learn a reward model and so while we thought before about different ways of doing this as a particular example involving language you might say someone might prefer an earthquake hit San Francisco there was mining minor property damage but no injuries versus a 4.2 magnitude earthquake hit San Francisco resulting in massive damage versus Barry has good weather but it sun has Wildfire wildfires and earthquakes so you can see in this case that these are places where someone might be able to provide different rankings in response to prompts so now you can think of the context as being a prompt and the output as being um all the actions or all the different responses you can have and people are going to rank them Now sort of building on that before you actually do po or something um you may want to try to check the quality of your award model and this is something that you'll also you can think about for homework 3 so in general depending on the amount of data you have and the complexity of your award model you're going to be able to do a better or worse job of being able to try to capture the underlying lat and reward model of people um so in this case this is looking at different model sizes and you know these are these are Big models like a lot of the models that people have thought about historically or things like linear models or you know then neural network models but these can be extremely large models they can be on the same order as large language models not uncommon to see like you know seven s billion parameter reward models and what they're looking at here is sort of validation accuracy and so what you can see here is when you start to get enough data and you have a big enough model then you can start to capture really complex reward models and so that's a useful thing to think about when you're thinking about your projects or you're thinking about homeworks of sort of what is the complexity we we need in order to start to capture human preferences okay and then once you have that now we have everything we need to do that pipeline so if you have tra you've gotten a lot of preferences now again the question is how many of those preferences do you need it might be a lot so if you look back here this is quite a lot of preference data now it's not the same amount of data that we generally need to be using to train an llm um but it's not like one or two either and in fact there's some uh and a lot of ongoing interesting work and trying to think about how do we reduce the amount of online preference data that we need in order to train these okay by online I just mean additional data compared to the historical so in reinforcement learning from Human feedback what we can do is once we've had that learned reward model now you can use that with Po and one of the important things to note here is that just like how we saw for po before in general we're going to need some sort of reference decision policy um that maybe we use from like super like Behavior cloning or supervised fine tuning and we want to regularize so we don't get too far from that when we're doing po okay um and so that sort of Divergence is going to be just as important as what we've seen in the previous work and one of the things that's been noted is that you know perhaps not surprisingly given the huge success of J chat gbt this type of approach can make a significant difference so by leveraging rewards and doing rhf there really was a substantial gain over previous um approaches even when you fix for the model size so that suggests that changing the optimization function we're using and using the Roo functions really can lead to substantial gains in performance so I think something that's important to notice here is sort of like what are we doing the reinforcement learning over and what is how are we training the reward model and comparison to what we've talked about mostly in this class this is really where you're trying to do something almost like meta reinforcement learning or multitask reinforcement learning so instead of trading an agent to do one task like do a backflip or you know solver grid World we're really trying to train a large language model here to do any possible task a user might want and so then when we're collecting data and we're doing sort of um comparisons you might have an enormous number of different tasks so writing a thank you letter to making a website to lots of different things all things that used to be previously considered different tasks will likely be involved in this so another thing that I think it's useful to note is that um this is a comparison from 2023 also from Stamford um there's also been a lot of other work this is a very on important ongoing area to understand how good these approaches are and one thing that's useful to know is that best of n is an alternative where you could for example use your reward model just generate n samples from your original model and then just use your award model to pick the best one according to your reward model so that doesn't use any reinforcement learning doesn't use po it's just using your reward model as sort of an external like expert to try to pick among all of your generations and what you can see here is that also does pretty well relative to PO now in general it doesn't do quite as well um but I think it's really useful to think about some of these alternative baselines particularly depending on whether or not you have access to actually training the model again versus you might have access to being able to train a reward model um and you might have access to an off the-shelf llm and you might be able to combine these it's a very active ongoing area to figure out what's the best way to train and refine these sorts of models okay all right so that was you know a f minute overview of how people use rhf to train chat gbt and now I'm really excited to have our guest lecture on Direct preference optimization yay all right okay well I'm super delighted to have Rafael archid and Eric here today to talk about direct preference optimization I really appreciate you guys coming um I know you guys have done this um Rodeo before at nerx um in terms of balancing between three people so for those of you that don't know direct preference optimization um got outstanding best paper runner up at NS this year which is the premier machine learning conference um it's also had a huge impact already on the um well really broadly on the L1 community um as an alternative to rhf so I think it's extremely exciting you guys will get to do to my knowledge the first homework um that's incorporating RF and DP uh which would be really great and what they're going to talk about today is this and they also just had a um newpaper dropped on our time just a few days ago about some extensions so I think it time that thanks so much y u well yeah thanks um so much Emma for having us um it's funny when when you talk about sort of the the impact of the paper you sort of want to say RL but I guess it's like llm Community or like what what even is the community anymore it's it's hard to draw the boundaries between things but I think it's so cool like to see how the boundaries are kind of breaking down between these areas um so yeah as Emma said we're going to talk a bit about rhf and DPO and we have a little bit of background that um I'll I'll do to set things up for um these guys to to uh bring things home and some of this is probably going to be reviewed um from things Emma has already covered but just to kind of make sure we're all on the same page um we are in fact talking about this setting of reinforcement learning from Human feedback and um as a like small piece of sort of background or setup here you know why are we talking about rhf why are we doing RL on language models why are we talking about it now um people did not start doing RL on language models a few years ago when chat GPT came out people have been doing RL on language models for a long time but um you know this this sort of chat gbt moment so to speak um is something that I think really brought um these these RL methods to language models into the Forefront of people's minds because there's sort of a sense in which things really started working you know for the for the first time in a way that maybe they they didn't before and a lot of this um comes from being able to start from a really strong pre-trained model that that already has a lot of interesting kind of um sort of skills um and pre pre-learned behaviors that we can that we can fine-tune and so we don't have to start from scratch when we're doing RL typically on these language models and that that makes it a lot more kind of access to to get some benefits from these algorithms okay so so RF we have this we have this three stage pipeline that is the thing that is sort of been popularized by by chat GPT so um in this in this first stage I think Emma actually showed this same figure in her slide just a minute ago so um this this isn't totally new in this first stage um we we were actually there's really a step zero here which is do the unsupervised pre-training this is when we just fit a big generative model of like a ton of text this is again we kind of learned some sort of metal learn in some sense some some skills we're going to select from um and then we're going to collect some some supervised demos so from from humans um we'll have some data set of prompts uh you know explain the moon landing to a six-year-old and a human is going to write sort of a sensible good uh uh demonstration uh response to this to this prompt and we're just going to do supervised fine tuning here and this is going to actually serve as that reference policy that Emma was talking about a few minutes ago the thing that we're going to uh constrain our model to um to to make uh learning a little um easier and also to avoid over optimization of our approximate proxy reward function that we're learning from in the second stage that's when we do the learning of the reward model so here's when we collect preference data so we're going to sample responses typically from this supervised fine tuned model that we learned in the first stage and we're going to ask a human or often a collection of humans to provide ranking annotations over multiple draws from that supervised fine tune model and we're going to use those preferences to learn a reward model so a mapping from a prompt uh a dialogue history and a potential response to a scal of reward and then in the third stage we're going to do policy learning so we're going to try to fine-tune that supervised fine-tune model model to generate responses that get uh that that receive high reward from that reward model okay so the first step is pretty uh straightforward supervis fine tuning we don't really need to talk about it very much um and again Emma already covered uh some of these things so um hopefully this is mostly review but you know of course ask questions if if anything seems funny um um like I said the feedback here the thing we're going to do is we're going to get preferences over responses from our model okay so we're going to end up with some data set of a prompt and this could be like a single prompt or it could be an entire dialogue history so multiple turns and then sort of the most recent user um uh message and then this is typically going to be only two responses that we're going to do give a binary preference over you can do rankings over more responses but um the the the returns can be sort of uh uh Plateau relatively quickly and it's typically maybe better to have more prompts and fewer responses per prompt I think it's worth mentioning briefly like why are we talking about preferences over responses instead of you know directly asking for reward annotations right like you know you could you could take your prompt in your response and just ask the human like one to 10 how good of a response is this um and I there are a couple reasons for this I mean uh first of all uh actually another set of slides we have of an example of this which I think makes it quite clear I don't think we have them in this deck but um you know if you if you take two different humans and you say you know here's a prompt that says write me a recipe for making a really good cake um and you ask two different humans um you know you have two different responses from your model and and for human a you say you know what reward do you give to this response and that response and another human you know you ask the same question uh you can end up with the same ranking over responses but actually a lot of disagreement in the actual rewards that you're giving so people are not really calibrated to each other in terms of the absolute rewards that they're going to be assigning um and it's also just uh sort of more cognitively uh difficult to assign this absolute number um in contrast to anchoring to one response and then just making a decision about is another thing better or worse so so in some sense um I I think of you know Gathering preferences as opposed to um asking humans to write high quality demonstrations or asking humans to assign directly um the the reward itself is sort of a way to um uh get higher return of annotation information per sort of unit of cognitive effort of the human labeler so we're going to get these preferences and and now we just have this Bradley Terry model which is a a very simple model of discreet choice in humans um which which relates a scoring function or in this case a reward function so that's this R of X and and a and a response y here um to to a a probabilistic decision over two discret choices okay so here our discret choices are you know we have uh the the the thing that was labeled preferred in the data set and the thing that was labeled this preferred in the data set and we we wanted to train a model uh some probabilistic model again our reward model um to to maximize the likelihood of this observed data and we need to decide on some model that relates a scoring function to these choices in order to do maximum likelihood and that is this bradle ter model which we can then simply do uh maximum likelihood in or you know use this negative log likelihood of loss okay so so we're using this Brader sort of conceptual model of of choices and this turns into a maximum likelihood loss so we're simply solving a binary classific problem so we have a binary binary classifier here where the the logit is just our reward model the the difference in the reward we're assigning to the The Chosen response minus the um the the dispreferred the rejected response that's just we're treating that as the logit of a binary classifier and doing maximum likelihood so once we do that we get a reward model um we finished step two now uh and now we need to just find a policy that that actually optimizes this reward and um really this is this is the RL bit so to speak um and and here we want to learn again Pi Theta this is our our policy that we're actually fine-tuning we're actually learning here and the objective here is you know we have we have prompts we have some data set of prompts or conversation histories uh and you know in expectation for for responses sampled from our policy we want to achieve High reward um but but it's not the full story here right if if we um you know if we just um optimize to to maximize the reward here like what can what can happen I'm not sure if you've talked about this ready okay perfect anybody have any worries about just optimizing this objective or are we good because this is okay we could we could tell if an AI like you can forget the rest of the objective perfect okay so one thing that can happen here is you remember this is not a true reward function right this is something we learn from a finite data set and so there's going to be some distribution in which this gives us accurate or or meaningful reward and there's going to be you know outside of that distribution there's no guarantee this thing is going to generalize meaningfully so what we typically end up doing is we actually have an additional constraint a KL penalty uh from our from our starting model that sft model or reference model to say I want you to maximize rewards but I don't want you to drift to far from the starting model because again our reward model was trained on preferences over samples from that reference model okay so if we if we drift far from the reference model we're sort of out of distribution for the data that our reward model was trained on so we basically can start getting bogus reward scores if our if our policy changes too much from that reference model yeah refence model Ever Changing uh it depends on the algorithm I think the sort of original canonical uh version of rhf no it was it was a fixed reference model but there have been a lot of work since then showing like ways to update the reference model and use a moving reference model over time yeah yeah data is coming from that reference model you mean that like both those like y w and y l they both coming from the same model uh again in the sort of original canonical form of rhf yes since then there people have proposed a wide variety of different sort of sampling schemes a way to select you know what pair of responses do you show the human to get a preference over but in sort of the original vanilla version of rhf yeah you typically sample two responses from the reference model get a preference over them and use that to learn your word model I've heard that like in practice but like do like responses like come from like the same model with different temperatures or like different models does that like in theory kind of mean that we're doing something wrong um well I wouldn't I mean I'm not sure in theory it means you're doing something wrong I think like one way to think about this is again like we want our reward model to perform well um across the state action space right so like you know if you think of our state space being our context space being the conversational history so far and our action space being the response like you want to have good coverage over this space so that you're going to get meaningful rewards out when you actually update your policy and so you know like in principle yeah we would like to be able to cover this space assuming we have a model that has high enough capacity to model all of it we'd like to cover as much of the space as possible so yeah like a more diverse preference data set is very helpful and there's some trade-off sort of between we want to concentrate our preference data set on the things that are high quality but also make sure we do cover a wide variety so we don't sort of overestimate rewards for bad stuff okay one more and then I'll I'll hand it off to these guys learning we know that even if you have limited data set but if you have a large enough Network then if you train it to near zero error on The Limited train data set it can generalize well on test data set as well why is start not like applicable here as for the reward mod well is applicable in the sense that like the the same sort of phenomena like like double descent and things like this like are still applicable in this case so I mean you you will get better performance typically from using a larger reward model um but you know there are limits to this I mean there's only a certain amount of information content in a finite data set of of preferences and so the you know the extent to which you can push that model to generalize to to new things I mean my preference data set only has questions about like what types of pets someone like U someone likes it's just not going to tell you anything about Quantum field Theory um you know no matter how big you make your model um so so there are limits but yes I mean you you you would expect some level of generalization Okay cool so so that that is a primary on r and um basically uh unfortunately what we end up with is this you know if we're doing PO for example this ends up being really really really complicated uh in the policy learning stage so there are a lot of moving pieces here and I guess you all will have the distinct pleasure of implementing this spere homework um congratulations um but there are a lot of moving pieces here and that was sort of one of the motivating reasons for for why um DPO came to be basically was that PO turns out it was a little bit tricky to to get it to work for particular problem that that Raphael was um uh initiating some research on so um anyway that that's sort of the the background on rhf and I'm going to leave it to archit now to to give an overview of thepo all right thanks Eric uh is this working okay cool just all right who's ready for the fun ma stuff um so you saw the scary picture here and really the question we wanted to start with is like do we need to do all this just to like find iner model according to human preferences and unsurprisingly the answer is going to be no so like be prepared for the ride um and yeah um we saw this objective earlier and kind of before we go into the math like I want to just give a high level picture of what is going to happen here um we had some reward function which kind of told us what humans like and humans do not like um and right now we're parameterizing that as a separate Network saying that this will give us a score for which answer is good and which answer is bad now really like a can we like sort of Leverage the idea that our language models have these probabilities over completions and the completions right now represent any distribution over the internet but can we like overload it somehow to basically represent oh can we only put probability on things that humans like and that's roughly the idea we're going to try to exploit is that there's essentially a mapping between the language model and the reward model itself one-on-one mapping that you can use to directly train the policy on preferences themselves and towards the end of this what you're going to have is a distribution of responses that are not just arbitrary text responses on the internet but responses that humans like and that's where direct preference optimization will come in um how do we do that that's where the math is going to be so we saw the rhf objective which is essentially we want to maximize the expected reward over completions and we have a k constraint to the reference distribution Um this can for now we're just assume it's any reward function um the math is going to hold for any function but in general it's the Learned reward function now I don't know if this was covered in the class or not but like it turns out that this equation or this problem has a closed form solution um okay um great I'm not going to derive it maybe I'll leave it as an exercise for people to like um but it's a fun derivation and it's not too hard so hopefully you should like um find the time to do it um but really like if you've ever heard of like boltzman distribution or something of this form this is really just atate um and this is not what we're contributing this is a known result for a while and it's very intuitive like it might look scary for a second but it's really what it's saying is that we had the reference distribution that we started with and we had some reward function and really what we're doing is we're upgrading the responses by the exponentiated reward so things which have a higher reward will have a higher probability according to the exponentiated reward now if you just look at this this is very simple but this won't be a probability distribution and the thing on the left hand side is a probability distribution so we normalize it by this partition function which is the zfx um think of it as just like summing over every completion for a given question X um now you can imagine that that's a very very intractable quantity like if I start Computing every sentence and try to measure the probability and then multiply it by an exponential or what that's just basically not tractable so this equation by itself is not very useful um and yeah I went over this this is exactly the definition of the partition function we're summing over every response y uh the PF is a distribution we started with and the exponentiated reward and beta is the temperature term trading of the reward and the KL uh constraint so this is intractable we'll we'll hold on to the partition function for a second and we'll see what happens to it but really this result is a relationship between P star and the reward R but now we can do a little bit of algebra and Shuffle it around and rewrite the reward in ter terms of the optimal policy itself um so what does this equation say we're writing the reward in terms of the beta log ratio where the ratio is between the optimal policy Pi star and the reference distribution we started with and then there's this PES key partition function that just continues to stay on there um I'm going to try to like develop some intuition here like this is important is that what it is saying is that if an optimal policy way puts more probability distribution on a response than a reference distribution the reward is higher does that come through and like if a probability is lower then the reward is lower and this is intuitively correct right this is how a reward function should also be if a response is preferred then it should have a higher probability and a higher reward so this is you can see like we are starting to develop a relationship between a reward function and the probability distribution itself cool so but the main problem here is that this is by itself not very tractable because the partition function as we said is like just completely intractable so maybe let's go back to what we were doing in the rlf process um the high level idea is that like we mean uh we are we have a loss function on reward functions and we're going to use this transformation and once we plug it all together we're going to get a loss function on the policies themselves um and if we go back to our L function for the reward bit if you remember the logit is the difference between the rewards of the preferred response and the dispreferred response this is what Eric covered just a little a little bit back now if you look at this this difference is not going to depend on the input itself or the partition function itself if we look at it explicitly so this is exactly what is going to happen here is we're going to take the equation that we took earlier Express the reward in terms of the policy we're going to learn and we're going to plug it into the reward modeling loss and once you compute this difference this partition function is going to cancel out because it only depends on the input X and it does not depend on the output we're Computing it over and when we do this we get our final beautiful loss function which we call um the DPO loss function and really is just a reward modeling loss and let's take a second to like see like what it is doing what we're trying to do is is like we have a preferred response YW and a dis preferred response y l for a given question X and we're trying to maximize this difference um that's how we would minimize this loss and maximizing this difference means that our log probability on the preferred response should be higher than the probability that the reference distribution puts on it and the log probability of the dis preferred response should be lower than the probability that the reference distribution puts on it does this make intuitive sense why this would like sort of change the probabilities in the right way cool and yeah the log partition function basically just cancels out did you want to and you think of this as being a benefit to the fact that you can shift the rewards by a constant so often that's considered not a good thing but here they're Levering you can just cancel out the partition yeah all right I'll hand it to rapael and he can go over the results all right can you guys hear me so this is sort of like the first um sort of control experiment we we run on this project and basically we took this IMDb reviews data set which is sort of like movie reviews and um we wanted to train the model to generate positive movie reviews so we use the pre-trained sentiment classifier as a go reward function this case we do know we have access to the underlying um reward score and then we generated a bunch of data from the base model which was pretty trained with sft ranked it based on the sentiment classifier and create like synthetic preferences and then basically we just took a bunch of baselines um across that data and we fundamentally were interested in comparing to what degree is DPO an actual good Optimizer of the core objective essentially there's this like reward k tradeoff um underlying all of this and we basically wanted to see how good like how good of a parto curve can we um extract from that we kind of see essentially DPO sort of the optimal trade-off here in this simple Road problem curve oh I see yeah um well Paro curve is is a general Concept in in economics and sort of decision analysis and things like that where we have tradeoffs uh between several things for example this case reward versus KL and we're interested in the optimal tradeoff that we can get and we say for example one method par dominates another method if essentially we can get something get more without giving up on something else so in this case for the same K we can get more reward using DPO than than another another method and we actually played quite a bit with the baselines here I mean we probably I probably spent like a couple months trying to push these po numbers and um essentially it works like poo kind of works and you get some results there but um it's it can quite catch up with with the DP objective and what I kind of want to include this curve here in this in this talk is um essentially I think even now basically almost all of the RF paper that you read are actually doing um evaluation potentially wrong because you go read these papers and you kind of get the win rates or you got like get the comparisons Etc but none of them really like plot this curves you for none of them you really don't know where along this like tradeoff you are and that that number in of itself doesn't really tell you much uh because um you know it's a question of optimization and you don't know how well that optimization worked or didn't work just by extracting one position position on this curve so I think that's kind an important point that uh the community is still not quite making as much but but I think when any of these new things come up I think this sort of the the fundamental question that should be asked do you think it was because the reward model is is specif or do you think it's like which part do you think it's where is in this Cas do you think model that good or the P optimizations so basically if you look at the purple thing that's kind of like the how the Box po uh TRL um and if you look at some of these our implementation things to do a lot better um so the core difference there and surprising to me people have Wroten like numerous papers about the same thing now and to me was sort of a footnote how we got this to work better we just sampled more answers per uh per prompt that was a question of variance right um and in the RF setting the variance problem is even higher because of the constant shift so we actually did some analysis around this one writing in the paper about like 60% of the reward scores are like noise essentially so signal to noise in like regular PO is about 40% and when you mixing that like in the whole process like the variance like completely explode so it's like very kind of like sparse signal to learn from there yeah sorry um how do we I'm just sorry this is just picture question but like I'm having a hard time knowing how do we read the graph like like is it better to have higher reward or like what is this graph actually telling us about each metric yeah so I mean obviously it's better to have higher right this is the cor concept of reinforcement learning you want to maximize reward but essentially from thef setup we maximize reward subject to a k constraint subject to some K cost what this graph is saying like for basically is plotting me for a level of K using each of these baselines how much reward can I get and you want that to be basically said PR optimal in the sense that like you want to get the most reward for a certain level of K and the other point I made is basically people compare only like win rates or essentially reward but they don't tell you at what K so you can like compare for example you know oops sorry you can compare this DPO point to this PO point and like this PO point will will appear better right because it has more reward but fundamentally as an optimization algorithm that's not the case this model is interpretable or you can canot can you explain what's going on under the hood or just optimization M what do you mean by inter so if you provide feedback right the behavior will change can you explain the whole process or you cannot explain the whole situation if you put in noisy data uh for example can you deug it explain the whole process yeah I mean I think that's more complicated question than than it seems on on the surface like there's Co lines of research on basically I with noisy feedback I multimod feedback plurality of alignment so it's not quite like answer I can give like in one sentence right it's a lot yeah expl again why we have a bad signal to noise ratio in normal PP uh it's a it's a long question there's like a whole section in the paper it's about half a page it's not like one like s kind of line yeah but essentially by samply more answers per response kind of like goes away um can you explain what the reward means for sentiment generation yeah it's basically the sentiment of the sentence and one is very good sentiment zero is very bad sentiment so like move this on hopefully a One sensor um just to for our appreciation of the graph about what kale Divergence trade-off would you choose in a real model here like is is it that like you might choose something like 10 so we're really in that region or is it somewhere much farther it's very much moral and data dependent okay yeah I mean this graph means absolutely nothing in a summarization set got I think maybe like just to like I think it's very hard to choose a specific Gale but usually what people do is measure performance on other benchmarks yeah they care about and usually they find that if the K is smaller the performance on other benchmarks is preserved so you typically try to like air on the side of like lower K and yeah I mean there's no specific number but like wherever you find like your MML performance is great that's where you like stop yeah in time um we had a bunch of other experiments in the paper um which kind of like show up B basically DPO works but I think really really like the Testament to to the algorithm is it's kind of be like widely adopted B the community and in larger scale um this was maybe a little updated haven't like looked at this recently but a couple of months ago this was basically the open a Leaderboard on on huging face basically leaderboard of open language models and I think nine out of the top 10 models were trained with DPO and is kind of the open source community and since then you know even institutions have taken this up in particular this is taken from the mistro paper that basically they used um DPO exclusively as their rhf algorithm um and as you kind of know basically you know some of the the mistro strong mistro models are somewhat competitive with gbd4 for example so we do definitely have evidence that this works at very large scales and for basically like last week we now know even llama 3 is using DPO as part of its optimization pipel planine interesting enough it's actually using it with mixed with other things so basically the the tldr is this kind of algorithm sort of works um and you know we're seeing it kind of like taken up more and being used for more and more things so this kind of like where the paper ends since then there's been like a ton of other works that we have done and and other people have done thought a lot about what to kind of talk about this from those Works um for example I heard you guys have learned inverse Max entropy inverse reinforcement learning you can actually derive DPO as a inverse q-learning algorithm in in a Max entropy RL setting it's obiously trivial but but is possible um and and that paper is called like your language is secret your language model is secretly a q function so you can U for example you can do that um I've heard heard you're going to use sort of RF on control problems uh I I don't know I haven't talked with the the T but actually DPO does not work for control under the classical formulation and you need formulation of preferences under regret rather than the reward functions so hoping they've taken that into account that's like whole separate other work but um I guess what I decided to kind of like focus on is this sort of DPO versus poo debate which is going to be like raging a lot on in the community in industry very much on Twitter um and kind of like want to give you my perspective for this because and I don't want to sound like entric but I think pretty much the entire debate is wrong uh I think there's like let's skip that from now um but basically there's two things DPO fits this implicit reward function which Arch show you can think about this as fitting a particular reward model and there are two questions there the first question is is this implicit reward function as good as an explicitly parameterized reward function a second question is like for this implicit reward model the DPO fits you can analytically extract the optimal policy so basically what I can do is you know I can get the DPO policy or I can take the DPO implicit War function put it into Po and run that optimization Loop under perfect optimization absolutely perfect optimization I'll get back the DPO policy directly if my PO is perfect but that is RAR the case with any sort of machine learning optimization so get something that's like suboptimal and does that like suboptimality induce some sort of regularization effect that makes my model like stronger um so these are kind of the two big questions I think in this debate um so kind of they've been kind of tackled recently um there's this thing callede came out reward bench which is a large scale evaluation of reward models and as DPO is both the generative and a reward model discriminative model we can evaluate DPO models as rewards and basically on several scores here we have this chat safety reasoning type of task so this for example shows uh scoring reward scoring preferences based on like dialogue and chat uh you can see the top four models are all DPO models and outperform for example proprietary models much bigger and sort of close Source ones and on reasoning the Top Model is this proprietary coher model uh but the next like five or old DPO models so and and you know obviously like there's always more work to be done more ADV to be done but in my mind this sort of work kind of like solidified this that the DPO Poli reward is about as good as like the classic AR reward like there's not you know we're not losing generality we're not losing capability for considering this implicit model versus an explicit parameterized one so the other big question is then does using a weaker Optimizer so po provide a better solution gives you some sort of regularization um and basically started to look more into this recently some of the first feedback we got on DP was like someone tried to train like a very large scale DPO model and what they said was like oh you know it does well and then sort of like it becomes more and more of a Bose and then like starts speaking more and more and at some point like reaches a point where just one stop and just kind of like goes off the rail becomes like just can't stop talking and and we kind of looked at this on two data sets one on summarization one on dialogue and what you can see how here is like the distribution of of lengths of answers and the blue distribution is the preferred answer and the red distribution is the dis preferred answer so you can see there's like a very slight bias towards longer responses like people have biases they prefer more like verbos sers they prefer more like verbos more like longer summaries etc etc but once we train with DPO under every column is like a separate level of regularization under any level of regularization this is blown way out of proportion it's not only DPO is allocating probability Mass within the distribution it's pushing basically like this green histogram is a DPO length it's pushing things way out of distribution and you see like now we have answers which are significantly outside of the distribution that's covered in our data set so what is happening there and there is concept of reward hacking I don't know if you've covered like reward hacking but there's a very famous paper from open AI called scaling lws for reward moral optimization and what they did there is essentially the sentiment experiment but but a larger scale they got some real human preferences they train a reward model like a very good very strong reward model and then they use that reward model to annotate some synthetic data synthetic preferences and then they repeated the whole RF process on top of the synthetic PR preferences and this is what they discovered um so basically what this graph is is the same graph I showed earlier for sentiment except that the xaxis is a KO constraint and the y- axis is rewards and these things that the dash things you see are the Learned reward functions in po basically like the expected reward from from your motor training and the solid lines are the actual go War models so what you're seeing from a reinforcement learing perspective it looks like the motor is doing really well it's maximizing reward quite a bit but actually its quality is either nating or going down and this kind of this concept of War hacking has become quite prominent since then you know both for practical purposes but for example the the AI safety Community is very worried about this you know the whole like paper clipping thing if if you've heard about it uh in the way that basically like the motel can find way to exploit these reward functions such that it thinks it's doing something good while it's actually doing something very bad and basically the these things are well understood this paper has something like 200 citations a ton of work has been done on like mitigating these things and the thinking there is like I have a in in classic carf I'm learning a reward function I have a proxy reward and I'm continuously quaring that reward with new data which might make it out of distribution which might kick it off course etc etc so you know it's not surprising that this this happens I think by and large the community has not realized yet that this happens in direct alignment as well because a there's no proy reward function you're directly optimizing the model on the data and B there's no um new data there's no like synthetic data being sampled it's all within the data set but what we have discovered ENT this is a new result that we are currently still developing is that actually War hacking seems to be quite prominent in in DPO and actually all of the DPO varment things like IPO and slick as well do this if you've heard of those um and actually might even be more prominent than than in po because PO is a weaker Optimizer so you have to like push really hard to like really hit those tail of their War function uh but DPO gives you the exactly optimal anal iCal function so in a sense it's sort of almost hacks in like in in an absolute way um so yeah this is currently I think part of the dialogue and and the kind of the research that the community is not quite figuring out yet and you know that's my goal to put these things out there that yes the same War hacking phenomena very surprisingly because it sort of goes against all the intuition we've had from before happens in these sort of algorithms as well um and oh oh right right so it's kind of the same type of plot you see on the left the x-axis a KO Divergence y- axis here is GT4 win rate so basically judgments by gp4 and each checkpoint each data is like a different checkpoint evaluated trained with DPO um and kind of similar to before you see that basically it's like different model sizes and and these are different data but sort of what I'm pointing out here is the pattern this shape pattern um you kind of see like the more you train you sort of like the higher K you go actually your performance doesn't improve goes down so it's sort of the same reward hacking phenomena um the theory tells you this thing should be monotone like you give up some K you get some reward but that's not the case um and kind of the point here is like this seems to be more prevalent in this this technically the dward function is just as good as any other reward function but if you're like optimizing it too much we might be in this like reward hacking phenomena and this is where where potentially like a PP optimization could be more stable or could be more beneficial because it's a weaker Optimizer essentially form of so yeah I think this is sort of like where we are with this type of algorithms right now um and I think this kind of like exciting work to be to be done again um you know in conclusion kind of like yeah I mean we saw all of these things um but I think it's kind of like interesting what the next steps are think like a ton of work has gone B to making RF robust um as we basically we're showing that like these alignment algorithms are very prone to ro hacking as well so I think a lot of work will need to be done to make direct L algorithms robust as well uh there's a lot more interest as as Professor Bruce you mentioned on now online fine tuning algorithms how do we like elicit preferences how do we actually like f tune these things efficiently there's been explosion of RF across modalities not just language models we've done Vision language models we've done diffusion models in particular stable diffusion 3 for example is also trained with DPO we've done text to image there's text to video work being done um potential like speech and musics our next Frontier to be tackled in a couple weeks we'll be releasing a paper on protein synthesis with feedback um and actively working on things like robot safety for things like large scale robotics Foundation models we're trying to do multi turn interactions which fast car jef cannot do um and things like agents to use um and and all those things are all kind of like basically things that in the pipeline and we're looking to so I think there's kind of like a lot of exciting things that that are happening in this field and still like it's been on for a while but I think only now we're just starting to get deeper into and store of like understand a lot of the the finer points of this I'm I'm sorry if we r a little bit over time yeah over that's we got some time for questions so to uh isn't like reward fing implicitly induced by the model itself it's work of itself it's a finite data type of issue right if you have uniform if uniform data coverage over everything hacking will go back but is like a finite data thing because you have like ratios exponentiated ratios there in the reward formulation and you're using that uh everywh because your model will try to maximize that it will essentially try to skew SC ratio right if like I'm wondering if you had some other uh reort function then maybe yeah it still happens so if you use hinch objective still happens if you use like a square type object still happens and Bally think about why this happens so if you think about is like you see that your half cheater running L you know like basically imagine you half CH and your target running Target speed of 10 and you get like running Target speed of eight that's better than run Target speed of seven learning Target speed of nine it's learning running better than Target speed of eight Etc and then you think about well probably running a Target speed of 11 is better than Target speed of 10 but you've never seen anything run at Target speed of 11 so you just extrapolate away and just SW it's basically you know like this picture right you think long things are better so like longer things always better too good question yeah um it's kind of a niche question but I'm kind of wondering so like what if for a particular prompt like all of the samples aren't that great but obviously whoever is ranking them has to rank all of them and doesn't have any way of indicating that even the best sample isn't that great I was wondering if there's a way to account for that like any sort of waiting that could be applied to the rankings that would indicate that the rankings are more or less confident overall no I don't have but like I mean um feel free to interject but like that's a great question like I mean I think General problem around like this is almost like the exploration problem in RL is if you do not like ever see um like good trajectories what are you going to learn without it um I don't have any easy answers frankly but like I think some things that are work is like there's other forms of feedback as well um so there's this is like comparative feedback where you're comparing two things but like you can give thumbs up thumbs down and then if all of them are just bad you can sort of indic it optimize in a different way such that um you're down weting most of the responses um but yeah this is a good open problem to look at I think the explanation Pro pointed important to in that like one thing that people ask a lot is oh how can DPO work because like with po you get to sample from your policy during training so you can like explore and like that has to be helpful right like DPO is just from your fixed preference data set and you're never sampling during training but I think your question actually points out the fact that in some sense because we have this issue of we're optimizing only a proxy reward we don't get to optimize the real reward the important exploration is actually the exploration we do when we gather the data that we're getting preferences over that we're going to learn our reward function from because if we do good exploration at policy training time but we sample some great trajectory that our reward model doesn't correctly label as good it doesn't help us um so yeah in that sense it's basically an exploration problem it's very important that's why I think like sort of multi- Turner sort of like iterative process could be really hard M yeah do you think U like a similar idea could be applied for say multi-step kind of reward like in which you get a reward after multiple steps and you have a preference at the final step but like the reward function was like explicitly comparing like two uh like references between exactly two can you repeat that so so I just didn't quite catch the question I was saying like if you have a multi-step kind of reasoning process and a reward which comes at the end of that would would this this idea kind of apply yeah uh it does work as said sort of like you can think of this as a ke learning problem actually that is however not true to to show but it does work like if you have a problem where basically you have a sparer word at the end like the mod does end up doing some sort of credit assignment on the intermediate tokens if you think of this as like a part token mdp um you will end up with something that does something interesting um for those intermediate steps it's not doing like explicit bootstrapping obviously but like you do end up with some sort of credit assignment and there are like I think there like several results now showing that if you have like sequence level you know rewards you you can end up um uh sort of doing something interesting even though you don't have this these inter Med rewards you have a question next oh yeah can you a few slides ahead like when you talk about synthetic data yeah just can you just explain again like what the difference is between like the real and synthetic like what they're doing in both yeah it's it's sort of the same sort of sentiment problem I was talking about before they they had real human data and trainer re warn function on this so they want to be able to measure the real reward function so they they get this gold reward model which is trained on these real human comparisons and they generate data from their base model and rank that data using this gold reward function so essentially they have access they can Quire the gold re War function and know what the actual score of this like synthetic Generations is so basically they can like essentially create these grows the reason we're getting reward hacking here is because we're not using like the actual reward function we're using like this synthetic reward function if you train on any like reward function with the finite amount of data and if you do like in the limit of like infinite data you would probably not see this phenomena but like because you're training on finite data there will be like errors uh outside the distribution that is trained on and some errors would like skew positive or like overestimate the reward some would skew underestimate the reward but because you're optimizing against that you'll end up um giving responses where the errors are skewing positive so that that's why you start seeing phenomenas where like your learned reward is increasing but your true reward is actually decreasing you guys think back to dagger where like they saw this like propagating her supervis and here's also another interesting tidbit of information all these checkpoints even the very high K ones um they have like quite low success rates actually have very low losses and very high accuracies as as Roar functions so basically the the quality of the RO function is not necessarily connected to the performance of the downstream policy you know quite surprising result of you have a question yeah so back to the um a pairwise comparison so if your object St you are comparing canot perfectly do this kind of pair wise comparison with so for example like say the game uh rock paper stos right say rock is bigger like prefer against but it's not like a perfect partially order SI then what can we do you mean like if the reward function is not transitive it's a great question yeah so there I mean there there's an interesting like outcropping of work that are that is basically trying to get away from the reward maximization uh framework and think of this as a game um where instead of saying I want to generate um responses that are the highest reward responses we should think of this at like the policy optimization level and I should search for a policy where the average win rate if I take the expectation of the win rate of sampling a an action from the policy that I'm optimizing and then I have some comparison or adversary policy I'm going to sample an action from that adversary policy what is the expected win rate of the action sampled from my policy compared to the action sampled from that adversary and so now we have to pick like an adversary policy class which kind of makes sense in your um uh Rec pris example right because like yeah there's not like an optimal action to take here it depends on what the policy of your adversary is here to know what's good and what's bad so um in this case it it exactly does address this issue of if you have only a partial ordering you you can't necessarily compare all pairs of responses we can still use that kind of data we don't have to like uh uh be bottlenecked by fitting a reward function first so there the me methods like um sort of like a Nash yeah like direct Nash optimization or Nash learning from Human feedback are like this like other kind of newer I guess like family of of algorithms that really interesting interesting that frog paper scissors doesn't actually have a deterministic Nash Point um it has like a stochastic one but the stochastic one is just like equal probability over everything um that has related to some deeper results they actually say that like examples like this sort of plurality of preferences are actually unsatisfiable so you cannot actually theoretically train and in practice train a model that will satisfy that set of references I think consider the pizza one way that that isn't motivated to is if you have different distributions of populations with different um preferences even if each of them are internally trans you know consist with transs to be they not cost yeah um so assuming that forward hacking is not happening what in DPO uh prevents it from like taking large steps and the optimization what do we mean about that like assuming reward hacking is not happening like is is there something that DPO is doing that's like preventing it from taking too large of a step in optimization I think the K regularization if you look at the beta term if the beta term is higher you the sigmoid essentially saturates after a point right uh and if the beta term is higher you have to increase the differences less to satisfy the loss so roughly the beta control how quickly you change the loss function but there are other parameters as well like learning rates and so on which also change affect this so yeah I think for uh this reward haing uh problem one of the methods that people usually try to use to address this issue is like using Ensemble models right is that something that could be done with like direct methods like DPO like Ensemble of dpos or I don't know something like that you could the problem with that is then you have to keep all the motos around but there's like smarter assembling things you can do yeah you you can you you don't have to have like complete copies of your entire model to have an ensemble for example so you can like Ensemble sub pieces of your your model or even represent your reward model as a distribution instead of a single scaler um and this starts tying back into these situations where we have um a a variety of preferences in our data that aren't always consistent with each other um you know one way of of modeling this data better is to say I have a sort of you know a non-deterministic or I have a a multimodal reward function instead um and and if you have a way of representing this with this like generative you know model architecture then you can still just stick this into a DP looking loss um here answer kind of answered my question but uh I just wanted to ask in general what were the promising directions for uh addressing reward hacking at DP well I mean there's like a number of reward hacking works on classic AR was like a huge number of those some of those transfer uh pretty pretty straightforwardly here's something I'm I'm kind of excited about um and interesting know that came from the open source community in a way that didn't actually understand they were doing uh they kind of like stumbl ac across this like very randomly by by a very questionable group of researchers on Twitter uh what they discovered is basically if you just take a bunch of like random like rhf models and you just like literally weight average them like they just become better like take the weights like take the average and just just becomes better and turns out this there's like a ton of work on this from like 2018 around the optimization landscape of these things um and you know they very randomly stumbled across in seems to work in but there's a paper called warm weight averaging reward models which kind of like makes that point for reward model so if you train an emble of reward models you don't keep the Ensemble but you average them like we average The Ensemble that significantly improves your robustness as a ro function and the same seems to be actually happening with DPO is if you train an sample of DPO models or take or preach your models and you weight average them that seems to actually significantly improve the question this as well and Twitter randomly stumbled across this without really understanding it but it seem to work for them and turns out there's like a really sort of deep reasons behind behind this so that's what one thing I'm kind of excited about and actually like after we we get this paper out we have right now something on order like 400 checkpoints or something the next thing we're probably going to do is try to see how how how much robustness we can squeeze out from some sort of like and people do smart things now like evolutionary merge and things like that like how much robustness we can squeeze from some sort of like average anding strategy maybe one thing that's sort of interesting also is that like we're starting with this KL penalized reward maximization objective which is you like that was the original policy learning objective is maximize rewards subject to this KL penalty and like the intuition is that yeah we want to keep the KL small so we don't like all optimize our reward function but like this is kind of a crude way of encoding this like P like desert AUM basically and what we like something that might be closer to what we really want is to say well like the the places where you know my reward model has high uh uncertainty you know that those are the places where I want to be conservative um but you know if I have something that's sort of like out of distribution but like my reward model is like really confident or you know there there like um the um I have low uncertainty over what the reward should be or basically like the lower percentiles of reward are still quite High um then it's okay to change my model a lot in in these places and so I think like One Direction I think is also interesting here is getting away from the KL regularized policy optimiz optimization objective which is nice because it gives us this one to Oneness from like policies to reward models um but is also like I think I don't know it's possible this is a bit too crude and it like leaves some performance on the table because we're over constraining our policy and I a quick question a quick point like as I said you can think about all this L is Q function essentially some framework I thing that's kind of interesting in pursuing is like initially dqn were like really hard to get working right and there's like you know after a couple years they will work great because a lot of tricks were used to make them stable make them perform and make them not bootstrap and make them not overfit Etc I think a lot of these things could potentially transfer from that to the i in particular the weight averaging thing is very much I think also inspired by results in dqn where you have like a Target function sort of like like I don't know if you guys did the homework already we have like a Target Q function which is like actually weight average sort some sort of poak averaging so it's kind of like staggered and that like seems for example to improv St a lot I think of similar result go for lq's so to speak but again so sort of in the pipeline of experiments to yeah I'm not sure if this was already touched upon but are there any risks with like overfitting and is there um like certain domains like medical domains where there's like very very small data sets is there a scope for this kind of for on those I this is essentially an overfitting problem right you have like limited data coverage extrapolating in the wrong way it's a little bit more trickier though like is that like I mean people have actually found that in dpu and other settings like this overfitting is somewhat beneficial So like um you can do multiple epochs and small data sets and for some of our experiments you can get a very tiny preference data sets as well and it still sort of works uh but like people do multiple EPO and they're very clearly overfitting but the performance still keeps on improving but again a lot depends upon how you evaluate these models and you're probably losing somewhere else so it really depends upon how you're going to use those one thing to keep in mind is that we've been talking a lot about reward over optimization or reward hacking which is this discrepancy between the proxy reward that we're actually optimizing against the thing we learn from feedback and the true reward that we don't actually get to observe but there's another discrepancy that we haven't really talked about that ARA just mentioned which is that um when we evaluate these models we in practice we're actually typically not evaluating average reward we're typically evaluating something like more like a win rate um which is you know comparing to some baseline policy or something or so so the the the setup is more like almost a satisficing rather than a maximizing kind of kind of situation and so that's like another layer of Disconnect between the thing that we're using as our training objective and the way we're actually the the thing that is actually providing utility for you know the human or like the person who's actually building this thing so there's another layer of where we can get over fitting in a sense to the objective yeah uh so my understanding is it's kind of like two stages the first one is the normal like supervisor training of the language model and then the second stage is the uh DPO training and then you use like the K Divergence is a means to make sure you're not like moving too far away from your original supervis model um it seems like two stages is it possible to like combine this preference zoning during the normal supervised training so like as you're like training the model from the start you're also picking into these um because it seems like you're using kale is like kind of a proxy for making sure it doesn't move too far away but if you do them at the same time maybe it'll help address that there there are a few works that have like tried to so you're talking about merging the supervised instruction tuning and the preference tuning part yeah cuz they're one after another right now right yeah yeah so there's a few works that I've tried to do that I think it's still an active area of research but like the general idea why like so maybe it's like useful to understand why we even do instruction tuning before doing like rlf is that it kind of when you start with a pre-trained model it will give you gibberish responses which are not even aligned with the instruction you're giving so the instruction tuning sort of helps us generate the right preference data set like where you're starting to follow the question being asked so typically in a very typical rlf pipeline when you don't even have a PR reference data set to begin with that's why you do the instruction tuning bit uh but like I mean if you have reference data sets already people are coming up with methods where you can both combine instruction tuning and preference uh learning bit into the same optimization algorithm they're not very different they're usually like some elements of like the Lost functions you already see um but it's still somewhat of an active area of research but like could you do like maybe a dages kind of thing where you train the model and then like you do find tun at one there there's methods which do that as well yeah um in my personal experience it didn't work very well but like um there are papers that claim that works really well so like um we also have problems trying to get it doesn't mean it's impossible yeah it doesn't mean yeah there's a lot of details that I'll go into this like I mean yeah yeah I also personally I'm somewhat suspicious of of these things because the optimization landscape is is so basically as you seeing from this over optimization things the optimization landscape is so complicated and conf like so many different pitfalls I think trying to combine this and navigate that in a sort of like a single shot optimization direction is pretty hard probably not impossible but pretty hard to me it's not really clear what the benefits from that are again I also think kind of goes back to the exploration question which is I think sort of how much it friended there is like in the like you know um on the first day like open AI said let there be an sft model and like there was no preference dat dat you set yet and so in order to actually get the preferences you needed a source of exploration to get the trajectories to get preferences over so you had to do one and then the other and so I think that's like the original way to think about that but now if we're trying to do this like in a single offline stage well now we're sort of stuck like like we're just stuck with whatever data we have in terms of the exploration and just like there's just only going to be so much you can do when you have purely offline data and so you know either doing this iteratively and but like being able to sample and then get new preferences over this samples like is useful to do so you mentioned the discrepancy where we're training to like maximize the reward function but then during evaluation we're um evaluating based on like wi rate so could could we just like use a different objective function to like optimize directly for wind rate is that possibility yeah I mean these Nash algorithms basically are doing that so so instead of deriving this as like we have some reward function and we're maximizing reward it's like literally I have some um either Baseline policy and I want to you know if I if I can only evaluate the preference function not the reward function so a function that takes two responses and says which one's better one objective I could come up with is the expectation under my policy of that preference uh uh function computed on one response from my policy and one response from this Baseline policy or one response from like an adversarial you know best like like worst case best you know adversary policy um and so now you're basically explicitly optimizing for either like average case or worst case win rate against some reference or comparison so how do that compared to DPO depends on who you ask I mean the the the paper introducing these methods like show improvements I mean I think one of the ways that it's helpful is that you're not again like going through a reward function and so you're not requiring um you're not explicitly training to have some uh complete total ordering over all of your responses and so like this can be helpful it's it's not as like constraining of a um um of a sort of framework to think about um at the same time like any policy we end up with compared to some reference model we can interpret as a reward function so like I'm not exactly sure how to think about the advantages there but yes like if you look at the experiments in the papers they will say like yeah we have improvements in like win rate which kind of makes sense right you're you're Ealing with win rate and now we're training for win rate instead of training forward maximization is not that surprising you can see improvements there's also another point that if you do consider Bradley Terry model to be true that this is your preference model this is the data generation model maximizing reward and maximizing probability of win rate are actually like evental so as I said like this reward Max meion thing because of the free parameter has very high variance so what open AI does in other papers but it's always like a footnote in like a you know 100 page paper is they actually normalize their reward functions so they substract some human Baseline so like the reward of the human completion or the human data is like zero and what this gives you is essentially actually the lock probability like then the reward function they optimize with p is the lock probability that generation is preferred over the human generation under Brad so that these things are very tightly coupled and the normalization part you know from our perspective actually doesn't change the optimal policy it just you know in things I've kind of seen and kind of experimented it actually significantly reduces the variance which is kind of the intuition there but actually there's very direct way to tie that with essentially maximizing probability of of winning essentially it's like a baseline yeah it's exactly a baseline essentially and you know this Baseline actually works the variance actually significantly goes down why don't we do one more somebody hasn't question so uh do the uh applicable to uh like multiobjective parl CH there's paper called Mo DPO which is stands for multiobjective DPO um and yeah you can basically like I mean yes you can you can do DPO in this setting where you uh basically like condition on a scalarization of your multiple objectives like a a particular waiting so you don't have to like learn any reward function or you have to like learn n minus one or than no I think it's what do you guys correct me I'm wrong I think you're still learning basically a like a waiting conditioned policy where you can pick the mixture like you have all of your different objectives and you can pick like you know what waiting over these objectives you want to you want to use you know to like which policy do you actually want to you know end up with how do you how do you trade these off and you don't have to like retrain for every single different uh scalarization there are others that do this with like uncertainty over the over the road model as wellers again thank you very much for having us than for coming all right good luck with the midterm everybody see you Wednesday