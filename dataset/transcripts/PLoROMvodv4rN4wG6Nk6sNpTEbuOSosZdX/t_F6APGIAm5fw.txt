here asking you about DPO and rhf for okay great why don't you turn to somebody and compare your answers I okay so there's still pretty good um there's a lot of dis agreement on one of these um for the first one it's false does somebody want to tell me which one does not learn an explicit representation of the reward function so they do not both learn one one of them does and one of them doesn't which one does yeah rhf learns and then D doesn't learn that's right yeah that's exactly right so this one does learn okay so this is false now it's true that DPO assumes a particular parametric representation for their W model both of them do but um DPO then inverts that so you can directly do policy learning it never has to explicitly learn a reward function in the same way that RF does what about the second one what do you think is it constrained to be as good as the best examples in the Parise preference data so I think this is false um does somebody who also said false want to say why why is this false yeah yeah yeah maybe because we're using like U policy approxim we're using a function to approximate it um yeah the gradient could like take a step which is like more positive that yeah exactly what said so um you're G to at least if we think about the rhf case we are using this information to learn a reward model if that reward model is good even um and can sort of extrapolate Beyond and generalize beyond the samples that we have when you do po using that reward model you can learn a policy that's better than your demonstrations so this can in fact go beyond the best sort of performance that's inside your data or if you think of it in terms of the reward um you know maybe some of the examples you're showing aren't that great but then you can use that to actually get a better policy and in fact you might think that's probably exactly what's happening with chat gbt because for chat GPT they initially got um sort of the fine tune model from supervised uh learning and then they showed those examples to people and people would uh pick between them and then it learned a reward model and then they got a policy that was better at generating those sort of responses so you could argue that uh chbt is an example that suggests yes this often can be true we can learn a good enough reward model such that if we do PP at least a little bit of it we can actually outperform the um the training examples um po DPO does use a reference policy both of them do and this idea will come up we've seen it a few times already and it'll continue to come up today this idea of thinking is essentially of how far can we extrapolate or how far can we interpolate from our data um and when do we need to sort of constrain ourselves to be fairly close either in the policy space or something else um so that we don't generalize to parts of the domain where we might have really bad performance we saw that in imitation learning we saw that in DPO we've seen that in PPO in all of these cases where we're thinking given the data that we have how can we um sort of generalize as much as possible but not further right okay so um we're getting into a part of the class which is probably my favorite part of the class though I like of course I'm biased I like all of it but um we've been talking about sort of learning from past human preferences um we first saw that sort of learning from past human demonstrations then we saw learning from past human preferences and today we're going to think just generally about learning from past data so that could be generated by humans or it could be generated by your robot or something else and then next time we're going to start talking about fast or data efficient learning and that's going to be useful for doing homework 3 as well because the theory question for homework 3 is focused on data efficient learning right so we'll focus on that now so in particular for today we're going to discuss like we often do sort of thinking of separating things into a policy evaluation question and then a policy learning question um because we've seen repeatedly that if we think about can we evaluate how good a particular policy is that we can often combine that as a way to sort of bootstrap improving or policy optimization all right but I want to start with just a question um which is can we do better than meditation learning and of course this relates to the question I just asked you and the refresh your understanding so I'm just going to give up sort of an example um in my lab we often think about Education data or Healthcare data um or other cases where decisions are being generated by humans or automated systems where you might have say a series of patients you could think of this as medical record data um and each of those people are getting a series of interventions maybe it's some medication maybe it's a medical checkup maybe it's a vaccine and then we observe some sort of outcome and in imitation learning um we've saw the idea of saying well could we try to mimic the best human or could we try to mimic expert data and so an important question is whether or not we can go beyond that and we just thought about one example where we might be able to go beyond that but I think that there's a huge number of places we'd love to be able to go beyond the limits of sort of at least the average human performance um Healthcare is certainly one of them um in America we pay a lot for our Healthcare and we don't have particularly good outcomes compared to how much we are pay so you would hope that maybe we could learn through sort of reinforcement learning or others are there better sequences of decisions we could make in order to better assist say a new patient okay so I'll just give a a little bit of backstory of why I started thinking about this question so maybe about a decade ago um I was collaborating with zaran papovich and his lab and my grad students he's at University of Washington and he had this game called refraction and fra refraction helps teach kids about fractions um it's one of the concepts kids typically find really challenging um uh when they start to learn math and so in it you have a spaceship and you're trying to fuel a spaceship by splitting laser beams um in certain ways so that you create fractions or subp parts of laser beams to fuel spaceships and um to sort of save the save the agents and in this case um so I think roughly around maybe 500,000 kids have played this game and what we were thinking about is how could we customize it to make it more personalized and adaptive to students so in particular there are all these different sort of game activities and game levels and we wanted to understand how could we use sort of information about how the student was working in one of the activities to adaptively select which next activity to do so this is a decision policy and you can imagine conditioning on all sorts of State features so State features could be like how long they took but it also could be things like we did they put down laser bams or what series of mistakes did they make you imagine it could be generally a really really rich context or state space and then there were lots of different next levels we could do okay so that was the question we were interested in um and in particular in this case we had access to about 11,000 Learners who had been giving activities in a a random order now that was because there was a human um designer who had designed a specific sequence through the game but we weren't sure if that was actually optimal or close to Optimal um and what we wanted to do is to see whether or not we could find using reinforcement learning an Adaptive policy to help students persist at the game for longer so this game was offered on something called brain po um which some of you guys might have seen before it offers lots of educational games uh for kids and a lot of kids use it for a little while and then they stop so it's an optional game and we had some evidence that suggested that if kids played the game they were likely to learn things but if they don't play the game they are not so we wanted to think about increasing uh student persistence in terms of like the number of levels and so we really wanted to go beyond expert performance in this case like beyond what the experts had done and so what we did is we used reinforcement learning and we wanted to see if we could outperform essentially Behavior cloning and to give a spoiler of sort of the types of ideas we're going to see today in this case we found we could learn a policy that increased persistance by about 30% and so that suggests that in some of the domains there may be essentially be enough um data and evidence to find new decision policies that are substantially better than what is being currently done and so that's what inspires me and my lab a lot is to think about where can we use natural variation in the decisions that are being made or you know past experiments that were run in order to find substantially better decision policies that are currently being used yeah not a super relevant question to the subject matter but just how to curiosity was it 30% distributed uh uniformly or was it just like the people who already played played longer were the ones that um stopped early would actually continue this is a great question so this is a really big challenge often is whether or not um who are you moving inside of this distribution so this is just an expectation like most of what we've been doing for k n and others we did not analyze that too much in this case um in another much more recent paper we have which I think came out in January or something we did a exactly that an an analysis to try to see who was actually impacted and there we're really excited that it was the lowest performers that were most impacted um and that was exciting because one of the big concerns is that a lot of these systems are just increasing the inequity Gap um and this is particularly a problem in kind of these optional ones because it's normally the kids that are furthest ahead that have the highest usage so a great question but it also sort of raises in terms of on the technical side these questions around understanding um sort of iting estimates for sub parts of the population and doing heterogeneous treatment effect analysis to figure out which groupings of contexts have different forms of Q values yeah ask in terms of the policy what was sort of being changed like the simplest level is it like the difficulty of the fractions is it how hard it goes as they're going up or yeah it's a great question so in this case I can't remember what the final exact policy was they using but the type of things that we're varying in this case is things are on the fractions so like changing the numbers as well as different things of sort of how tricky it is graphically to do that um so there was a couple different things that we could manipulate as well as you can see just like visually here these look quite different so one thing that we found in some other work in um a game called Battleship number line which I was excited because recently my son uses Brain Pop and it just popped up and I was like we worked on that so that was exciting um in in Battleship number line which is another thing to do with fractions um we found there that variability was incred incredibly important for persistence and so just changing sort of how things look in that case how big the battleships were um also makes a very big difference to persistence and engagement I think that's actually an interesting question too in terms of including us um the history and the state features to try to you know um capture stuff like people caring about variability so great questions um we'll we'll talk a little bit more about this example later to talk about some things that we tried or that didn't work in this domain but this is just to highlight that I guess we shouldn't set our expectations too low so I think that imitation learning is amazing and of course if you're trying to imitate imitate the best surgeons in the world that's incredible but there are many cases where we think we can go beyond Human Performance particularly in cases where kind of like our high level principles don't inform what we should do at a more micro level so for example here we might have general princi principles of learning science but it doesn't say you know which activity to exactly do when and that's where being data driven can be really helpful okay let me give you another example another place thing that we think about a lot is Healthcare we've collaborated a lot with finale joshu valz at Harvard and her lab um this is an example thinking about um hypotension and trying to optimize different policies for that uh there's a really amazing data set called mimic that comes out of I think MIT and MGH Mass General Hospital which has lots and lots of electronic medical record systems and so what um these guys did in this particular paper is to look at Behavior policy so that's this Flatline and to see if they could learn policies using a method called popcorn that they thought would be much better and again here you know the results depend on the method and some of the hyperparameters they're looking at but the important thing just to notice here is that a number of these policies are substantially better than Baseline suggesting again that there may be domains where we can Leverage The intrinsic variability in the data and identify things that are working much more successfully in the systematic way so when we think about doing this generally I would call this sort of offline or batch or counterfactual RL and it's counterfactual because what we're trying to do is to estimate or learn policies that don't exist in the actual data collection strategy so we have the setting now rule assume um like in imitation learning that we have a data set of end trajectories so we're going to assume now we're going back to the standard mdp setting um and it's not pairwise preferences we're just back to having sequences of states and actions and Rewards okay all right so in particular we may have things like this where we have data from one policy and data from another policy and we want to think about how we can learn from that thinking about the state distribution of what's actually best okay now I'll just highlight here two reasons why this is hard um so we're always trying to estimate a counterfactual here over what might have happened for that wasn't tried so in this case we we don't know for this patient group what would have happened if we gave them that treatment or vice versa so just a reminder this is the fundamental problem of sort of um causal inference and this is going to be a big challenge for us here particularly when we try to go beyond the performance of the policy we saw in the past okay um so data is censored and of course in general again we're going to need generalization because we don't want to have to enumerate all the possible policies okay and I do just want to High here that in addition to education Healthcare you know if you want to think about climate change or many other areas there's just a huge number of scenarios including robotics because it's often really expensive to do robotics experiments where these types of ideas are helpful okay now one thing you might be wondering about is when I'm talking about this and I'm talking about trying to understand the performance of a new decision policy that was not used to gather the data you might start to think back to Q learning there's a lot of work on off policy reinforcement learning from really you know the very beginning of reinforcement learning so you might say why don't we already have the tools that we need to try to tackle this problem of learning better policies and in fact as we saw you know chat GPT if we learn a reward function and we do po that is do off policy learning so so that's one example so why can't we do this why can't we just do Q learning or some of the other methods we've seen um one thing to remember is a little while ago I said sometimes we have this deadly Triad of bootstrapping function approximation and off policy learning that sometimes when we combine all three of these things things can fail and that was part of the motivation for PO is that we don't want to go too far from the distribution let me just talk a little bit about what can happen here in the context of Q learning sort of model free learning so this is um the bcq um Behavior constrained Q learning from Scott pu jimoto um and what this shows here is that so these are a bunch of different methods this is DQ deep Q learning um uh this is behavior cloning uh this is the behavioral policy so what they did is they gathered some data um and then they tried to use different methods to learn a policy from it and um this is D ddpg and what they found with this is that some of the methods did really bad even given the behavior data dqn does about the same as the behavior data but what they found here is that by being a bit more careful and using methods that were explicitly designed to handle this offline data in this case bcq behavior um constraint Q learning they could do substantially better and so that suggests that we don't probably just want to use if we know our data is fixed and we know we're not going to get additional data that it may be worth it for us to use different types of algorithms in order to handle the fact that our data is constrained and we're not going to be continuing to get fresh data so that that so motivates why we're going to need new methods all right so now what we're going to do is dive into policy evaluation and then we'll talk about policy optimization afterwards okay so in Pol batch policy evaluation what we're going to be thinking about is we have a particular policy of interest and we have a data set and we'd like to be able to use that data set to estimate how good that policy is for one state or on average over a set of starting States okay so similar to what we've seen for policy evaluation before okay one thing I want to highlight um this is by Phil Thomas who I had the privilege of having as my postto a few years ago he is a um a professor UMass Amherst we generally want to think about sample efficient methods for doing this so in this case he was working with Adobe and they have you know 10 million to 20 million trajectories doesn't matter too much what you what these lines are the key thing here is that um this is sort of the behavior policy and you want to be learning policies which you're confident are better than your behavior policy and this is just to highlight that depending on the methods you use you may be confident at very different points so just meaning that data efficiency and having good algorithm is going to matter a lot Yeah by Behavior you mean the policy that was observed in the current data set exactly Behavior policy great great clarification question when I say Behavior policy today what I mean is the policy that was used to gather the data set that you have so I'll just write that out on here so this is so we're going to assume Behavior policy is the one that was used to gather your data okay all right let's first think about using models so this is actually the first thing we tried to do with refraction we thought okay great um Travis mandal who is the grad student leading the project we have all this historical data let's just try to learn models from it um so we're going to look at we're going to cons you know represent the state space in some way um and then under different actions here in this case that's just different levels and there's only a finite number of levels and activities let's learn a Dynamics model so in this case the idea is that we have that existing data set and we're going to learn an explicit Dynamics model and we can learn explicit reward model now in our case our reward model was known because it's persistence so we could essentially get a reward every time the student did didn't quit the game but we didn't know the Dynamics model and so that's what we're using the data to learn now as you might imagine we had to make a lot of choices here about what state representations we would use and so we lot thought about lots and lots and lots of different state representations okay but um once you have that then you can treat this as a simulator so now you have your simulator of the world because you have a Dynamics mod reward model either you can do this analytically like in some of the methods we saw in one some of the first few classes um or you can use dynamic programming or um Q learning uh Q evaluation with those um to explicitly learn what the value is but really you can use anything you can even use like Monte Carlo methods because um you can try to learn from this simulator um an optimal policy so you can either try to learn an optimal policy or you can evaluate a specific one you can do either of those so I'll just write that here so you can either evaluation or learn a new policy with any other oral method because now you have a simulator okay let me show you what happens all right so the first thing I'm going to show you is the following what we have on the x-axis here is different state representations of this environment so these are obviously really small state spaces like we don't actually think that human learning is encapsulated in terms of five states or 10 states but you can just imagine sweeping this right like so these are some of the state spaces we considered where we use really really condensed State spaces or much more complicated ones what this show is showing here is normalized score and this is log likelihood and this is um held out so what this is saying is as you might expect as you increase your sort of State space complexity you get a better fit on the data you can better predict predict the next state of the student if you use a more complex State space and that's not totally surprising right because we think that human learning is complicated and so we really think we are getting a better Dynamics model and again just to emphasize here this is cross validation so this is on a held out set it's is not training error okay so so we're doing better in terms of this now what are we doing with these once we have these oh yeah go ahead number yes yeah so the data set is fixed here what we're trying to do is given the data that you've seen before like there are all different ways we just have clickstream data there's tons of ways to model that as state spaces we're just doing model selection now what we were doing here then is once we had that simulator we were trying to learn a good policy um and then we were evaluating the performance of that actual policy now I'll tell you how we actually evaluated that policy shortly but this is the important thing so this is Shing that the models that we're getting are actually better okay but here's the problem if I take this policy which really or if I take this model which really is a better model it really does fit the data better and then I do say dynamic programming with it and I extract an optimal piie star so that's the procedure I take my model I learn an optimal policy and now I want to know how good that actually is in the real world if I evaluate that in the real world even though the model itself was actually better what you can see is the actual value of that policy is is getting worse okay so I've got a better simulator but the policy I get by optimizing for that better simulator is worse okay so this is the actual unbiased reward estimator and I'll tell you shortly how we do that because of course under the model's opinion the model thinks it's you know the policy it's helping produce is great let me just make sure that sort of the pipeline of what we're doing there is clear so what we do is we are getting we're going from data to a model of the Dynamics model and then we add in a reward function and we extract a p star for that estimated Dynamics model but that's just under the simulator and then what I want to know actually is what the true value is of that policy I've computed okay and what this Mo this graph is showing is is that even though under my model is getting better the actual performance of the value I'm getting out is is getting worse now when we first saw this we were kind of confused we weren't quite sure why this was happening and in fact there had been some work um a few years prior to this in sort of the educational data mining community that suggested doing exactly what we were doing here which was build a model then use it to simulate and learn a good policy and then deploy the policy that looked best okay but what are what our work here suggested is that was not a good idea now the reason for that is because the model is misspecified now that means that under this model missp specification the value it's getting when it computes the optimal policy so when I get so you can think of there being two things here there is one thing which is V hat of Pi hat star which is its own estimate of how good its value is and then there is the true value of it and these in general are going to be different and these in particular are going to be different if your estimated model is bad so it's going to think I'm doing great this is going to you know help students persist till the Rend s of time but if the model is misspecified meaning that even with infinite data it will not converge to the true model of student learning then that estimate will be wrong and as you might imagine here 20 State model of learning is not that great yeah you are improving St right yeah yeah so it's not saying that um it's not saying that some of these policies might not be good policies what this was arguing to us so it's a great question it's not that inside of these there might not be pretty decent policy classes um you know you could argue that Education Works because there's decentish policies I mean I don't have perfect models of all of you guys' learning um but they're still sufficient for us to be be a to learn and communicate what is AR what I'm arguing here is that we should not just use um the accuracy of the Dynamics model as um uh like a proxy for which of the values or which of the policies to pick this is arguing that we need separate independent estimates of really we want to basically in some ways you know kind of like what we saw with Po and policy learning we would like to directly evaluate the performance of a policy instead of using as a proxy sort of um how much our Q function is changing or how accurate we think our Dynamics model is yeah so when we evaluate policy we execute it it and there's a real environment or estimat the policy performance using our as so there's two things we either we can do it under our simulated model or we can do it under our real model we don't want to have to do it under a real model because we want to know which policy to deploy before we actually deploy it otherwise we could kind of be doing online RL so what I'll shortly be giving you is a way to get an accurate estimate of how good the policy is before we deploy it I haven't said how to do that yet I've just argued that using models alone might not be good do your problem yeah so about model Miss specification uh is one way to think about this just like you're kind of overfitting um your Dynamics modeled by increasing number of states that you used to represented or but we're not it's here's a question go we're not overfitting here because it really is a better fit it's still just not a perfect fit in other ways my might say this is it's not realizable this is not the real model of student learning and under this that means that there's still essentially significant bias um when we do this learning now one thing I just want to note is like modelbased learning can still be helpful one thing that we may want to do in this case is explicitly build different different models when we know we want to evaluate different policies so normally when we fit a model we try to minimize the loss under the data distribution of the behavior policy so like if you have a bunch of data and you fit your Dynamics model you're essentially trying to optimize for the accuracy over your behavior policy but if you know that the policy you want to evaluate is different you can actually change ENT you can weigh your um your errors separately so this is a paper that we did a few years ago with um finel Dashi and um yalo and gotsman and others which just highlighted this that you could change your loss function and essentially upwe your accuracy over the state and action pairs that you think you will encounter under a different policy and that can help a lot so what you can see here is this was for a medical domain and um what you can see is that this green here is ground truth and what we found in this case so ours was our model here and um this is just a you fit for the behavior policy and what you can see is by essentially reeing your data you can fit Dynamics models that that much better fit the type of um Dynamics you'd see in the future okay but now I'm going to introduce um sort of model-free methods and then we're going to get into important sampling there's other ways to try to do this policy evaluation that hopefully have different limitations or less limitations compared to the model based method so one of the first methods um that I'll talk about here is fitted Q evaluation so fitted Q evaluation is going to look pretty similar to deep Q learning but there's just going to be a couple important differences so our data set here is a bunch of just different tuples of State action reward next state recall that our Q function Q Pi is just going to be the immediate reward we got from being in that SI tupple so whatever we saw in our data set and then we'll put in plus gamma * V Pi of Si + 1 and then what we do is we try to minimize the difference between this under a parameterized function just like what we saw with deep Q learning versus The observed data tles so you can think of this as our Target and this is called fitted Q evaluation it's closely related to something called fqi which is fitted Q iteration which I think was around 2015 [Music] 2005ish um and so this is very similar to what we've seen with uh deep Q learning before we just fit this function the key thing here is that we want it to be for just a single policy Pi so we're not doing an argx okay so this is how the algorithm works for fitted Q evaluation we sort of initialize our Q function randomly it could be a deep Q you know a deep Q Network it could be something else we compute the targets where when we put in the next q we have to use the policy we're interested in evaluating so we're we're only doing this for the actions we would take under the policy we care about we build our training set of sort of x's actions and our output Q Target and then we fit our Q function and so again the key difference here compared to dqn is there's no Max we are fixing this part only for a fixed P but aside from that it should look really similar okay and so one of the so this was something that was very closely related to a common algorithm for doing off policy learning which is fitted Q iteration EXC me which is very related to deep Q learning and one of the things people wanted to understand is whether this thing that was working in practice um actually had some theoretical grounding behind it like could we say anything formal about how good this approach was so just to give you an illustration of the types of guarantees that we can get in this case what we want to look at in this situation um is to think about sort of what is the generalization error okay let me put this in here okay so I won't go through the whole paper I just want to give you an illustration of the types of guarantees that you might get in this setting what they would like to know in this case is to compare the difference between the value that you will compute under this procedure versus the true value of the policy this is your normal discount factor and then there's a whole bunch of things that are additional let me highlight some important things here n here is the number of samples you need so n tells you about how much data you're going to need need in order to do this so this is sort of how much data much data okay Epsilon is um sort of your desired Target act accuracy okay this is one of the really important things so we're going to have something called a concentr ability Co coefficient concentrate ability coefficient is going to be the difference essentially between the distribution of State action pairs that you have in your data set and the distribution of State action pairs you would get under your desired policy so we saw this before with po of thinking about sort of these Divergence in the state action distributions State action distributions okay and it's also related to what we'll call overlap later so I won't go through all the details in this case but I want to just give an illustration that people often think about trying to understand if you have a data set of some Behavior data how accurate you can hope to be of evaluating the performance and of a policy depends on your discount Factor because that says sort of how accurate you want to be and how much you care about long-term rewards how much data you have in terms of your target error and how closely related your state action distributions are from your training set to your test set or your desired policy okay now one of the challenges about this approach is that it generally still relies on the markof Assumption so we're still assuming our data is all Markoff and it relies on our models in these case the sort of Q functions being wellp specified so what do I mean by that it means that we really can fit the Q function like there's some existing Q function in the world for our policy and we can really fit it and if you say for example let's say that this is your state space it's just one dimensional and this is what like your true function looks like you know you could imagine that look something like this and let's say that you are restricting yourself to fit a line like that with just two parameters so in that case even if you had infinite amounts of data you're still going to have a lot of error you're not going to be able to fit the Q function so these methods assume typically realizability that if you had infinite data you could fit the function the problem is that you don't have infinite data okay all right so now we're going to see a really beautiful method called important sampling which allows us to deal with this we've seen sort of brief ideas about this before but I'm curious if anybody who's seen this in other classes who's seen important sampling before okay so just a couple people this is one of the favorite ideas in cs234 According to some past people um all right so the idea what is the motivation so important sampling is an idea from statistics um that we have imported over into reinforcement learning why would we like to do this well we want a method that doesn't rely on the models being correct meaning that we can actually you know that fit things with a two- layer you know uh deep neural network or stuff and that we don't have to rely on the markof Assumption in the state space we're using we saw before that we could use Monte Carlo methods to accomplish this for online policy evaluation and now we want to do this for offline data meaning that we have data from a different distribution from the policy we want to evaluate and the key challenge as has often been is data distribution mismatch okay so here's how important sampling works let me just specify what this means let's say we want to try to understand the expected reward over a distribution of States so for this part you can just think of X is equal to States and R ofx is equal to the reward of EST State this works for very very general distributions but you could think of that here is just being rewards all right what we're going to do is the following this is what we would like to evaluate so you could think of this here is maybe being P of X could be equal to the probability of reaching X under policy so you might really want this you might want to know what is the expected reward I'm going to get under this policy where I know what my reward is for each day or I have samples of it and then I have this probability distribution the problem is that you don't have data from that so we but no data from P of X so that's the the general challenge we're in we want to see how well like our alternative policy would work for like helping students persist but we have no data from that so here's the trick let's multiply and divide by the same thing I'm going to introduce a new policy and its distribution Q okay so Q of X is a different policy this is a different policy maybe it's going to end up in different states with different probabilities okay so let's rewrite this this is going to be equal to Q of X time P of X over R over Q ofx RX okay right I haven't changed anything yet this is exactly equal but if I have data from Q ofx I can approximate this expectation with samples so this is approximately equal to 1 n sum over I = 1 to n of X sampled according to Q of x p of x i q ofx i this is super beautiful what we've de said here is that I really want to estimate the expectation of something over say policy this policy P I don't have any samples from P what I can do is I can just take samples from my policy q and I can reway them so it says if I was you know really likely to take to reach a particular X under policy q but less likely under this one I'll weigh that data less if I'm much more likely to get to a state XI than I was under here I'm going to upweight those samples so this is beautiful and it's unbiased so this is an unbiased estimate and we'll extend it in a second to think about multi-time steps but just for single time step right now this is how we can do this gives us an unbiased estimate and as we'll see shortly we can extend this to multi-time steps and we don't have to make a Markoff assumption right so this is a really lovely idea so we can compute this expected value under an alternative distribution and it is generally an unbiased estimator under a couple assumptions the first is that the sampling distribution Q so like our alternative policy has to be greater than or equal to zero for all X such that P of X would be greater than zero what does that mean in practice that means that if you could reach a state under your policy you care about with a non-zero probability so let's say I don't know your student could get to this particular level with non-zero probability under your target policy then there has to be some probability you'd also get there under your um training data set this is sort of reasonable right so this says that like if I want to think about I don't know a policy that like um uh recommends restaurants versus coffee shops I can't use that data to then estimate how good it would be to go to the movies I just I've never done that there has to be for anything that we're trying to estimate here we have to have non-zero probability for that X the second thing is a little bit more subtle but comes up a lot in real empirical data which is called No hidden confounding and that means that essentially you have to know all of the features that were used to kind of um Define this distribution so this doesn't may not seem as clear in this part but I think once we start getting into multi steps and the sequences it becomes really relevant so let me give an example okay so imagine like a healthcare setting so if we go back to the electronic medical record setting um we often are interested in what would have happened to a patient if we did a different action so we want to know what that counterfactual is one of the challenges there is that we will have certain features that are in are in our electronic medical record system we will see an action like you know someone was taken to surgery or some drug was administered and then wece the outcome in order for important sampling to work all of the features that were used to make that decision or pick that action have to be known and that's called No hidden confounding now why is that well it might be for example you might see that there are certain patients that um that are sick and then a particular action is taken and maybe they die and you might see other patients that look like they have the same features and a different action is taken and they live and in that case you might think oh maybe the the decision was just bad that's possible but it's also possible that there are just hidden additional features that you don't have in your data and that meant that the first person was much more sick and that's why they got that particular treatment versus the other person so it might be that like you know that there's important reasons that are not part of X that are being using used to define what the action is in the data set excuse me and in those sorts of confounding scenarios then if you try to use important sampling you will not get an unbias estimator this is really important and really hard in practice it comes up all the time and in facted one of the things we were just doing on a paper we just put online we are trying to think really really carefully about whether or not there would be additional compounding beyond the the features that we had in our data set so in that case we had done an experiment to see whether or not offering students access to GPT for um would increase or decrease um participation in the class and exam scores and only some people used gp4 a lot of people that were given access to it did not use it and so an important question there then is well is there something intrinsically different about those students who are using it that also would confound their their test scores and so this issue of noan hidden confounding comes up a lot particularly when sort of actions are optional or being made by humans now if you're in sort of muok or something this is easy because if you have control over the simulator you don't have to worry about it but it's important to start in practice all right let's take a second and sort of check your understanding so we haven't really talked about Bandits yet um don't worry about exactly this we're going to be doing policy evaluation so let's say we have a data set for I'll just say samples for samples from three actions okay action one is a brand newly variable where with probability 0.02 you get a really high reward else zero the second one you get probability with probability 0.55 you get reward of two lse Z and the third one with probability 0.5 get a reward of one else zero your data is going to be sampled from a particular Behavior policy so this is our be what we've been calling a behavior policy where with probability point8 it pulls this action else it pulls action two the policy we want to evaluate Pi 2 pulls action two excuse me else it pulls action one this question asks you to think about what are true about the performance of those policies whether or not we could use the data from PI 1 to get an unbiased estimator of Pi 2 and whether or not the rewards being positive or negative might impact that for the third one is kind of hard and might require looking back at the equations on the previous slides water for a second and then all right why you turn to a neighbor and see what you got a couple layers I actually swwa P1 and P2 yeah three I did I did it in my head a bit so if you go yeah so we have two there so that's one plus whatever we P action one which would be two which should give us around but thenes us at the time yeah gives us and then otherwise you two US yeah so it wasn't as high in my small calculations and then yeah you never see thing [Music] yes okay so they have to be completely over all right let's go through this so the first one requires um a couple nested expectations so let's go through those and make sure I get my math right um okay so for the first one Pi one so there's two levels of stochasticity here we have a stochastic policy and we have sto we have rewards with stochastic actions so let's first um or stochastic rewards so let's first just figure out what the expected reward is for action one this is equal to 02 with reward 100 so that is 2 plus else you get a reward of zero so the expected reward for Action A1 is two I'll just write that as here expector reward for Action A1 we can do the same calculation here so the expected reward for A2 is just going to be equal to 0.55 * 2 which is just equal to 1.1 and the expected reward for A3 just equal to .5 okay so generally policies that P more weight on action one are going to be better now let's see look at what the expected value is of Pi 1 so what pi 1 is is it's going to say with probability 8 we're going to get the reward of A3 plus 2 we get the reward of A2 the reward of A3 is .5 so it's 8 * .5 + 2 * 1.1 so that's about how much that one is so this is like approximately yeah okay so we'll we'll um this one is like approximately like. 2ish I'll double check the exact path it's roughly like that okay now let's do it for um so this is the reward for for pi 1 we'll do the same thing for pi 2 this says with 0. five it pulls gets the reward the expected reward for R of A2 +0.5 it gets the reward of A1 so that's equal to.5 * 1.1 +5 * 2 so it's approximately equal to 1.5 is I think I was off by two when I was chatting to some people before um yeah I I I got 6 for first one 65 for the second one I do my ma um I think this is I think it's going to be more than that because the expector reward for this one is two and so this one has to be two okay I thought I thought it's two yeah yeah so I think this s so I think this ends up being roughly 1.5 I can double check my math but I think that's right okay so Pi 2 does have true higher rewards so this is true um the second is we can't use Pi 1 to get an unbiased estimate of Pi 2 why is that so this is true also why can't we use Pi 1 data from PI 1 because it never pulls it never does action one that's right so it never does action one so it's like saying um you have data about all these restaurants and then you ask it okay I also have a policy that's not going to go to this new restaurant and you have no data from that so we can't get an unbiased estimate of the average award okay this one's hard um uh this is false okay it turns out that um uh you can still get an unbiased you stand you can still get a lower bound on the performance of a policy using another policy which doesn't have complete overlap if the rewards are strictly po positive so if the rewards are always greater than equal zero you can do this why is this so we have a paper on this from a few years ago now um just for the for why this happens essentially you can think of it is if you're Behavior policy doesn't include some of the actions that you want to evaluate it's like putting zero Mass on those okay because if you think back to what is happening here it's like you never sample them right so you have zero probability Mass on some things that you want to evaluate like you want to include a policy that sometimes recommends movies and you never do so it's like putting zero Mass on that if all your reward is positive that's essentially just lowering your estimated value okay so it turns out that if all your rewards are positive you can use a behavior policy that doesn't have complete coverage with your target policy but it'll be a lower bound the reason why that might be useful is because if it's still the case that your target new evaluation policy is better than your behavior policy even though it might not have full coverage you may still want to use it so it's like oh it doesn't matter whether those recommendations it makes for like those new movies is good or not it's already a better policy so we can do that okay great all right so it turns out that we can also do this for RL policy evaluation so I just showed you a much more simple setting of this um and I'll highlight too here that import sampling like many things in stats and math Etc um goes by many different names you'll often see things like inverse propensity waiting so if you take econ classes people often refer to these things more as like ipws or inverse propensity waiting um when I learned about them I learned about them is important sampling often also depends whether you're using these to design ways to gather data or whether you have historical data okay let's see how we can do this for reinforcement learning so in reinforcement learning we can do exactly the same thing so I have what I want to have these are now my trajectories and as we've seen before we can think of the value of a policy is just being an expectation over all the trajectories that could be generated by that policy from initial start State times the reward of those trajectories so this is the reward of a trajectory tow this is the probability of a trajectory under the the desired policy so what we can do in this case is the following we can just multiply and divide by the same thing like what we saw before so we're going to imagine that we have data from a different policy so I'm going to call this pi b so we've now introduce my behavior policy okay so I'm just going to rewrite that so I again just have this weight this is just reweighing what's the probability of me getting a particular trajectory under my behavior policy versus my target policy okay so we have that here and let's let me write it out here first okay so now we know let me put this from here we know from before that if we have samples from our Behavior policy we can approximate this expectation by a sampled expectation and we reweight these the next thing is to make sure that we can compute what's the probability of a trajectory under our Target policy versus our evaluation policy and we've seen things like this before so just remember what we can do in this case is that the probability of a trajectory given a policy and action is equal to the product over = 1 to the length of the trajectory the probability the transition probability I'm just going to write it as a deterministic policy for Simplicity deterministic for Simplicity but you can extend all of this times the probability that you would take that action s oops actually yeah I'll rewrite that you don't want it I don't want it to be deterministic that will be misleading okay put here okay all right so this is just the probability of us taking the action given the state under our policy and the transition probability for every single time step the nice thing that we can see in this case we can write that out for both the behavior policy and the um the target policy and as we've seen in some other cases this will cancel so you don't need to know the Dynamics model so this is beautiful and Incredibly helpful under similar conditions to what we just saw as long as you have coverage which which means that you will visit the same sort of trajectories maybe with differing probabilities all you have to do is reweight them so they look more like um the policy that you want to evaluate and we assume that you know this cuz this is just the um your policy probability just says what action would you take in this state and so this is known if you're doing policy evaluation okay so it's first introduced for RL to my knowledge by doing a preup Richard Sutton and um sender Singh in 2000 and there's been a lot of follow-up work in Leverage of this it's super helpful we don't need the markof Assumption or anything okay requires very mle assumptions it's unbiased um and it corrects for distribution mismatch so extremely helpful I won't do this now but you might want to look through this later just to think about given everything you know about um Monte Carlo methods Etc like what might be some of the limitations of doing this I'll just briefly say there's been a whole bunch of extensions one thing is called per decision important sampling similar to a policy gradient we think about the fact that um in terms of the reward and the decisions later decisions that are made can't affect earlier rewards so you can reduce the variance by being a little bit more um strategic and where you put your weights and we saw similar ideas to this in policy gradient so this is called the per decision important sampling and it it helps to have Better Properties uh in terms of long particularly for long sequences in general the variance is pretty high like for most Monte Carlo methods um I one thing to know is there's concentration inequalities like the hting inequality that you can use that will generally scale with the largest range of the variable if you want to start to get confidence intervals over these values um and this can start to be pretty terrible for long Horizons for important sampling so I'll post afterwards what the solutions are for both of these check your understandings but it's pretty informative to think about exactly how bad this can become okay there's a to deal with this there's a lot of different extensions um one thing is that you can if you do have Markoff structure you can think about State distributions instead of trajectories and that can be very helpful and there's been a bunch of work in that direction One work that we've done and others is called W is taking ideas from statistics on W robust estimation and using these um to reduce the variance um in these methods as well as trying to blend between methods that make a Markoff assumption and methods that don't right I want to finish Now by talking a bit about how we can use these ideas and others to think about offline policy learning so I think there's sort of a couple important ideas we went through so far today one is that you can just build a simulator from historical data and you can use that to learn but it may be biased and that bias um may be substantial when you're trying to use it to pick policies we can do model-free methods um uh but we're going to want to be careful about those and we're going to see more of that later and you can use important sampling to get an unbiased estimate but it might be high variance not going to think about those sorts of ideas in the context of when we're actually trying to pick a policy and do optimization so I'm going to go back to this issue of coverage because I think it's important to emphasize so let's imagine that you have antibiotics mechanical ventilator and a vasopressor all things that are often used in um uh sort of an intensive care unit and you might have different probabilities of these interventions and let's say you want to evaluate a policy that frequently does mechanical ventilation as we've been talking about your data has to support the policy you want to evaluate so if this is your behavior policy that works because every single action you want to try you have a non-zero probability of trying that in the data if you have this policy that doesn't work that's the same as the example that we saw before so if you never use a vasopressor in your behavior data you cannot evaluate how good that would be in the future now when I draw it like this or in the example that I gave in the check your understanding it's pretty obvious because there's a finite number of actions and it's pretty clear like if we didn't take action the vpress our action that we can't evaluate it but in real data sets it often gets really hard to understand what does it mean to have sort of sufficient coverage so in general this is going to be hard cuz we're going to want to say well you know is it okay if it's zero I definitely can't do it but if it was like right here is that sufficient you know if like one in a million times I use a vasopressor is that going to be okay does it have to be in my actual data set or does it just have to be there was a chance of me doing this um so all these issues are kind of exactly how much data support you need come up all right so up to around 2020 um most of the methods for join off policy evaluation kind of model based or model free assumed overlap so so um if you're doing off policy estimation it means for your policy of interest but for off policy optimization it often assumed all policies so every single policy you could imagine in your domain had to have coverage with your behavior policy now if your behavior policy is random that's fine but if your behavior policy is say like how Physicians operate or how teachers operate or some sort of you know policy that's not completely random that wouldn't always be be um satisfied and in general many many real data sets don't involve complete random exploration and this means if you assume this and use these methods and it's not true then you might end up sort of going into parts of the domain or ending up taking policies that go into parts of the domain where you have very little coverage so I'm going to introduce an idea and it turns out this idea um there was a number of groups that all started thinking about this at the same time and I'll cite a few others of them in a minute um we s we call that's doing the best with what you got so the idea was how can we leverage data sets where we only have partial coverage like we still want to do as well as we can but within the support of the data and this is similar to kind of the K constraint or po clipping that we've seen before but this is all going to be entirely in the offline case where we don't manage to get any additional data and the key idea that we're going to think about here is just being pessimistic so that when we don't think we have sufficient coverage or we or we have high uncertainty over what the reward might be in a particular state or action we want to be pessimistic with respect to that uncertainty I want to highlight that just even when our paper came out here there was sort of increasing interest in offline RL but what we noted is that like there was still quite a few challenges and I just want to illustrate that with a really simple example so this is known as the chain mdp and we might talk about it more when we talk about data efficient exploration um this this this is not exactly the same as all the chain mdps but there's number of them and they're used to illustrate the hardness of sort of learning good policies so the idea in this setting is you have an initial start State S1 and then or sorry s0 and then under one policy mu you have um a probability of going to S1 S2 Etc and then also with another probability you have a probability of transitioning to S10 it's a really really small mdp just a very small number of states the important thing to note here is that all of these states have deterministic reward except for this one so in reality this has an expected reward of 08 you always get 08 when you get to that state and this one has an expected reward of 0.5 so it's a worse state but if you go there because of stochasticity some of the time you'll get a one there which means when you have finite data you might think that state S9 is better than S10 that will just happen with your data so what we could show in this case is that's a bunch of the other algorithms for doing conservative hrl and I won't go through all of them but happy to talk to them offline have this weird Behavior where as you this is your behavior data set so as you increase the amount of your behavior data you would hope in general that you get a better better estimate of a new policy and you get actually better and better performance this is the success rate and we found we had this weird Behavior where for a lot of the other algorithms they would start off and they would learn the optimal policy for this domain which is to go to S10 but then as you got more data from your behavior data set they would get misled because sometime you would have seen S9 and it would have given given you a one and if it saw that it would say oh no I don't want to go to S10 I want to go to S9 instead so with intermediate amounts of data these other methods would get confused and learned a bad policy and it was only as you started to get a lot more data that they would end up getting back and realizing what the best policy was and so that was somewhat concerning right because you would generally hope that you get some sort of monotonic improvement as you get more and more data from your behavior data set but here we were seeing that some of the previous methods had this sort of unfortunate behavior and it turned out it it didn't just happen in like for these particular examples but like we could show some other types of examples where we got very similar types of Performance challenges for other methods so the key idea is pretty simple which is just be pessimistic if you haven't seen a state action very much so we defined a filtration function which is just a simple threshold that says let me check for this dat in action pair it's kind of what my density is how much I've seen it if it's greater than a threshold this is going to be one okay so what this threshold is doing is trying to account for your statistical uncertainty you have if you have finite amounts of data so if you haven't seen things very much this is going to become zero if you've seen things a lot it's going to be one that's all we're doing and then we can just combine this with like Bellman backups so just like sort of your dqn or Bellman operator we can just apply it so that when we are looking at your reward plus gamma times your expected discounted sum of rewards we look at the States you might get into and if those States we don't have very much data for then this whole thing becomes zero and so it's like saying if I transition to a next state for which I don't have much data I just pretend its reward is zero and then I back up from there which essentially means I don't want to take actions that transition to States for which I don't have enough data just pessimistic and it's going to um uh be a lower bound if your rewards are all bounded by zero it's just going to be a lower bound on your potential reward okay so since we assume that our rewards are all positive and you can always just shift them this is going to become a pessimistic estimate for all of those doues and you can do this for either policy evaluation or for um so you can use this in kind of like policy gradient type approaches or for um Q learning type of methods and it turns out this helps a lot so let me just we call this marginalized Behavior supported policy optimization um I'll just highlight what was because one of the key things of this paper was the theory that we showed with it as I said a lot of the previous methods sort of had to make assumptions over coverage that like your data covered any possible policy that you might want to evaluate and under that you could ensure that your the policy that you learn is close to Optimal ours does not make that guarantee it only says let's think about all policies that we could reasonably evaluate that have sort of enough coverage we are guaranteed to find the best policy within that class and we have some um I'll skip through this now due to time um but under some assumptions then we can also give these kind of finite sample guarantees similar to what we saw for the fitted Q evaluation okay all right and I'll just highlight that those do include the function approximation so these aren't for tabular okay so this is what's pretty cool to see so this in this case this is Hopper um this is the behavior policy this is the behavior policy used to gather the data what you can see here is if you use ddpg um that actually does worse than the behavior policy if you use Behavior cloning it was um a little bit better about the same used a particular vae we then compared to bcq which I mentioned uh briefly before uh Scott fujimoto's work and you can see that that and our approach in green both do substantially better again highlighted in some of these cases the data does support you learning a much better policy and you should do you should try to uncover that by using these methods that sort of explicitly think about your uncertainty now um I'll skip this just due to time there's some interesting sort of theoretical reasons why model base might be even better at the same time um there were three papers that all came out but nurs ours was one of them the same year with all basically very related ideas ours was a model free based approach and work by some of my colleagues uh Chelsea Finn and uh t Yuma and others um learned a model- based approach where they penalized model uncertainty during planning and they had some very nice results in D4L cases ours was a bit more theoretical and model-free theirs was a little more algorithmic um and empirical and also had some really nice um and was focused on model based approaches I'll just highlight that um another method that came out similarly around the same time was conservative Q learning and that has also continued to be very popular since so that's another another way to think about sort of being conservative we almost out of time so I just wanted to do sort of share kind of how do these different approaches compare pessimistic approaches in general um do better than Alternatives all of these have some form of um pessimism these are modelbased this is the be the sort of behavior constraint Q learning some nice work bear from um Sergey lavine's group from Berkeley and cql which is also from Berkeley um the different methods tend to do better or worse in different settings um I think that in general the key thing to understand from this part is that it really can be be beneficial to think explicitly about uncertainty and use that to sort of penalize and constrain your function to be in the parts of the domain where um you have support and again this is pretty similar should definitely make you think back to sort of Po um instead of having constrained updates so many of these different settings we're really trying to think explicitly about um sort of coverage and how far we can use the existing data we have but particularly here where we don't assume you don't get any additional data you're just going to deploy a policy at the end we want to think about uh you know exactly how much support we have okay all right I will skip the last part because we're going to be out of time if you're interested um just want to highlight that you can extend these ideas to think about there being constraints so we had a science paper a few years ago thinking about what if you want to make sure your performance is improving compared to baselines and in particular we used um like a diabetes insulin management simulator it's a really cool simulator is improved by the FDA to replace early stage animal trials and you can learn new ways to do insulin um delivery and what we wanted to illustrate in this case is that by thinking explicitly about your uncertainty over the performance of new decision policies you could quickly learn a policy that you were confident would be better than the existing policy so just highlight that to say that there are lots of cases where you'd like to do this offline policy learning but do so in a way where you have safety constraints or constraints over the performance all right let me just summarize this part so in terms of things that you should know or be able to do excuse me you should be able to Define and apply important sampling for policy policy evaluation and understand some of the limitations of these prior Works um you should understand why offline RL might be able to outperform imitation learning should know sort of this idea of pessimism under uncertainty and be able to have some application areas where you might want to be doing offline RL or offline policy evaluation so particularly in kind of high-risk settings that can be important what we'll be doing next is going to start to talk about how if we can gather our data how we should gather our data in order to really efficiently learn policy I'll see you on Wednesday