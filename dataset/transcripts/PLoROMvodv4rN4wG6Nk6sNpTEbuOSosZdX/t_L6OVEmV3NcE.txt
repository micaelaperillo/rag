hi everybody welcome back we're going to go ahead and get started with a refresh your understanding okay hopefully everyone had a chance to think about this a little bit more um so let's go through the answers the first one is true so if you are trying to evaluate the value of this is in the tabular case so this is where we're assuming we're going to sample each coule at random and we do a q learning update we do this an infinite amount of times um we know for standard tabular learning we can converge to the true value of a policy um uh under as long as our learning rate schedule as such so if there's an exists a learning rate schedule under if you're decaying your learning rate um at the right level then you will converge to the True Q value in the tabular case because there's no function approximation that's happening there in the second case this is also true so we talked a bit um about how we could think about doing these things in a batch way where we do it over and over and over again we take our existing data and we run it through our either TD learning update or our sarer update or other things um and we said that the TD learning updates if you do it in a batch way are equivalent to just taking um a certainty equivalent model which means you estimate the Dynamics model and you estimate the reward model excuse me um from your existing data and then you do dynamic programming so that's what we saw I think we saw that in lecture three this one is false does somebody want to say why it's false this one is not true there's a number of reasons why it could be false I I want to share why why is dqi not guaranteed to necessarily convert to the optimal Q function yeah I mean would you need to enforce a certain number of iterations to for it to have any chance of converging at all so good point so it related that so certainly if you don't do enough iterations but even if you do an infinite number of iterations it also might not be guaranteed to converge me tell me why you went infinite so right you certainly need a lot of iterations but even if you had a lot of interations you still still might not be guaranteed to converge I think here it helps to think about what we often call realizability which is we don't know what the functional form is of Q and so you could think of the fact that I'm GNA draw it in as if the state space was onedimensional but in general of course like the state space is like this Vector um or it's like you know images and so it's really high dimensional but imagine that it was onedimensional even here you don't know what what your V function or your Q function might look like and so if you are using the wrong approximator like if you are using say a line instead of a multi degree polinomial then no matter how much data you have you're not going to converge to the optimal key function or you what be guaranteed that you because just can't even realize it so in general um and there's all sorts of additional instability things that mean we can't be guaranteed it's going to converge so we're not guaranteed it'll converge but empirically it often does pretty well so we'll see we and if you look at the empirical results it often does really quite well great so so far in the class we've talked a lot about these value function based methods where we thought about having an explicit representation of the expected sum of discounted rewards starting in a state or starting in a particular state in action and so we talked a lot about value functions and Q functions um and now we're going to talk a lot about policy search and so we're going to still think about there being this policy which is a mapping of states to actions or a mapping from States and actions to a number between zero and one such that it sums to one we always have to do at least one action in every state but we don't necessarily have to have an explicit representation of the value function anymore so these have been very popular and important and if we think back to what ourl algorithms involve they involve optimization delayed consequences exploration and generalization and we've seen examples of the all of these ideas so far and we'll go a lot more into some of them as we go through the course but one thing you might be wondering about is you know could we play the trick that's often done in U computer science and try to reduce reinforcement learning to another problem so could we do something like just like online optimization so we know that we uh don't know how the world works and we're trying to find a good control policy but could we do something like sort of um online optimization where we're trying to search for a good policy and in this way you can give policy gradient being related at a high level to this type of idea it's not a reduction based approach but it's sort of thinking about sort of well can we just directly search to find a good policy and policy gr methods have been extremely influential particularly over the last 5 to 10 years so they're used for lots of areas they're used for things like you know sequence level training with recurrent roal networks that was based on reinforce which is an algorithm we're going to go through today um it has been used for things of end to-end training of deep Vis Muto policies so this was really influential work in the robotics Community about a decade ago I'm just going to show you a quick quick video of it so this is work that was done by Professor Chelsea Finn as part of her PhD thesis along with Sergey LaVine and others at Berkeley see if this will work with audio so what you can see there that what they're doing is what they're going to be trying to do is learn from like so they showed you a really they showed a big Network and what they're trying to go is go directly from pixels to learn what the robot should do and this is one of the first examples of people trying to do this directly from images let's just go back to some of the tasks that they're using and so that was part of the motivation too is that they want to be able to learn these tasks in way that will generalize so this is another example of sort of trying to do direct policy gradient methods um in order to go from like really um large complex State spaces into direct decisions uh now in homework two and we haven't covered po yet but you're going to be implementing proximal policy optimization which is one of the methods that build on the sort of methods that we're going to talk about today um and that was used PO was used as part of training chat GPT so as you can see all of these algorithms have become incredibly influential in part because they can scale really well to extremely complex inputs whether it be images or high dimensional robotic tasks or even things like natural language and so they're very powerful they're often used in conjunction with things like State action values as we'll talk about later but you don't have to use them with them so they're really useful sort of class of things to know about so in particular just like how last time we saw that you could approximate a state action value or a value function with a set of parameters so we can do function approximation um in those cases we thought of directly learning a value function or a state action value function and then it generate a policy from the state action value so something like EG greedy where we either take what the Q value suggests as the best action or weact randomly and what we're going to do today instead and I'll try to be careful about not using the same um we used W before to parameterize our state action values and I'm going to try to be careful about using Theta just to make it clear um we're going to directly parameterize the policy and we're going to try to learn parameterized policies so we can think of these as like you know deep convolutional neural networks which at the end will output um either an action or if we have an action as input we'll output a probability and the goal in this case as is normal is that we want to find a way to act in the world that will give us High reward so we want to find a policy with the highest value function V pi and we're again not going to be focusing on model based learning so we're so going to try to directly learn from experience we're not going to assume we have access or that we're explicitly building a model and I think one of the things that's helpful to think about is there's these sort of different views or lenses into reinforcement learning so this is a nice picture from David silver who's an amazing person in reinforcement learning he was you know one of the main leads on alphao and a number of other incredible papers so we can think of it is like you have some methods which are value based we're explicitly building a value function we have other ones that are policy based and the ones that are in the intersection are often known as actor critic methods who here has either implemented or heard of actor critic methods before okay so some people and underground they're extremely popular and in actor critic methods um you will often combine between the benefit it's a value based and policy and so for example alphago is a actor critic method in the sense that it is often having explicit representation of the policy and of a value function okay so we'll get to actor critic methods later today we're going to focus on policy based so now that we're going to most of the time we've thought about policies so far we thought about deterministic policies or egedy policies and now we're going to think much more generally about stochastic policies um and that's going to be important because as we saw last time if you only have a deterministic policy it's much harder to learn about actions you don't try whereas now we're going to think about having stochastic policies where you're going to be getting information about lots of different actions so let's think about a particular example also to kind of illustrate some of the things that policy gradiant methods are going to help us handle so who here has played rock paper scissors okay most people I think it's called Rambo in um Chinese it's a very popular game throughout the world it's a stochastic game where you know or what each side can pick um a particular strategy and the state you could think of there being a state you could keep track of what your opponent has done over time so think for a second about whether the um a deterministic policy can be optimal if you're playing this game repeatedly so raise your hand if a deterministic policy can be optimal raise your hand if you think a stochastic policy is optimal okay someone who said stochastic explain why yes two is like circular like there's no one one best one that that's right yeah so there's no best like there's nothing that strictly dominates all the other strategies and also if you're deterministic what can your opponent do like if I say I'm always going to pick um paper what does my opponent do they're always gonna pick like the other one like rock to to beat me so anything you do that's deterministic can be exploited by your opponent if you are playing repeatedly and so the optimal thing to do here is to be sarcastic the optimal policy has to be sarcastic here otherwise all deterministic policies are strictly dominated by good stochastic policy okay and now you might think well all right well that sounds different than what we've seen so far but one of the challenges here is the system is not Markoff um so it's not stochastic what your um adversary will play next they're not random or it might be if they're playing stochastic policy excuse me but in general the sto that they can react to What You've seen so far um and it's not just like you know a random environment like a coin flip on the next time and in this case actually a uniform random policy is optimal it's a Nash equilibrium right okay so that's one case where having a stochastic policy would be really helpful so you could just have a fixed stochastic policy and it would be optimal but you couldn't necessar write this down easily as a q function and just take the ARX there's not a deterministic policy for this environment that is optimal and so it's less clear how you would write that down directly in terms of a q function in part because the system do not mark off so here's another example where we might want to have stochastic policies and it's where we have alias or partial observability so imagine this case where like you have a robot that's walking along and you know maybe they have sensors so they can tell how far they are from the walls but under Those sensors these two gray boxes look identical because like from the agents point of view if they have only immediate sensors both of those places will look identical and so they can't distinguish those gray States and imagine that um you know you just because you have a feature representation that just tells you about what your what you have a wall to the north to the east to the South or to the West so those two gr States would look identical if that was your feature representation so you could have a value based reinforcement learning representation where use an approximate value function where you take in this as the state representation or you could have a policy based one that takes in those so the challenge here is that if you're value based you have to do the same thing in those two gra States because you you can't distinguish them so from your perspective it's like you're in the same place no matter which of those two places you're in so if you're going to do a value function based and then extract a deterministic policy you would either always have to go say to the left in those cases or always go to the right and neither of those would always be good okay so under alosine meaning that we don't know whether which of the two GR States we're in when we're in one of them ad opal policy will always move west in both States or east in both States and either way it might get stuck and never be able to reach the money but um and that's what's going to happen if we do a value based reinforcement learning approach so that's not great you're going to you know Traverse this for a long time you're not going to be getting high reward what could you do if you wanted to have a stochastic policy so that allows you to act R you know randomly or you know stochastically in any state what do you think would be the right thing to do in the gray States if you could have a stochastic policy uh with just some probability you go either East or yeah exactly so um so you could just randomize it so an optimal sastic policy will randomly move East or West in The Gray State because it doesn't know which one it's in and half the time that'll be the right thing to do so um so that means much more the time it'll go into here and it generally will reach the goal State pretty quickly so this is another case where the system is not markof this is notk off and the state features so because we have alosine meaning the system is partially observable is not a markup system one way to handle that is to treat it as a partially observable markof decision process Michael Cocker talks a lot about those in his classes um but an alternative is to use a stochastic policy and you can also act very well here so those are two examples of sort of the type of thing that might be able to EAS be easy to handle with policy gradient methods or stochastic policies that might be hard to um tackle with the type of methods we've seen so far okay so now we have to think about if we have policies and in general we're going to want them to be stochastic how are we going to learn what are good policies like we have this you know now we have a function space over policy and we want to learn which of them have good values so if we're in an episodic environment we can use the policy value at the start state so we can just say I'm going to similar to the Monte Carlo methods if I start in this state I run this policy what would be my expected reward be until the end of the episode we're going to mostly focus on the episodic case today um but you can extend these to sort of more of an infinite Horizon case all right so once we think of it in this way we can really think okay this sounds like an optimization problem so we really just want to find um the parameters that maximize the value so you could say here you can think of this as being your thetas so I'm just draw it one dimensional but in general this could be you know all the parameters in a deep neural network and then this is V of theta or of a particular starting state it might look like this and what your goal would be is to find the parameters of your policy that maximize the value function and so this is an optimization problem but it's a hard optimization problem because we don't have that function you can only estimate it through data and so you can imagine like you start off you have no idea how how Theta maps to be then you have to learn that over time so once we think of it as an optimization problem where we don't know what the function is there are a lot of different methods we could think about to try to solve this problem and what we're going to focus on today mostly is ones that are going to exploit something about the structure of sequential decision processes but there are methods that completely ignore kind of all of this um and in particular you can even use things that completely ignore gradients so you can do things like hill climbing or genetic algorithms or cross entropy methods where you may not think of sort of any of the type of structure of the parameter space and in some cases that can work really well so there's a really nice example um by my colleague uh step Collins who's over in the mechanical engineering department he does some really interesting work on yeah yes but you can it's a you can make it a distribution over actions so you can you can output something between zero and one you can have input action and so then you would have a stochastic policy out the netork you would then compute up for all the actions then you would have to have some yeah you'd have to pick a random number and then use that to select yeah yeah good question so my colleagu uh Stephen Collings over in mechanical engineering does a lot of work on exoskeletons there are lots of reasons exoskeletons could be really helpful particularly for people that have motor impairments but one of the challenges of them is that uh you have to them actually help people walk so um if you clamp something on to say your leg then the way my physical configuration is may not be the same as your physical configuration I'm pretty tall and so you'd really like to make sure that this helps each individual in the best way possible but you don't want it to have to learn over the course of like you know two years how to best optimize to someone because they're not going to wait that long to use it so what they did is they use policy methods policy search methods to quickly personalize the parameters of an exoskeleton um and they called this humanin the loop exoskeleton optimization so the idea in this case is what they're trying to figure out is what is the parameters of their exoskeleton and what they're looking at is essentially how much it helps you walk so how much it reduces the effort needed to walk and so what they could do in this case they're not using like a gradient bath method they're using just CES which is so they train is of continuous optimization is they'd have people walk under sort of a few different control parameters they would see which of those seem to be most effective and then they would move the policies they' try in that direction with some stochasticity and I think it was within maybe two or 3 hours using this they could find substantially better policies I think it increased metabolic efficiency like maybe by 20% or 30% it was pretty remarkable and so this was published in science about um seven eight years ago but that's another example of a place where you can do this sort of like online optimization you don't necess have to think about the temporal structure of the policy yeah start with a default policy and then try to improve that before great question yeah so in all of these cases we're going to have to assume that we initialize our policy parameterization in some way just like how we initialized our value function to zero to start now we're going to or if you had it for deep uh for the Deep Q Network it would be whatever your neur network parameters yeah great question now it's just useful to to know about these because um they often work pretty well so I think sometimes we like to leverage the structure specific to say our markof decision process but in some cases just leveraging these ones which may not use very much structure at all can actually do really well so it's just good to keep in mind that there are a lot of ways to do online optimization all right so this is often a great Baseline to try the great thing about this is it can work with any policy parameterizations even if it's not differentiable because it's not using gradients so it doesn't need to be differential and it's also often very easy to paralyze so CES for those of you who not who haven't seen it before um you'll have a number of different policies you kind of try in parallel and then you'll use that to sort of update and shift to another set of policies and that's what they did uh Professor Collings did and in a lot of cases the problems that we think about are places where you know you'll have many customers you'll have many many robot arms and so you can par things one of the limitations is that it's often less data efficient because it's ignoring the temporal structure so if if you have temporal structure or you have a gradient gradient information it may be more effective to use that all right so what we're going to focus on in this class is differentiable methods so we're going to focus on places where we can do stochastic gradient descent including on the policy parameterization so if we have like our policy parameterized by a deep neural network we can propagate through that and update it update those parameters so we're going to um we're going to focus here mostly on methods that do use gradient descent and that often Leverage The sequential structure of that we're making a sequence of decisions and we want to optimize to make those sequence of decisions so to do that we're going to explicitly Define the gradient and we're going to write down the value function in terms of as a function of the policy parameters so that we can be clear that this value function um you know relies on those policies and we're going to focus today on episodic markof decision processes where we go for a single episode stop reset and keep going right so now what we're going to do is we're only going to be trying to get in general to a local maximum now it's possible you're lucky and you're sort of convex in the in the space of the the value it's bace of the policy parameters but in general we're not going to assume convexity so at best we're going to hope to just get to some sort of local Maxima in our space so if we have again if we only had one parameter and we have something like this where we might get to here we might get to here in general we're not going to make Global optimality guarantees this is in big cont been contrast to the tabular cases we saw before we were guaranteed to get to the optimal Q function optimal value function now we're just going to hope to given our policy parameterization let's try to get to What's um a local Optima in that policy parameterization so it's sort of a policy specific policy class specific guarantee and it's only a local Optima and what we'll be doing is we're just going to be trying to sort of take the gradient of the policy with respect to the parameters okay and as usual we're going to have a step size parameter so we're going to take the gradient of the value fun with respect to the parameters and take a small step and the key thing is going to be thinking about places where we can do this All directly using sort of smooth functions okay now one way you could do this of course when you see this now you probably immediately think of Auto diff methods and think about we can just back parate Etc but it's worth noting that when these methods began to start to get popular um they didn't necessarily have Auto diff yet uh I know they there was research then still um and what one of the things people started thinking about for this is how you could use this for robotics so this is a nice paper from 2004 so 20 years ago by Peter Stones group it was right around then I think maybe RoboCop was maybe 6 years old then or something um I think they started back in 1998 or so so they're these little quadruped robots and the goal was to think about having getting robotics to the stage where you could have robots play human players I think that was the goal by either 2030 or 2050 I forget but what they was going to start with it was quadrupeds so one of the big challenges at the beginning because you know everywhere you start with the beginning challenges and you go from there is just getting them to walk fast enough so if they're going to score goals and they're going to compete you need them to walk quickly and so there's this question of just how do you learn fast walks you know so that they can sort of trying to teach robots to run and what they found here is that they could use policy methods and policy search methods just to learn a faster way for it to walk and so they parameterized sort of the curve of how the the foot moves as a set of parameters and that defined the policy for moving those joints and then what they did is they just had these walk back and forth many many times and what they would do is they'd have them walk um you know with some particular policy parameters they would see how fast they walked they would do finite different methods so they weren't trying to explicitly do auto dip or anything there and then they would slightly change the policy parameters and repeat and they substantially they learned a substantially faster walk during that and I think it took maybe around four hours or so but they just had to replace the batteries a couple time so just an example to say like you know it's lovely to have autoi you can do really complicated things now but these methods can work even in really basic settings particularly where you think you have pretty bad models of like how the world works and so now you can just be directly data driven and why is this as a hard problem for those of you that haven't done robotics it involves a whole bunch of contact forces um you know the ground may be uh well there they have to learn on this particular ground you may not know because it's commercial Hardware you may not know exactly all the parameters that the designers put in so you can just be data driven okay as opposed to maybe having like a physics simulator all right so just to summarize so far the benefits of policy based RL is that um we're going to have often better convergence properties often going to be able to guarantee that we get to a local Optima whereas we didn't have that for deep Q learning um they're often really effective in high dimensional or continuous action spaces and you can learn stochastic policies but the methods we've seen so far might be more inefficient and higher variance and we often only get to something of a local Optima and we'll see some things to help with kind of the inefficiency in a second all right so now what we're going to dive into is how do we do this when we are willing to have differentiable policies so the hope is that we can actually compute the policy gradient analytically so we don't have to do it with finite differences and we're going to focus on policies where it's differentiable as long as it's non zero so we're going to assume that we can always compute the gradient of the policy parameters themselves and there are a number of different classes we can do this for um and there are many popular classes uh including of course deep neural networks so popular ones are often softmax softmax is used all the time I'll explain what is in a second gaussian um and neural networks and again just to be clear here what I mean by a policy class is what is the functional form we are using to give us a probability of an action given a state so are we having something like well I guess we can just see on this next slide what what these will look like okay so we're going to assume I'm going to give you some examples of those of what you know what softmax um and G neural networks look like in a second in terms of how how we differentiate them but these are just different ways for us to parameterize what is the probability of an action given a state um actually I guess I'll give a quick example with Gan for Gan you could imagine let's imagine I have a robot and I'm trying to figure out say um how much speed to apply then you might have a policy class that says the action I take is equal to a Galan centered around 0.5 with some standard deviation so it would be a stochastic policy um and it would say the average amount of speed you're going to apply is 0.5 but you're going to have some variability around that that would give you some stochastic Behavior sometimes the robot would go really slowly sometimes it would go fast so it would go in a negative Direction okay so let's keep assuming that the policy is differentiable whenever it's non zero we know the gradient that still doesn't tell us how to solve policy gradient methods yet because what we want to do is take derivatives of the value function so we want to say I want to find the maximum the policy that has the best value function which means I'm going to need to take the derivative of the value function with respect to the policy parameters okay so remember that the policy value the value of the initial starting State under a policy is going to be the expected dis expected sum of rewards we don't have to use discounting for most of today if we assume it's finite let just say we're assume we're in the episodic case and so this is finite so no discount counting for now so we don't need discounting for now because it's always a finite length so we're never going to have infinite reward so the policy value is just the expected sum of discounted rewards when we follow the policy parameterized by Theta till the end of the episode starting from the state s0 and there are lots of different ways for us to write this down so one ways for us to write down B of Sr is equal to well it's equal to the state action value averaged over the probability of us taking each of those actions under our policy so this here just says what is the probability of me taking this action starting state s0 if I have policy parameterized by Theta times what is my Q value of starting in that state taking that particular action and then following that policy for the rest of it okay so this is one way to write it but we can also think of a quite different way which is let's think about trajectories I'll write down what these are in a second okay so this is a trajectory what's a trajectory that's going to be s0 and then it's going to be an action and then S1 dot dot dot dot sampled from PI Theta okay so another way we can think of the value and then this is going to be the reward for that trajectory that and we've called RG before so we I'll just write down that in case so another way we can think of the value is we say well let's just sum over all possible trajectories we could reach under this policy and what would be the reward for each of those trajectories and I'm just going to take a weighted sum now of course you might be thinking that's totally intractable and yes in general um if H you have a really long trajectory then it's going to be inct and you have a really large State space and you could reach many states in general it's not going to be possible to actually enumerate this but this is mathematically well defined this is just an expectation over the reward of trajectories and we know whenever we see expectations that we can approximate those with finite samples you can think of just taking n samples just like what we saw with Monte Carlo methods and using that to approximate a trajectory so in general this is inter able General intractable but we can approximate by sampling all right so this is one way we could write down so this is another also valid way to write down what is the value starting the state and following the policy okay so I've written that doubt more neatly here P of T Theta is the probability over trajectories when you execute that policy starting State s0 and that is the sum of the words for a trajectory in this class we're going to focus on this latter definition but inside of set and Bardo they have a nice way to think about policy gradient methods that starts from the other definition so you can always look at that but both are totally valid definitions all right so now we're going to focus on thinking about likely Hood ratio policies so we're going to be thinking about this case where we have a distribution over trajectories and then what is the sum of rewards for each of those trajectories so we have our value function and now what we want to do is find the argmax so that we maximize us having probability of getting trajectories with high reward so that's nice so instead of just thinking about the value function we now can think of it as okay I want to have policies that induce trajectories through the state space through this state in action space they give me high reward so what we're going to need to be able to do is to take a um a gradient through the right hand side okay so that's what we're going to do now just okay so we're going to take the gradient of this because once we have the gradient of the value function with respect to the policy parameters we can update our policy parameters to increase hopefully the value of the policy that we're at okay so what we're going to do is we're going to say we're going to take the gradient with respect to the right hand side we can rewrite this by pushing in the gradient okay now R of to doesn't depend on the policy parameters that's just what is the reward once you've told me what a trajectory is so we can put that on the other side this is the only part that depends on the policy parameters and now I'm going to play um a trick I'm going to note that this is going to be equal to well I'm going to do something that's going to seem not very helpful for a second and then we'll see why it's helpful just going to multiply and divide by the probability of a trajectory I haven't done anything I've just multiplied by one and I've happened to multiply by the top and bottom by the probability of that trajectory but then I'm going to note that the derivative with respect to log of the trajectory in Theta is just equal to one over the probability OFA time the derivative with respect to the trajectory in Theta okay because the derivative of log is just equal to one over the value times the derivative of the thing inside the log so that looks exactly like this okay so that's the trick that we're playing here and so we can rewrite this then as the probability I'll tell you why we do this in a second okay all right and let me just rewrite it one more time so so it's more easy to see why did we do this okay the reason we did this is that in general it's going to be hard for us to think about or it might be tricky for us to think about how do we propagate our derivative through something that's an expectation we had an expectation over all the trajectories waited by the reward of those trajectories we now want to want to take a gradient with with respect to it we want to end up with something that is computable from samples um because it's easy for us to get samples we can actually run our policy in the environment so by playing this trick what we now have is something that we can also sample because this is now an expectation over trajectories of the reward of the trajectory weighted by the gradient of the log of the probability of that trajectory okay and we'll talk soon about how you compute this part but this expectation can be sampled okay because this is just a probability over trajectories and so we could sample say 100 of them and approximate that outter expectation so that's one of the right reasons why this is I'm just writing this out more neatly here on the next slide this is called the likelihood ratio this term here and so that's one of the benefits to doing this is that we want to end up with something that is computable we want to be able to get this gradient with respect to the value function for the policy parameters and so this is going to give us something that we can app approximate with samples and we can compute okay all right now you still might be a little bit concerned because all right maybe you think yeah I can maybe compute this by like writing things out in the environment but I'm still going to have to take this derivative and how am I going to do that and what does it end up depending on so let's do a Next Step so as I said what we're going to do here this is an expectation this is an expectation and we're going to approximate that expectation with an imperical estimate okay so we're just going to instead of actually taking you know all possible trajectories particularly in the case of vision um input you could imagine that would be completely insane so we're just going to approximate it by taking n samples but we still have to handle this okay so that's what we're going to do next so this first part should all seem clear the second part should Le certainly for most of us would not be clear yet about how we do that second part okay so what do we do with that what we're going to do now is we're going to decompose that latter part into States and actions so remember that what this means here is this is going to be a particular trajectory we get by following a policy for teps or until the end of the episode okay so let me just write remind ourselves what T is going to look like here so this is going to be like time step here I'm using the subscript As Time step okay so let's just write out what a trajectory is and what those probabilities are approximating the probability of the trajectory just to be one over n no good question yeah or oh sorry yes for for this part yes yes we're assuming that we're s for each of the trajectories we're using like a Monte Carlo estimate we're just using 1 over M but you know if some trajectories are more likely than others they'll appear more in that set of M it's good question okay so let's now try to express what the probability is of a trajectory okay so the probability of a trajectory we can write out as follows so we're going to still have that outside log we're going to do the following okay going to say mu of s not is equal to the probability of s okay that's just like what is our probability distribution over our starting State okay so that's me and then what we're going to have is the following t = 0 to T minus one we going to have our policy so this is going to say what is the probability that I pick the action I picked given the current state I'm in times the probability of St + 1 given s0 to t a z to T so what I've done is I've just written out what is happening in my trajectory here is I start I have some distribution over C in this initial State under my policy I have some probability of taking a z and that's here then um I'm going to assume for a second that rewards are deterministic but you could add in a reward term here and then I'm going to say well what's the chance that I get to State S1 given my history given the previous States and the actions so I've just written this out as um a joint probability okay and now what I can do is I can use the fact that log of a * B is equal to log of a plus log of B so I'm just going to decompose all these terms okay so I'm not applying my gradient yet I'm just going to have log mu of S Plus su/ t = 0 tus 1 t = t 1 let put the L sorry it's a bit messy I'll make sure to add a clean version um so what I've done is I've just decomposed my log but now this is really nice because this term is not a function of theta this is just my initial starting State distribution it has nothing to do with my policy so this drops out does this part depend on my policy yes does this part depend on my policy no so when we take the d of it it disappears so that is beautiful because now it means we don't have to know about our Dynamics model okay so the only term that is still around after this is this thing all right so this is great because now we don't depend on our Dynamics model we have written down what this term is as a function of um so we're just doing this term right now as just the sum of the derivative of the log of the policy at that particular point so we're sort of summing up for each of the different actions we took along the way what was the log of their probability and taking the derivative of that whole term all right so we don't need any Dynamics model which is great and I'm just going to say here I'm going to make sure that um something is consistent here oh yeah with all the meth uhuh the PSS t+ one for the um part why do we look at the entire history and not just the Past St in action great question so what I've written out as so this question is a good one I wrote down here the Dynamics in a really general form I am writing them down and I'm not making the markup assumption we could make the markup assumption but what I wanted to point out here is that you don't have to make the markup assumption does not matter so because the Dynamics model are independent of your policy when you take the derivative they completely drop out whether they are markof whether they are non-markov Etc and so that's really nice it shows that in this case it's not making the Markoff assumption now I did make the Markoff assumption somewhere I made it here because I assume that um I made the Markoff assumption in this sense I assume my policy was Markoff my policy is only depending on the current state um but your policy also could depend on a history of States you know you could have like a current neur uh neural network work or any of the other representations you might want to choose there um and you would still then this would just depend on your history good question all right so I just want to go and I want to make sure that I wrote it down neatly in terms of the most general form that's why I'm skipping those right now one of the things to note here in terms of just notation is that people often call this thing here a score function so this derivative with respect to log of the policy itself we often call it score function okay so in general um we um the nice thing is that it's generally not very hard to compute the score function so if you have a differentiable function we can compute the score function pretty easily in many cases let just make this bit smaller okay so let's see what that might look like for a couple different policy classes so one thing we could do which is a pretty popular thing to do is do a softmax policy so the idea in this case is that let's take a linear combination of features so 5 sa dot product with Theta and then you could say the probability of Your Action is proportional to the exponentiated weight so you take the exponent of that dot product between the features and then you normalize it and that gives you generally a stochastic policy um you can also have a temperature parameter in there if you want and the nice thing about this is that we can write it um we can take the derivative of this very easily okay so we can just do that quickly here just to illustrate so what this is this is just to illustrate that like it is often very feasible to take the derivative with respect to the policy parameterization okay so this is just going to be the derivative of the log of e to 5 of s t Theta so we can do this here and we can rewrite this here as okay and so this is just going to be equal to five s so I'm just taking the derivative of this for a particular Theta and so we can just rewrite that 5 okay so what I've done here so I've taken the derivative with respect to this function for a particular Theta and then what I've said here is well you could notice that this here is exactly just equal to my Pi Theta of essay so it's like I'm getting this waiting over the features okay on the next slide nether okay so the score function for the for the softmax policy is just going to be equal to the feature sa FSA minus the expected value over the policy of the features yeah oh great question well I could be for example you could think of it as like if you have um a large neural network that's doing some representation it could be the last layer like the second last there and then you could just do like a linear dot product that yeah it's a good question you can or in case of customers it could be a whole bunch of different features and then you have different we there all right so this is also possible to do for other functions so for Gans um uh we often want to think about that for continuous action spaces which are really useful for robotics where you might have like continuous torqus or continuous accelerations Etc you can think of there being a mean which is a linear combination of some State features your variance might be fixed or could also be parameterized and then your policy is a gan so like you maybe you're sampling some particular action dependent on your state along with some variance and then you can again just directly compute what the score function would be in this case in close form but in general you're often probably going to be using this with deep neural networks and then you can just use autoi to do this it's just to illustrate that there's a number of different functional forms where you can compute this analytically okay all right so just to recap this what we've shown so far is that we can have policy methods where we have a direct parameterization of the policy we can write down the value function as being a weighted sum over the trajectories generated by that policy times their reward it turns out that when we want to take the derivative of that we can reexpress it so that we just think of we only we don't need the Dynamics model and we're weighing these score functions so now let's just do um a small check your understanding about uh likelihood ratio score function policy gradients and so what I'd like you to do is say does it require that your reward function is differentiable can you only use it with markof decision process is it Ely useful mostly for infinite Horizon tasks a and b a b and c none of the above are not sure let's just take a second to do that all right we have a good split at the end nobody is not sure um but there is a lot of spread so why don't you talk to your neighbor and see if we can come to more consensus all right Sor interrup some good discussions but I want to make sure we get through reinforced today so um so this a little bit of a tricky one in fact when I was giving it to one of my Tas I forgot to put the none of the above and he was like wait what so um so it's none of the above um it and so the first one's part of the actual elegant aspect of policy gradients so as you can see here you need the policy function to be differentiable but the reward function does not have to be the reward function is not a function of the policy in the way that we've written it here so that's pretty elegant so that that has motivated people in a really wide range of areas where you really might have very complicated reward functions um to be interested in using what we're going to see soon which is reinforce which is based on this idea because you just need the policy parameterization um to be differentiable so that's really cool um B doesn't have to be markof because as we saw the Dynamics model drops out and so was just saying in that case it doesn't um doesn't appear at all so doesn't need to be mark off you don't need differentiability um we are assuming that it's finded Horizon uh so that we can actually episodic so we can get M more than one if it was infinite Horizon we'd only get nals 1 so so all three of these are false so let me just make sure I Circle that okay so just to give brief intuitions because to make sure that we get to reinforce um you can think of if this is a generic way of writing this down we sort of have some function times the derivative of log of some other probability function and you can think of this first part is measuring how good a sample is and what the idea is is that when you have the derivative you're trying to sort of move up the log probability of samples that have high reward because you generally want policies that visit parts of the state in action space where we get high reward so that's kind of the intuition and the nice thing is that that F doesn't have to be differentiable it could be discontinuous could be unknown as long as you can get samples from it um so it could be extremely flexible to what is that reward or objective function so I put a couple slides here um I believe remember if it was John Schulman who originally had these ones um I put some credits at the front but you can think of sort of com taking a combination between what the probability is of your input of your X as well as your function so in our case that's going to be the reward function this is is generally going to be the reward function over trajectories and this is going to be our policy it gives us probabilities of the trajectories and so you can think of combining between these two to actually change your parameter space just to give a little bit of intuition over what the what this sort of um gradient estimation is doing okay so in general we can also write down a policy gradient theorem which says we could either use something like episodic reward or we could be trying to look at average reward per time step or we could be trying to look at average value and in all of these cases we can end up writing something that looks really similar to the equation I showed you before which is the derivative with respect to these value functions or something like a value function is going to look something like the derivative with this score function the expected value over the trajectories you're going to get of the log of the parameters times the Q function or the um the return for that particular State action pair following the policy okay and there's a nice D derivation in s and Bardo about that at a high level I think the useful thing to know here is just that um there's many we can extend it Beyond just thinking of kind of like this sample of return we can think of they being Q functions all right now what I've shown you so far uh is something that is correct and we can turn it into an algorithm but it does not leverage much of the temporal structure so what do I mean by that so what we've written down here is a valid gradient it's unbiased um but it can be very noisy so we're estimating this by Monte Carlo method because we have these M samples and as we know from Monte Carlo methods before they are unbiased but they can be very high variant and so some of the ways to make this more practical and what I mean by that is the better estimate of the gradient um and hopefully with less data because ultimately we're going to have to be using this information to update our weights to try to get to a good policy so we want this to be data efficient is we can try to leverage the temporal structure and we can also include baselines right so let's first see the temporal structure so what we've done before is we've summed up all the rewards from a whole trajectory and we've multiplied it by the sum of this score function for the whole trajectory that's what we've done so far we can instead think of it as what if we have the gradient estimator for a single reward term okay so this is just you know for one time step we can think of it for there which is we have that single time step times the score function for the remaining time steps or or for the for the time steps up to that point okay so it's like we just think of the partial trajectory until we got that reward so we want to think about s of the Dera of this this is the reward we got at this time point so instead of having this whole sum we just think of well what is sort of the trajectory that we got up to that time point and all of their score functions okay does that make sense have questions about that part okay so this is like for a single time step T Prime and so now what we can do is we can sum this overall time steps so instead of having the sum of all overall rewards times this we can say well we know that for one time step it is equal to the expected value of the reward for that time step multiply times these score functions up to that point so let's just rewrite it like that now we're just going to some over the rewards we got for all time steps all right so now what we can do is we can do slight rearrangement okay so what we can notice is that for each of the points so you can think of it as I have so this is t 0 1 2 3 and you can think of all of these sort of score functions I have at each time point so the score function at time Step Zero is going to appear for r0 R1 R2 R3 dot dot dot okay so that's what I've just done here I've said this first term is going to appear here in with a reward function of all of the subsequent time points okay because that decision happened and then we got a reward and then we got a whole bunch of rewards later the on the first time step it can affect the reward you get at time step one and all the future after that okay so this term here this score function can influence um the one that we get on time step one can influence time step one all the way out to the end of the episode the one we get on time step two can influence time step two all the way out to the end of the episode so essentially this is like saying my reward on time step three cannot be impacted by decisions I make on time step four time only flows one way so if we think about what those score functions were and like we think of like the trajectories that were generated they're a temporal structure and so it means that we cannot have um if we change the policy parameters such that decisions in the future change that can't affect my reward on earlier time steps okay so this is leveraging the temporal structure so this just allows us to rewrite the equation so that now we have for each of the different score functions essentially which of the rewards they influence and the reason this is important is because here you could see that we are multiplying each of the score functions by all of the rewards and now we're only going to multiply them by the rewards they influence and so in general that's going to be way less than having the full set of rewards so this is going to reduce the variance of our estimator without causing any bias just leveraging the fact that decisions in the future can't affect your rewards in the past all right so that is one of the first things that we're going to um do in this case so we're going to write um so remember in this case that if we sum up all the rewards from the current time step to the end we've just called that the return we've seen that before from multi Carlo so we can just rewrite this expression like that and that gives us the reinforce algorithm so this is the reinforce algorithm that has been incredibly influential um in NLP and Robotics and many many areas okay and so what this says here is that the way we change our parameter is just our learning rate times our score function times the return we got from that time step till the end of the episode so we still have to wait till the end of the episode to update anything but what happens is we run a full episode with our current policy and then for each time step we slightly change our policy parameters by using a learning rate the score function for that time step plus the return we got from that time step till the end of the episode and then we just steep through that for the whole episode and that's given us T different updates to our policy parameterization and then we just repeat over and over and over again and what that guarantees to us is that eventually we will land in a local Optima of um the value function for the policy parameterization so this is called Monte Carlo policy gradient or known as the reinforce I believe this was in roughly 1992 it's about 30 years ago there's been many many many Poli grading algorithms are built on this idea okay now when you're looking at this you might still be concerned that if from remembering back from the Monte Carlo methods we've covered that this estimate G can often be pretty high variance so in general if you're just directly kind of averaging over sample returns that might be high variance so one of the next fixes we can do and we'll get to this more on um Wednesday is to introduce a Baseline and see I'll just say so the the goals here is that we're going to hopefully try to converge as quickly as possible to local Optima so we want to reduce the um the variance over our gradient estimate and so the Baseline is going to allow us to hopefully reduce well in general yes reduce the the variance over this estimation process and kind of the we'll see two ideas next which is introducing a Baseline and and then thinking about an alternative to the Monte Carlo returns so those are the ideas that we're going to go through next I guess I'll just do one more thing and we'll we we'll go through the proof of it next time so I'll just introduce the concept of Baseline and then we'll we'll prove it next time so the idea in this case is that we're just going to subtract something off and we're going to subtract something off that only depends on the state this is only depends on the state okay this is not a function of your policy only depends on the state and it will turn out and we'll prove this next time it's pretty elegant that for any choice of something that only depends on your state the gradient estimator is still unbiased so you could subtract off anything there that is only a function of your state and you didn't change the bias of your estimator which is kind of wild um and we'll prove that next time and but the goal is that we can hopefully reduce the variance of our estimated gradient by subtracting off the right thing okay and just intuitively the way to think about the Baseline is that you don't necess just care about whether or not the gradient is positive or negative and whether returns were um good or bad you might care about like well how much better or worse are these returns compared to something else I could have done like I want to know whether this policy a is better than policy B and maybe both of them give you positive returns one of them gives you 100 and one of them gives you 90 but you'd really like the one with 100 so you'd really like to move your policy parameters in the direction of stuff that is better than other Alternatives and that's kind of the idea of a baseline is to say like well maybe I know that I could probably always get like 90 for this particular State how much better is this policy for this state compared to something I could do on average and so we're going to sort of intuitively increase the log probability of an action proportionally to how much its returns were better than expected where kind of the Baseline is giving you that expected value and we'll see formally on Wednesday how that by doing this with the Baseline it doesn't introduce any bias so this is going to be one of the ways that we're going to get better uh better ingredients the other thing that we're going to do on Wednesday is we're at least going to start talking about po which is the second which is part of your homework too bless you um which is going to involve more ways to kind of be more efficient and effective in the policy that we do I'll see you then thanks