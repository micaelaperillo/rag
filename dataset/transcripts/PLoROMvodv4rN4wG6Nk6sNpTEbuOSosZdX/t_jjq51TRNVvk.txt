hey everybody welcome back we're going to get started with a refresh your understanding poll you can go to Ed and to see all the polls for today um just remember to log in first so that we can log it for participation points the two questions ask you to think about what we talked about last time in terms of Markoff decision processes um and what sort of guarantees or type of properties they have yeah you you what the again yeah great question tabular mdp is where we can write down what the value is of a state as a table so like you can just have one end for what the value is for each state this is in contract to like neural networks or things like that where you don't have one parameter per state okay we'll take like another one or two minutes we have a good amount of controversy on these questions so I'll see how this converges I remember seeing this a to the power s number in in a previous context was it in the context of policy yeah does someone want to remember um why is a to the S important I remember why it is yeah remember your name is it like the number of total possible policies exactly right exactly right so there's at most a to the S potential um uh deterministic policies all right so most people selected the correct answer for the first one um which is this is true so ASM totically this will value iteration and policy iteration are correct in the tabular discreet markup decision process case and they will ASM totically both converge and compute the right value function the second one looks like it's pretty evenly split so I'd like you to turn to someone near you and um and argue for what you said for the second one Li you all right so maybe your has Chang [Music] all so the answer is true um and about half of you said that do someone get want to tell me why this is true yeah is it yeah um I'm not sure if this is correct but I just I think the value iteration might not be um um guaranteed to converge so that's if it's not guaranteed to converge it could just be unbounded so that certainly would be the case but fortunately it is guaranteed to converge if gamma is less than one um so but you're correct that it that it um Can require more iterations does anybody have an and remind me your name so my thinking was that uh in uh like in policy itation we know that in each step we're going to improve to a new policy but um in each value step you might not reach a new policy you might take m multiple steps to reach a new policy that's right so um in policy iteration um and I talked to some people about this too there can only be a to the S um so in policy iteration there's at most a to the S because you only go through each policy once but for Value iteration it can be more and I'll give you an example um so and one way I would think in general if you see this sort of question is to think about well can I can come up with a counter example to say where this would be different so consider a really silly mark off decision process where there's just one state in one action so if there's one state and one action there is literally one policy you can only do one thing and there's only one state to do it in um so that means that policy iteration is going to take one round but for what value um iteration is going to do is it's going to keep going until the value function stops changing or stops changing within like a very small amount and so what would happen in value iteration and feel free to go back to your notes from last time is we would start off and let's say the reward is 1 and Gamma is9 and we initialize the value function to zero so then if you use a geometric series and if you haven't seen that before just come chat with me about it I'm happy to say um the varar of this date is 1 over 1 minus gamma because you get 1 plus gamma * 1 plus gamma s * 1 dot dot dot so because it's like you're always staying in that state and you're always taking that action and you just day in and day out that's what you get forever so so the actual value function it get the you get eventually is 1 over 1 minus gamma um or you know about 10 um but after the first iteration of value iteration V1 of s is just one so that means you know one is not close to 10 or not that close to 10 so we haven't converged yet so we'll have to continue to do a bunch of iterations of value iteration whereas at that point um policy iteration would stop because it would just evaluate the value of the one policy we have which is to take that one action and you'd be done and I bring this up just to illustrate that like both even though both of those algorithms are guaranteed to converge the right thing eventually they can have quite different behavior in the short term did you have a question I was just going to ask is it only converging in the limit or all of them are only converging limit yeah so all of them are converging um like ASM totically as you do this over and over again good question great well welcome back um uh if you just came in feel free to go through the questions later what we're going to be doing today is to kind of continue in this more simple setting where we don't do any function approximation um but we are now going to think about the fact where we don't have models of the world and what I mean by that is that we're not given a Dynamics model and we're not given a reward model our agent just has to try things out in the world to learn how good they are and we're going to start with model free policy evaluation um in the case where we still have a small enough number of contexts or states that we could write down a value for every single one of of them separately so that's why we call it tabular um I'll just say one thing in terms of logistics so office hours are on the website we'll try to keep that calendar updated it's just a Google Calendar it'll include the location um uh and if you go to Q status for the one1 office hours we'll make sure that the zoom is either on that or on the web or on the website um I'll have office hours starting this week on Thursdays uh and mine are for project and conceptual questions so feel free to come ask me about anything in class um I I won't be going through code but you can go talk to the Tas about that um or you can come in and brainstorm about projects or or general questions about reinforcement learning um I'm sure some of you are starting to think about projects uh just in general some people are asking so what's kind of in scope um I put something on Ed about that but also in general it can be like a new idea for reinforcement learning a new application um it can be something you're doing in if you're already doing research and reinforcement learning it can also be replicating part of an existing paper and this is really helpful actually it's helpful for the whole Community because there's a lot of work going on um and people are making different choices about hyperparameters and seeds and things like that so it really is very valuable too to see what things we can replicate does anybody have any questions about that or other logistics of the class before we get going all right so let's get into policy evaluation um so as I said what we're going to be doing today is to think about how do we learn through direct experience how good decisions are and we're going to assume that we have a fixed policy so again like our boss says how good is this way of you know advertising to customers or U maybe you're in a setting where you're trying to see how good is the patient outcomes from the current protocol and the idea is that we're only going to be using data from the environment so um and today this experience is going to come from executing that policy let me just move this up so you can see a bit better okay today we're we going to assume that when we get this data it's from directly executing a particular policy later we'll think we'll talk about other sort of relaxations to this and so I'm going to try to motivate today why this is a useful thing to do um and what sort of properties we would want to try to compare different algorithms so it will turn out that this type of thing comes up all the time it comes up when we actually want to make decisions and learn better policies um and it's going to be an important part of much more complicated algorithm like deep Q learning and policy gradient and other things where we want to sort of be able to see how good are current things so that then we can take gradient steps or improve our policy okay so this is what we're going to try to cover today we're going to cover multicolor policy evaluation temporal difference learning certainty equivalence and batch policy evaluation and maybe raise your hand if you've seen temporal difference learning before okay raise your hand if you've seen Q learning okay great Q learning is the control version basically of temporal difference learning so you'll see a lot of similarities there okay all right before we dive into this I just want to recall a couple definitions we'll have we're going to use G for the return which means that from this particular State what is the discounted sum of rewards we get for a particular episode the state value function is saying on average what is that reward we get and the state action value says if I start in this state take this action and then follow this policy what is the expected discounted sum of rewards so we saw last week that we might want to do dynamic programming for policy evaluation when we do have access to the models so again what I mean by that is that where someone gives you a function for the reward and a function for the Dynamics model and so we saw we could do this sort of bellman like backup for a particular policy this was different than the Bellman equation because there's no Max not trying to take a Max over different actions we're just taking whatever action is specified by the policy and in this equation here this is for like a deterministic policy otherwise we need like some additional averaging over all the actions that could be taken by that policy and just to remind ourselves here um before we converge this policy this V Pi K is like an estimate of the value of the policy it's not the actual value yet it's just like an estimate and it's hopefully improving over as we do more iterations and what you can and another just good thing to remind ourselves of is that this is here this sort of expect expected discounted sum of rewards what we're doing in this equation is we are plugging in this term as an estimate of the expected discounted rewards for the future so we're saying like oh we've got this estimate of the value function we're going to plug in and say my reward is my immediate reward plus my discounted sum of future rewards and this is what I'm using for my discounted sum of future rewards and so this is known as bootstrapping because we're plugging in one estimate in order to help us do another estimate and we'll see a picture of this graphically later right Monte Carlo policy evaluation is a really simple idea but it's very useful um and it is commonly done so as essentially the idea with POC multicol policy evaluation is we are just going to simulate or act in the real world so this is just saying you've got a policy which means you know what action to take in every state today we'll mostly focus on deterministic policies just to make it easier um so I'll just say that for for most of today assume Pi is deterministic but all these ideas can easily be extended just easy to write that down without having to do an expectation over actions everywhere okay so what's the idea in this case well the value function is just an average over the returns it's like an expectation over the trajectories or the returns you could get by following the policy and therefore the value is just the mean of returns and we know how to approximate means we just do things a bunch of times than we average and so as an example of this it might be something there someone says okay when I want to know if we um say give a particular set of patient treatments um and maybe those treatments take a year for example and someone wants to know on average how good is that well what you could do is you could have a 100 patients I'll go through that particular protocol for a year and then average their outcomes and that would be an example of Monte Carlo policy evaluation is you just execute the policy for many different episodes and then you average okay and one thing just to note here is that you can have cases here where not all the trajectories of the same length so imagine in the patient case I just gave you might have that some people drop out of a trial during the year or maybe they finish their treatment successfully and and so then they're also done so all the trajectories may not be the same length but essentially you can just think of it as just have many many trajectories you know maybe this one has a gal 10 this had a gal 5 this has a gal 10 you just average over all of them and that is your value function okay now one of the benefits of this is that when we do this we're not actually benefits or drawbacks and we'll talk about this this is making no assumption that the system is a Markoff decision process it's just averaging so your system might not be Markoff and what I mean by that is that you in general will have a finite set of features to describe the state so if we think about that patient example I just had maybe you have different vitals of the patient maybe you have static demographic variables but we do not have all the features that are probably going to describe how someone's going to react to a set of treatments and so because of that you may or may not think that in the features you have access to the system is Markoff but this doesn't require this state to be mark off it's just averaging you just roll out your policy many times and you average now a really important thing here is that it can only be applied to episodic mdps so what do I mean by that I mean your episode has to end in order for you to see what the total return was so if you have sort of horizon lengths or episodes that last for a year that's okay it's a little bit slow but you could do that but if you want to just think of how good a policy is if you just are going to act forever and never stop this wouldn't work we're not without some additional approximations somebody have any questions about either of these two things about it not assuming the state is marov yeah um you're talking about like Medical Treatments so um I mean does this only work if like the treatment only lasts like I know a same amount of time for every patient like six months um because if they have different Lanes right like how can that be episodic great question yeah so what is like well if it can it be episodic if the episodes are different length it could be so it could be that you have like a fixed policy and maybe that policy says if someone doesn't respond to this type of treatment we do this additional type of treatment and in fact that's very common um as long as the episode is guaranteed to end like you know that treatment could only last day for a year total you can still average overall those outcomes you just sum over the the return for those different ones yeah for each of these um trajectories are we supposed to begin with this like different state or we can actually start with the same state great question so um if we want to get a value function for all states we need to see all the states inside of these trajectories and we'll talk about how we estimate these in a second so I think this will be answered okay so for example how might we compute this so we would like to get the value for all the states that are reachable inside of your policy so what you could do is we can initialize two different variables One n of s is just going to be the counts the number of times that we've updated our estimate for State s g ofs here is going to start with zero which is we've never seen any returns from this state so what every visit Monte Carlo does is it samples an episode so this goes up to sometime step T TI this has to be finite but it could you know be different on different episodes and then we compute the discounted sum of Rewards for that episode okay so starting at time step T excuse me how much reward do we get till the end of the episode and then for every time step until the end we see if this is the first time that states been visited then we update the total number of times we visited this state for the first time per EP you know in each episode we increment the total return and we average so that just like Steps along and it just says okay you know maybe the first time I reached if you think of the Mars rover example see I have it in the next one no I think I put it so for a lot of today I've moved a lot of the worked examples till the end of the slides but if you want to go through them later I encourage you to um so for example in the case of the Mars rover you might imagine you start in state like S3 and then on that particular one you get a reward of one for that episode and so then you would average in one starting in that state as three till the end so this is first visit Monte Carlo evaluation which means you only update a state at most once in each episode so if you had something like this S1 went to S2 went to S3 went to S2 went to S3 so in this case dot dot dot you would update S1 once in this trajectory and you would update S2 once and S3 once even though you in fact visit those States multi times you only update them for the first time you visit yeah so could this have the problem where if the state is really rare uncommon then we get a really bad ass or we just never visit it at all so if you don't ever visit a state under the policy that's okay because then you don't have a value for it but then it's kind of undefined because you never would reach it um uh in this case as says if there's a state that's really rare for you to reach inside of your policy it might take a lot of trajectories in order to get a good estimate of it so maybe there's some rare side effect um of a treatment plan and it's going to take a lot of trajectories um to to observe that that was one of the challenges with the covid vaccine is that you know of course it was a finite number of people is pretty large number but pretty but finite and some side effects don't show up until you get much many many more it's true generally for treatments even if on average they're totally fine that you won't see some of those rare side effects until you get an enormous number of trajectories now covid vaccine certainly had you know the benefits there way out let way um side effects but my point is just to highlight that you know depending on how frequently you see the states it will take you more or less um number of total episodes in order to observe yeah so kind of it doesn't matter that I saw like S2 again or S3 again in this trajectory for this algorithm no um I mean it probably affects the reward still it's just that you don't use that data so an alternative is called every visit where every time you see this dat in that trajectory you update it and as you might imagine there let's say you see a it's a really long trajectory and you see s to many times then you would update for all of those so I'm just going to show you of three common different ones so this is sort of the this is a work example you can go through later if you want which is if you imagine this is the Mars rover the rewards are on either side this is the particular trajectory you can compute the first visit and the every visit Monte Carlo estimates so both of those are totally reasonable things to do perhaps more common is what's known as incremental Monte Carlo okay and this does kind of what you would expect it to do which is you maintain a running estimate for what is the value under a policy for a particular State and you smoothly update that as you get more data so what we would do in this case is you keep track of the number of times you visited that state and then you weigh your old estimate by the number of times you visitate minus 1 NS plus your new return you just observed div by NS so that's just sort of your way you know you're kind of constantly updating your value function for this state as you get more data and for those of you who have done machine learning which is probably most of you this should look pretty familiar this is kind of like a learning rate this is your updated value and this is your old value and in fact that's what we're going to see here okay so you can think of this in general it doesn't have to be 1/ n it can just be any Alpha here so any sort of like Alpha here is um just a learning rate and we're just smoothly updating our estimate of what is the value function for a particular State and we'll see lots lots and lots of algorithms like that similar to part what you saw in machine learning the key thing here is the the estimates that we're using which we often call sort of you know we also might use the word targets often is we have this estimate here and then we have our old estimate so this is you know one sample of what is the return starting in this state till the end of the episode so I think it's helpful to think a little bit about what this looks like pictorially and this also relates a lot to we'll see this we talk about things like alphago so I think it's helpful today and it'll look somewhat familiar to you if you've seen things like Mini Max trees or expecting Max trees who here has seen expecting Max trees before okay so maybe one so most people not so this might be a useful sort of um uh representation so I think one way to think about this is what we're trying to do is we're trying to um think of what the value is starting in a certain State and we want to take um we know what the action is we're going to take cuz we've got a fixed policy so that says we start in this date s we take this action a this is prescribed by our policy so this is pi of Pi of s is going to equal a and then after we do that because the world might be stochastic we're going to have a bunch of next States we could reach so we have probability of S Prime given s and a and what we're trying to do when we do um policy evaluation is we're trying to get expectation over all the potential Futures we might end up in by following this policy you know and maybe in some cases there's really good patient outcomes hopefully most of the time and maybe sometimes there's less good patient outcomes and we want to do an expectation over all of this so we can think of that as a tree as just we start in a state we take an action we look at the branching factor of all possible next States and then we repeat excuse me so if we think of what the policy evaluation diagram is doing for each state we know what the next action is we take and then we Branch again in terms of States so this is like S Prime and this is S Prime and so we can just think of like kind of this tree of possibilities going out but we don't have any there's no branching on the actions because the actions are fixed by our policy so then if we go all the way out and then what we want to do is we want to figure out what the value function is can think of this is you know an expensive way to do dynamic programming um what you would do is you would take an expectation over the states and then you would propagate those values back up to the root and if you don't find this a useful conceptual way to think about it it's fine it just I think it can be helpful to then think about what these different algorithms are doing in terms of approximations so this is sort of what we would like to do in order to get V Pi of s but we want to do this a much more computational umly efficient way and also sample efficient way so what Monte Carlo policy evaluation is going to do is it's going to look like that particular equation here and what it is doing here is it is going to approximate of these full Expectations by a sample so in particular what it's doing here is it's updating the Value Estimate by using a sample of the return to approximate an expectation and we do this many times you know we average over many such returns so it's kind of like saying you have this enormous branching tree you could do an expectation over all of that sort of explicitly up from the roots or you could just sample many times and that's also going to approximate the tree and the more samples you get the better that's going to be as an approximation of the tree and this type of ideas has been um used in many different types of algorithms there's um some really nice work in the mid 2000s like by Michael Kern and others and then similar ideas were really the foundation that then led to some of the advances of like Monte Carlo tree search and then that went into alphago so this what this is what Monte Carlo tree search is doing so notice it's not doing any form of bootstrapping there's no dynamic programming that's going on here it's just rolling out and then what we have here is it is using this here as sort of U this sample as an exp as an AO okay all right so that's how Monti Carlo policy evaluation works one natural question in this case is how good is that estimate so we're going to see lots of different ways and lots of algorithms for trying to do policy evaluation and so you might know ask like well how do I pick them on them like what are the properties I should think about so one pretty basic property that you might want is consistency which means that as you get more and more data does your estimate actually converge to the true value of the policy for all the states and this is something you probably want in many cases at least because otherwise it means that even if you had infinite data your estimate still going to be wrong now as we start to think about more complicated settings we might have to St you know um sort of be satisfied with this less good objective but here for right now we're going to we're hoping we can just write down the value of every state as an entry and a table that we should be able to get consistency a second thing we might want is computational efficiency we'd like this not to be too expensive for us to compute we'd like us not to require too much memory and we'd like it to have statistical efficiency which is sort of essentially how does the accuracy of the estimate change with the amount of data and what that means here is like sort of more formally we'd like to know how quickly do these things converge um as you get more and more data and then in reality we often care about empirical accuracy just how what is our mean squared error for our types of our estimators so how good is Monte Carlo well let's just first quickly remind ourselves that the bias of an estimator is the so if we have an estimat Theta which we're going to be thinking of is like our value function approximate approximation it's going to be the difference between on average what our estimator is versus the True Value that's our bias and the variance of an estimator is the difference between this and its expectation squared and the expectation of that and the mean squared error is going to be variance plus bias squ okay all right so generally we would like an estimator that has low mean squ error which means we want it to have like lower zero bias and low variance something to think about if you're less familiar with these is um whether or not if an estimator is unbiased is it consistent it is not necessarily consistent just so you know right so what we would like here is that ASM totically the probability that our estimator so n here is the amount of data we're using to construct that estimator the probability that as we get an infinite amount of data that our estimate is different than the true value by more than Epsilon it has to go to zero okay so we would like it to be consistent so how does Monte Carlo fa on these sort of um properties well um first visit is unbiased so it's an unbiased estimator of the DU policy and by the law of large Lumar as the amount of data you have goes to Infinity per state so you know if you have really rare States you're still going to need a number of samples to you know to estimate them but as the amount of data you have goes to Infinity you'll converge so it's consistent and it's unbiased okay every visit Monte Carlo is biased one way to think about that is um in the first case all your data is IID independent and identically distributed in the every visit case imagine that you visit State S2 and then like you know four steps later you visit S2 well their returns are going to be correlated because they're both in the same trajectory so they're not IID anymore that's just some intuition for why it might be biased but it's also consistent and it often has better means squared error because you get to use more of your data inside of a single trajectory to do more updates and then incremental Monte Carlo methods depend on the learning baate as you might expect so see that here so um let's imagine that we are going to have our Alpha parameter which is our learning rate which is trading off between our new estimate and our old estimate it can actually change per time step so just like how you can generally Decay your learning rate you can change your learning rate here and if your learning rate is such that if you sum up all of its values for a particular state it goes to Infinity but the square is less than infinity then you will converge to the True Value so and again this is a Prett the these are um a pretty common types of criteria we'll see for some of the algorithms we have that sort of under some sort of smoothness guarantees for the um uh learning rates we'll have some decent properties yeah for my Ming those conditions aren't met like do you definitely not have a guarantee or are there other conditions that can give you a guarantee and those are just like some of the great question so asking you know um is it is it required to have these conditions these are sufficient um they aren't necessary always a lot of that will depend on the particular problem domain too and like what the Dynamics and the reward is um to my knowledge I'm not sure if there are other really General conditions like that but there might be for specific problem classes it's a good question now one of the problems with this is that in generally it's a pretty high variance estimator so you're kind of getting certainly like with every visit or sorry certainly for um uh first visit Monte Carlo you're only updating the state at most once per episode so it can take a long time so you can imagine that if you have very different outcomes from the same starting state so maybe you know most of the time you have pretty average outcomes but maybe one in 100 times you have a really bad outcome it's going to take a long time for that estimator to converge so in general this is a pretty high variance estimator even though it is often unbiased and it is consistent and then the other big requirement is that it requires episodic settings so you have to wait till the end of the episode to update your estimate and for here right now that might not seem that bad but when we start getting into control de and decision- making you might want to use the data you have already in that episode to change the behavior of the agent so you can imagine something like if you're um doing like uh self-driving cars or something you're already getting some evidence that um the car is not working as expected within a single episode that might be really long you might want to use that information to change how your steering for example okay all right so just to summarize here what it does is it's um it's not using the Markoff process it's updating your value function estimate using a sample of the return to approximate the expectation and under some pretty mild conditions it converges to the true value of the state and in some cases it will turn out that even if you actually know the true Dynamics model and reward you might still want to do this and I think one thing that's useful to think about here is um yeah systems which you think the markof property might be violated at least with the features that you'd be using to represent the state all right now let's go on to temporal difference learning and this is again sort of related to Q learning which we'll get to in the next lecture so set and Berto which is a textbook that um is an optional one for oh yeah and is it um that a good question so if we don't know the rewards model how do we calculate the rewards for the trajectory ah great question so the Assumption here is that um it's kind of like you either are in a real setting where you can sample these from an article or something in the real world is giving you these so you may not have an explicit representation for the word model but you can get them so like your customer buys something or they don't or you have um you know a side effect so you don't NE have a parametric model but you are getting real rewards it's a good question anybody else have any other questions about Monte Carlo before we go on to temporal difference learning and I'm going to call it just temporal difference learning now um and then I'll specify that it's actually td0 for most of what I'm going to talk about so just specify mostly discuss tt0 and I I'll specify what I mean by the zero shortly so certain abto which is one of the optional textbooks for the class um says if one had to identify one idea as Central and Noble to RL it would undoubtedly be temporal difference learning and what their point is is that it really is sort of it's a way still to construct estimators both for control um and for policy evaluation and the idea is if we think back to that tree I showed you and I'll show you some more there's going to be a way to sort of combine between the idea of sampling to approximate expectations and bootstrapping to approximate future returns and we'll see that in a second it is model free meaning you don't need to have like a parametric representation of the reward function or the Dynamics model and the nice thing is you can use it in episodic settings or in infinite discounted Horizon settings you just like set up off your robot and then it's just going to have to learn to act forever okay and one of the key ideas is that we're going to update our estimates of the value of a state immediately so I'll put Pi here because we're still talking about a policy after every single Tuple of State action reward next state all right so let's see how that works so again remember our goal is just to compute the expected discounted sum of rewards for a particular policy now let's think back to the bman operator so if we know the mdp models and we have a particular policy we could write the Bellman operator like that and what we were doing in incremental every visit Monte Carlo is we were updating the estimate using one sample of the return and the idea now is to say well this was one sample but maybe we could just maintain we have access to a value function why couldn't we look up and instead of having you know what the rewards were in the state till the end of the trajectory we have we observed a particular reward we got to a particular next state why don't we use the value function for that state so what we're doing in this case is instead of using G we're plugging in the immediate reward plus gamma times the discounted sum of future rewards using our current estimate of the value function for that next state we reached and here the reason one of the things is that we don't need we don't have to wait do this immediately as soon as we reach S Prime so as soon as we reach S Prime we can you know as soon as we see the next state we can immediately update the value of our current state so we have to wait till the end of the episode we can use this for infinite Horizon problems okay so this is what that looks like and we're of going to call that the TD Target and again that should look like machine learning it should look like um uh what we just did with Monte Carlo that what we're plugging in here is we're saying we're taking our old estimate and we are moving it we are shifting a little bit by our learning rate towards our Target which is our reward plus our discount some of future rewards using that plugin estimate and when we think of sort of how much our estimate is changing we often call that the td0 error which looks at excuse me how different is my current estimate at the value of a state versus the estimate that I'm plugging in and again if you've seen Q learning before this is going to look really similar to what we had but there's no Max or things like that you'll see those soon okay so the td0 learning algorithm just looks like the following you sample a tuple of a state action rewards uh next state you update the value for that starting State and you repeat and so your T goes to t + 1 and then you get the next coule just do this over and over and over again so like in our Mars Rover example you have sort of State action reward next state you update and then you just shift along let's see what that might look like here so in this case let's imagine we have a policy where we always take action A1 we're going to make our discount Factor one to make the math easy um and we're going to assume that any action from State one or S7 terminates the episode and then what we see in this case is we have the following trajectory we start in state S3 we take action A1 we get a reward of zero so this is the reward we transition to State S2 and so so forth till the end of the episode so what we would have in this case is that we would make it um so that the first up we update we would do be V of S3 and what we would say is that's my old estimate of V of S3 * 1 - Alpha I've just Rewritten the above equation here because there's this was basically one and this is Alpha * minus B plus Alpha * the immediate re plus gamma * V of S2 so that's what that would look like and here my imagine that I'm I I've initialized all of them to be zero to start so this would still just look like zero okay and in fact the only what would be the only state I would update to not be zero in this episode for it to be updated not to be zero either its immediate reward has to be one or it has to be transitioning to a state whose value is not zero yeah so what we're seeing here is that um in this case we have state action reward next date and so this is the TD update and what I was saying here is that we've initialized all of them to be zero which means that in order for their value to change from being zero either their immediate reward has to be non Zer or we have to transition to a state whose value is not zero because all of them their current value is zero yeah were you going to guess which state it's upd yeah which one it was well when you're in state state one you have a reward one that's right yeah so this is so none you don't see any reward here until you get to State S1 I'll just highlight here so at that point is when you update your value function that's the first first time that you get to anything that's that any reward becomes non zero so in that case what you get is S1 is equal to V of S1 1 - Alpha + Alpha * 1 + gamma B of s Terminal S terminal is always zero so it just becomes Alpha * 1 so why am I make I do a lot of algebra here I want to do it because um if you work this out and I won't go through it here but I think it's a useful exercise the TD episode TD estimate you would get for your whole value function at the end of this episode is quite different than what you get with Monte Carlo so TD updates after every single tupple every single state action reward next state Tuple and so that means when you reach the end of the episode if you look at what your value function would be and I've written the value function here as um just as a vector but this is the value of S1 this is the value of s 7 so I've just written it as a vector this is what your value function would be it would say oh my current estimate for S1 is one and everything else is zero but if you look at first visit Monte Carlo it's quite different okay and if we make gamma equal to one here which I said it would be it would be 1 one 1 0 0 0 0 why is this because Monte Carlo waits till the end of the episode and then it uses the returns to update any state that was visited once in that episode and the reason that's important is that now actually we have we filled in a lot more things because we knew we observed in that case that not just did we get a reward here but then we saw what S2 got which was also a reward of one and what S3 got which was also a reward of one and the reason I bring this up is that there's going to be different choices about how these behave particularly when you don't have a lot of data at the beginning which may be more or less data efficient or sample efficient and ideas of sample efficiency will come up a lot we'll see that a lot later on um but we'll see it on uh Thursday as well all right so what does this look like in terms of the tree so we go back to our tree which is like expanding out potential Futures what we can see here is that TD is updating the value estimate using a sample of St + one to approximate an expectation so in reality if you're doing dynamic programming you would want to do a weighted expectation over all the next States you could reach weight by the probability of getting there what TD is doing is it's just sampling one of those and that sample is you know an approximation of that expectation so we're going from this to sampling the next state but similar to dynamic programming it is then bootstrapping so unlike Monte Carlo which goes all the way out to get a sample of that value function here we're just plugging in V so this part looks the same so TD does both sampling to approximate expectations and it boot straps by using your existing estim of the value function all right so let's just do a check your understanding um so this is a poll so what I'd like you to think about is how this learning rate might affect things so whether different choices of this is going to weigh the TD Target or more uh more or less than the pass V estimate um and what might happen when your state space is stochastic meaning that when you start in one state you might end up in multiple next States what does that mean about convergence and the implication for learning rates as well as thinking about deterministic markof decision processes deterministic Market decision processes what I mean by that is that oops P of S Prime given sa is equal to 1 for exactly 1 s prime meaning that there's no stochasticity when you're in a state in action you always go to one particular next state so that's a deterministic barup decision process so just take a few minutes now and um look into this and you should be able to select all that are true but if you can't let me know you cannot okay right well then I mean again these are only for your thoughts so just try to like write down for yourself which of these you think are true and then we'll talk about them the second I'll check into these for next time that yes that's what I just heard sorry about that so I'll try to fix that for next time just try to have in your head of which ones you think are correct and I'll ask you to compare with someone in a second thanks for letting me know for all right turn to your neighbor um and check and uh particularly focus on the last two and see if you agree on your answers for those like let's say MVP a single no I'm GNA make [Music] you like construct pry yeah I think people no have any all right great had some great discussions um okay so for the first one this is going to be false because if we have Alpha equals z then we don't care about the TD Target at all just just totally drops out so we're always we never update um in the second case this is true because this means if Alpha is equal to one then this part and this part cancels out we just have this so that means whenever we see an update we always update uh we totally change your estimate potentially the third one is a little bit subtle um this is true does somebody want to give me an example where this might occur yeah yeah um if you have like two states where they just keep pointing at each other is that the cas yes and in particular if there's um if you could would go to either of those state with some probability yeah so so if you're in these are cases so imagine that like I I to think of it is like a coin flip so imagine that you have one state where after this you either go to a state maybe it's 50% probability you get plus one and 50% probability you get minus one and then your problem just resets so imagine it's like a really short problem you start off you get zero re W you transition to a state and then your episode resets so in this case either on that um round you're gonna get plus one or you're GNA get minus one so you're either get plus one or minus one here and you'll just flip back and forth between the plus one minus one plus oneus one plus one one so that's just to highlight that if you do have systems which are stochastic the fact that in your target you are using a single sample of that stochasticity to approximate the expectation um can be bad but that does not mean and I guess this gets to I think was it like that was asking before that like in many of these cases it it's sort of the ne um there's the sort of cases where like it might be possible that this would happen but it won't always so in this case there do exist deterministic systems um where um even if Alpha is equal to one you can converge so again think of something that I like often to think about really small mdps to get some intuition for this if you have a case where there's just a terminal State and there's no more transitions so like you get to some point where you always go to some terminal state dat and then it's plus 10 there there's no more updates then just plus 10 there's no more expectation and in general any case where you know if there's no stochasticity and you're near the end and there's no more stochasticity in that episode um those can be cases real St converge okay great so this is just and I I encourage you um to go through some of the worked examples if you want to just to see some more comparisons over uh you know the difference between Monte Carlo and TD methods in this case just to summarize what we're doing in TD learning we're bootstrapping and sampling we're sampling to approximate our expectation overall the stochasticity we're bootstrapping because we don't want to use a full return um we are in taking V to approximate that can be use an episodic or infinite Horizon settings it is generally lower variance we're doing lots and lots and lots of updates um it is a consistent estimator if your learning rate Alpha satisfies the same conditions specified for incremental Monte Carlo policy valuation I here today only introduce td0 What td0 refers to is you take the immediate reward then you immediately bootstrap and plug in the value of um the next state so we did r+ okay we did r + gamma V of S Prime versus summing up all your discounted rewards for the whole episode in general you could have something kind of in between so you could have plus RT + 1+ gamma squ so you could have something like this so in general you could say like do some sort of combination of like using partial returns and then bootstrapping um there's a lot of different TD methods that kind of interpolate between taking one step and then plugging in the value versus only plugging not using any value function approximation and if you want to think about this graphically it's kind of like thinking about do you plug in V of S Prime here or do you plug it in no way lower yeah is there like is there empal like um estimate of what like a good trade of for for like computational complexity versus uh performance like have they found like a good number for it good question unfortunately in many cases it will be depending on the domain um one thing I think to think about here too is that you can think of this part doesn't require the Markoff assumption right so if you have a system where you're not confident but maybe you're like well maybe it's like I'm willing to say that I'll plug in a markup assumption eventually because it's going to be lower variance um but want to preserve the fact that maybe it's not markof and sort of have a short Horizon um often people do use something in between the two so they often do consider this in between for for multiple reasons but it gives you some of this flexibility it often is um a lower bias it's a great question all right what we're going to do now is think about also how some of these ideas relate to dynamic programming which is what we saw in an earlier lecture because we could use this also for policy evaluation we know how to use it for policy evaluation if we are given the models but some of you guys might have been thinking well we have data now if we have data because we're taking the policy in the environment couldn't we use that to estimate a reward model or couldn't we use that to estimate the Dynamics model and that's what's known as certainty equivalence um approaches so the idea here is that you're going to be getting data as you execute this policy and you can compute a Dynamics model from that data so you could use like a maximum likelihood mdp model remember right now we're in the tabular setting so we can have a parameter for every single state in action so we can just count we can just say how many times was I in this state took this action and transitioned to this next state divided by the number of times I was in that state in action so this just gives you um a maximum likelihood estimate of the Dynamics model and you can do the same thing for the reward model and of course as you might imagine you can do this with much more complicated function approximators like deep neural networks too but the idea is that once you have this model and it's called a certainty equivalence model because we're now going to ignore any error in these models so we have finite data these models will definitely be wrong but let's ignore that for now so once you have this maximum likelihood mdp model you can just compute the value of a policy using the same methods we saw last week because you have Dynamics model now and a reward model and you can see some examples about this at the end of the lecture slides so one of the benefits of this and this gets to question is this is really data efficient so I showed you an example for the Mars rover before where we only updated one of the states with TD learning we updated three of the states with um uh Monte Carlo what this does here is it tries to update it computes the Dynamics mod reward model for all states and actions and then it tries to update all of them okay so it's going to compute a value for every single state now the downside of that is that now we're doing policy evaluation with a full model which is either going to be something like s s a for iterative methods or maybe even worse so it's computationally expensive but it's really data efficient because as soon as you reach any state for which um you get say a positive reward you can kind of propagate that to any other state that you know is possible to reach from there it's still consistent it's going to converge to the right right thing for Markoff models um and I it can easily generally be used for all policy evaluation which we're going to get into yeah sorry what is NSA in this equation sorry great question NSA here is the number of times we've been in that state and taken that action yeah so this is counts okay okay yeah this seems like pretty similar to Carlo I think so um is just the difference that you're Genera like probability as opposed to like calculating G great question um we're going to hold that thought for asking how similar is this to mtic car not it's actually going to be pretty different we're going to see this in a second um we are going to be we're going to be using we are using our data similar to mon we're using the data to we're going to use the data here to compute models and then propagate information but they're going to end up making some interesting different decisions so let's see that now it's a great PR okay so now let's get into batch policy evaluation so I've said like there are these different methods they might have different computational complexity they might be more or less data efficient so one thing that you might imagine doing and we'll see this a lot shortly a lot more next time is well if I have some data how could I best use the data that I have and this comes a lot up a lot in the research that me and my lab do because we're often dealing with patient data or student data or legal data or others where like it's really expensive to get the data or it's costly or could be harmful and we want to get as much information as we can out of the data we have so when I say batch what I mean is imagine that you have a set of K episodes and now what you want to do is you want to do value you want to do um policy evaluation just with that data so what we're going to do is repeatedly sample one of the episodes that we have of those K and we're going to apply Monte Carlo or TD Z to that episode we're just going to do that over and over and over and over again and we'll see this a lot more next time as well and so the idea is to just understand um if we do this given that finite amount of data what will Monte Carlo and td0 converge to in terms of the um evaluation of the policy so let's go through that this this really nice example from sardo to kind of illustrate this we have a really small domain we have two states we're going to say gamma is equal to one there's no discounting and we have eight episodes of experience so in one episode we started in um State uh a we got a reward of zero we transitioned to B and we got another reward of zero okay so in this case you can think of it as like this had a trajectory like that in some episodes we started in state B and we just got an immediate reward of one and we observe that six times so in six trajectories we just happened to to start in state B and we got a reward of one and then in one trajectory we started in state B and we got a reward of zero so first imagine if you ran TD updates over this data in infinite amount of time what do you think the estimative VB would be remember that what it works for there is we have 1us Alpha times our old estimate so I'll just put V plus Alpha * our immediate reward plus gamma times the next state but here it's just terminal because we always terminate after B we always terminate so you never get any future discounted rewards in this case so what the updates look like for TD learning is that you would have 1- Alpha time your old estimate plus Alpha time whatever reward you get in B and imagine you just iterate over these over and over and over again somebody have any guesses of what the reward would be for v um would it be 75 yeah somebody else want to explain how why it's 75 is see some nods yeah cuz in this case we had eight episodes in two of them when we started in B we got zero and in six of them we got one so we just average those rewards and imagine you know we're just doing this many many many many many many times So eventually you would just converge to this estimate being 75 what about for Monte Carlo so let's do Monte Carlo for v b what would that look like so remember for Monte Carlo it would be 1 Alpha * B of B plus Alpha * G where you start in state B is it going to be the same thing is it going to be different so Monty Carlo we're just going to we're averaging over all the returns we get starting in that state is it like so when we start at B so then wouldn't it be 6 over s uh 6 over 8 oh 68 again yeah um but don't we start at a in the first one uh in one episode we start an A but we're just trying to compute the value of you right now we'll get to in a second yeah think ahead yeah so the Monte Carlo estimate so we're just trying to contract so just to what we're trying to do here is we're trying to see will these two algorithms converge to the same thing or not we're going to start off and look at what the value of B of State B in Monte in TD we would converge to 75 because the immediate reward is either one or zero and the discount sum of future words is zero because we terminate and in Monte Carlo we just average over all the returns we get when we've started in B and that is also 6id 8 all right so now this the hard one okay what about V of a what will we converge to in these two cases so let's do check your understanding and you can respond in the poll and feel free to talk to someone next to you and again the intent of this is to think about are these actually Computing the same thing or not in this case and remember this is a different setting than I told you before that both of these things can be consistent but that was if you get infinite data what this is looking at is if you only have a finite amount of data you just go over it over and over again either with Monte Carlo updates or with TD will you converge to the same thing and if you're not sure or you're confused feel free to put that in the poll too for okay there's lots of different answers here um so this is a great one to talk to somebody nearby you so I you see if you're getting the same things and can use our collective intelligence think get old [Music] I'm hearing a lot of good discussion so I'm fir to interrupt but this is kind of a fun one so why don't we um let's start with uh TD so raise uh someone want to explain why it's 75 for TD there are multiple people that got that yeah would you want explain what you and your partner I think yeah so I think if you just like um look so uh we're just looking at like this your name sorry yeah there's like just one episode where the a where we're at a um and so in that episode um the immediate reward is zero but then we have to do plus gamma time um the reward of the next state B um and then V Pi V is 75 we gotten the previous part so Valu to 75 that's right so Monte Carlo is not that estimate so TD gives you 75 what does Monte Carlo give you it's not 75 and again multiple of you guys got it correct oh sorry I I have a question is gamma the same as Alpha here oh good question um uh G no here I'm assuming that gamma is one and it's a great question so I someone else was asking this too so um I'm assuming that Al is set correctly for these to converge oh yeah it's a good question someone else had that too so I'm assuming that we're like we're going over our data in infinite amount of time but we're decaying Alpha correctly as we do that a great question somebody want to explain what Monte Carlo is it's not 75 is it zero yes it is great someone want to explain why it's zero so B to Carlo is zero yeah we've only seen one trory a shows up and that's right remind me your name I'm yeah what said is exactly right so we've only seen one trajectory and I know some other people made the same observation so we've only seen one trajectory where there was a at all we for mon Carlo we just average over all the returns we've seen we that so that's only zero so I bring that because even though um ASM totically all of these things converge to the right thing under some mild assumptions um with finite data which is what we're almost always going to have in reality even if you go over it multiple times they are converging to sometimes totally different things and here is what they are converging to in general Monte Carlo is converging to the minimum mean squared error with respect to the observed returns so it's just going to set it so it minimizes the error between the observe turns it's seen and um H its value so in this case that would be B of a equals z so that is the minimum means squ error td0 converges to the dynamic programming policy for the mdp with a maximum likelihood model estimates so you guys remember how we just talked about certainty equivalence what we were doing here is we're taking all our data it's a it's a the answer you get from td0 if you do this batch process is the same as if you had computed your maximum likely could markof decision process from the data you have and then you did dynamic programming with it okay so this that will be exactly the same as this and so in particular it is leveraging and using the markof Assumption and that's why it could actually chain these things together so you can see here for Monte Carlo it doesn't doesn't know that the value of a has to be related to the value of B in terms of this boot tring relationship but TD is making That explicit it's using the Markoff decision process to say the value of a has to exactly be equal to the immediate reward you get in a plus gamma times the states that I could get into which is always B so the value of B so TD learning is explicitly baking that into the the solution you get whereas Monte Carlo is not Monte Carlo is just trying to minimize the mean squared error for the returns you see so they can end up giving you very different solutions and depending on whether your prop markup property is really satisfied or not you might want one or the other awesome so this just summarized quickly sort of some of the different properties and approaches and just highlights here that temporal difference really does explo exploit this Markoff structure and that could be really helpful if you want to leverage that to get better estimates of earlier States like in the case we just saw so just to summarize we finished going through policy evaluation with um uh tabular settings and then on uh th uh Wednesday what we're going to do is uh talk about control and we'll start to talk about function approximation as well all right thanks see you then