all right welcome back um we're going to start lecture four in reinforcement learning so we're going to be covering today Q learning we're going to cover deep Q learning this result came out in roughly 2014 um and I remember it being a really big deal because one of the big conferences neural information processing systems Deep Mind came and had this like amazing demonstration that they were able to now have an agent that could learn to play video video games really well and an important thing to note here is like they're doing video games from Pixel input so like they're just getting the same input is what we do and what the agent was learning to do is to control the game um through this and through reinforcement learning and so we'll talk today about the algorithm that they did to do that um and we'll build up to that point and this is a short video they show to just illustrate how the agent is learning through direct experience to try to optimize the score and so what it learns in this case is it starts to learn particular strategies that allow it to do really well which may or may not be the same ones is what humans would use and so it was pretty incredible this is one of the sort of most impressive successes of reinforcement learning at this point um particularly at trying to do tasks that humans can do as well um and from Pixel inputs and so we're going to see today sort of how that algorithm works all right so but before we do that let's start with a quick check your understanding um these are posted inside of Ed and this asks you to think about the policy Improvement stage so we're going to be talking today a lot about learning through direct experience um and scaling up towards function approximation with doing that but first let's think about uh when we're doing this what sort of form the policy has um and then as we do this evaluation we do this repeated evaluation and policy Improvement um what happens in this case these are the first two questions on the polls sorry I just joined CL it's on Ed yeah what's your name thanks yeah so if anybody's not as you new the class you can go to Ed you should be able to get to that through canvas for all right we have good agreement on the first one this policy is stochastic under the assumption that um for each state there's a unique Max and it means that the new policy will be deterministic so almost every I think everybody said that correctly which is great um so now so this is the the answer for this but there's some disagreement about the second one so why don't you turn to a neighbor and compare what you got for whether you can compute Q Pi I +1 um by using this to generate new trajectories and remember what I mean by this is I want to know whether or not you can get the state action value for every state and action pair under this new policy so I want to know if you can compute Q of sa under this um new policy so I'll give you a hint if a policy is deterministic how many actions does it take in the same state one right so are you going to get any data about any other actions in that state so can we compute the Q value of all actions in that state no that's right yeah so this is false we can't compute it because if we have a deterministic policy then we only ever take Pi of s so we would only take Pi of I + 1 of s that would be the only action we' ever take in that state because the policy is deterministic it only takes that one that one action and so that means you're just not going to get any data about what it would be like to take other actions in that state and so that's useful to know because it means that if we had models of the Dynamics or if we had if we um if we had models of the reward and we could do some other things then we might be able to compute these Q values but here if we're going to start thinking about just learning this from data and from direct experience that if we have a deterministic policy it's not going to give us any data about trying different actions in the same state and so that's going to introduce some challenges that we have to tackle when we're trying to get data about the world um in order to learn an optimal qy function great so what we're going to be doing today then is try to think about building on what we learned last time about policy evaluation um where we're trying to learn directly from experience to be able to evaluate how good a particular decision policy is how do we leverage that information to then actually learn an optimal policy to actually learn a good decision um you know a good policy uh without having to model of how the world works so we don't have access to an explicit parametric representation of the Dynamics model or the reward model and then we're also going to talk about uh value function approximation and in particular we're going to talk about Q learning with deep neural networks AK DQ which led to this really seminal result in like having machines that can just play directly from Vision to learn how to play games like like Atari um but I'll just pause here in case anybody had any questions or logistic questions before we dive into this all right and we're going to cover a lot today um because next week we're going to start policy gradient methods um and we're doing that because we think that that's a really important thing to focus on um so but there will be quite a lot today uh and you're welcome to reach out I've put a bunch of worked examples at the end in case people want to step through some of those with Mars Rober and others all right so these are we're going to we're going to discuss a bunch of things and we're going to start by thinking about staying in the tabular land so staying where we can write down the value function as a vector and then trying to learn how to make optimal decisions in that case so let's first just talk about the idea of generalized policy Improvement so we've s seen before this idea of alternating between policy valuation and policy Improvement and now we're going to think about that for slightly more General cases of policies so what we just said here is that if the policy is deterministic we can't compute the state action value for any action that's not the policy and so um what we'd like to be able to do now is to have kind of more coverage and to do that we're going to have stochastic policies because if the policy is stochastic then we'll try multiple actions in the same state and we can use that data to estimate the Q function so we're staying in what we're calling model-free policy iteration meaning we're not trying to explicitly build build a Dynamics or reward model we're just trying to directly estimate a q function and once we have a Q function then we can extract from it an argx policy or something else okay and we're now going to be using an estimated Q because we will be estimating Q from data directly from experience all right so this is going to introduce this General challenge of exploration which is we can only learn about the things we try in the world this is just like the you can't know how much better or worse your life would be right now if you're drinking coffee at Koopa same thing like we we can only learn about the actions that we take um and the and so we need to learn about actions by trying them so we need to explore but the downside in general is if we try new actions we are spending less time using our knowledge to make good decisions so you might imagine that you can act random randomly always and that would work for like learning a lot about the world and learning a lot about Q functions but you wouldn't be finding you wouldn't be acting using that knowledge to try to gain High reward so this is known as the general challenge between like exploration and exploitation um how much time do we spend exploring and getting new data about things that might be good versus how many time how much of the time do we exploit our knowledge of how the world works according to the data we have so far to try to make good decisions and this will come up a lot this is there's a really deep questions around here about thinking of um how do we quantify our uncertainty in our knowledge and then how do we propagate that uncertainty into the value of that uncertainty for Downstream decision- making so we'll see a lot more about that later in the course and this is continues to be a really active area of research this is not at all solved um but here we're just going to start to see some simple methods to try to tackle this um challenges sort of balancing between these two things so one of the simplest things you could imagine doing is what's called Epsilon greedy and the idea with Epsilon greedy is you're going to just spend some of the time doing things randomly and some of the times doing things the best way you know how because um you're kind of exploiting that knowledge so if we just have a finite number of actions because right now we're still in the tabular case so we just have a finite number of states and a finite number of actions then Epsilon greedy policy says um with high probability so we have some Epsilon here epsilon's going to be less than one could be like probability could be 0.1 for example so with high probability you're going to take whatever action maximizes your Q value in your current state so you're going to kind of exploit your knowledge for whatever your state action value says and you're going to do that with probability 1 minus Epsilon and then otherwise you're going to take an action at random and so when you pick an action Rand uniformly at random it might be one of the same one as the argmax or it might be a different one but either way the main idea is that essentially you spend 1 minus Epsilon percentage of the time being greedy with respect to your knowledge and Epsilon per of time acting randomly so it's like you know maybe you say like okay I'm committed to trying out new things at my restaurant so once a week I will try a random dish and the other six days I'll pick whatever I like like whatever I've liked in the past and has always been good so this is a pretty simple strategy this is not trying to have a deep notion of uncertainty or trying to quantify that but um but neveress this can be pretty effective so in particular we can prove things about policy improvement with Epsilon greedy policies so what we proved in the past is that if you do policy iteration when you know the Dynamics and reward models you are guaranteed to monotonically improve so each round of policy iteration either you would stay the same in which case you found the optimal policy or you wouldn't change it and in that case um or or you would improve but when we did that proof we assumed policy Improvement using um a deterministic policy and it turns out the same property holds with Epsilon greedy policies so if your policy is always like an Epsilon greedy policy you can also get this kind of monotonic improvement so in particular and I'm not going to do the full proof today but I'll leave it in just for time but um what this shows here is imagine that you have a Q function like you have some policy Pi I and you have a Q function which tells you the state action value for that policy pii and Pi I is eg greedy which means some of the time it acts greedy with respect to that Q function and some of the time it selects an action at random so that's what it means to be um an EG greedy policy with respect to that Q is it it's making those decisions when it's being greedy it's with respect to that Q function so what this says is that Pi I + 1 is a monotonic Improvement um so that V pii + 1 is greater than uh V Pi I and we can prove this here so essentially we're trying to prove in this case that the new policy that you extract through doing policy Improvement which is still an egedy policy is going to be better than your old egedy policy and the main idea is just to say you can kind of also do policy Improvement when you don't have deterministic policies but you have these kind of egedy policies and you could still get monotonic Improvement and now I'll leave that um I'll put that at the end uh for for later post proof okay so this is just to highlight like here's one thing we could do and we're going to see that this is actually going to be a pretty helpful thing to do this is one thing we could do um to try to get data about other actions so so we're not just taking a single action in a single state but we actually have some probability of trying out multiple actions and just to make that concrete if you think back to our Mars Rover example there are only seven states so if you act in it for a long time you'd repeatedly reach the same States what this EG greedy policy is doing is saying like even when you get to the same state you might take different actions and so over time you're going to get data that allows you to estimate the Q value of that whole policy so now we're going to see is how we can use these ideas of kind of EDD policies to actually do control so what I mean by that is that we're going to try to learn optimal ways of acting in the environment and we're going to start we're going to have the same scenario as last time so we're going to either have Monte Carlo approaches where we simulate in the world and then we use that to try to improve or temporal difference approaches which more directly try to use the Bellman and Markoff structure okay so let's start with Monte Carlo so remember what we had before we used to have this Monte Carlo policy evaluation algorithm where on we would repeatedly Loop we would sample the Ki episode so we just like sample a series of states and actions under a particular policy okay and then you could compute the return from each step till the end of the episode and then what you would do is you would bless you you would update for the first time you visited a particular State action Tuple you would update the Q value by a weighted average between your old estimate and then your new Target which was just the sum of rewards you got starting in that state in action till the end of the episode okay so this is cod we often call as like our Target and we were using that because um we knew from Monte Carlo that uh a sing that that what we want to do is really estimate the value of starting in this state taking this action and following this policy to the end um to the end of the episode that uh we can get a sample of that by doing this and that's sort of this sample is an unbiased um approximation to the true expected sum of rewards you would get starting in this state in action and going till the end of the episode yeah we can apply we're going to see that yes exactly yeah so when we thought about this before we thought the policy was like a deterministic policy or that was the easiest way to think that but now the policy could be stochastic and so it could be e yeah great question okay so now policy this policy good well here we'll we'll go on to the next one okay so this was my Monte Carlo policy evaluation now what we could try to do is Monte Carlo online control so what I'm going to do here is I'm going to introduce a different an additional line here at the bottom which says after I do an episode I'm going to potentially change my policy so you can think of this is like my policy evaluation part and this is my policy Improvement and again I'll just write out what this means so what this means is that for for each state each s um the policy for S is going to be equal to ARG Max Q of sa with probability 1us Epsilon else random so that's what I mean by I say we're doing the policy Improvement step is we take our Q function we say either you would take the argmax action or you would act randomly sorry what are we looping over in the outermost Loop is it yeah this would be yes this would be K yeah yeah so this is just you can think of the loop here and I'll write that down um Loop over the episodes so it's like I play one game of Atari and then I update my policy evaluation and maybe I change my policy then I do another round of Atari so I like Play Break Out you know a million times sometimes more than that in some of these cases yeah and right is it yeah it's still yeah I'm going to confused by this last line that you have out there so I mean isn't it implicit that you are using the a new you're using a new Que in the on let's say you're done with iteration number k when you're sampling the next episode you're using the updated ah great question okay so maybe I should so what this says here is that um initially you construct so your Q initially is zero everywhere you could initialize in some ways but your Q is zero everywhere and you're going to slap something that's egy with respect to that now if your Q value is zero everywhere it means that all of your actions are tied you have no information you basically are just acting randomly what this says is that the way we Act is always with respect to our current policy so the first time are you can think of as like motor babbling right like your agent will just like randomly Press buttons it'll move over the screen it'll do that till it wins or loses the game and then it will update its Q value and what this is saying is that the next time you're going to change what that policy is that you're using to act so hopefully it won't Babble quite as much it's like oh well sometimes I hit something and then I got an increase in the points so maybe I'll try to do that action again great question okay so I've have not said anything about that yet I haven't said anything about what the properties are of this yeah in the back and remember your name is it required to do this on policy or could you do this off policy like collect a number of demonstrations and then update later great question yes you can definitely do off policy and we'll see that in a couple slides yeah okay any questions these are great okay so you should be be skeptical that this is naturally going to do anything reasonable but it's certainly something you could run right like something that you could write down in a computer so this is a process so then a question would be and I I put some this is an optional worked example you can go through it just to think about like how it would actually update these so some important properties are how expensive is this does it converge to the optimal qar as well as what is is it is its empirical performance let's think first whether or not we think this is a good idea um and whether or not we think that this procedure here is guaranteed to become a good estimate of um the optimal qar so this is another check your understanding it's on Ed but what I would like you to think about here is that given the process I've just shown you here do you think that the Q value we're Computing is an estimate of the current policy and do you think it will ultimately become qar and if you think it might or might not under some conditions that's fine too you can put that in there yeah like K changes with that's right what is q k QP K is the true State action value function for the py K policy so what is expected discounted sum of rewards if you start in state s take action a and then follow p k yeah thanks for the clarification you my meting why you I have not said anything about whether I very oh oh um oh yeah yeah here I am yes I don't yep why are we doing that we'll talk about that forgot that I had already put that in there yeah we can talk about it okay and one thing just to note here and I think this is question to so just to be clear here as you're thinking about this so this is like an approximation to policy iteration so we're kind of doing policy evaluation and then policy Improvement but it's it's helpful to think about kind of how much time we're spending doing policy Improvement versus policy evaluation so what this is saying here is that you're going to sample one episode and then you're going to do policy evaluation okay this is all just one episode so it's like I'm going to play one like I'm going to play until I win or lose at breakout once under a particular policy and then I'm going to change my policy then I'm going to play with my new policy once and I'm going to change my policy and some of those games might be really long or some of them might be really short yeah so this is like just this is like representing after so then I think I might just be confused when they're just like if I'm playing a game the epis just follow one after the other there just there's just one K episode there's one kith episode yeah so like K is like like I play so if K is one I'm going to play my first game and I'm going to play it until I win or lose until the you know the game so maybe breakout finishes or maybe I'm playing Tetris and like I fail and I died and that and that is one episode and I'm going to use that to then update my Q function then I'm going to change it and say okay my next round I'm going to play differently then I play Tetris again until I fail and then um I see what the total point points are I update my Q function and I repeat and some of those episodes might be really short so maybe the first time particularly for these agents the first time they play Tetris maybe they lose in like 10 steps might be a really short step um later maybe they play for a long time but in general I've not told you anything about how long these episodes are they might be really short but they might be really long okay and I think one useful way I find to think about this is that think about a they're really short like really really short like I take two steps and I just fail I did something really dumb so in that case think about whether Q would be a good estimate of Q Pi K like would it be good if you've only seen two states or would it be pretty bad so why you turn to someone near you I think most people have voted if you are have written something you haven't but why don't you check and see what you think for e e one time so don't C okay awesome I'm hearing a lot of really good discussion but I'm going to interrupt you because I want to make sure we get to dqn um so this is where so one of the reasons that I bring up this particular example is that you know here it's tabular things are a little bit smaller so it's a bit easier to see but essentially what I kind of want you guys to get out of today is that it should be sort of shocking that reinforcement learning works um and we're not going to have time to go through all the Deep mathematical reasons for why it does work sometimes um in this class but I'm happy to give people pointers but so there's several things that are really kind of odd if you start to think about this when when you go through this so first of all Q is not an estimate of Q Pi K it is not because it is averaging over policies that are changing every episode or potentially changing every episode right because um in in fact in general it will be right because we're decaying Epsilon so we're changing Epsilon each round which means we're making things more and more deterministic but in addition to that our our Q might be changing so essentially like I'm just trying a policy one round um and then I update my queue and then I try something again and it's sort of like you know extreme example of this would be like flipping a coin once and deciding whether what it's bias is or something like that like that's just not very much data to do this evaluation and you're also you're averaging this over many many different policies so Q is not an estimate of Q Pi K it's this weird weighted average of all the previous data and all the policies you you've done before like is the last latest k k minus one well but not really right because it's I mean you've averaged in that part that part is from PI K plus one but this old thing was over all of your is like this weird weighted average of all the other policies you've tried so yes it is that but also like that plus all the other policies so it's this weird thing right um the second thing is that we're only doing and I was talking to some people about this we're only doing one roll out to try to evaluate a policy and you might imagine there's a lot of stochasticity like even in something like you know um some games there's like random roles of the dice and stuff like that which means even with the same strategy you might get different outcomes each time um so it' be like if you know you drove to SF and you did it once and there was no traffic and so you're like oh I can always get to SF in like you know I don't know 20 minutes on the highway but for those of you that tried to SF you would know that often there's lots of traffic and so you would need to average over many rounds of doing this to see how good a particular rout is so the weird thing here is that we're just doing like kind of one roll out we're averaging into this weird Q thing which is now going to be this weighted average over all the policies we've done and we have this weird Epsilon thing and it is should not be clear yet that we will necessarily converge to qar like we are getting more and more deterministic over time because we're reducing Epsilon so reducing Epsilon here towards zero eventually we're going to converge towards something deterministic but you may or may not be convinced yet that the thing we're going to converge to is actually qar so fortunately there are some sufficient conditions under which we can guarantee that this sort of thing will converge to qar and it's really it's quite beautiful that this works okay so one is what's called greedy in the limit of infinite exploration or Glee so the idea in this case is that if you can ensure that all state action pairs are visited an infinite number of times meaning the number of counts that you have for a particular State and action pair goes to Infinity for all states and actions this is for all and the behavior policy and what I mean by the behavior policy is is this is the policy you're actually using to make decisions in the world um and it will be important there'll be distinctions between this and other policies soon which is why we call this Behavior policy if the behavior so if you s Paul State action pairs an infinite number of times and your behavior policy converges to the greedy policy which means that ASM totically the action you select in a state is exactly equal to the ARG Max of your Q function with probability one so you're just getting more and more deterministic so then then you were being greedy in the limit of infinite exploration that says that you're exploring everything an infinite number of times you're always continuing to try all actions in all states but you're getting more and more deterministic so this is what it means to be Glee if you have a Glee algorithm um and I'll just note here like a simple way to do this is to do EG greedy where Epsilon Is Res reduced to zero at the following rate so we'd have this so that's sort of a simple one you know and visit and that should hold as as long as you have an egy strategy then you will be able to visit all states and actions so so you're you're going to be visiting all states and actions under this GLE strategy then under that the Monte Carlo algorithm I just showed you four tabular representations will converge to qstar which means as long as you Decay Epsilon at this rate you are actually converging to qar you're getting more and more deterministic you're still visiting all states and actions an infinite number of times and this procedure is guaranteed to ASM totically get you to the optimal Q function which is pretty cool and it should be somewhat surprising all right so that is Glee and that is one one of the reasons why we like to think about EG greedy algorithms because they have this nice property that we can prove that we are going to get an optimal policy even though all we're doing is we're acting in the world and we're getting this data now what you should be thinking about at this point is that all right here's the Monte Carlo approach to doing this there's probably going to be a temporal difference uh approach to doing this and that's what we're going to see now so now we're going to look into temporal difference methods for control okay so one of the interesting things is that um there's going to be two different types of algorithms that we're going to focus on for temporal difference for control and the idea in these settings is that we're going to alternate between two steps again kind of this policy evaluation versus policy Improvement and one of the key things to think about in this case is sort of how much time are we spending doing evaluation versus Improvement and what are we trying to evaluate and what are we improving with respect to so the idea now is that we're going to compute qy using temporal difference updating with an EG greedy policy and then we're going to do policy Improvement in the same way that we saw before for Monti Carlo methods so we can do this EG greedy thing where we are greedy with respect to our current Q value and the first algorithm we're going to see is called sarsa and the reason it is called sarsa is it is State action reward next state next action is short for that s r s sarsa that's an easy way to think to to remember why this method would be called Sara because those are the um tles we need in order to do updates we need s a r s prime a prime to do an update and this is going to be an on policy algorithm and this is related to um what was suggested in the back remind me your name yeah exactly what said so can we also use sort of off policy data and we'll see that really shortly but um Sara is going to be on policy and what we mean by that is that it's going to be Computing an estimate of the Q value of the policy we're using to act or the what policy we're using to make decisions in the world so let's see how it works so in general the the form of Sara is the following um we are going to iterate our Loop is going to be such that we start off so this is the a the we start in some State this is the S we take an action a um we observe reward in the next state and then we Loop and we take the next action still according to the same policy and then what we're going to do is we're going to update our Q function given this topple of sorsa essentially and what we're going to do in this case is it's going to look similar to what we saw before so we're going to have our updated one is our old Value Plus Alpha so this is like our learning rate our Target St + 1 A+ one minus Q of st8 so this is the Target and it's it's going to look similar to what we saw for td0 where we plug in our immediate reward plus our estimate of the expected discounted sum of rewards starting in that next state and one of the important things to notice in this case is we are plugging in the actual action we took in the next state so we're saying what is the expected discounted sum of reward starting in this state and taking this action well one one uh estimate of it is the immediate reward I got plus gamma times the Q value for the state I reached plus the action I would take under this policy next and that's one of the reasons why it's called on policy because it's being specific to the action you would actually take under this policy all right and then after the the then the next thing we do is we do policy Improvement and what we would do in this case is again similar to what we saw in the other one so for all s this just means for all for anybody who's hasn't seen this notation Pi of s is equal to R Max over a q of sa with probability almost abs and then what we do is we update our time step we update our Epsilon and then what we're going to do is just repeat so then we're going to um go then we're going to go to the next we're going to take our next state take an action and repeat this updating so this is called Gap um quick question uh like do we I I have bit confused about setting Pi of s do we say Pi is a deterministic policy that is one of this with this probability and the other one with the other probability or are we saying it's a stochastic policy that can it's a stochastic policy yeah so it's a um it's a stochastic policy at the very beginning it's totally random you just take any action in any state later you're defining it with respect to your current Q value and you're either Being Greedy with respect to that Q value selecting action at random yeah so um one concern that I had was like What if we reach a Dom State and ah good question okay and um this actually came up in another conversation earlier this morning yes so if it is if you reach a terminal State then you just reset so if t t + 2 is terminal reset episode and S so if you ever reach a state where it's Turnal what would happen next is then your whole whole episode just resets you sample as initial state from the world and then you repeat so just like if I like finish my game I failed at Tesla it reinitializes the world so these are still sort of assumed to be continuing processes yeah um I'm wondering what great question so um and what we're going to see in just like a slide or two and you guys are probably half of you at least has probably seen this before we're going to see Q learning and that's where it's going to be off policy okay very quick question uh like the uh when it's s t plus2 uh do we do the step seven or do we skip it for one step and do it in the next one cuz like if it's terminal or in general like if it's terminal if it's terminal you would halt here and then you would reset the whole thing then you would need to take an action observe or or next state and then jump into five so You' sort of have to reset to two great question all right so let's see um well first let's talk about whether this is guaranteed to do anything reasonable um and then we'll going so I've written this out neatly here and then there's a worked example for the Mars Rob at the end of the slides okay so one thing to note here to is that now we've defined a general learning rate so that we have a general learning rate here okay and we also have make sure I keep this in here we're going to keep updating our Epsilon okay so is this a good approach so we can think of a couple different things here we can think of the computational complexity so here after each tle we're doing an update and in fact we know that that's in general only going to um change the Q value for the states and the actions that we're updating so we just deserve doing that that small update each time we don't have to sum over all the states so there's nothing that depends on the state space siid per update but of course we're doing this many many many times does this converge to the optimal Q function so what we have here in this case is we have this weighted um combination between our last Q function and like this new Target and again Q is an estimate of the performance of a policy that might be changing each time point so it's similar to Monte Carlo like we're just like we're constantly changing the policy in this case and so that should feel a little bit concerning and empirically it often does quite well but Q learning is more popular so what are the convergence properties so it turns out that in terms of some of the mathematical formulations this relates really strongly to stochastic approximation and this is a deep literature with lots of really amazing results kind of the the in the finite State and finite action case it's going to converge to the optimal qar for sarsa if your policy sequence satisfies the condition of Glee so we're going to visit all states and actions an infinite number of times and we're getting greedier and greedier over time and we have to put in a condition about the learning rates the step sizes so in particular they have to satisfy the Ruben's Monroe sequence so they have to satisfy these two things which is their sum goes to infinity and their squared this an infinity and we've seen this before and an example of this would be a = 1/t satisfies these conditions so these results really sort of rely on these really nice results from stochastic approximation because it should be a little bit surprising you can think of this as like kind of there's these different there's these mixing processes that are going on because our policy is changing our estimates are changing how can we be sure that it's is essentially going to be stable enough that over time we're actually going to converge to something that's both fixed like we're not just going to oscillate forever and that it is optimal so it should not be at all clear why this would necessarily work um and uh the this is where we sort of rely on those results from stochastic approximation that also had to be extended to think about these particular cases during a number of really beautiful papers from the 1990s so there's those the 1992 and 1994 papers that show this so some really cool results that illustrate why this is possible okay so sarsa for tabular settings um under some mild conditions is guaranteed to converge to qar so now let's see if we can do off policy learning so off policy learning is the idea that now we're going to be trying to estimate and evaluate a policy using experience gathered from following a different policy so so far we've been thinking about like Monte Carlo methods and starsa where we're at least sort of kind of trying to always approximate the value of the most recent policy or averaged over all those policies but now we're going to explicitly be trying to estimate qar at all time points okay so in Q learning we are going to try to directly estimate the value of pi star which remember we don't know because if we knew what pi star was then we wouldn't have to do any of this learning with another Behavior policy pi b so we're going to be acting in one way and we're going to be trying to use that data to estimate the value of an alternative policy and that's what how what Q learning does so in Q learning the key difference is that instead of trying to think about what is the action we actually took on the next time step we're just going to figure out what is the best action I could have taken because we know for the qstar value it is the estimate of the optimal expected reward you could get if you take the the current action and then act optimally from now on so really you would normally like to have something like this so sum over S Prime probability of S Prime given sa a Time B star of S Prime that's what you'd have in like the Bellman equation and what we're going to do here what Q learning does is it approximates that by this Max and that is different than what sersa does because sari used the actual action and CER says I don't really care what actual action you took I care about what is the best thing you could have done there because that's giving me a better estimate of the maximum expected discounted sum of rewards I'd get from that state till the end of time so that is what Q learning is doing so it looks really similar to the sarsa update but our Target is going to be the reward I got plus the best reward that I think I could have achieved from that next state okay all right so then we get an algorithm that looks extremely similar to what we saw before but we have this Max over the next action and then I'll just make sure I think I forgot to write that so whether we're doing Monte Carlo or Sara or Q learning in all of these cases we're interleaving Gathering some data under our current Epsilon greedy policy and then using it to update a q value and because we don't know what the actual Q function is we're sort of doing this weighted approximation between um our current estim of the Q function and the Target that we just put in and we do this over and over and over again so similar to sarsa the conditions to make sure that Q learning in the tabular case so things get a lot more complicated once we go into into the function approximation case but in order for Q learning um with EEG exploration to converge the optimal qar you again need to visit everything infinitely often your step sizes has to satisfy the robins Monroe sequence um and one important thing to notice here is that you can estimate qar without being Glee which is different than sarsa because you're always doing this Max so so even if you act completely randomly so just like infinite exploration not being greedy you can learn qar because in your qar estimate here you're always doing this Max a that's an important difference compared to sersa but if you actually want to use that information to make good decisions in the world you need to become greedy over time and be using that information to actually select the best action according to your Q function so in for EG algorithms with Q learning you normally Decay your Epsilon over time so you're getting more and more deterministic and you're taking your estimate of what qar is and using it to make decisions okay we're now going to go into function approximation I'm just going to pause there in case people had any questions um so foring great great question so um if there are no ties in your Q function like as in like for any action there is or any state there is a uniquely best action it'll converge to a deterministic policy um if there are ties it'll generally you know pick between those arbitrarily there'll be like an infinite number of optimal policies if there are ties in your Q function great question all right so now what we're going to do is we're going to layer on function approximation on top so this was all assuming that we just had this table where we could write down the value for every state and action separately and now we want to use function approximation so we can start to do problems like Atari so the motivation for doing this and I know for those of you who've taken machine learning this is probably clear but it's nice to think about what this means in the context of reinforcement learning so what are the things that we might be storing or trying to to manipulate that might be the Dynamics or reward model the value function the state action value function or the policy and if you were thinking about pixel space you do not want to write that down as like one different value for every bless you for every different um possible image in the world so we're going to want compact representations like what we can do with neural networks so that we reduce the memory we need to like write down those Dynamics models the value function or Q or the policy we reduce the computation and ideally we might even be able to reduce the experience and I think this last point maybe is a particularly interesting one to think about so you can imagine like if you're agent is learning to play an Atari game or play Breakout it um it might want to know that like oh well if these pixels are slightly different here most of the time you might still take the same decision and so then instead of having to learn from scratch what to do in each state you can get this sort of generalization and that could be really important in terms of reducing the amount of data we need to learn to make good decisions all right so how do we do this what we're going to try to do is we're going to essentially do the same thing as what we did before but we're also going to have to incorporate a function approximation step so let's just think about how we would do this if we had an oracle so what I mean by this is we're not thinking yet right now about all the learning and like Gathering data we're just assuming how do we fit a function to represent our Q function so let's imagine that you had an oracle that for any state in action it would give you the true value for a particular policy and that state in action so it would tell you like that's three or that's seven so then you could say okay now I've just got a supervised learning problem I've got input tles of states and actions and I have output values of my Q function and what I want to do now is just learn a function to like a regression function to say given the state in action what is the output and so imagine that you're in a case where like you know we have a continuous set of states and we only have one action then you might just have all these different points and maybe you just want to learn a function that predicts the Q for every single state you just learn like a parametric function or it could be a deep neural network and in general just like in supervised learning the objective is going to be to find the best approximate representation of Q given some weights or given some neural network architecture so we've got like you know some neural net and we're just going to fit this to try to if we had these points but of course we don't have these points and we're going to see how we're going to handle it we don't have these points but this is kind of the intuition is that if you had these then you could do the function approximation step by saying okay well how do I I am going to handle generalization by using like a linear function or a deep neural network to say for each of these states and actions what is the output so just to highlight here in this class generally we will be focusing on methods that use stochastic gradient descent to try to fit these functions and again I expect most of this is familiar for you guys if you've done machine learning if you haven't you can come talk to me or any of the Tas um generally we're going to just use mean squared error and we're going to try to fit a function that minimizes the mean squared error we're going to do gradient descent to find a local minimum and we're going to do stochastic gradient descent um just to you to compute an approximate gradient so in this case I have here is just to write that out really quickly you would have something like this you'd have WJ is just the derivative of this like that so I'm going to take this equation star and I'm just going to take the derivative of it which is going to be two so we're just going to take the derivative of this and essentially that just means we're going to have to take the derivative through our Q function representation like using autoi per deal networks and then we can use this to update our our weights all right so we'll do stochastic gradient descent to do this and the main thing is that that's what we're going to be doing to plug in um in order to do policy evaluation or to do control so of course in general we don't have those we don't have for each state in action what the Q value was if it was we wouldn't need to do any learning we need to learn that from data and so the idea is that we're going to do model free state action value function approximation so just like what we've been seeing before we we're doing model free state action value function um now we're going to actually do that but just do an approximation we're instead of writing it down as a table we're going to write it down with these parameters function parameterized functions okay so the idea now is like similarly we just saw before all these methods either where we use Monte Carlo methods or difference methods to try to do these approximations um now what we're going to do is that when we do the estimate update step we're also going to fit the function approximator so just like in the algorithms we saw before where we do like policy evaluation and policy Improvement now when we do the policy evaluation we're also going to just refit like our whole Q function for example okay so let's see how that could work so for Monte Carlo value function approximation we're going to remember that our return G is an unbiased but noisy sample of the expected return so we can think of us having like this state action return State action return Etc and so you can substitute in those G's for the True Q Pi when you're doing your fitting so let's see what that would look like so in this case remember what we would like here when we're doing our function approximation is that this is the real Q of the policy but we don't know what the real policy real Q value is so we're just going to plug in our observe return so you know we want would like Q of sa but we don't have that so we're going to plug in the return that we just observed and then we'll just do the derivative we'll be plugging that in for our derivative and then we update our weights using that derivative with respect to minimizing the me squitter so this would just be for policy evaluation if you have a fixed policy you would just do this at each time point so after you see um after you get a return then you would update your Q function you do this many many times and for some of you this might start to look redundant but I think it's just useful to see that essentially the structure of all of these algorithms whether it is policy evaluation or tabular or function approximation is extremely similar we are just either sampling an episode or sampling a tle we are going to do one step which is like policy evaluation where we update our estimate of the Q function maybe optionally do function approximation fitting and then we're going to use that to figure out how to act next if we doing control we'll see an example of that shortly okay so that is Monte Carlo okay oops okay all right for temporal learning it's very similar but now we are going to um have this weighted sum where we plug in we bootstrap so we plug in our current estimate of the value of S Prime so this is the same update we saw before this was for tabular cases and now we're going to do it for function approximation okay so let's first just see how we do it for function approxim it's just useful I think when we look at this to think about all the different ways we're doing approximations we are sampling to approximate the expected value over the next state we are bootstrapping to plug in what the value of those states are and now we're also going to do function approximation because we're going to represent the value of a function with some weights okay so we're going to have these weights all right and again we can just do stochastic gradient descent to fit our weight function to represent that value function okay so you'll get something like this where as long as you're if you're in a terminal State you'll restart the episode otherwise you'll just be doing these Computing the gradient with respect to your minimizing your mean sored eror and updating your weights and then the then we're we'll see how we do this for control so for control it's going to be very similar now what we'll make sure to do is we're always going to be using the Q function instead of the value function and we're now often going to be doing off policy learning again like Q learning so we'll again do stochastic gradient descent with respect to our Q function we're going to sample the gradient and we'll have very you know an algorithm is very similar to the one we've seen before so we can either use sarsa where we have our Q function where we always plug in what is the actual action we took next or we can have q learning where we plug in a Max over the next q function and raise your hand if you've implemented deep Q learning before okay so 1% but most people not yeah okay so you going to imagine in general this is any form of function approximator but often this is going to be like a deep neural network okay now one thing I just want to highlight here is that again just sort of being in terms of being concerned whether all this is going to work there's a lot of approximations that are happening here so particularly for Q learning and it's led to what Sarto um the authors of the book that is optional textbook for the class called The Deadly Triad and what they say is that if you are doing bootstrapping meaning that you're plugging in an estimate of what is the value of the next state and you're doing function approximation like you're using like a deep neural network or linear function and you're doing off policy learning where you acting in a different way than the the data you're getting under those cases you may not converge at all like you just your Q function May oscillate you may not converge to um anything and um you are certainly not guaranteed to converge to qar so it's just good to keep in mind that that could occur um I think for some intuition for why this can occur the Bellman operator if you think back a couple lectures ago we proved as a contraction meaning that as we apply it repeatedly we went to this fix point in the tabular setting but the problem is is that like when you do a bman backup that operator is a contraction meaning that if you apply the bment operator two different things their distance gets smaller afterwards value function approximation fitting can be an expansion which means if you take two things and then you try to do value function approximation like you f align to this one and align to this one the distance between two points afterwards can actually get bigger than before you did the value function approximation so there's a really beautiful example of this in a paper by Jeff Gordon um from 1995 I will just Jeff Gordon 1995 has a really nice example of this where you just can kind of visually see when you have these two functions and these like points that after you do this value function approximation you've actually made the distance between them bigger and so that means that you have this thing where you're kind of alternating between something which you know is a contraction and driving you um driving you towards a fix point and something which might actually Amplified distance differences and so because of that it's not always the case that you're guaranteed to converge to a fixed point so this is something important to know however I think it's also kind of a um it's an important part of the history of the field in that in the 1990s there was a bunch of work showing that this could occur that like even with some really simple settings like linear value function approximators you just approximate things with a line um that sometimes you could get these kind of oscillations or lack of convergence and so people were really concerned about using function approximators with reinforcement learning but then what happened is that Deep Mind showed well actually there are some ways to tackle this and we can do really amazing things with it and so I think it's a useful like a useful lesson from history over the difference between like what can occur in maybe some sort of hard you know not ideal cases versus what actually occurs in practice and so we shouldn't let like sort of some of the negative examples limit us from considering what might work in some other scenarios so let's see that now let's see dqn okay so the idea with dqn is we're going to use these ideas to actually play Atari so we're going to take in images of the game we're going to use convolutional neural networks and we're going to um have a really big deep neural network to represent the Q function and do Q learning with it okay so the idea was well we knew that sometimes like Q learning with value function approximation can diverge and 's a number of different issues but one of them is kind of this stability thing so we know that there's correlations between samples your data is not IID which is what you would normally want for when you're doing function approximation and the other is that you have this kind of non-stationary Target thing which is like when you plug in say with TD learning you're plugging in gamma plus sorry R plus gamma times the value of your next state and that value of the next Tate is constantly changing as you get more data so what dqn did is they said well what we're going to do is we're going to use experience Replay in particular we're going to reuse tuples over time and we're also going to get fixed Q targets and both of those things ended up making a really big difference particularly one of them we'll see in a second so the idea of experience replay is to say in general if I think about um states that are nearby their Q function might be pretty similar and if I'm doing lots of updates that's breaking my IID stuff that I want for my function approximation so another thing you could do is just have a replay buffer of lots of all the different tuples you've seen in the past and you just sample from one of those and then compute a Target value and then do stochastic gradient descent and this might be really helpful anyway just in terms of data efficiency because it means that instead of like taking your data and using it once and then throwing it away you keep it and then you can replay it just like how we talked about sort of batch learning last time so an experience replay can be useful because we're both replaying our data and so we can sort of squeeze more information out of it and also we can select from very different parts of the past history which makes those updates more independent okay so this is um and in general we're not going to keep the buffer for all time we might keep like the last million episodes or things like that okay so that's one thing we could do um now the other thing is that if we think about what's happening in this case the way we change the weights is going to be um in general the weights appear here and here and here so this target value is a function of the weights itself because you're using a value function approximation to represent the value of your next state and so the problem is that in general this is going to change on your next update because you've just changed your your weights um and this can also lead to instabilities because if you think of sort of supervised learning you know your XY pairs your y is changing for even for the same X over time because you're changing your Q function okay because this is a function of the weight and so as the weights change this sort of Target value is going to change even for the same input so the second idea is to have fixed Q updates and what the idea here is and so remember this is like when we say the target weight this is it's going to be what's we're using for Target weights is that the weight the the weights or the parameters we're using to to estimate the value of the next state we reach we are going to not update those as much so we're going to have our Target network using a different set of Weights than the weights that are being updated so you can see here that we have a w minus meaning that like we're trying to make this more like supervised learning where we have a fixed output y that is not changing um while we're trying to update our W and so if you think about sort of the example we want to just sort of draw it like this here's our states here's our Q function um right now we'd like to sort of make sure that these points when we're like trying to fit a line that those y's are not changing a lot what we're trying to fit the line and in general because they're a function of the weights themselves they might be moving and perturbing and so what we're saying is no we're going to fix these so you can think of this as just being you know a fixed number for a while and then do multiple updates on this W to try to fit that function and so what that means is we just have to uh we we keep around these Target weights and we keep around um the other weights and this allows us to do this is what the is called the fixed Q updating so if you think about what the pseudo code would look like in this ta case is it's going to look pretty similar to the things we've seen you're going to sample an action you're going to observe order in the next state you're going to store the transition in a replay buffer so you're going to keep track of it then you're going to sample a random mini batch of tles from the past you're going to do something you know keep track of if episodes terminated otherwise you're going to say my target y that I'm going to try to fit in my function approximator is my immediate reward plus the Maxum reactions of my Q function with my target weights so I use my deep neural network to predict the value of that state action pair and then I'm going to do gradient descent on the difference between these predicted y's and my current estimate my current weights so this is just the function fitting part and then you repeat this and then periodically you update your target weights okay and I just want to highlight here there's a bunch of different choices to be made you have to decide like what function approximator you're using you're using a deep neural network um what's your learning rate how often to update the target weight uh how big should your replay buffer be there's a lot of different choices that you have to make all right let's just take a quick second here to see if this Parts give chance to think about this part you may have just SE the answer but that's okay um which is okay in dqn we're going to compute the target value for the sampled State action reward next States using a separate set of Target weights so does that change the computation time does it change the memory requirements um are you not sure put that in here we're now going to maintain two different sets of weights to do our function approximation e right um yep I see almost every converts the the right answer very quickly it is doubling the memory requirements so you have to keep track of a second set of parameters um H it does not change the computation time just changes the the memory requirements so we just keep around two copies they have your deep neural network one with the old weights one with the new ones um and then the queue updating with respect to that is the same all right let's see what that actually does so the kind of the key Innovations for dqn where we are going to use deep neural networks that had been done before but um not with I think this is the first really big example with convolution convolutional neural networks um it's going to maintain these really large episodic replays and then it is also going to have these fixed targets all right so what they have here is they're going to do these series of convolutions output and they're going to Output a q value for each action and and they're going to use that to make decisions and I think one of the things well there's multiple really remarkable things about this paper one is that they got extremely good performance across a really wide set of games so instead of only having a few Benchmark tasks they looked at the whole Suite of performance they are learning a different policy per video game but it is the same neural network architecture and I believe all the same hyperparameters too so the idea with that is to say like could we actually have sort of the same type of architecture in the same way that we don't swap brains when we do different tasks um and but have the same learning algorithm learn to be able to do many different types of tasks and so I think that was pretty impressive that they showed that that was possible so you have the same algorithm same hyper parameters but it could you know learn to do well in many different tasks I think one of the interesting things about the paper is to consider what were the aspects that were important for Success so here's just um a subset of algorithms uh sorry subset of the domains this is six this is a few of the games and they also compare to using a much more simple function approximator and what you can see here is that the Deep neural network is not actually better right like the Deep neural network does not look better than um uh than the linear case so it's not clear that just using a more function approx like um it wasn't just that they used a much more careful function approximator and the second thing was whether they used this fixed q and that helped um so you can see now that they are exceeding the performance of using a more simple function approximator so this idea of kind of keeping things stable is helpful um in terms of oscillations but keeping using the replay was incredibly helpful so they went from like you know three or 10 up to 241 um or in some you know something from either roughly three times as good and sometimes even more like a couple orders of magnitude so it was incredibly helpful to use an experience play buffer and maybe this isn't so surprising because it means that they are just reusing their data a lot um but it was really you know incredibly important I think that's really helpful to motivate why thinking about sample efficiency and re reusing your data is helpful um and then combining these ideas led to even bigger benefits so it was helpful to have both the fixed targets and the replay buffer but if you could only pick one the replay buffer was just enormously helpful all right so as you guys know um uh there's been an enormous amount of interest in reinforcement learning and deep reinforcement learning since there was some immediate improvements kind of within the next year or two um one is called double dqn um uh and that also it's a very simple change just maybe one or two lines and it does increase some of the requirements but for uh memory but it is a really helpful approach so um it tries to deal with the fact that you can get some interesting maximization by it issues and happy to talk about that offline um but so there was a few different immediate next algorithms uh but then there's been enormous amount of work since and I think it really led to huge excitement in how we could couple these with really um impressive function approximators so just to summarize um the things that you should understand is to be able to implement td0 of Monte Carlo one policy evaluation so things like we talked about last time um you should be able to implement Q learning SARS MC control algorithms again in tabular settings you should understand and what are the issues that can cause instability um so things like function approximation bootstrapping and off policy learning and have an intuitive sense for why that might be concerning and then also you should know some of the key features in dqn dqn that were critical and then next week we're going to start to talk about a very different way to do things which is just policy gradient methods it is similar again to this you can see how important policy iteration is it's going to be similar to policy iteration um and it's kind of similar to policy iteration of Monte Carlo and certain ways and directly trying to work with the policy I'll see you then