all right well we work on getting these started I'm just going to write a couple things up about General Logistics can continue to work on this for a second for all right okay great well why don't we dive into this um let's see so I think everybody agreed from the first one which is great so this is true so this is true um there was was a bit of uh dis bit of disagreement about this B and C so why don't you talk to your neighbor for a second and see if that changes your mind or resolves the communic confusion [Music] I mean just we need some all right so um the first one is oh this is false and this is false um so dagger if you think back unfortunately required the human to keep around forever uh they would constantly be getting asked hey for the policy that the agent followed was this an optimal action or not um and behavior cloning does not require knowing the Dynamics model it allows us to reduce reinforcement learning to supervis learning and the idea is that we take the expert demonstrations and we just try to learn state to action mappings and so we can just treat it as a standard supervised learning problem great all right so I think um you know as we go further in the course we get to go to more and more exciting topics and today we're really going to start to see how you know skipping all the NLP side but how do we actually get to reinforcement learning that can do some of the amazing things that we see large language models doing so for example um you know when I Was preparing this lecture I was like please write me a program to demonstrate how rhf Works be brief in your explanations um and then show me the code and within like you know about 5 Seconds it generated me code that used um Q learning other things to generate an actual example of how rhf which stands for reinforcement learning from Human feedback which is how they trained chat GPT amongst a whole bunch of other things to do um so it could generate me a small example of how to do that code that you can run so that's pretty extraordinary this was not possible um you know two years ago well I started offering this class in 2017 so when I first started offering this class this was definitely not possible this only really became possible with Chad GPT so it's pretty phenomenal that we now have ai that can do this um and the question is how do we get there and what sort of RL techniques are being used to help accomplish this so that's what we're going to start digging into now so today what we're going to do is we um are going to continue on from imitation learning and talk a bit about reinforcement learning from Human feedback and then next time we're going to have um a guest lecture from one of the authors of the direct preference optimization work which received uh best paper runnerup at neural information processing systems which is kind of the premier machine learning conference um so he's going to come talk he's one of The Graduate students here at Stanford and this is sort of have become really I guess maybe like it's it's starting to replace or exceed performance on rhf on a lot of benchmarks that's super exciting it'll be great to have him and in fact because everybody here always is innovating which is awesome he was like oh we actually have a new paper coming out an archive like next week that shows how we can extend this doll in all these different ways um so I asked him if he had time to cover that a little bit so there's there's a lot of work to be done in this space to think about how do we better use RL in combination with these incredible function approximators of large language models to create you know a sort of the amazing performance that we could see of a system that could do something like this so that's where we're going um what we're going to focus on today is to continue talking about imitation learning and I think imitation learning is a nice way to build into this because imitation learning is one form of using human feedback to try to train uh try to train reinforcement learning agents and then when we get into rhf well that'll be sort of a different way to leverage human expertise so to start we're going to go back to imitation learning and to talk a lot today about Max entropy um iners reinforcement learning so let's just remember where we were last time what we were talking about when we talked about imitation learning was the idea of taking demonstrations from people and these either could be explicit demonstrations like I show the robot how to pick up a cup and it records all my movements and then you can use that for later training um or it could just be natural trajectories so like you take electronic medical record systems and you just look at the decisions that are made from doctors and we use that um to try to either equal doctor performance or exceed doctor performance so often we just have observation data which may either it's been done in normal sort of business as usual or that is explicitly being given um as a demonstration trajectory and this is just going to be the sequence of states and actions we're not going to have rewards um in general and so the idea was well it might be easier in some cases either because it's just sort of natural data traces that are being generated as they're part of their normal work like electronic medical record systems are um or because uh it's hard for people to write down a reward function that kind of captures all the complexity of what they're trying to um do in their objective so that was one of the motivations for this and we saw a few different ways to try to think about this setting last time including Behavior cloning where we just map things back to supervised learning and we try to learn in a policy directly to match the expert we saw dagger um I'll put that on here too so another thing that we saw kind of in between these two was dagger which tried to address a challenge of behavior cloning which is that when you make mistakes in your supervised Learning System you may end up in parts of the state and action distribution that you don't know you don't have good coverage so we talked about this kind of race car track example where like once you go off you've got a distribution mismatch and we'll hear more about distribution mismatches um in RLS later in the course and there we wouldn't necessarily know what to do and so what dagger said is we have to keep an expert around and then they will always tell us what you should have done so they kind of a coach go back they replay you know how you did on that hockey game what you should have done at each moment um and there there's a lot of really interesting questions of kind of thinking about those counterfactuals and then we thought about this broad question of well could we recover the reward from looking at these demonstrations and this could be useful within its own right to try to understand the you know the objectives that people are using when they're making their decisions for different areas um as well as potentially for learning a better policy or learning the policy and then can we also sort of you know once we have that R generate a good policy or generate a good policy directly so one of the ideas that we talked about in this case is well what is sufficient to be able to accomplish mimicking um so in particular we said well if we want to get a policy that matches the expert that is equivalent to generating trajectories where that distribution over those trajectories is the same as what the expert would have done so we kind of think of this strong relationship between policies to trajectories which also is to like know States and actions because we can think of there being a policy that induces a distribution Over States and actions and two policies that in um induce the same distribution Over States and actions will have the same reward because we're assuming that the rewards are only a function of the states and actions and so we we talked about how people had sort of leveraged this assumption to think about different ways to try to learn reward features so for example if you have a set of features depent by your policy so this might be Mew which could be things like you know um uh how quickly a call service agent responds to calls how many times they use positive sentiment things like that of course in the case of a robot it might be how many times it hit a wall how far it went others any of these sorts of features you could imagine that your reward function is just a linear combination of those features and so we saw so these features are just things that people can come up with for every problem great question so um ask you know are these features like people are writing down per problem um historically yes I think one of the big things with deep deep learning has been like let's at least go as close to the sensors as possible so can we use like just images instead of features on images um but in the case of something like say online marketing a lot of them would be potentially predefined so you know is and what web web pages you looked at and what things you you know search queries you did so you would have to still enumerate a set of features in this case that you're defining your reward over but ideally it's sort of as close to the sensor level of the data you're collecting as possible or at least that often has a big Advantage so what we saw here is that essentially because we assume if if things are linear and we assume it's just like this unknown weight Vector so this is a vector this is a vector um we could say if you could have make sure that your distrib over features is really close and if you bound the norm of the the weight Vector then being really close and features is the same as being really close in reward which means if your policy can induce the same features you can get the same reward this is a recap from last time but it's useful to think about as we go forward so one of the big challenges we talked about last time is that there is not a unique reward function that is compatible with the observed data even if you assume your observed data is optimal so we talked about how even the zero reward is compatible with any policy you might see and so in general it's not going to be identifiable we can't just like say oh if we observe these trajectories and we know the policy is optimal this is what the reward is there's too many rewards that are compatible and so what we're going to spend a lot of time on now is to think about one choice for how to kind of break that ambiguity okay this is where we left off last time and what we're going to um focus on now this um is maximum entropy IRL Gail is also this is the second one is known as Gail this is also a popular poach this is developed by um uh Stephano oran's group here at Stamford uh but we're going to start with Max entropy because also it's there's a lot of other follow-up things that could be useful from this idea okay so we're going to talk about Max entropy inverse RL this came out in 2008 and it um goes first with the principle of Maximum entropy raise your hand if you've heard of this before in the context of probability distributions okay a few people more than I would expected cool all right so remember that the entropy of a distribution P so think of um this is a a probability distribution so remember we've got this this is something so we'd have like some overall I see if you have a discrete State space this is just a probability distribution okay so the entropy of a probability distribution is minus the sum over all of the states the probability of that state Times log of the probability of the state it helps capture sort of you know how distributed our distribution is and what the principle of Max entropy says is that the probability distribution which best represents the current state of knowledge what do we mean by current state of knowledge is it is there you know if we start if we have some previous data the one that we should pick is the the so the probably distribution we should write down is the one with the largest entropy given the constraints of the precisely stated prior data so you can imagine you have your expert data and what this says is that and we haven't talked about what these probability distributions will be yet but what this says is that we're going to try to write down distributions over um we're going to look at trajectories in particular that are compatible with our observ trajectories but otherwise have the highest entropy and so intuitively you could think of sort of if you have some data you want to find probability distributions that are consistent with that but have the highest entropy on um given that they're consistent so we're going to end up with something where you have constraints yeah I don't understand the motivation imation I'm trying to not deploy it's expensive I the what what are we trying to do with invitation yeah what's the motivation for doing this because you already have access to an expert by learn uh well the idea is that you have access to trajectories over the from the expert so you don't have access to the expert at all point you don't have their policy you just have observations so you can imagine something like um if I'm an expert doctor you could look at all of the ways that I do surgery and you could like look at all of my movements and stuff and then what I want to do is have a robot that can imitate that and so I need to distill it and make it sort of you know into an explicit parameterized policy set yeah okay cool all right so this is a this is an interesting idea um this is of saying this is one way to break ties like there's a whole bunch of different reward functions a whole bunch of different ways you could maybe be compatible with the observed data um let's pick ones which have the maximum entropy okay so this is just a choice you could make be consistent great question okay so hold on to that for a second I'm going to say yeah so question was what does this mean like how do we actually make this mathematically formal and and algorithmic and we'll see that in like the next slides good question we're going to write this down in a formal way okay but this is the principle and so this is what Brian zird and his colleagues um thought about in terms of this method okay and I'll just say a little bit about the motivation so um Brian was a grad student at the time at C melon and they were interested in trying to understand Taxi Driver behavior and so what they wanted to do is you know when you're driving there's lotss of different constraints particularly if you're a taxi driver you want to think about distance and potential traffic and you know tolls and all these things and so what they wanted to do is just to take trajectories of people driving through the streets of Pittsburgh um and then try to infer what the reward function was that taxi drivers were using as well be able to have a policy that did as well as like good taxi drivers um so this was sort of part of the motivation and they weren't again they had to deal with this question of how do you you can't just learn unique reward so like let's just try to find something that's got maximum entropy and let's see what this means in this case all right so in the linear reward case what we're going to be interested in or how we're going to think about where Max entropy applies is to say we're going to have distributions over trajectories so we're have distributions over trajectories and we want to find a distribution over trajectories that matches our observed distribution over trajectories from the expert but otherwise has really high entropy so what we're going to what you could be learning in this case is a probability distribution over trajectories that has the maximum entropy subject to the fact that it is a true probability distribution so that's one constraint so this is just subject two you think of subject two or such that but the other ones are constraints and the other is that in this case we're going to say we're going to want to match the features and we saw before that matching the features was equivalent to being able to match the rewards in the case where you have a linear function so in the linear reward case what we want to do is we want to say I've got my distribution of trajectories let's say mu is a function that just takes a trajectory and outputs a set of features and we'll talk about some other choices for that soon and we just want that to match what the features were we observed from the trajectories from D where D is a data set from our experts this is like our from our experts okay so this is how we would write that down now I haven't told you yet how we're going to learn the reward function I haven't even told you how we're going to learn this but it's this is sort of this is where the maximum entropy assumption is being applied it's saying what we mean by maximum entropy is we want to think about getting a distribution over trajectories that is compatible with our EXP data but otherwise has the maximum yeah remind me one more time um when you say distribution over trajectories is does that mean distribution over policies that create that trajectory or is that something else great question that sort of isomorphic so you can just think of it directly as me you know distributions over State reward State action State action Etc or you can think of it as it's implicitly going through a policy that is generating those yeah and we'll become clear too about like sort of where the policies come in great question okay so this is what this would say but we haven't got into rewards yet um and we need to think about how do we go from this to thinking about learning reward model learning policies okay so in general we don't have rewards but if we did have rewards what we would like to do is to get a distrib a policy that induces trajectories that match the same reward as our expert okay so we would like to excuse me get a a policy that has as high reward as our expert if we knew what those were like if we had away like this RFI then we would say we want our distribution so we're let's say we we're going to learn um a distribution of trajectories we want this to be the same as what the expert is and I'll just highlight that here I'm using this P hat for expert okay all right so this is expert okay so this is this looks almost the same as above except for I've said well let's imagine that we don't necessarily have to have um a linear reward function in general we just want to say we would really like that whatever our distribution of trajectories is is that it matches the reward of the experts because we know the experts is optimal so if we achieve this we're good okay so we would like to be able to solve this problem we don't know what R is still so we can't do this but we're just going to look at what would be the solution to this problem so and where we're going to go from this is that we're ultimately going to end up with an algorithm that does something like the following we are going to assume we have a reward function or compute one once we have a reward function we're going to learn an optimal policy and then we're going to update our state or trajectory features to update our reward function and we're going to do this many times so we're going to be thinking really a lot about the relationship between um reward functions St optimal policies optimal policies to distributions Over States and actions distributions Over States and actions to how can we update our reward function okay and we're going to step through all of those steps okay and in the original paper they're assume the Dynamics reward model is known all right so let's step through the first part because I think it's really helpful to see um often when people talk about Max entropy then they introduce this sort of exponential family and it may or may not be clear where that comes from okay so remember that we have this constrained objective so we have this thing here okay all right so what we would like to understand in this case is given a constrained objective if we knew the cost What would be the form of the distribution over to okay because remember what we've got here is we have a Max so what this thing is is this is an objective this is an optimization problem that says the right distribution over trajectories you want is the one that maximizes that expression there and what we're going to do now is we're going to see what would it be like if we if we knew all of those things what would the sort of structural form look like okay and then we're going to use that to to make some other steps okay so now this is just to get intuition over this functional form what we're going to do is we going to rewrite this as using lrange multipliers Okay so we've got P here we're going to introduce Lambda okay and I'm just going to write this as follows over and I think this is illustrative because it'll make it really clear where these sort of structural forms come in that we're going to use all right so I'm just writing down our first lrange multiplier and I suspect most of you have seen this but if you haven't feel free to come up to me afterwards okay we're just rewriting the constraint optimization problem okay so we rewrote our constraint optimization problem as a single equation and now we're going to take the derivative with respect to this okay because remember we want to optimize this so we're going to do D going to do it with respect to our trajectories so we're just going to get log of P of to plus P of to time the derivative of log which is just one over P of to Plus okay and in the third case the third one doesn't have any P of to it just This this term does here so you'll get Lambda 1 R by of to yeah does the summation go away weing yeah exactly exactly what said so we're going to assum we're taking we're try to get what is the derivative with respect to one like this probability this particular trajectory to okay that's why everything goes away and the important thing to notice here is this goes away because there's only a p hat there it was from the expert so that term just disappears it's not a function of P of to at all okay all right so now we want to set this equal to zero set this to zero because we want to find the Max and then we're going to just do some algebra okay now we're just going to exponentiate okay why did we do this cuz I wanted to illustrate that what this means is that the probability distribution over trajectories which maximize the entropy subject to some con subject to some constraints is exactly proportional to this is the proportional side the exponential of the reward for that trajectory okay which means that in general If You observe this you would put sort of exponential more weight on things that have higher reward subject to a constraint that you have probability distribution okay all right so what this shows here okay is that if we want to take this principle of Max entropy then what we end up getting is that the functional form over our trajectories is this exponential it's Pro it's proportional to an exponential okay and that's an exponential family for those of you who have seen this before or seen exponential families so this is like the structural form this is the distribution that maximizes the entropy and so once we know that we can leverage that to now start to try to learn a reward function okay so let's see how we do this because remember we just did this assuming that R RI was known like for a particular RI we're not taking a derivative with respect to F here at all it's just a derivative with respect to P of to okay all right so what this means is that we can think of maximizing the entropy over the probability distribution with respect to is equal to maximizing the likelihood of The observed data under this particular Max entropy distribution okay so I'm going to just write out what that would be here so remember that's what we saw here we saw that if we Max entropy the functional form we get looks like this normalized exponential okay so in particular we'll just write that out again here okay so what we get is we say the probability trajectory probability of a particular trajectory I given some reward Model F is equal to 1/ Z of and I'll Define that in just a second e r to I okay where Z of five is our normalizing constant because we have to have a wellform probability distribution okay so let's say this is structurally like what it looks like and notice that we can also write this in terms of States okay so this is also equal to e to the sum over all the states inside of your trajectory okay where I'm sort of abusing notation a little bit to both use RI of to or RI of State just to mean the reward you get from a particular State or the reward you get from a whole trajectory so notice we can use each of the and this is our thing so so why is this helpful okay so we don't know what the reward function is we don't actually have that right yes but what this means is that since we know what the functional form is of the probability of to under the max entropy principle we can now say okay I'm not going to worry about this part I'm going to assume this is the structural form now my unknown is just F now I'm going to try to maximize the likelihood of my observed data by changing the parameterization of f so this observation and when I say this observation I mean that the um that the probability over tow that maximizes say entropy constrained constrained entropy looks like a normalized exponential means we can now estimate or learn are by by maximizing the probability of our observed data so we're going to treat this as a maximum likelihood problem all right and I'll just note here you know this is a a a really elegant observation this came all the way back from Janes in 1957 so when people were thinking about what does it mean to maximize the entropy of something subject to some constraints they realize that you could make this you could convert it to this exponential family and then once you have that now you have something where um your uncertainty is only with respect to this fi and in fact this type of insight at a very high level will be related to what you'll see next week in terms of direct preference optimization where sometimes we might be able to reparameterize our objective function to be able to get rid of um might call them almost nuisance parameters things that you might not care about directly where you have sort of one parameter you really want to learn okay so let's see how we can do the maximum likelihood so now what we're going to try to do is we're going to try to actually learn that reward function and we're going to leverage the fact that we know the structural form of this probability distribution over the trajectories so what we're going to do is we're going to say we're going to maximize F of log the probability over all of our data this is our expert of the probability of each of those trajectories so we're just saying we're going to tryy to maximize the probability that we observe the data that we did under our reward function okay and because of our structural form we can rewrite this as follows this is going to be a sum so I'm just going to say log of product is this the same as sum over the logs then I'm going to plug in what my form told me that my probability distribution has to look like four my trajectories okay all right so this is just me plugging in that sort of Max entropy form of the trajectories and now I'm just going to split that apart so I'm going to rewrite it the log and the exponential cancel and then I have log of the normalizing term okay now um notice that in terms of this part so notice that this is independent of um T star okay so this is you know independent of all those we end up with two things we have Max over Pi SU over toar and our data set of R minus the size of our data set Times log sum over tow Okay the reason that happened there is this was all inside the sum this sum this was completely independent of tar so I could bring it out the number of trajectories I have in D is just the cardinality of D excuse me all right so now what we can do is take a derivative okay so I'm going to I'm going to call this whole thing j of f because it's all parameterized by my particular um reward function going to take the derivative of that because in general we're going to do everything with gradient descent as usual okay so this is going to look like the sum for all my trajectories in my expert data set the derivative with respect to my reward function minus let me see if I can make this nice and big for this part okay okay so we're going to have two things we have this log so we're going to have to take the derivative of that this godo on the bottom okay and we're going to play a small we're going to observe something in just a second okay so then we're going to still have our SU to e to the R5 to times the derivative so I'm just taking the derivative of that whole term okay all right the important thing to notice here so I just took the derivative of both of these parts this thing should look a little bit familiar okay this is in fact just the exact expression we got for what is the probability of a particular trajectory okay let me just put that in okay so note this is just equal to the probability of to given five let me make sure I put given five right because that's just this normalized exponential divided by that so we're going to have this when you when you put that uh I shall be careful let me just make sure I write that carefully so this is going to go to this because this is equal to e the r t / the normalizing constant so we can move this we can move this outside part into here and then that expression in there which is this is just equal to probability of tow given five okay all right so this back to here okay so we just end up with the following we get the derivative with respect to the reward function for every trajectory inside of our expert data minus the number of different trajectories we have times the sum over all trajectories the probability of that trajectory given by times the derivative with respect to by that point okay and that's our gradient step so what this would say here is if you want to take a step towards um optimizing then what we would do in this case is you could compute the derivative with respect to your word function all right we got to we have a few more steps to go so the next is this is all in terms of trajectories we'd like to get in terms of States so for that we can just observe the fact that as before the probability of trajectory can be broken down into its components okay so this is just equal to tal 1 to length of your trajectory the probability of your a given s is like your policy and the probability of St + 1 given St and a okay this is just a probability of a trajectory and we've seen this before um if we have that the probability of a trajectory is proportional as we've seen to e to Theus RF of a trajectory and we know that we can write that also as equal to e to the minus sum / s inside the trajectory of R Pi of that state okay so then we can think of plugging that in for our derivative and what we get is the following so just the same derivative now but in terms of States instead of the trajectories so this is for all the states in St of your expert demonstrations you take the derivative with respect to that State minus D sum over the states probability of the state given that in the trajectory and then the derivative with respect to that state okay and why is this interesting this is interesting because basically what we're getting in this case is we're getting stuff that looks like us trying to match the distribution Over States um that we see in the data set now when we think of doing this one other thing to note is where do these sort of State densities come by come from so essentially you could think of it as I have some observed States and actions and I'm going to think about under a different policy what states and actions that will induce if you know the Dynamics model and if it's tab so if it's it's if tabular so if you think back for a few weeks ago tabular and the Dynamics are known then you can actually compute the state action distribution directly using dynamic programming okay so and Pi is given then you can actually just directly compute the states and actions so let's see that so you could say mu1 of s is equal to P of ss and then for T = 1 do do do T so this is time indexed okay so this is like again remember a high level what we're trying to do here is we're learning that we are getting um we're trying to match the state action frequencies between our observe policy from our experts and what we induce under our reward function what this is going to say is that you're going to try to estimate a reward function you're going to try to compute an optimal policy given that reward function and then you're going to try to count and see what your state action distribution looks like under that resulting policy if it matches your experts you're done otherwise you need to keep changing your reward function your policy and the resulting State action distribution until they match okay all right so I'm just going to go through briefly so you can see how we could how this is computable right so what this would say is that your distribution of states on the next time step depends on your distribution of states on the previous time step the probability under your policy so this is your policy actually I'll be a little careful there the probability of the action given the state and the probability of the state given the state in action and you can use this then to sum up over all time steps what your sort of average density is for a particular State okay so what this means is that when you're trying to actually compute the derivative of your objective function with respect to the reward then what you end up getting is that you can plug these in and if you have so you can write down this you're going to get so you can see that it's fairly involved but it is definitely possible sum overall your States probability of the states given by T and your reward function and this will simpli y a bit if you have um so let me just write out if your R is equal to just this times some features then when you take the derivative with respect to the FI you just get the features okay so this would mean that Dr 5 S so if linear just equal to your features Okay so I know this is a lot of algebra but what this is saying is that your derivative about how you want to change your award will just end up being a sum over all the features you have inside of your data um minus this additional term okay so you sort of compute all of this with respect to um your observed features and um the features you have in your data set all right so how does this all work when we put this full fully together um what we have in this case is you give as input some expert demonstrations you initialize your fi and then what you do is the following you first compute an optimal policy given that r f EG was something like value iteration you compute the state visitation frequencies you complete the gradient on the reward model and then you update your reward Model Five and you repeat over and over again me just sorry the tight go okay all right I'll just write out what that equation is here so your derivative here would be your sum over all your trajectories inside of your data set your features for each of those trajectories minus the sum over the states probability of the state given your current current parameterization oops and the features for those States so this is under a linear a linear reward which is what they derived it for okay and so this is what you would do over and over again all right so let's pop up a level and check our understanding given all of this what steps in the above algorithm relying on knowing the Dynamics model is it Computing the optimal policy is it Computing the state visitation frequencies is it Computing the gradient or nothing requires it I told you that they did say they assumed access to the Dynamics model and I'll just write out that gradient again right here so let's just take a second to sort of review the algorithm check in any questions we might have about it all right and all the slides are on the web so you welome till I go back though I guess might be helpful to most of them I've just been writing out here but you can also just think back about like value iteration and how I was just starting to show that we could use dynamic programming to compute the state visitation frequencies so remember you probably all remember value it and then this was the type of equations I was writing out so to compute the distribution over the next time step of the states we doing things like summing over the distribution for the previous time step as well as the probability of action given a state as well as the Dynamics model so that's the type of dynamic programming algorithm they were proposing there to be able to do this and then you can also just think back to what we need in value iteration to be able to do this all right why don't you talk to a neighbor and see what you got there's a lot of uh variance e e all right good so this is a nice sort of um reminder of algorithms that we've seen from long ago so the answer in this case is one and two so to compute the optimal policy generally with value iteration you need to have access to both the reward model and the Dynamics model so this one is true is true and then the Dynamics programming algorithm we looked at also required access to the Dynamics model in order to kind of back up and say if you're in this state now what's the distribution Over States you're going to be in the next time step um once you have those you don't need this for the gradient so I'm just bringing that up there once you have all the state once you've computed all the frequencies and you have this you don't need it again so um assuming that you've done these two things you don't need it again for the gradient but it is being heavily used um and as you might imagine that's also a pretty strong assumption so like do we actually know what the Dynamics is um the Dynamics model is for say like a human physician making decisions or a surgeon it seems quite unlikely um you might know it for like Moko or something but uh but probably not in general okay so let me just summarize where these things are um this approach has been incredibly influential um as we said the initial one used linear rewards and assume the Dynamics model is known uh but there was um a lot of follow-up work to this including some really nice work by Chelsea fin Finn um who is faculty here now and have been for all but it's part of her PhD um she showed that you could use General reward and cost functions you know like things like deep neural networks and others so you could use much more complicated um functions and state spaces where you're not going to be able to use dynamic programming to be able to enumerate sort of the um distribution Over States and then also she remove the the need to know the Dynamics model so they had a really nice paper in 2016 showing how to do this with really sort of very general Rich complex State spaces which has also been highly influential but I think this idea of saying like how at a high level that the challenge was what do we do about the fact that there are many reward models that are compatible with people's behavior one thing you could do is you say well the one we're going to learn is the one that has maximum entropy and this provides sort of a a recipe or an approach to trying to tackle that problem and it turns out that can be very effective in many cases and in Brian ze's approach um they ended up using it for trying to model sort of taxi drive cars uh taxi car drivers Etc it's been used in many cases since so let's pop up a level um we're finishing sort of our introduction to imitation learning what we've seen is that um imitation learning is this nice approach where if you have access to existing demonstrations and it might be hard to write down the reward function you could try to learn from those what optimal behavior is to try to match that the behavior you have access to um in some cases it can greatly reduce the amount of data needed to learn a good policy we haven't talked a lot about that um precisely but there's some really nice work on the theory of imitation learning in RL and thinking about some ideas we'll see later in this course around sample complexity of like is it provably harder to learn from optimal demonstrations versus in the RL setting um so there's a lot of really nice aspects for imitation learning the things that I think you should know in terms of going forward is you should certainly be very familiar with behavior cloning because it is a technique that is used very frequently so you can just reduce RL to supervise learning when you have demonstrations um but it's also good to understand what this principle of Maximum entropy is doing how that relates to distribution over trajectories and how that is then sort of formed into a maximum likelihood optimization problem to learn the reward model okay and I I think one thing to notice in this case is like when they did this they are not claiming that that is actually the reward model used by people it is the reward model that is compatible with people's demonstrations that maximizes that you know and a distribution that maximizes entropy so it is not necessarily claiming that it is exactly mention mapping human preferences awesome so now we're going to get into this is you know one example of human feedback or human input to trying to use that to make good sequences of decisions under uncertainty but there's actually a huge number of different ways to do this um and so now we're going to this class and next class we're going to talk some about sort of human feedback and reinforcement learning from Human preferences and I think you can think about this from many different levels you can think about it in terms of how could humans actively try to help reinforcement learning agents that they are saying trained to do something like maybe they want to train the robot how to clean up their counter in their kitchen um and they have a particular way they want to do it and so they might be actively trying to help an agent do a particular task or we might be just trying to align say large language models with our values or intents um and so then could we sort of provide information that's going to shape their behavior across many tasks okay so it is relevant to both of these different types of objectives and I'm going to go through sort of some different ways that people could be using human input in terms of these sort of training so one thing to note is that people have been thinking about this for quite a long time I like this work by Andrea tamas um and Cynthia Brazil from MIT and they had this thing it looks pretty primitive now but um this thing called Sophie's Kitchen and the idea in this case is that you would be trying to teach um an autonomous agent how to do you know like make a recipe or do some basic different tasks in the kitchen um and of course as you can see with this we've come along way in the last 20 years which is wonderful um but the there kind of key Insight here was well maybe we could uh learn much faster if you have a human like instead of having an agent that's sort of trying out things like Epsilon greedy and sort of exploring in the world by itself that's not how humans do it most of the time most of the time we have things like schools or guardians or friends that are giving us lots of feedback and help when we're trying to learn tasks and so their Insight was to say well let's try to do more effective and efficient robot learning by leveraging the fact that you could have a human in the loop that's sort of providing feedback to the robot and in this case I think one thing that's important to note is that the robot is getting two different forms of input that they're trying to maximize they're both getting input from the human and they're getting input from the environment so for example I don't remember if this exactly was in that particular domain but you could imagine something like maybe there's intrinsic um reward if you drop something like a big cost but then maybe they human also says that's good when like you know you stir you make the right recipe so there's two forms of signals that are being used to train so this is an example where it's more like dagger you have a human in the loop and like they are trying actively to help the the robot all the time another version of this is the Tamer Freight work um from Brad Knox and Peter Stone over at UT Austin and what they were again looking at is like well maybe we can train agents to do things much better much quicker if we're willing to be in the loop um and these are all different approaches than the dagger approach in this approach so what are we looking at this was again this was older so this is looking at Tetris um a video game we trying to sort of Stack blocks and um and clear lines and what you could see in this case is a lot of the previous work like sort of policy iteration and it doesn't matter exactly what these algorithms are but there other sort of competitive algorithms at the time were at game three getting nothing like they just weren't clearing any lines um but after while they could start to learn you know much better like they could get many more later and what they found here is that by using human feedback um they were learn they were taking human feedback and they were learning an explicit reward model so one thing you could imagine you could be doing is doing something like modelfree RL where like you're getting signals from the human and you're using that to update the agent's policy but then you drop it you're not doing any parametric modeling of the reward model in this case they are trying to explicitly build a reward model from the human feedback and you could see that they could get much better performance very quickly but you know just like kind of maybe the problem with dagger people aren't going to stay around for thousands of games and so you may not be able to exceed performance at least in this case um of if you allowed the agent to train for much longer but I think this is another example of where so this is sort of a place where they're starting to do modelbased approaches where you are actually explicitly training a reward model from Human feedback and I think it's nice to think about there being at least at least one um sort of Continuum of the type of support that humans could provide really probably is multi-dimensional but at least one which is you could think of if we humans are willing to provide data at all to train RL agents one might be I'm only going to give demonstrations that I'm going to do anyway as part of my normal behavior or maybe that I'll do once and then another extreme is something like this dagger or this constant teaching where I'm like willing to be a coach for my agent and I'm just going to sit there the whole time and one of the things you might wonder is like well what's in between this is clearly a spectrum and one thing that a lot of people have thought about quite a bit over the last 15 years is whether preferences um pair comparisons is that sweet spot so the idea in this case is that you're not going to ask people to do constant teaching you're not going um but you are going to ask them to do a bit of work and in particular you're going to ask them to be able to compare different types of behaviors and which do they like better okay and this is kind of in between on the level of human asirs so one of the first places this was discussed a lot was recommendation ranking systems so Yan Yu who's a professor now at Caltech um together with his PhD adviser Thorston ws and others at Cornell did some really nice early work on thinking of um if you have recommendation ranking systems so imagine you have two different retrieval functions and you put in some query and this gives you this the retrieval function a gives you this series of outputs and and retrieval function B gives you the other and you'd like to learn because you are Google or Bing or something like that You' like to learn which of these two is better and so the idea that they came up with in this case is well we can ask people which one is better and in particular you could ask people to compare say maybe the first um the first um item returned or the second or the complete ranking which one is better and that's something that might be much easier for people to do than to specify a Scala reward function for like how good it is that like CS 159 is returned to your query like is that 17.3 or is that 2006 or is it minus 7 billion like it seems very hard to ask humans to do that but they probably can do the comparison and they can say well it seems a little bit better so that's one area and that was sort of one of the early areas of people thinking about um how could you get feedback a recommendation systems so that we could make them better but there lots of other applications and as you can see Robotics and driving is one that people have thought Lots about so this is worked by d d Zig another one of my great colleagues here and what they were doing here is to think about if you're training a a car to have different behaviors on the road um how do you get input from humans about which types of behaviors are going to be acceptable so for example most of us would probably prefer the thing on the left than the thing on the right because the thing on the left does not involve a car accident but it is hard to write down an objective function for all the things you want to do when you're driving including like if it's healing or if like a car suddenly stops in front of you or you know it's pretty subtle and so what dorsa and her colleagues showed is that um people can do the sort of preference Pairs and in fact she has done uh she and her lab here has done lots of really interesting work on thinking of which preference pairs do you show to people so that you can quickly try to get a sense of what their preferences are to try to sort of respect this human effort aspect so these are just sort of two of the examples of the places that people have thought about this um and of course chat GPT is another and we'll see more about that in a second so in general PA wise comparisons might be in this sweet spot because it's often easier than writing down a a rward function and it's much less work than sort of dagger and having to constantly you know say you could imagine in recommendation systems like what is the perfect response to this query of like I don't know like which courses involve this might be really hard for people to write down but it's easy for them to do the comparisons now how do we think about this well one way we could think about sort of how do we mathematically model this is the Bradley Terry model and as we've seen with trying to understand modeling scaler rewards when we start to think about having people compare preferences often we will still be really interested in understanding sort of a latent reward model so the idea will often be that we assume that people have some reward function in their head that maybe is hard for them to articulate and what they can do is produce preference pairs that are compatible with that underlying reward function now they might be noisy you know a lot of us all make mistakes we all make mistakes and so we're not going to assume that necessarily what I say is perfectly corresponding to my latent reward function but it's going to be noisily related to that okay and so Bradley Terry model is one of these types of models that tries to relate kind of internal preferences over items to how we might compare them all right so let's first start off with a simpler setting before we get to RL of Karm Bandits we're going to see K like Karm Bandits shortly in the course but for right now you don't really need to know what it is except for there's only actions so there's no States right now just like you have K different actions that's all and they all have different rewards okay and what we're going to assume is that a human makes some noisy pair wise comparisons where the probability she preserves prefers bi so Item B compared to BJ is like the exponential model that we saw before our exponential models come up a lot so it's going to be the exponential of the reward for bi divided by the exponential reward for bi plus the exponential reward for BJ okay so what will be the probability that I prefer bi to BJ if actually their reward is identical to me like if actually I'm like ah I don't mind whether I have I don't know like deep dish pizza versus flat I mean I actually do have preferences but um but imagine that I don't so what would the probability be in that case according to this model so my internal reward for both of them is like plus 20 because I really like pizza so what would that be for this probability if the two items are identical 50% 50% right yeah so this is automatically normalized um so 50% at most if I like one thing much more I do like dejure a lot more so like it probably more like that would be say um 100 versus like 10 and in that case my probability would be more like say 0.9 or 0.95 something like that um so this is just a particular model it is noisy uh if you read the P if I put a link later to the reinforcement learning from Human feedback paper they make some additional assumptions of how people make preference um pairs uh on of this model but this is the basic model that a lot of people have been looking at recently to understand how internal latent rewards relate to external preferences one of the important things to note here is that this model is transitive which means that if I want to know what this sort of probability is between I and K so those are two particular items I can deduce it from my probability for I to J and my probability from J to K so you can kind of chain things so this is a a transitive probability model okay so this is was you know introduced roughly 70 years ago um it's a very popular Model H and it came up you know early on in terms of recommendation systems and others so another thing that's useful to think about is okay in this setting where I just have K different actions I can take and I want to understand I want to learn what the reward is for somebody for all of them if I want to think about kind of finding a maximum one you know say like what's the best arm what is the best action I might want to try to understand how under these different preference models what it means for something to be good so in the class so far we've often talked about just maximizing the value function and we want to find a policy that's good now let's just think about if I have you know K arms which of them is best so a condos sort winner is one where for every other item you prefer item I to all the other items so I like of all the types of pizza if I like deep Dr most that's the condur winner okay and it it does mean that it has to that those probabilities have to be you know probability one it just has to be greater than 05 it means I have to beat all of the other options and and I'm bringing these up right now because there's also been sort of some later discussion of how do sort of all the recent rlf work relate to ideas from social Choice um and computational economics about what are we Computing you know what what is the sort of underlying objective we're Computing and and how are we um uh distinguishing between different sorts of of responses LMS could give us so at the second thing so this is a pretty high bar right this means that there has to be one thing that uh beats everything else a cop plan winner is a little bit less it just says it's the winner um if it has the highest number of par wise victories against everything else so that doesn't mean that you have to prefer it to everything else it just it means on average it is beaten it beats the others okay and aort border winner if it an item is a border winner if it maximizes the expected score where the score for item BJ is one if you prefer bi to BJ it's 0. five um if they're equal and it's zero otherwise so it's sort of like this discretization of the wins and loses and typically algorithms for K armed or dueling Bandits again we'll go into what Bandits are more later what they focused on doing is trying to find this like I don't necessarily need to find an item you know say like a ranking system that is always better than everything else I want to find one that on average beats everything else and they often would construct these kind of pairwise matrices where you could think of do these different actions beat these other actions all right so how would we learn these so the question is we have all of these noisy par wise comparisons and what we want to do now is to see if we can extract these underlying reward functions why would we want to do that well once we have these underlying reward functions we could figure out which arm is best or which action is best and in the reinforcement learning case we could try to optimize for that reward function okay so how do we do that what we're going to do is we're going to assume we have endles of the following form we have item one item two so item I item J and mu where mu is one if you prefer the first thing mu is zero if you prefer the other thing and it's 0.5 if you don't care so this is just like a classification task you just think back to your supervised learning um we just have it's it should look very much like sort of a logistic regression task and you can maximize the likelihood with cross entropy okay so we got to map it back to sort of a standard logistic loss where we say these are you know these reward models and in general we're going to parameterize these as like deep neural networks or some other complicated function could be linear just depends um but once we have that then we can uh try to try to find the set of parameters that maximize the likelihood okay so that's how we could fit a reward model when we are given preference pairs um and observe preferences now you might wonder how do we do this in RL because in RL we have States we have multiple actions we have trajectories the idea is pretty similar in some ways to what we are seeing with Max entropy and that what we're going to do is to say well we have a trajectory if we have a trajectory we can think of there being a series of Rewards so the reward of that trajectory is just the sum so I plug in all of those sums and I prefer trajectory if I can get higher reward for that trajectory than the other according to the same model so I essentially just map it back to as if it was kind of like a bandit okay just now that I have like two different trajectories we'll see an example of this in just a minute okay so what do we do we now are going to ask people to compare trajectories we'll use that then we're going to learn our reward model and then once we have our learned reward model we can do po or something with it so this gives us a reward model for our domain and now we can try to optimize our policy with respect to it so let's look at an example so in the reinforcement learning from Human feedback more precisely called dbrl from Human preferences this came out in 2017 and they wanted to train something to do a backflip and what they noticed here is they needed about 900 bits of human feedback in order to learn to do this so let's see what it looks like all right okay so what someone's going to be doing in this case so remember they're trying to train this sort of little um Moko like thing to do a backflip so what they're going to show people is they're going to show them little Clips um and they're going to say is the thing on the left doing a better job of trying to do a back flip or is the thing on the right and they're just getting people to click left right left right left right left right and so they're not having to say what is the reward function for doing a back flip they're just saying I don't know this one looks closer to back flip you know or better okay and so what you can see here is that um some of them are going to be much better at like getting close to doing a backflip right so some of those is actually pretty good and what they are saying is that they only needed about 900 examples in order to train it so that it could actually learn to do a back flip which is pretty good okay particularly if you think back to sort of like um deep Q learning and like the enormous amount of data and training that they're often doing for um say trying to learn Atari Etc which is literally millions okay so this is really cool this is possible to do um and this is something that you're going to be doing in your homework so in homework three you're going to be doing both rhf and DPO I'm really excited about this assignment this the first time we're doing it um so you can actually see how this works so you can see how we can actually learn from Human preferences we're not making you do the human preferences we're going to give you a data set um for for how we can actually train these agents now I know we're almost out of time um but I'll say just a little bit about this I'll probably have a bit of time on Monday before um we have our guest lecture but just I want to give you at least a little taste so this was in 2017 that paper was and it was there was attention to it but I feel like in many ways it wasn't there wasn't a huge amount of work on that until much more recently recently so I just want to share a couple slides from um Tatsu Hashimoto's NLP class so if we just think back I think I showed this slide on the on the very first day of class how is rhf being used for chat GPT what they're doing there is they're getting demonstration data and they're doing supervised learning this is basically what we would call Behavior cloning then they're going to get this comparison data and Trader reward model now in their case they might not just use two you could actually have people rank between like say four or something like that you can extend these models to do that so you train you get labelers to do that then you train the reward model and then you then use po to actually update the large language model now one thing that I think is important to note in this case is that this is all really an instance of kind of meta reinforcement learning in the sense that what they're going to be trying to do here unlike where we've seen like you know you want to train something to do one task like being to do a backflip they're trying to learn in general a reward function that covers kind of all the tasks that people might want to do with large language models and so it's sort of this multitask problem right and so when they do this they're going to give you sort of a new prompt like write a story about frogs and then they will want the agent to do well on that which is likely a task it has maybe never seen before in its data so I think that's also another important thing to note here is that the reward models that are being trained now are things that probably would have been considered UM multitask settings before but now we're sort of lifting them and saying your task is just to do whatever humans want to do with this chat chat GPT in terms of answering questions and so how do you train a reward model that will be good for any of those so we'll continue talking about this next week we'll talk proba either before or after the guest lecture um a bit about how we actually do this uh but it basically just follows exactly along the framework that we've just seen there and I'll see you then thanks