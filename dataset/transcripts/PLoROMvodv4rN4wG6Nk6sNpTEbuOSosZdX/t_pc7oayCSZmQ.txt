all right it should be up in a second you can go Ahad and get started on your refresh your understanding all right when you turn to somebody near you and see if you got the same answers for this this question ask you to think back to what we were learning about last time in terms of posteriors over what the parameters might look like for a multiarm bandit so check with someone nearby you and see whether you got the same idea yeah optim oh I okay we're going to go ahead and come back together um and go through the answers for these all right so the first one of these are true okay because in this case for beta one two where we're weighed more towards um an arm that more frequently gets um something like a zero instead of a one then we're more likely to sample these three parameters um the second one is also true because if you have a flat uniform over all of the different arm parameters you're more likely to keep distribution um and the third is false because when you have a one one prior that's a uniform somewhere between zero and one so you could get a so the true arm parameter could be a zero or it could be a one or anything in between okay and then the second one asks you to think about sort of using Thompson sampling to sample arms um and so the first one is true so given these priors you could sample either of those values for the underlying uh parameter for your beri variable the second one is false so let's assume that the real parameter here is4 and 6 what this um question is asking you to reflect about is that Thompson sampling is not guaranteed to give you an upper confidence bound so up it may instead just select a parameter that is consistent with your prior um and for these particular sample thas it will happen to tr choose the true optimal arm for this round awesome so I want to just let's see if we can make all the a work want to briefly show you this nice example w see if we can make this all right so I wanted to show you this nice example of somewhere where you might want exploration so we've talked about exploration so far in terms of cases like you're um an online Advertiser and you'd like to figure out um which ads work for people um it comes up in healthcare I want to show you an example of an application which we thought about in collaboration with Chelsea Finn and a bunch of wonderful Stu grad students recently so this is the breakout assignment um this is an assignment that's used in Stanford where students actually encode the game so in this case you know you're not the compared to the settings we're at where we sort of assume you have the environment and then you're learning an agent to act in that environment here students are actually creating the code to make the breakout assignment so to S make the game environment and this in generally is often really engaging and fun for students particularly when they're learning to program many people like computer games so this is a really great opportunity for people to learn and could be really engaging um and a lot of different people use these type of assignments so it's not just at Stanford but many many other places including code.org and others use this assignment to try to teach students about programming here's the problem um even though it teaches lots of different sort of introductory computer science Concepts there's a challenge which is if you want people to learn from writing this assignment you need to be able to provide them with feedback and providing them with feedback involves grading the assignments so in this case we normally have like a rubric of different things that the program is expected to do correctly like is the paddle drawn correctly is the ball drawn correctly when you bounce does that respect the desired you know um uh transition Dynamics things like that and so normally just like when you guys get feedback from grade scope someone has to go through and play the game to do this okay and so that is really expensive because that means people people have to figure out okay when the ball bounces here does it actually do the right thing um and then you have to do that for each of the different rubric items so there for example it kind of jittered right it didn't do the right thing so what you can think of here is that essentially someone is manually designing a mental policy a grader is designing a mental policy in their head for how to play this game in order to uncover whether the game Dynamics are correct and so the way we normally do that right now is you know each individual grer figures out how to do that and then they play this so this means that it would take probably around 8 minutes per submission so you can't just do a unit test in the normal way because like you actually trying to figure out how the game behaves in different scenarios where it might take multiple actions to even get to that scenario so if you think about sort of doing 8 minutes per submission if you have like 300 submissions in a course and there's actually many many more people than that that have played this game on code.org or to code this that's an enormous amount of grading time that's an enormous amount of human resource time and that means that some of the people that offer this challenge to students don't grade it at all like just too expensive um and so that means students gets the opportunity of trying to do this exciting assignment but they don't get any feedback back um which can be you know really hinder their learning process so there's a lot of things that make this hard it's you know sort of a stochastic setting there's not kind of simple heuristics and um and there are multiple errors so my student um uh Chris Peach another professor here and I started thinking about this problem a few years ago of saying couldn't we design like a reinforcement learning agent to play this game and what we want is that this reinforcement learning agent can explore the parts of the domain so that we can try to uncover how they're doing and whether the game is coded correctly so we did this work um and then we did an initial approach to this and then Evan Who is the author of this um Extended this to try to think about rubric items so the idea is that instead of having humans graded what we're going to do is we're going to replace humans by a machine learning agent and in particular what Evan did is he built on r i and Chris's initial work and said let's actually phrase this as um think about how we can use meta reinforcement learning and exploration the reason this is an exploration problem is because you want to learn um an RL policy here so that in a new environment you can click quickly use behaviors to grade the assignment and so that's where efficient exploration is coming in so you don't want the staff to take you know 20 minutes to try to grade it you want to as quickly as possible whether for an agent or a human figure out what strategy you should use to play the game in order to correctly grade whether this is a a good environment and so Evan had a really nice nervs paper building on our nervs paper these are both machine learning contributions of how to um well there's a series of papers there's a first paper on how we could do this at all there's a second paper by Evan who was looking at trying to do um explicit um exploration really fast exploration and then we joined forces to think about how we could do fast exploration in this setting and then more recently we published a paper showing that this could actually significantly reduce grading time and actually improve accuracy when you combine this with humans so I just give this as an example to illustrate another sort of exciting exploration case where if you can design agents that can learn quickly and can quickly explore an environment it can end up being really helpful and we'll come back to sort of dream and this idea of kind of meta exploration later in the course um later today so today will be our final lecture on Fast and efficient reinforcement learning and then next week we're going to start talking about Monte Carlo research which was one of the key ideas behind alphago um I hope that homework 3 is going well well feel free to reach out to us with any questions um and feel free to come to our office hours all right so just to remind ourselves about where we are we've been thinking about different Frameworks for evaluating the correctness of algorithms and how efficient they are at learning and making decisions and so far we have focused mostly on Bandits which is this much simpler version of reinforcement learning where the decisions we make don't affect the next state so we saw how to do that for for um for both standard Bandits and basian bandits and today we're going to start to lift all those ideas up to markup decision processes so we did that by Design because a lot of the ideas around sort of optimism under uncertainty or posterior sampling or Thompson sampling can be lifted up to the tabular Markoff decision process case and then all of these ideas also then can be extrapolated up with some care to the function approximation setting so that's where we're going to go today um the main approach is for trying to act efficiently in markof decision processes and we're going to start by focusing on the tabular setting will again be optimism under uncertainty and probability matching or Thompson sampling and we're going to see ideas of how to do that in this setting okay so here is one of it's not the oldest algorithm um to do sort of probably efficient um exploration in tabular markof decision processes but it's um sort of one of the quintessential ones and I think it illustrates a lot of the really nice ideas so this kind of a lot let's just step through it so the idea in this case is that we're going to um be making decisions in a tabular Markoff decision process we're going to be taking actions with respect to some specific Q function that I'm going to Define in a second we'll observe the reward in the state we're going to update a whole bunch of things update that special Q Tilda and repeat the key thing that we're going to be trying to do is similar to what we saw for the upper confidence bound algorithms we're going to think about how do we construct an upper confidence bound on the Q function so that's going to be sort of the key we're going to be doing this is an upper confidence bound algorithm so this is going to again use the idea of optimism under uncertainty and we're going to think about how do we bring this to GPS okay so the key idea in this case is what we would like to do is we'd like to construct an optimistic upper bound on the Q function this is a model based approach which means the way we're going to do that is we're going to try to construct um optimistic estimates of the reward function and optimistic estimates of the Dynamics model it shouldn't be immediately obvious what it means to be optimistic with respect to the Dynamics model and and we'll go through that in a minute in practice what we're going to do is the following the reward is the easiest to start with so in the reward case we're going to maintain we're going to maintain counts of how many times we've taken an action in a particular State we're also going to maintain counts of how many times we've started in a state taken an action and went to a particular next state and we've seen these ideas before for tabular markof decision processes we've used them for certainty equivalent planning um back in the first couple weeks of class so the reward model is perhaps closest to what we've seen for the Bandit before for the reward model what we're going to do is we're going to compute the empirical average over in this state and this action what's our average reward we've seen so far and then we're going to um think of there being an upper confidence bound to that okay what we're also going to do in this case is we're going to U maintain an empirical estimate of the Dynamics model now when we do this we're going to do the normal Bellman equation except for we're going to include a bonus so this part should look familiar to what we've seen for hting which is when we are going to compute a Bellman backup instead of just using instead of like in certainty equivalence we would just use the empirical estimate of the reward function and the empirical estimate of the Dynamics model instead of doing that we're going to include this bonus term this is this bonus term okay and there's there's a few different ways to do kind of modelbased interval estimation I'm picking one here that just sort of uses a bonus term but um I'll talk about some other one there's a number of variants so what this is saying is when I do my bman backup of what is the expected discounted sum of rewards from starting State s and taking action a this is going to try to approximate qar going to plug in my empirical estim of the reward I'm going to use my empirical estim the Dynamics model and then I'm going to add in a bonus and if I have not taken that state in action very much that bonus is going to be really large because those counts of the number of states and actions is going to be really small so this will be large if the counts are small the key difference compared to what we've seen with Bandits before is this is a Bellman back up so we will then repeat this many many times so you do this for all states and actions and then you back up you do this many times so intuitively what's happening here is this is like pretending the the expected discounted sum of rewards you'd get if you start in a particular State and take a particular action is much higher if you have not visited that state in action very much so that's where this optimism comes in you end up adding in this bonus term here and this bonus term will be really large so beta is defined up here this bonus term this is a 1 over 1 minus gamma so if you imagine that all rewards are scaled between zero and one um and Gamma is really small you can think of that as kind of being like H times that sort of special term divided by the square root of the number of times you've been in that state in action so what that means is when you do these repeated Bellman backups it will drive your policy to visit parts of the state in action which you have not visited much okay because those are the parts where you're going to have these really large overestimates probably overestimates optimistic estimates I should say you have these optimistic estimates of how good the value could be in those States and the reason this is important is because it might be um so this is going to work when you're doing sort of a series of episodes or you know you're working in the same mdp for a long time what will happen is that your md you will explore in your mdp and it will drive you to cover the state and action space if you think that you know that it might possibly have good rewards in those places so this will drive exploration and by doing these repeated backups here you're propagating your optimism under uncertainty backwards so that you develop this policy to drive you that way so this is um one of the quintessential algorithms um for doing uh tabular optimism under uncertainty based planning it is also a pack algorithm so we talked about pack last time in pack pack it means it's probably approximately correct I'll just write that out again just to remind ourselves probably approximately correct but now we're going to talk about in particular markup decision processes so we talked about how an algorithm is probably approximately correct if most of the time it makes a decision that is close to Optimal and only makes mistakes on a polom number of times so we talked about that last time we saw that you know you don't have to guarantee this you could make mistakes forever like if you're acting randomly that's not a you know you would continue to make m forever MBI is a pack algorithm so what it says is that it says that let's let sort of script A toote MBI ev's policy at time step T and St denote the state at time t with high probability the value of the action the algorithm takes is at least the value of the optimal action for that State minus Epsilon and it's true on all but a finite number of steps with high probability so this is the number of steps okay and the important thing here is this is a polom in the sets of the state space the action space 1/ Epsilon and 1 over 1us gamma now I always encourage um my research students to plug in for bounds because theoretical bounds are beautiful um but it's nice to know whether or not they are all related to practice so for example in this case you might imagine let's say we have S = 10 and a = 10 okay and you said Epsilon is equal to 0.1 and Gamma is equal to9 all right so let's just work out what that would be that would be roughly 10 3 * 10 to the 9 or 10 12 so that's a lot okay right so what that would say is that we are sure by using this algorithm that we will only make step mistakes on this 10 State mdp 10 to the 12 time steps now I don't know about you but I would hope that in you know a 10- state grid world mdp that we could learn to act substantially faster than that so I use it to highlight that um these bounds while this might officially say this is a pack algorithm they can be um pretty conservative in sort of how many mistakes you might make now in practice often this optimism under uncertainty algorithm can work very well it doesn't say you will make this number of mistakes it just is an upper bound on it but it's good to plug these things in just to sort of see um how tight or not you think it is relative to Real Performance all right so this is a pack algorithm um the paper goes through an interesting proof of it to sort of show the different um components but one of the key ideas is something called the simulation Lemma and that I'm going to go through at least briefly because the simulation Lemma is one of the one of the many core ideas when we think about doing efficient exploration a and the key idea for the um uh the simulation Lemma is the idea that we can relate the accuracy of our models to the accuracy of our learned Q function okay so that's the key idea it's going to say we have bounded yeah I guess I'll just leave that so if we so we can bound there sort of a really if we if we just ensure that we have good predictive models we can relate our error and our predictive models back to our value function so let's do that at least sort of sketch so this is going to be for for tabular settings okay so we're going to assume that we're back in like a finite set of States finite set of actions okay so this is one proof of the simulation LMA um we're going to assume that we have pi as a fixed policy and we are going to assume that we have a Max Norm um on the reward so we're going to assume if you remember back um let me do R1 minus R2 we're going to assume that we have two different mdps so mdp1 and mdp2 and these might have slightly different reward functions and slightly different Dynamics models so remember that if we have the infinity Norm can express this as like this is going to be the place where the two reward functions differ the most over our finite State space so you know if one of them gives the rewards of 1 2 7 3 and the other one gives rewards of like 2 6 1 7 we would figure out for which state do the two rewards differ the most let's assume that's upper bounded by Alpha we're also going to assume that we have an upper bound on the Dynamics model okay so we're going to assume that t of S Prime given sa minus T2 of S Prime given sa is also bounded I'm going to assume that's bounded by Beta okay so that means from the point a view of your predictive models the two mdps differ by you know you can bound the the amount and we're going to show if that's true then we also are going to only differ in their estimated Q functions for a particular policy by bounded amount so what we have is we want to compare what is the Q value of under model one for um State sa and then follow following policy Pi versus Q Pi 2 okay and the reason this is going to be important is because general what we're going to have is the R1 and R2 are going to correspond to our uncertainty so if you think back to the hting inequalities I told you about we talked about how our empirical estimate could differ from the true estimate by a bounded amount so that should make you think about this part R1 could be our empirical estimate of the word and R2 could be the true unknown one and hting can give you an upper bound on what that Alpha is and similarly we can get a bound on the Dynamics model error as well okay so the idea will be to say if you end up plugging in say like an empirical estimate of the reward model and the Dynamics model how far away could your estimate of the Q function be from if you actually knew the true reward model and the true Dynamics model okay so that's why we're we're doing this okay so this is the difference let's just write down what that will look like so this is going to be R1 of sa a plus gamma sum over S Prime T1 S Prime given sa a T1 Pi of S Prime okay- R2 S Prime T2 and just use the definition of the Q function there to write out what is the difference between the two Q values okay all right we're going to Upper bound this okay as follows we're going to just use the triangle inequality first okay so we're just going to say this is less than equal to R1 minus R2 so I use the triangle inequality plus gamma time the difference in the second terms AE in parenthesis okay I just use my triangle and equality I split the two terms okay um remember this we've already said is going to be less than or equal to Alpha cuz we've upper bounded our so so then we have to think about the second term then we're going to do something that we often do in reinforcement learning which is we add and subtract zero um or we add zero and we're going to do that by trying to relate between right now we have the Dynamics model of one thing the value under the Dynamics model of model one and the value function of model one and so now we want to have some in between terms so we can directly think about the difference in the value functions under one particular Dynamics model and the difference of the Dynamics model separately okay so let's what we're going to do in this case is we're going to say this is last Al to Alpha plus gamma and then I'm just going to add and subtract some terms okay so sum over S Prime and I'm just going to use shorthand here so that I can fit everything okay so I'm just going to introduce add and subtract zero careful with my make sure that's clear okay so I just introduced I added a new term that is kind of this intersection term um and I added and subtracted it okay and the reason that's helpful is now I can just think of terms where they only differ in the Dynamics model or terms that they only differ in the value function okay so this is going to be less than or equal to Alpha + gamma * Su over S Prime T1 of S Prime Times the absolute value of B1 Pi Prim as V2 okay plus gamma time S Prime T1 of s primeus T2 of S Prime all right so what have I done there I've just rearranged the terms I just move these two here and then I move those two there and I'm starting to apply um my absolute values a lot to just repeatedly do the triangle inequality all right so what is this this part looks a lot like this thing okay so that's going to be like a recursive term okay so we can turn this part into the following this is going to be this part here will be less than or equal to Alpha plus gamma let's call this difference Delta I'm going to call that Delta okay and then if I do that I have Su over S Prime just T1 of S Prime okay so this could just be like a Max difference in your value functions um and then the second term I have I'm going to use the fact that my value fun funion is upper bounded so here R Max / 1us gamma so if you get the maximum reward at every single time step and your discount factor is gamma this is an upper bound on Vmax so that allows me to take this term out and then that just leaves me with my difference in my dynamic model okay so I have this that was this term okay all right so that's what I have in this case now um and so this has to hold for all states and actions so here I'm going to here the Delta that I've defined Delta is kind of like the worst case error over any of these right so over any states what's the maximum difference between the value functions so that also has to hold on the Q side so we get this we get Delta is less than or equal to Alpha plus Gamma Delta plus gamma B Max beta going to subtract that so now I have 1 minus gamma * Delta is less than equal to Alpha plus gamma V Max beta or Delta is less thanal 1 gamma Alpha plus gamma B Max beta okay what is what have we just shown we've said the worst case error in the value function for the same policy between one model and the other model is upper bounded by 1 over 1us gamma times the error in your reward model plus gamma time your maximum value time the error in your Dynamics model so that is one version at least of the simulation Leva it comes up in lots of different other areas too people use it for a lot more advanced complicated settings but the critical idea here is to say if you can bound your error in the Dynamics model or the error in your value function or or and the error in your reward function that also means that your CU functions can't be too different okay so that's like the that's the main important point of that and so this idea in general is a is a excuse me a helpful one because it means as we explore and we learn these predictive models but better we can be sure that our value function is also simultaneously getting better and better over time and getting more accurate and in the proofs of pack algorithms that's often used to say you can't sort of infinitely learn in particular States and actions and not end up with a value function that is getting more and more accurate okay all right so now I'll pause there in case anybody has any questions before we move on to basian uh markof decision processes yeah uh so here we Define the difference in the value function as Delta but why we could also represent the difference in Q function as that yeah because we end up um we use this this has to hold this is an upper bound to this term and this has to hold for all of the states and actions we could ever be at and that also has to hold for this for any state so you could make a here be Pi of s and that so then yeah should factor of the number of states um because we're summing over all the states or it's um because just from the aor bound I thought that's just for one state for example the Dynamics just for one state is less than or equal to Beta then the proof it's selling over a bunch of States um good question so what happens here is this is just a assuming that given you have a bound on this like for every single like this tells you this is for every single S Prime you have a bound um what will end up coming normally where the number of states will come in is when you start to think about how much data you need to achieve this and you want this to hold for every single state action pair and normally that's where you will end up getting a dependence am Moda of data in order to get get sufficiently small confidence intervals and with your union bounding to make sure all of these bounds hold in terms of this part the state space doesn't appear but this is just for the simulation limit it doesn't then tell you all the way to how many samples you need to achieve this okay that's sort of another part of the proof you could probably imagine already given what you know about hting that you could imagine having some way to compute how many samples you need to get Alpha sufficiently small and the Dynamics model requ you know kind of a similar idea and you can do this for other sort of parametric models too like gin Etc anybody else have any questions on this part before we go on to basium all right let's step on to basian okay so this are some of the so in all these cases you know we weren't using any notion of um priors or any of the things that we saw last time so now we're going to think about how we can lift some of the ideas we saw from last time to think think about this from our cough decision processes so just as a refresher remember um the other way we can think about this a common way to think about trying to do efficient exploration is to imagine that we have some prior knowledge over how good we think different states and actions might be or how we think the Dynamics might work and then what we're going to do is try to use that information to figure out how to act um and we saw Thompson sampling as being one method that was an efficient way to try to make decisions when we have these priors and these posteriors okay and now we're going to think about sort of lifting these ideas to the sequential case so what we saw before is that we'd have these priors over the model parameters like in this case say the Dynam the reward models and that if they were conjugate excuse me then after we would actually observe a reward we had this nice closed form expression for the betas so we could think of these as just being kind of the number of successes and the number of failures and I talked about but didn't actually illustrate that um you can do this Brothers sorts of things like gaussians Etc all right so you might think this should work clearly for the reward part of the um of a markof decision process can we do this in general so this is what we did to straight again to remind ourselves Thompson simply for multiarm for multiarm Bandits involved us maintaining this prior we would sample from that meaning we would get like a particular set of values for our coin Clips like what you saw before we would then act optimally um with respect to those observe reward and update our posterior so now what we're going to do is all a very similar thing but we're going to maintain prior over Markoff decision process models so we could have a reward model and this right now we're going to again start start with the tabular case so we're going to start with the tabular case there's a finite set of states and actions so in this case you could imagine maintaining a different reward model for every single state in action um and being able to sample from it so you could sample like a parameter for every single one of those and we're going to see how we can use that to actually do sort of something very similar to Thompson sampling for the sequential process case Okay so the idea now is that we're going to maintain a prior over all of the Dynamics models and all of the reward models we will sample from that now if you remember what I just showed you in the case of Bandits once we did the sampling of the parameters it was really easy to figure out a decision because we just in the like in the case of the Bui bits as soon as you know that this coin flip you know is going to give you one with higher probability than this other one it tells you how to act for a Markoff decision process it's more complicated because as soon as you see the Dynamics and reward model you don't know how to act yet so you actually have to solve a planning problem so it's like you sample a Markoff decision process once you're given that Markoff decision process then you have to do planning like value iteration or something like that um to actually get your qar once you get your qar then you can sample you can you can select the optimal action given that computed qar so computationally it can involve a lot more work than what we saw in the Bandit case okay then the next question you might have is how do we do this sampling so this is the psrl algorithm it was invented by Ian osband and Dan Russo and Ben vanroy um who's here at Stanford and these guys were here at Stanford when they invented it the idea is as follows and I'll talk about sampling the Dynamics model in a second there's going to be a series of episodes at the very start of an episode given your prior your current posterior you're going to for every single state action pair sample a Dynamics model and sampler a reward model given that you're going to compute qar for your sampled mdp once you have that sampled mdp you are going to act according to that policy for the entire episode so this computes a qar for the entire episode you're then just for T equals 1 to H you're going to assume your episodes are finite going to act according to your qar observe your word in your next state you're going to repeat okay at the end of the whole episode you're going to take all of the data that you just got God and you're going to update your posterior so for the reward model it can be very similar to what we saw last time where you know you just sort of update your counts um for the Dynamics model it's probably um it may not be clear what you would do in that case so in this case what we would often probably choose to do all right let's just WR up here so what we'd often do is we'd use a DL model a DL model is a conjugate prior for multinomial okay multinomials is what we can use for our normal Dynamics model here because a multinomial allows us to express what is the probability of going to any of the next States given this current state in action so in general we would have one multinomial for each day in action pair we are now in the basian setting and so now so we would have one per sa pair okay and this specifies our probability distribution over all the next states that specifies P over S Prime given s and a for all S Prime okay has to sum up to one that's the multinomial part the part where we're being basy in is we're assuming we don't know what all these parameters are and so we have a prior over them and the dlay is a conjugate prior which means that if we start with the dlay over multinomial parameters we observe something so let's say we let's say we're interested in understanding what happens when we're in state one and we take action one and we observe that we go to S3 seven times and we observe we go to S7 three times well I'll do two times what that means is that at the end of that episode we would use that data to change our durle distribution over those multinomial parameters very similar to what we saw for the beta distribution okay and I don't expect I mean it's an interesting thing to do but I don't expect you to um do that in this class you know some of you might want to for part of your projects but the key idea here is that it is conjugate so it means that the posterior you get are in the same family as your priors and so you can use this to sample multinomials essentially it's just sampling Dynamics models so we do this in this case um and we do this over and over again and like the really key things to notice here compared to what we were seeing before is that we have to sample this entire mdp and we have to compute its optimal value before we act so we do all of this computation before the start of an episode and you might yeah I'm a about this sampling the mdp but then sampling Dynamics and reward models part oh this is just explaining that okay this is like a comment you sample the mdp what this means is just for each of the state and actions um yeah good good clarification question to sample mdp what I mean is that you're going to define the mdp so that means we can completely specify an mdp given a known State and action space and a discount Factor by specifying a Dynamics model for every single state in action pair and specify model okay and then we'll have to compute the optimal action now one thing that you might wonder about is is it important or necessary that we do all of this once per episode okay so when we talked about um basy and bandits after every single observation we updated our posterior so we would you know try buddy taping the toes and we'd see that that helps someone recover and then we would update our prior this is a bit different right it we are only doing this every eight steps now you might think maybe that's computational you might think that that's being done for another reason but it's something that um that's an interesting thing to think about let me see if I have this on the next slide yeah so let's do a check your understanding and then I'll give you talk a little bit more about um why why this is done in psrl so this asks you to think a little bit about sort of doing St exploration in mdps and in Thompson sampling in the algorithm I just showed e want you compare your answers to someone near you e okay thank you okay yeah so now it should be back on what I was saying is that in um Maria's uh deo's work she was thinking about concurrent reinforcement learning which is something we've also thought about and for this much more realistic setting the idea is whether you might need to coordinate exploration and how frequently should you update now one of the challenges of this setting even before you can get into before you get into concurrent reinforcement learning is that if you update your prior a lot within a task like within a single episode you're essentially sampling different mdps within the same episode the reason that can be bad is it now is going to totally change your behavior and there may be some cases where you essentially thrash so let me give an example so one of the sort of canonical hard Markoff decision processes that people talk about is a chain okay it's really just sort of like an illustrative one and there are lots of different slight variance of chains um and the idea is that you might have something I've shown stuff that's similar to this before where you know on one side or on the other side oh it's not reconnecting just for in the back there to reconnect um that you would have high reward on one of the other sides what you could imagine in this case if you were thinking about it being like a basy and Bandit is that some of the times it might pick a Markoff decision process where this is the good the best state and some of the time it might break a markup decision process oh it's still not showing on the there um thanks hopefully that'll come up um and some of the time it might pick one that is here so if you start off acting let's say that you first sampled an mdp where this is the best state you do your planning and then your agent's going to start going this way okay let's say You observe that there's some zero reward here and your Thompson sampling updates and now it says hey this is the best date okay okay cuz you just have some prior over the model parameters and so your agent turns around and it's like oh I shouldn't go this way I should go this way okay and then as you're doing that it's getting more rewards and it's updating its posterior and so then it samples again and it's like oh this is good and so it can Rel lead to this kind of thrashing Behavior because you're it's sampling a new Markoff decision process each time and so your agent can end up kind of toggling back and forth between its sort of ideas over which mdp it's in so it's for this reason that often you will end want to essentially commit to the Markoff decision process you're in for the whole time you don't always have to do this but that's one of the reasons why this can be helpful this is uh this commitment is also in like the 2013 nerves paper that we well not paper but the algorithm that we saw earlier right they both commit um this yes yeah this is yes this is the so far sorry this is just exactly the same as the psrl algorithm I'm about to tell you about the seed sampling but yes this is just in the 2013 so yeah exactly this is the um in psrl itself it commits and this is one of the reasons for that um and so in Maria's work she sort of discusses some of the important benefits of it and then she thinks about how would you actually maybe try to couple and coordinate exploration if you have many agents that are going through the same environment as at once and it's for sort of in some ways like it relates to this idea too you might want everybody to sort of commit to exploring different parts of the space because if you have you know many agents in the same domain you might want to say you're going to think that the best reward is here you're going to think the best reward is here go explore and then we'll sort of unify our poster afterwards so she has a nice demonstration of that and then she extended it to the Deep learning case shortly afterwards but see if I can play that e e e e okay So eventually it happens right but then you can get to concurrent ucrl where in this case it's much um you can start to if you don't do something smart again this can end up being not very effective and let me just see if I can skip ahead to the last part seed sampling okay good okay so seed sample compl in her case is what they're doing when they essentially do concurrent reinforcement learning and you might have even missed it because that part is really fast okay so I'll move it to this just so I can talk over it at the same time okay so this is seed sampling which is what their idea was and this s of just talks again about doing strategic coordinated sampling so you can see in this case we're leveraging the fact that you've got concurrent agents that are exploring the environment they're committing to it but they're committing to it in a way that they coordinate that so you don't get all of the agents so here by 324 all of the agents have shared information about where the cheese is and everyone solved all right so that just illustrates why you both need sort of this committing to a particular exploration strategy and then if you're in the case where you also have concurrent agents which is very realistic that having this additional um coordination is really helpful now I think one of the interesting things to note there is that this is a nice place where there's some is it connecting hopefully it'll connect in a second where there's an interesting disc um disconnect between Theory and experiment it's still not maybe there's like a problem with a connector we'll try to get that fixed for next week um there there's a disconnect between theory and practice because theoretically you don't need to do this exploration so we have a paper from 2015 showing that if you don't do coordinated exploration it's still totally sufficient um you can still get basically almost a linear speed up but oh good finally game yay all right so that covers sort of how you can do basian exploration um in and sort of optimism under uncertainty in the tabular Markoff decision uh process case but of course what we'd like to be able to do is to do this for much more large State spaces and realistic problems so this is a very much an ongoing area um again you'll see this sort of similarity to the types of ideas we've seen before very popular ideas or optimism under uncertainty and Thompson plan they're not the only ones but they're probably the dominant strategies people try to use S I may have just not caught this but like what spe like what is actually different between the two algorithms like what is the difference in like C samp Point like between 2013 and 2018 yes so two things one is that um the psrl does not think about concurrency so they just assume there's a single mdp you have a single agent in it the other case assumes you have like M agents all in the same mdp so like the like the mice trying to find the cheese there's not just one Mouse there's a whole bunch and the idea with seed sampling is also to think about how do you choose which mdp the H sync they're in to distribute the exploration okay in the other case you don't have to do any coordination because there's only one agent okay good so in terms of generalization we're going to think about this the reason why this starts to get more tricky is a couple things one is that for optimism under uncertainty means we have to have a notion of uncertainty and it just gets much harder to represent uncertainty when we have deep neural networks um similarly for Thompson sampling as we start to move up to really complicated domains we need posteriors over really complicated settings and that's also computationally challenging and hard to approximate so let's first start with contextual Bandits um and some of you guys will probably be doing some of this for your project so instead of having our multi Bandit now we're sort of halfway between a markof decision process and abandoned so we're going to assume we have States but the action we take doesn't influence the next state and so now if we think about rewards we'll have a reward per action and state and just like we what we've often done before if we have a really large state in action space we're going to assume that we use some sort of parametric representation to model the relationship between State and action and output rewards perhaps not surprisingly there is an enormous benefit of doing this so if you think about um a setting where this is the number of arms you have okay if you did something like upper confidence bounds and this is Regret regret is on the Y AIS so if you did something like upper confidence bounds and you have a th000 arms and 4,000 pools sorry you have a th000 arms and then um you're you're pulling these over time so this is I think just regret after a fixed number of time steps unsurprisingly if you have a lot more arms to pull you'll have a lot more regret because in Upper confidence bounds in the things we've seen so far um you don't share any information across the arms if on the other hand you use something like linear UCB which assumes that your arms uh are represented by a set of features so showing someone you know like a trump campaign today and a trump Campa a different Trump campaign tomorrow might have the same effect um because they're going to have a shared set of features about Trump at least would be one thing that would overlap you can leverage that structure and so what you can see in this case is that if you leverage sort of like say a parametric lar representation in this case um even as you scale up the actual number of arms if your parameter space is still the same then your regret doesn't scale badly so for example so this is K but you know your Theta in this case your Theta in this case might just be low dimensional so we might have sort of a Theta which is in Rd so we have a d diens representation okay and this just shows that this can be really helpful like in general you want a leverage structure so one common thing to do is to model the reward as a linear function um of course this could be built on top of a deep neural network or on top of a large language model or something like that you can often just use some really complicated representation of the state in action space and then say for the last layer my actual reward is going to be a function of these complicated features um producted with some Theta parameter and one common thing is to assume that it's just this linear function plus some noise and the nice thing about this is that if your features are interpretable then your reward function is also very interpretable because you can just think of like relatively how much do each of those features contribute to your reward all right so one thing to think about in this case is in these settings um well I'll go a little faster this part cuz I want to make sure we get to the mdp part too but when you have this even if you know you have kind of a linear set of models you can use them to represent sort of more complicated functions because let's say Technologies again so let's say this is your reward model okay for three different actions this is A1 this is A2 and this is A3 um and this is what your reward is and this is your state space okay so let's imagine you had a linear representation then you could represent policies that are just joint linear because you could if you were taking the max here this is what the value would be of your policy because it would say A1 dominates for this part of the state space A3 dominates for this part of the state space and A2 dominates for this part of the state space so linear ones I guess the main point here is that even if you have a linear Pol linear reward model it doesn't mean your policy has to be linear your policy will be disjoint linear it can be made up of these sorts of functions okay so it's it's fairly flexible okay how would we work in these cases well in this case what it means to have uncertainty is we need to have uncertainty over this linear Vector um so we want to capture uncertainty over Theta through some sort of uncertainty set and there's been a lot of beautiful work to try to quantify the types of uncertainties we have through things like um uh the elliptical potential themma and things like that which give us basically just sort of an uncertainty set over vectors okay and you can do this in a computationally tractable way and what this means is it gives us a principled way to get an upper confidence bound on the reward function given that we have uncertainty over linear model and this was shown to be very useful for news article recommendations about 14 years ago and you can also look at chapter 19 so these are really useful this is one way to sort of represent um a contextual Bandit setting when you have you want to handle generalization we'll now talk briefly about how you might do this for markof decision processes okay so if we think back to the MBI algorithm for finate Satan actions we have to modify a few things okay so if we think about this we were keeping track of counts and we were doing this we were building a model SE separately for every state in action so this count based term here that we're using as a bonus um we've already seen how we might be able to do Q functions with like deep neural networks um but the big problem here is the count base bonus like we have an infinite number of states if you think about Atari or something like that you certainly don't want to count you're mostly only going to see one Atari screen once ever um and so these sort of count-based bonuses aren't very realistic and so we're going to need ways essentially but you know why do we have the account-based bonuses we have the account-based bonuses to try to quantify our uncertainty over how well do we know the reward model for this particular state in action and how well do we know the Dynamics and so one of the ideas when deep RL came around was to think about could we lift this idea and try to quantify our uncertainty in the deepl setting okay so we're going to need to move beyond having these very simple counts to think about something that's sort of a higher level representation of that now if we could get that and I haven't told you how we can get it yet you could imagine that a lot of the algorithms we've seen before could be extended fairly easily so in particular um if you think about something like function approximation with q-learning we could imagine just adding some sort of bonus term in here so instead of having our empirical reward plus gamma times you know our Target like our our observed NE state in action with some you know parameter weight we could just plug in some bonus that's kind of what mbab is already doing it's just that our bonus before was determined by our counts and now we need some other way to lift that so we can do that for much more general settings but once we have that we could imagine plugging it in here okay so there's a lot of different approaches that have been developed to try to think about something of sort of density or quantification of how much um how many visits we have or how much certainty we have over different parts of the state in action space so one of the things that Mark bellam and others did which was pretty successful is they tried to build sort of pseudo counts over um parts of the state and action space so you can imagine maybe you've been some particular rooms in a video game many many times and so you try to essentially reduce your uncertainty over those there's all sorts of important details here around like whether you normally in mbib every round you update all of those counts in reality if you think back to deep Q learning we maintained a buffer of State action rewards next States now you would need to include those bonus terms in there too and if those bonus terms are changing how much do you update your buffer just to give you a sense of some of the different wrinkles one has to think about okay but the high level important thing is that this matters a lot so in Mont Zuma's Revenge which was early on considered one of the hardest Atari games probably still is um if you did search a standard dqn for 50 million frames which is a lot it never got past the second room it just with Epsilon greedy exploration it was not strategic um it just got very bad performance but what Mark B and others showed is that by incorporating sort of a a notion of count based lifted to the generalization case you could do far far far better okay so that's just to highlight that there are ways to lift up this notion of sort of optimism uncertainty for this type of setting there is similarly ways to Thompson sampling so um we've done some work there where we think about sort of particular representations and parameters um Ian osband who introduced psrl then tried to lift it up to the Deep Q learning case they did it where they were just bootstrapping samples as an approximation um that is a pretty um pretty coarse um approximation of uncertainty something else that often worked pretty well surprised ly well given how simple it is is essentially to do something just at the last layer so at the last layer do something like basy and linear regression to try to get an uncertainty estimate and then sample from that so this is a pretty simple thing um one could try there's a lot of work to do this um let's go back to thinking of other really recent approaches which try to think about doing this not just for one task but many tasks where you need to do generalization so early in this lecture I introduced the dream algorithm to you which was um we later Ed to actually do grading of the breakout assignment the notion in dream is that you have many different tasks and you're going to learn how to explore in them efficiently so that was one example where we're now really thinking about how do we develop efficient exploration strategies by leveraging structure over the tasks where an agent is going to do a series of tasks similarly in some of our recent work we introduced decision pre-trained Transformers um this was again a metal in case the idea is that your agent's going to do a series of Bandit problems or a series of RL problems and we want to learn how to optimally explore in those settings so I'll just show you briefly kind of how it works the idea in this setting is we're going to use a pre-train Transformer one of the interesting things is you can map reinforcement learning to supervised learning similar to behavior cloning but instead of relying on the data you collected in the past if you can compute what would have been the right action to take there you can train it to predict that optimal action it turns out that when you do that we can exactly map that back to doing the equivalent of Thompson sampling so in all the settings for which Thompson sampling has theoretical guarantees this decision pre-trade Transformer can inherit those guarantees which is pretty cool the nice thing too is that empirically it can allow you to get take advantage of structure that is present in your domain that you didn't have to code so let me just give you an example of that so what I showed you earlier in this lecture is that if you have a domain where you have some linear structure if you're if you give that linear structure to your algorithm then you can do quite well so that's the GRE line here so this is the amount of data you have over time and this is your cumulative regret lower is better so most historical algorithms have assumed you give that structure to your Bandit like you know you write down oh there are these 300 features that you need to pay attention to news articles and people to figure out what the reward will be if you give it that structure and that structure is right you often do pretty well you could not leverage that structure and you would get something like this okay so this is a Thompson sampling algorithm which just assumes that it doesn't have that linear structure one of the cool things that we found by this approach is that in this setting if you really have a linear structure in your domain and you're doing many tasks and all of them have this linear structure what our decision pre train transformer will learn is that even though you're not telling it it will realize it can more compactly encode that structure and so when you deploy it on a new task you will get Behavior almost as if you gave it the unknown structure so I think this is really interesting because often one of the brittle aspects of machine learning is that we had originally wrote down these sort of representations and of course one of the really amazing things for deep learning is that we're trying to not write down specific representations as much and get much closer to to the input raw data and this is illustrating that in terms of sort of sequential decision- making and meta exploration for multiple tasks we can do something similar here where we can sort of inductively learn that that's a more compact way to represent the domains and get this sort of much more efficient exploration in new tasks all right so just to conclude um we're wrapping up our notion of sort of data efficient reinforcement learning today you should understand sort of this tension between exploration and exploitation in reinforcement learning I haven't used these words they're not great words so I don't use these um I haven't used these a lot of there but exploration meaning you're taking time to learn about the domain and exploitation meaning that you're leveraging that information to make good decisions in the context of reinforcement learning um you should be able to Define and compare different sort of Notions of good whether like empirical convergence regret and pack um you should know for the algorithms we've talked about you know do they have for example does greedy is greedy um sublinear regret which it's not you should understand sort of the proof sketch I did of why upper confidence bound is sublinear and regret all right and then next week we're going to talk about uh sort of alphao and how do we think about doing smart adaptive tree search in really large games see you then