all right welcome back welcome to the last lecture for cs234 what we'll do today is we'll do a review and a wrap-up and we're also going to discuss the quiz a little bit but before we get started I just wanted to remind us where we are um so last time we did the quiz today we have sort of a review of the course and looking forward so what we're going to do today is we're going to sort of do a combination of the quiz cap and then looking forward to sort of um reviewing some of the things we've done in the class as well as looking forward so we're going to jump into the quiz the quiz we'll have um back to you guys within about a day and we're just going to step through some of it because I think it's a nice summary of sort of some of the different aspects so we go to the quiz going to start on question three so the quiz as everybody knows so was comprehensive it covered the entire course we're not finished gradient yet um but we noticed that there were some uh problems that people had more had um more challenges with than others one quick clarification is that when we said um a justification for for your choice we expected you to put something different than the choice itself so we wanted you to actually provide an explanation or rationale for why you picked what you did so let's just step through the quiz and inside of the solutions we'll also do that you might have noticed that the second question was identical to the midterm so it was a good chance to refresh if you hadn't remembered that from the midterm the third one um was slightly tricky so I want to just make sure to go through it just it's a nice way to review po so the third question really asked you to think about proximal policy optimization which was something that you implemented and one thing that might have been slightly confusing or a good thing to refresh is that we really emphasized in class that PO allowed us to use data and multiple gradient steps and when it made multiple gradient steps um those would be off policy but the very first step that PPO makes is always on policy so this is true because if you've just gotten the data and then you're doing a policy gradient stuff on that that part is considered off on policy after that you're you trying to take a further step so if your if we had sort of a one-dimensional policy your first step is going to be on policy and then any further steps you take are now going to be off policy using data that you collected from the previous round so I know that that was often a good thing to make sure to refresh and the second part was F we do not have any guarantees on B and the third part is true and we want to emphasize here that we're only doing important sampling over the actions what po does and what some of the other algorithms it was inspired by do is that they don't try to directly handle the state distri distribution mismatch and instead they try to create a new policy that's close enough that they hope that um the fact that you're going to be visiting different states under a new policy it's only going to be slightly observed okay and the last is that you can use lots of different types of advantage estimators and so the um D is not true you could use generalized um Advantage estimation but you could also use other methods as well and throughout this if anybody has any questions feel free to ask me so the fourth question was given by Our Guest lecture um for those of you that had a chance to attend or to to watch it later um we he talked a lot Dan talked a lot about thinking about the alignment problem and thinking about what things are important for that um the first part is not true it's generally um hard to think about sort of there's different ways to think about autonomy but that was not what we were focused on the second one is true so one of the things Dan talked about in his lecture was the fact that often when we think about preferences and Alignment we're often are focusing on people's individual preferences like someone says they like option one instead of option two but that focuses really on the utility to a single individual as opposed to the implications for the broader society and so the second one is true because as Dan brought up moral theories give us a way to think about more broad benefits to society and to collections of individuals instead of to individuals he also talked a lot about how autonomy is often a core principle when we think about um the value of different decisions we can make and so the idea of an II agent to um uh to allowing people to have some autonomy would say that an AI agent um that thinks about someone's suboptimal decision so it might be that you know somebody really wants to do something that we know is not very good for them that an AI agent that aligned and allows that person autonomy would still maybe support that um because in the interest of sort of upwe the degree of autonomy and the last one is also true because you could think of this as a form of paternalism so if the agent decides what's not really a good idea for you to smoke and so I'm not going to tell you where you can buy cigarettes um that may or may not be true of course we know that smoking is causally associated with lung cancer so you could imagine that case it's not in their best interest but it that would be considered a form of paternalism and so that would undermine user autonomy yeah I don't know if I agree with see even with the explanation because it seems to me that best interest and suboptimal decisions are like definitionally well best interest entail like Optimal decisions and you're saying we should let User make a suboptimal decisions I don't see how those are if the it is in fact in the best interest of the user to make decisions then those decisions are no longer suboptimal which would be a contradiction it seems to me I OB you have that F it's an interesting question right so I think what it says here is that there is different no Notions of what the objective is um and there are different Notions of what is considered optimal or not optimal so there might be some cases where for general population or even for humans in general there is one part of your reward function that says this particular decision like smoking cigarettes is not considered to be optimal because of long Health outcomes however you might have another part of your reward function that talks about the importance of user autonomy and so if you value user autonomy higher than say um someone's Health perhaps in this particular instance for for that particular constraint then in that case you might say well if I'm supporting um that person so if the user's best interest if the best interest of the user is to more value their ability to have autonomy than for them to make this particular Health decision then you would give them the information about where cigarettes are yeah yeah I was also like just the first Clause since is in the user's best interest I thought like it's really hard to generalize about what a single user's best interest was and so that was not a true statement and like by the start because maybe some people aren't best making their own decisions about things and so I wasn't sure how confidently you could say that yeah it's interesting question so what Dan argued is that it is generally a principle that everyone does needs some amount of autonomy and so if you go with that argument then you would say if we believe that it is important for all of us to have some autonomy then under that that should also allow us the freedom to make bad decisions some of the time and in that case an llm that is supporting us also needs to be able to respect those bad decisions and you could disagree with this you could disagree with like that as a premise in terms of the type of theories that promote that everyone should have autonomy you know and we give different people in society different amounts of autonomy children generally have less than you know adults um but but if you assume that that's the case as long as you assume that like it's always important for um every individual to have some amount of autonomy that would include allowing them to make bad decisions sometimes our justification like that's not something you can assume how would that I don't know how that exactly was I didn't gr this particular question but um you can definitely like see what they what they in terms of that and yeah we will look at everyone's justification in terms of that good questions all right the next one that I wanted to go through was Monte Carlo treesearch um so this is another one that uh people um there was a little bit of um differences over in terms of whether this was something people had some questions on so the first one is true so Monte Carlo Tre search um the MCTS the m in the Monte trech stands for Monty not for marov so um it and the way that we described it in class you can use it in both so what we do in Monte Carlo Tre search is we sample from the Dynamics model to get to a next state and as long as we can sample from that whether that's a Markoff model or if you required all of the history so far in the tree to make that dynamics that would be okay so um there's not something inherent in Marty Carlo research that means you always have to have a Markoff system um the second is true so the way that Monte Carlo research uses sampling is it samples the next state and what it does there is it means that instead of having to enumerate all possible States you can just sample a subset of them and still get an accurate estimation of the um uh uh expectation and the fourth is false so this is not true because um in this case like in a lot of settings including Alpha go the reward model is known so we're not trying to learn the reward model but upper competence bounds are still useful because they allow us to prioritize among the actions because ultimately we want to be thinking about taking Maxes and so in these cases you um upper confidence bounds like upper confidence bound trees are using UCB to sort of actively think of how we're expanding out the tree and then the fourth one is also true because that's exactly what Alpha zero does is they use selfplay to improve uh the network to predict values and action probabilities yeah the is it's false it's false it's false yeah yeah I'm a bit confused about the force wording it says Monte card research is used in Alpha zero but doesn't Alpha Z use a different variant of a research that is not mon I am a bit confused Al uses Monti car research it uses Monti car research with selfplay to train a network that predicts values and action probabilities that's part of what it does wasn't there a different thing that had the confidence for oh upper confidence trees yeah yeah so it also well it uses a particular form of upper competence trees as well so upper competence trees is a type of Monti car tree search it's like Monty research is a super set of ucct okay so and let's let's go through these two because all of these are true and so I think this is a useful one to go through as well all right so um CH gbt did learn rewards from humans preparing preferences over prompt output Pairs and then use po to train a better prompt so in fact they did ranking but they did do this um in general for long Horizon problems um and really large action spaces that would be somewhere where a forward search would be really expensive to do so using something like Alpha zero which essentially builds a subset of the tree search can be really helpful um in pack probably approximately correct methods we are guaranteed to learn an Epsilon optimal policy but the Epsilon might be non zero which means we're not guaranteed to learn an optimal policy so if you want to get a if you're if you're okay with your kitchen being like slightly messy which I am um then it would be okay to use a pack RL algorithm that would make a finite number of mistakes but most of the time would keep your kitchen pretty neat but maybe not perfectly neat and then offline RL may be particularly beneficial for healthcare and other high stake settings where online exploration um might be risky or or very expensive so in this case all of these were true all right and then the next one I was going to talk about is nine where we go through some of the theore IAL properties so in this case um what we have is that optimism um in this case um the first one is not true because we don't have any guarantees in general for the reinforced algorithm uh the second one is true for the reasons we just said a pack algorithm guarantees your Epsilon optimal so this is but not asly fully optimal um in the the third case it is not guaranteed to have subl regret so this is false and the reason again for this is that if you just get an Epsilon optimal policy you might make Epsilon mistakes for the rest of time so like Epsilon time T which would still give you a linear regret the fourth is also false so in general you can think of just minimizing regret which is the difference between your policy and the optimal policy or maximizing expected cumulative rewards and they're just the same thing when you just subtract from the other you either maximize cumulative regret or minimize maximize cumulative reward or minimize regret and then um uh in the fourth case this will not necessarily be a pack algorithm so the only one in this case is the only B is true and the reason for this is a pack algorithm has to make a finite number of mistakes so it normally has to be polinomial and so even this would say this algorithm would be consistently converging to the optimal policy but you don't as know how long it would take so it could be very expensive and then um the final question has us think about which algorithms could generate The observed reward um raise your hand if anybody wants me to go through that I'm happy to and step through it or otherwise we'll move on to the next p allowed people to just think about how different algorithms would run and whether or not not they could generate this observe data all right well we'll release the solutions for the quiz um over the next day and we'll also release the solution um the grades mistakes or is it that your toal mistakes should be within somebody finite number of mistakes so great question so in terms of PEC what we normally require is that the number of mistakes made well with high probability on all but a finite number you will be Epsilon optimal and that finite number needs to be a polinomial function of your problem parameters including like one over Epsilon the size of your state space the size of Your Action space Etc doesn't always tell you when those mistakes will occur they may be at the beginning or they may be later I was thinking if you perhaps it still be fine as long as there some is yeah I mean you you certainly could do that it wouldn't be pack unless you could guarantee with high probability that that total number of mistakes would be small yeah it's a good question one thing some of the work that we have done in the past is you may or may not know what Epsilon you want to commit to in advance and so we've also developed algorithms where you could think of this occurring for different epsilons maybe as you have different amounts of budget you might want to be able to pick Epsilon because if you get a lot of data maybe you can get more optimal question anybody else have questions about the quiz all right well feel free to we have normal office hours this week so feel free to come to our office hours next week we will not have office hours anymore um but if you have any questions about the quiz or about your projects feel free to come see us all right so I think it's always exciting to um go back to the beginning of the quor and kind of think back of all the things that we've covered um as well as looking forward in terms of the field so when we very started the very first lecture slide might look somewhat familiar we talked about how reinforcement learning is fundamentally the question of learning through experience to make good decisions in order to optimize our long-term reward and so that's really the central question that it tries to start to answer and we talked about there being a number of different learning objectives in the course and so what I hope that people will walk away from in this class is to understand what are the key key features of reinforcement learning and how does that change compared to supervised learning and AI planning a lot of other areas or unsupervised learning um to understand how if you're given an application problem whether and how you should use RL for it what algorithms might be appropriate to be able to implement and code RL algorithms and you guys have had lots of practice with that and then to understand how we would compare and contrast about what it means to have a good RL algorithm and what are the sort of ways we should evaluate algorithms themselves as a way to help us understand if we're making progress so thinking about things like regret sample complexity computational complexity empirical performance does it converge to the optimal policy does it converge at all um and then also to understand the sort of exploration exploitation challenge in terms of data collection and these sort of fundamental challenges we have between the data that we gather allowing us to learn things about the environment and about different decision policies versus using that information to actually obtain High rewards and so throughout this you've had a chance to think about this on the quiz on the midterm on the homeworks and then now also in your final project so what I'd like to do now is to sort of again revisit um this second question because I think really as you go forward this will be um when you use reinforcement learning this is going to be one of the things that you'd constantly have to do which is decide for any new problem you're looking at is it appropriate to think about reinforcement learning as a tool to help you solve that problem and so I think to do that it's helpful to go back to the motivating domains from the first lecture so that so three of the domains we talked about a number of different domains throughout the class but here are three of the domains that we talked about on the first lecture so the first one is Alpha tensor this is Alpha tensor and in Alpha tenser the idea was to figure out a more effective algorithm for learning to multiply matricies and kind of the amazingly beautiful thing they did in that case is that they actually are doing reinforcement learning to learn algorithms which I still still think is really incredible it's extremely creative and so what they want to think about in this case is if you want to multiply two matrices this is just 2 x two but they go beyond that um what is sort of the way we should operationalize that so that we can think about the particular products and sums that we're doing in order to reduce the amount of computational complexity we need to accomplish this in a correct way and what the researchers at Deep Mind were doing when they were thinking about this is they were thinking about a common task that comes up everywhere we multiply matrices all the time for almost all of AI and machine learning um and so we're really relying underneath is's constantly that cost that we're doing and so they're thinking about can we essentially invent better algorithms for doing some of these really basic substructures so I think that was really exciting and I think this is one of the domains that um that now you have some of the tools to be able to do the same types of algorithms is what they use to solve this bless you so what I'm going to do right now is I'm going to revisit these three and then I'm going to ask you to think about how given what you know now how you would formulate them and some of them I've talked about a little bit more or a little bit less but I'll first just give you a quick refresher so you can think about given what you know um you might for this so the second one was plasma control and this is much more of a like a controls like more like the mojoko type of task that we saw where they're trying to manipulate and control these different plasma and they want to think about a control policy to allow you to achieve different types of configurations and then the third one was thinking about how do we figure out who to test given finite resources so this is for covid testing so if you have a bunch of people coming off an airplane and you have a finite number of tests who can you test to better understand who might be sick and restrict the like restrict the spread of covid and this is a process that's happening you know every day as people were flying into Greece into different airports and then you would send off those samples for to labs and then a few days later you would get the results and those people that you asked to test could come out of quarantine and so what I'd like you to do now and I've posted a poll for this is to think about the following so I'll I'll label those as well this is the alpha tensor this is plasma and this is co testing if you can go on the poll and say which domain are you choosing is it a bandit is it a multi-step RL problem what type of problem is this what setting are we in is the problem an offline setting or an online setting or some combination what do you think the state date action and rewards might be and what algorithms do you think you would use to try to tackle this problem and we'll take a few minutes to go through that e for for e e right we'll give a few more minutes and then share some of our thoughts and I think one good thing to think about in this too is like are there problems with distribution shift that might come up are there cases where we'd want to be conservative with respect to the results that are being generalized um or do we not have to worry too much about distribution shift in these cases could there be unsafe States or too risky or things like that so I think we actually have a nice breakdown um raise your hand if you did plasma is there someone else here oh so we do have another person doing plasma but they are remote then raise your hand if you did Co okay maybe you want to go near those guys so you guys can all compare your answers and then did you guys both do Alpha tenser okay perfect well so why don't we take a minute and talk to your neighbor and I'll also come around and see if you guys came up with the same formulation [Music] think so I said was also like May I'm not sure like how to okay what I put down was like total number of test cases of like country but like that some we that would be the hope at least yeah unless you really proxy like [Music] com closer to offline there's like this batch setting with delay you have to make decision they're really Levering so it's all online but yeah I was like I wasn't well then maybe like soop what do you think in terms of you want to do exploring really ground for distinction yeah cuz they really want I mean you kind of get my expion that like they have features of people so like you know you get it's not like everyone with get the same but in general if there two people is I think what you guys so many ways you you so the problem in general yeah if you went but I or things like that right because if someone's going to go on a farm doesn't matter as much but they're going to go to Soul then you know I don't think any together um I really like these domains I think that they're really interesting domains to think about sort of what is the implications for um how we model these and the choices that we have to make and what are the algorithms that would work so let's go through some of them because I know that a lot of people pick different ones so Alpha tensor because I think nobody because I know there's some people who are watching online remotely because we have more answers um as well so I think uh it's really interesting to see the perspective I'm not sure that there was very few people that mentioned Monte Carlo research but actually for Alpha tenser it is something I guess maybe the alpha should hint um at that like Alpha zero Etc so they do they it is something where they're using reinforcement learning and policy networks Etc but they're also combining it with kind of alpha zero like technology so they use Monte Carlo treesearch in this case so they have Monte Carlo treesearch carar so this is a reinforcement learning problem it's multi-step because the idea is that you want to take a series of steps until you solve the multiplication problem and what the what the steps are in this case is you could think of it as like algorithmic Steps so like you know which parts of like your if we go back to here make it make this big for a second if you think of this when you do multi matrix multiplication you have A1 * B1 and A2 plus A2 * B3 A1 * B2 A2 plus B4 Etc you can think of there's all these different products and sums and you could do them in different orders and you can kind of refactor that so when you think about sort of all the operations you could do um you're going to have to do a ser series of operations such that remember what they're trying to learn here is not how to multiply two particular ones but they're trying to learn the algorithm that will always correctly solve in the minimum number of steps so here their reward function is the number of steps the number of um computations that you have to do and of course it has to be correct and to me one of the Brilliance of their ideas is how do you make sure that you're only searching within the space of correct algorithms um and so there's some really nice properties for this particular problem that allowed them to do that so other people had also noticed this in the past and then what they said is oh given that we have that given that we have a way to verify that you know and only search in the algorithms that are correct now what we could do is just optimize for length and so the way that they do that in this case is they're going to very similar in certain ways to Alpha zero be able to search through um different uh using like policy networks and and value Network so you can see here they have a neural network with both a policy head and a value head similar to what we saw for Al zero but they are going to do this forward search now one of the interesting things about this is that compared to what we saw for Alpha go in Alpha go because I saw some of we talked about this with some of you um and saw that in your notes at runtime they're not going to do search anymore what they're going to do at this point is they're just trying to find the best possible algorithm and then in the future they're not going to do any additional monticolo research unlike what we do with playing go because the assumption is at that point they have the algorithm and they'll just apply it to multiplying so they don't continue to do Monte Carlo treesearch kind of at runtime this is all something done just to find that best algorithm so this is a case where we would have Monte Carlo tree search and we would also have policy networks policy and value Network and where they're sharing again this is sort of you know a single neural network so you can get shared representations here very similar to Alpha zero um and then they they can play in this case yeah overcome distribution shifts here ah so well they are trying to have so all of the algorithms they search through are correct so there's no distribution shift in that like they will always be correct for a future problem it's just that it may or may not be that they found the very most optimal one so there's not kind of the same problem that you might write into different states the nice thing here is it's just a series of operations it may be that the search is stuff that they didn't find the optimal one so there might be still better algorithms that are shorter I don't think they prove this is a lower bound to my knowledge um and so there's not it's a great question there's not going to be a problem with like when you deploy this on a new matrix multiplication that you might get something wrong it's just that you know it may or may not be the most optimal way to multiply that particular um to so I think there that's sort of the the cleverness of having the policy Network be or having the the space they Searcher always has my correctness yeah inter Al Le something it yeah it's a great question so what you know was there s some like kind of high level insight and in particular high level Insight you could translate to other problems not that I remember um I think that they I don't remember there being any sort of particular like aha moment now this means for all these other type of problems we can do this um it' be interesting to go back to the paper and see if there's anything that I missed in that case so I think what they found in this case to what I remember is that they relearned a couple different well-known algorithms for trying to like during the search process um they learned a couple algorithms that are known to be good and and more effective and then found some others that hadn't been discovered before and so I think this is also an interesting question because there may be others of utility functions for Downstream use of these algorithms and so in that case you might want these approaches to sort of provide you a set of solutions a set of algorithms and then people could pick you know which ones they thought were best all right so this is a you know a multi-step RL problem and here the state of the system would essentially be what are the operations you have so far so like one of the operations that um that you've done on the input to matrices specified as tensors um and then how far do you need to go until you can get the complete solution and the reward in this case assuming that you've conditioned everything on being correctness is just length all right so next let's go to learning plasma control for Fusion science and I think this is a really interesting one I I appreciate it that I saw for a lot of people saying like we don't want to do you know like Epsilon greedy on real hour with with plasma that's probably a bad idea for all of our health um so there's need to be like some form of offline phase and that's exactly right like that's certainly what they did in this case um I think it's interesting to sort of think about how it's represented and sort of the different types of controls you'd be implying in this case which generally will be real valued um so it's a very different problem than alphat tenser let's look at sort of what their architecture was so in this case one thing that they also really emphasize in this is that they had to spend quite a long time time they have to think really carefully about what is the objective so this is an interesting one it's not just like minimize you know the number of computations to solve two matrices it's saying we want to be able to kind of manipulate plasma into particular configurations and so you could imagine in this case you might have lots of different reward functions and you want to be able to quickly learn policies for those so what they do to amarate the um the offline safety issue is they build a simulator and I was just coming to someone that uh on a recent panel I was on I was talking to a mechanical engineer that said that's one of the reasons they were really interested in Ai and machine learning is they like to make simulators of really computationally expensive physical processes and so they here have a simulator that is fairly High Fidelity but not perfect and is High Fidelity enough that they think it'll be useful but low Fidelity enough that you could do optimization over it so what they're going to do in this case is they are sort of solving the offline Case by constructing here not necessarily from data more maybe from a physics model a simulator so we're going to do kind of like modelbased RL model based in the sense that we have to have a model or a simulator but then what they're going to do is enactor critic method so they are going to do actor critic in this case um where they have a control policy and they're also going to learn uh so they have the actor here and they're also going to be learning um a Critic so I thought this was pretty interesting for why they took this particular architecture I'm just going to read you a little bit about that part let me go down there um so so a couple things so I thought one of the things is they use an act critic method that is related something else we saw but not exactly the same it's called No and I'll write that out in a second um but one of the things that I thought was interesting is they said in our simulating period we can do sort of whatever we want and we can have a really complicated critic when we are deploying this it has to be real time so some other people I think it was brought this up when we were chatting about it this is like you know self-driving cars and that like you have to really fast controllers you can't do Monte Carlo research and wait for us to decide like the Plasma's going to do something and so either you're controlling it or it is doing something else if you're not making an act of control and so they needed an actor AKA a policy that is really computationally fast and so what they said is that inside of their actor critic architecture one of the reasons they wanted to let do that during the training is they could require their actor to be pretty low-dimensional and so I have a pretty small Network to specify the actor or the control policy which is what they're going to eventually deploy but they could have a really complicated critic and so they can leverage the fact that in the offline setting they can you know really in a complicated way many parameters specify their value function because this is all offline and so this is sort of a nice interesting asymmetry between like you know computational efficiency and what are the affordances you have offline compared to online so they have a very complicated critic and they have a very simple actor and so then they train the actor to try to find a good point in that policy space using their really complicated critic um and so they said the representation of the control policy factor is restricted as it must run on tcv with real-time guarantees but the critic is unrestricted so I thought that was pretty interesting that they had this now another thing and this came up in some conversations is as you might imagine if we go from offline to online there is always the problem that it might not translate and again we're dealing with plasma so we want to have some sort of safety guarantees so here are the ideas we've talked about before about sort of having more trusted regions or having pessimism come up and the way that they handle this is by putting inside of the reward function so they essentially Define areas which they think could cause bad outcomes and then they put that inside the reward function to lead to a policy that veers away from that area and I think again that's a pretty common idea that if you have safety this comes up in robotics and other ones too Claire toml Tomlin up uh uh ber does this to a number of others you put that inside of the reward function so the resulting policy avoids those and so here they're doing that not necessarily because reaching that particular part would be bad but because you're getting close to a part where it might be unsafe or where you don't trust your simulator so let's go back to here so in this case it's an actor critic actor critic This is complicated this is simple this has to be simple for Speed and we all do this with a simulator we put penalties in the reward to avoid inaccuracies in simulator or let see outcomes and so this is very similar to S this pessimism over the places where we're uncertain whether because of data sparcity or because of um known problems in our simulator have yeah I guess you with like consider reward hacking or like sensitive rewards how do you like you guess like when you like can't reward how do you like avoid like or how do you like double checks I assume like they really don't want to be makes here it's a great question so my guess in this case is it ends up you were just pretty conservative like um I think of just how how far away now I assume in this case maybe because some of the physics simulators that they have access to that they could play with some of sort of saying like if you how negative do you need to make some of these or how out of bounds or how hard of a constraint is that so that you could be very confident that before you deploy this you make sure that this doesn't reach there at least in the simulator you could see whether or not you're violating those constraints or like if you have these penalties if you're sufficient not to reach parts of the area that you think you might want to avoid whether that will translate to your real system is is an important question you know so yeah it's a great it's a great issue of how you now I think it's it also introduces the really interesting question of whe whether you can verify so there are other methods this is not some most of what we've talked about is not those but where you could verify that you're not going to reach unsafe regions and this would certainly be an area you might want to do that all right the third one was efficient and targeted covid-19 border testing bar I should have also mentioned so this is also a multi-step RL problem so absolutely the controls you're doing affect the next state and that's the whole point and then you want to manipulate the plasma into a particular location so it's a definitely a multi-step system this one is thinking about how do you do efficient and targeted covid-19 border testing and even though it's via RL it really is a bandit problem in this case so it's a it's a repeated Bandit problem this is a batch Bandit with delayed outcomes so let's make this a little bit bigger so again remember to think back what happens in this case people come in um Greece has some information about those individuals before they show up we have finite numbers of tests we can run uh and process we have a policy for each individual coming off that plane whether or not they're going to be given no test or they're tested um you get the results 24 hours later and you use that to update your policy so I think this is a really nice example of like this backat banded process who you test today does not affect who arrives tomorrow on a plane so it's a bandit problem um but we have this delayed outcome problem that like you don't observe the outcomes of what who you just tested for a while which means that algorithms like Thompson sampl may be helpful um H and then in addition some of the other really big challenges in this case is that you have a lot of constraints you have constraints for multiple reasons so we have constraints over the number of tests we can run you also can have different constraints depending on where you're arriving in Greece and where you can send things so there are different testing sites which might have different capacities and in some cases also you might have I don't think they dealt with this in this paper but you might have fairness constraints too like maybe it's best to test all the women but maybe that's considered unfair um and so you may have a number of different constraints that you can think of as restricting your policy class so it's a pretty interesting interaction problem here and also because of the fact that it's budgeted it means that a lot of your outcomes are coupled in a way that they might not be so for example if you give me a test if we only have one test that we can do in this room and you give me the test then you can't give it to any of you um and so there's this interaction too in terms of uh you know the data that we get to observe um for for the right so I think this is a really interesting case and it was really interesting that it ended up having a sign significant benefit one of the things too that's interesting about this is what how we Define the reward one thing that we were talking about in our smaller groups is that really you would like to understand how this is impacting Downstream Co outcomes and you can measure those but you can measure those really late like you can use those as a way to evaluate how effective the overall program was but not necessarily a reward you can use to optimize and that's often a really common challenge the rewards you get immediately that you could use to change your policy may be different than the downstream outcome you care about and on Friday I was at an experimentation workshop at the business school here and I was giving a talk and I was really excited and interested to see how many other people were also thinking of this challenge of short-term outcomes versus long-term rewards that you really care about and I think this comes up a lot in advertising other areas too companies like Netflix and Spotify and others we're talking about this common challenge where you have to make policy decisions um or you know policy update your policy way before you can maybe observe those outcomes and so if you have to wait a really long time it limits how quickly you can experiment and so in this case too you might really care about these Downstream ones but one of the points of this paper was to argue looking at that lagged information was allowing people to make sort of not as good decisions and so you need these sort of shorter term outcomes so you have any questions about this one so I encourage you to if you haven't read any of those papers they're really beautiful papers if you want to read any of them or all and then just finally if you remember all the way back we talked about chat gbt at the very beginning of the class and I think you should feel excited now that you really understand this whole pipeline of what's possible the first is of you know training to supervis policy which we could think of as Behavior cloning um uh the second is doing direct preference solicitation we did it with two pairs and then doing Po and we also of course did DPO as well so I think now even though we didn't do with large language models you really have a sense of sort of the whole process you could use if you were to train large language models and do the sort of fine tuning all right so now we're just going to kind of wrap up with some of the the main ideas and then looking forward so if we think about sort of the main characteristics of reinforcement learning this idea of learning directly from data to make good decisions we've been thinking a lot about optimization delayed consequences exploration and generalization and I think a key thing just to remember if you didn't remember anything else from this class is that one of the big differences of reinforcement learning is that in general the actions impact the data distribution certainly of the rewards we observe but often also of the states we get to reach and that's just very very different than like supervised learning or unsupervised learning where you know the data we get doesn't you know you always see the label um or or you just have a static gener distribution of data so this is both a huge opportunity and a huge challenge because we have to think a lot more about distribution shift so in terms of the standard settings we've seen um we've SE we've talked about Bandits where the next state is independent of the prior state in action as well as general decision processes where the next state might depend on all the previous actions and states or it might be markof and it only depends on the immediate State and the immediate previous action we've also talked a lot about the online offline sort of settings where either you have historical data only and you're trying to learn better policies from that or where you can actually actively gather your own data and I will highlight there that I think many real world settings are sort of often between these two many so in many cases you might have a large pull of offline data and then you might be able to get a small amount of new online data this comes up in robotics it comes up in some of our work we often call this sort of experimental design so that you sort of you might have offline data then you can design an experiment to gather a small amount of data to try to learn a good decision policy so I think in general we can think of this as an entire spectrum between these two extremes now what are some of the core ideas we've seen well of course we've seen a lot of different ideas but I think it's nice to sort of pop up a level and think about sort of the common themes and Chelsea Finn who teaches deep RL also had a really nice light on this so I found that my thoughts were aligning with a number of hers as well so one thing is just to like you guys be really familiar with the fact that when we have function approximation which we're almost always going to need because we want to handle hard problems hard complex problems and we want to do off policy learning but honestly we often want to do whether we're online or offline um and just remember off policy learning just means that we want to take some data that was generated from one decision policy and use it to think about how another one might work whether in terms of gradient steps or in terms of fully offline learning and this is generally just really hard so you could argue that a huge number of papers in reinforcement learning just think about this problem it's just incredibly hard and the reason is that whenever we have a new policy we're going to get a new distribution over State action rewards and that means that it may not match our current data we have a data distribution shift and the reason we want to do this the reason we want to have that use the offline data is because we want to be data efficient and this is true even if you can be online because as we saw for things like Po if you follow sort of the Theory or if you follow you often have to really be incredibly conservative or just have bad performance for a very long time but the problem is is that when we combine these two in general we're going to be doing generalization or extrapolation and whenever we do that we need to be worried that like the values our predictions of how good a policy will be will not match its actual performance and so over and over and over again we've seen how do we try to mitigate this and in different types of methods so like in po the way we control this and this is an online method is we control it with clipping we just can't take too big of a step inside of our gradients um and that allows us to make sure that we're limiting this extrapolation problem in the dagger case we mitigated this by getting more expert labels we knew that there could be um a data distribution shift when we started to follow our Behavior clone policy and so we just try to get more labels when we get into states where we make decisions different than the expert so we can kind of cover the distribution of States we reach under the learn policy and things like pessimistic Q learning which came from my lab cql which came from Berkeley and mopo which came from other colleagues of mine here at Stanford all introduced pessimism into offline RL again exactly to kind of limit this extrapolation problem where you're sort of overly optimistic about what will happen so I don't think you should think of these as being the only ways to solve this problem I think it what they should sort of inspire you to is to think wow this is a problem that comes up really throughout all of reinforcement learning and we have some methods for trying to handle this but this is certainly not a solve problem some of the other core ideas that we saw a lot was this idea of the sort of different ways we could think about the main objects in reinforcement learning so we had this sort of models values and policies and sometimes people ask me like do we really need all of these are these all useful ideas I think some of the application areas we were just going through illustrate why these might all be useful ideas so models are often easier ways to represent uncertainty so if we only have finite data and we're training something about a value or a model or a policy often it might be easiest for us to represent that uncertainty with a model so may have an idea of why that might be why might it be easier to represent uncertainty for a model rather than like a q function or a policy you could disagree with me too that but um I can give you why I think this might be easiest we're building just like a Dynamics model or reward model might why might that be an easier place for us to represent our uncertainty about how the world Works compared to trying to represent our uncertainty over the Q function or the policy is it just because like when when you're making when you're do like example like uncertainty in your policy is uncertainty both about like your world but also uncertainty about giv your assumption about the world what you think the best action is so you're just dealing with like joint uncertainty whereas the model of the world is kind of like it just more like specified problem you have like one source of uncertainty yeah I think that's great that's a great intuition that's what I was was going for here so to repeat what said here like when you think about policy uncertainty there s of that kind of combines and wraps up this idea of uncertainty over how the world works and uncertainty over what you should do to make good decisions given that World um and same for the Q function and there are ways to directly represent your uncertainty over the policies and the Q functions but models it's a prediction problem and so we have lots of tools from supervised learning and from statistics and data science to think about modeling our uncertainty when it's just a prediction problem like you know what what state will happen next or what reward while I get in this state there's no planning or decision- making yet it's just prediction and so it's sort of a nice place for us to reduce or leverage the beautiful history of work in all the other fields of how we could do this easily instead of then having to propagate that through um so I think often this is an easier place to represent our uncertainty of course there's no free lunch and if we have it there and we want to think about our uncertainty over policies and value functions we still have to propagate it but it may be easier for us to represent that and drive ourselves towards it they're also really useful for things like Monte Carlo research like you can use models for Sim ators you or for plasma if you may be able to use these PLS as a place to think about risky domains or or or to be very data efficient the Q function in some ways is kind of the you know central part of RL in the sense that it just summarizes the performance of your policy and you can use it often to directly act because you just take an argmax with respect to the Q function so it's a good way to summarize like how good things are and policies are just ultimately what we want to have like we want to have good decision- making we often want to know exactly how good that is and that's maybe where the Q function will is one particular nice thing but ultimately we want to try to make good decisions in the world I think another thing that's come up repeatedly is sort of this question of computation um versus data efficiency and I think one thing that it's really useful to remember is that in some cases they are the same so in this class I've often talked as if they're sort of totally different but in many situations if you have a simulator data is the same as computation you're either using your computation to maybe do more planning or try to get to a better policy before you simulate the next step or you're just simulating more steps and so I think when you look at papers if they have a simulated domain and they're trying to do something really fancy in the back it's useful to remind yourself that if it was a real problem you wanted to solve you could either take that same computation and just have maybe 10x more samples or you can do 10x more computation between each sample now in some other cases um we really do have limited data you know we just fortunately do not have 7 billion people with um Co like there's just a finite number of people um and there's a finite number of students and so as long as you really want to be data efficient when you do that it's often trading off for computational cost so we're going to try to squeeze everything we can out of the data and when we do that we often are going to rely on methods that are much more computational intensive and also as you've seen in some cases you have real constraints on this like in plasma like in self-driving cars like in robotics there are sometimes cases where you have to have fast computation because otherwise there is a default there's kind of a hidden action which is you have to make a decision at every time point if you're not doing that something optimal something else is happening there's some default action that's always occurring now what are some of the open challenges I think there's aot lot of open challenges I think RL is a fascinating area but RL has not yet had the applicational impact that we've seen in some other areas of AI and um and engineering and I think this is for a number of reasons um but one of them is that you really want methods that are off the shelf and robust and reliable and many RL algorithms have hyper parameters you have to pick you know the learning rate you have to pick there some of these are the same as normal machine learning and others of them are different and one of the challenges here is if you're online even though in our world like when we're doing a homework you might be able to try it with different hyper parameters in a real world setting like for healthcare or for customers you would just have that one trajectory and so in that case or one deployment you can't sort of optimize those parameters and so I think that there's this real need for sort of automatic hyper PR tuning model selection by that I mean how do you figure out what architecture you use how do you even write down the problem um and generally robust methods you know model selection ter like you know the the sides of your normal Network Etc and just general sort of robust guarantees that we're not going to suddenly have one run where your performance is really bad the other is that we often need things that are going to be able to span this data versus computation efficiency and we don't normally have very good ways to allow a practitioner to say like okay this is how much I care about this or that be really nice if we could have have sort of like Paro Frontiers and you could say well you know if this is computation and this is data you might say Okay I want to have things that are always somehow optimally trading off between those two and depending on my application area I can pick where I want to be on this curve and I also think this hybrid offline online case is a really important one where many organizations might be willing to do a little bit of additional data collection but not fully online learning I think there's also some just really big questions for reinforcement learning we focused a lot on the Markoff decision process formulation that's where it comes out of the 1950s and Bellman that's how I learned about it many people learned it and it has some really nice intellectual properties but it is not clear that this is the right way to solve data driven decision- making this is one framework so I had um a professor when I was a grad Stu grad student uh who said that the whole world is you know a multi-agent uh partially observable markup to process where you're doing learning but it doesn't mean you want to solve it like that and so while in many times we might be able to F you know model things in these kind of stochastic Markoff decision process ways that may or may not be the most efficient way to represent the problem it's just like how you could always represent a bandit as a really complicated RL problem but if your state next states are independent of your previous one why would you do that so I think there's some real questions over like are there better formulations um I think a second thing is that historically in reinforcement learning and even throughout most of this class we focused on I'm going to learn from this one task from scratch but of course that's not what humans do we constantly are building on our PRI experience we are sort of imperfect agents for learning across many many many tasks and what we've seen from generative AI um like sort of uh large language models Etc is that doing many many tasks might be really powerful and that's been relatively understudied in the RL setting and it might be much more effective we've seen even in like Alpha zero um and Alpha tens and others that these shared representations can have huge benefits and so those might be really productive ways to think about accelerating the speed of decision- making and learning good data driven policies I think a third thing is thinking about Alternative forms of feedback assuming you get scaler single scaler rewards is pretty limiting particularly now that we have large language models you could imagine having really rich feedback or really sparse feedback like thumbs up thumbs down or preference pairs or really detailed examples about how something is wrong or what your preferences are and now that we can start to have language as as rewards I think that's a much richer opportunity and people are starting to explore this already another is sort of just what settings we're in most of this class we thought about stochastic settings take an action from a state you get to some next state generated sort of stochastically from some sort of like you know uh indifferent process but that's not very common in real world settings in many real world settings there are other stakeholders or multi-agents um that might be adversarial or might be cooperative you know you might have a teacher that's helping the agent learn something or you might have an adversary that's competing with that agent and so those settings are also really important to consider and I think another question too is throughout this class we've been thinking about sort of integrating and doing learning and planning and decision- making all at once everything and that's wonderful and elegant but there are many approximations to this so in some other fields they often do system identification I mean like you might learn how the Markoff decision process works you learn your Dynamics model you learn your word model you stop you plan and so while this offers some flexibility it also introduces a lot of complexity and again in some areas there might be some really good alternatives to this and finally this is one that's perhaps closest to my heart which is I think that there's just an enormous room to do better data driven um uh decision- making in domains that could benefit so I think there are lots of application areas we've talked about in class but there's so many areas where I think our society could benefit from better decision- making and so it' be incredible to see more of that impact whether it's from the Frameworks we've covered in class or from others and I think one of the wonderful things is that you guys are very well equipped now to go out and start answering these questions or other ones that you think are important all right I'll just close with two more slides one is that if you like reinforcement learning there is a lot of people at Stanford who think about reinforcement learning there are lots of classes there's at least another five um so there's deep RL with Chelsea there's decision-making under uncertainty with Michael Michael and I both offer Advanced courses and need a decision-making or RL and Ben Ryan Roy often also offers a an advanced RL or bandaid class so there's lots of places to learn more and finally thanks for being part of the course it's great to get to meet everyone it's been and we're really excited to see your posters on Wednesday thanks than