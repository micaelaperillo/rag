hey everybody we're going to gohe and get started and we'll start with a refresh your understanding um thinking back to DPO and Harley Jeff all right why don't you turn to someone near you and see if you got the same answer particularly for um the third and fourth one but PR up all right so let's come back together um the first one is true the DPO model does assume that um we have a particular model of how people are responding to preferences in particular Bradley Terry model the second one is also true even though we've been thinking a lot about when we actually have preferences um we can also use this in cases where um someone just directly provid provided you reward labels and so rhf is a paradigm is totally compatible with the idea of just getting rewards from some way um but normally when we think about that human feedback it's normally from preferences um the third one is an interesting one does somebody want to argue why they think that is uh not a good way to learn about the reward model for board games there's multiple optim point yeah I feel like there's like multiple yeah I think that that could be one I was seeing something even simpler does anybody else want to add why you might want not want to do this for Board of games it might be really hard to compare like in a game like chest where the reward is at the end and like self play yeah so it might and also because we normally know what the reward model actually is in games and so like if we know that at the very end we can say this is a plus one or this is a minus one um there is no reason necessarily to assume we want to look at like two game States and ask a human to try to judge which of those two is better we we know the ground Troth reward and so it's probably better just to directly use those um and it may be that those sort of pairwise rankings for intermediate game States might not be very reliable too um and then the last one is also true DPO and rhf can both be used in extremely large um with extremely large policy networks all right so where are we um last time we talked a lot about Monte Carlo research and we talked about Alpha zero and what we'll do today is we'll talk briefly to kind of finish up that part um just quite briefly I want to clarify a couple things that I mentioned last time so the rest of the day we're going to have a guest lecture almost the rest of the day we're going to have a guest lecture by Dan Weber um uh think which is a way to sort of introduce the last part of our course which is to think about where the rewards come from um in terms of how we make judgments about which rewards we might want to prefer or not and we'll talk about that today and we'll talk about that after the quiz as well but before we do that I just want to do a little bit more on montol reseearch so let's see if we can get take two for the video first so I think that sort of captures it's a nice you know we don't normally get documentaries made about the work that happens in artificial intelligence at least not yet uh but I think that it's a pretty powerful exhibit of why people were so excited about this result um and so of the implications it had when you can exceed the best performance in the world at something by computers um and we've seen examples of this in the past for those of you who heard about it there was an IBM Watson for Jeopardy case a number of years ago and I remember being in the audience when that was happening um you know many many people watched it in different watching parties um at the time and I was in one of those watching parties and it was a similar moment in AI sort of thinking about what are the levels uh that we're going to be a to achieve an AI and what are the implications of that for sort of human expertise and Human Excellence so Bon research of course so of course they did win the game deepbind did win against Le at all um let's not just go back and sort of think a bit about what Monte Carlo treesearch and Alpha Z zero are doing so this is another refresh your understanding and I'm also doing two of these today just to give you example these are the types of questions you also might see on the quiz so we'll do another one of these and then I'm going to clarify a couple points uh about Alpha Zero from last time e why don't you find somebody near you and compare your answers e e e okay good I'm hearing a lot of discussion about this which is good um so the first one is true the first one is it does approximate um a forward search tree okay um the second one is false and I know this is a little bit subtle so monticola tree search tries to approximate the forward search tree but as you might remember the forward search tree can scale exponentially with a number of states and the number of actions because you're expanding by both of those at each level um and so what monticolo research does is it uses its Dynamics model to sample a next state so you don't have to enumerate all the possible next States so it uses sampling to help with the state branching Factor but it doesn't tell you what to do about the action branching Factor one thing you can do is you do upper confidence trees and then that tells you how to use a bandit to figure out you know which action to select next in Alpha zero we see that even that is likely not to be sufficient when you have an enormous branching factor and so you may need some sort of additional weight like the probability like a policy to to select to monum those actions the second one is the third one is also false um so this was true in the original Alpha go and I think in the least SE doll one too that you saw in the video they did have two networks but in Alpha zero they just have a single Network and it outputs both a policy and a value so it just has two output heads the third thing is true so kind of amazingly even if you spend 40 days and you've got you know many tpus Etc to learn a policy output and a value output in the network they still do at kind of at test time you know like if you're playing lease at all they still do additional guided Monte Carlo research at that point and it's makes a big difference so I I think it was something like going from like an ELO score of like 3,000 to 4500 or 5,000 I'm going to get the numbers wrong but uh it was a huge gain by doing a little bit more of extra local computation and the third thing the fourth thing is also or I guess the next thing is also true which is selfplay does form provide sort of a form of implicit curriculum learning because the agent is always essentially working with an opponent that's very similar to its level in fact it's itself so it's exactly level and that means that sort of the density of reward it gets is going to be much higher than it would get if he was playing an opponent that was much higher or much lower okay the other thing that I wanted to clarify is so when we talked before we talked about um selecting a move in a single game and we talked about how it both maintains this Q can it maintains a qsa pres of a certain node as well as this upper bound here which is going to be proportional to this policy function it gets from the neural network divided by 1 plus the number of samples so I mentioned in class I just wanted to make sure to clarify this I mentioned in class that I thought that all of the s's here are just the nodes um but s is a little bit weird of a notation because you could imagine it could either be the state space or the node like that part of of the tree search I look back on it this is actually the node so I I was correcting what I said last time but I just wanted to make sure that was clear so they're thinking of sort of each of these points as being like a particular sa but in theory and again I'm not a go expert so I'm not sure how often this happens you could end up at the same sort of game State lower down in the tree and you would maintain totally different statistics for down there so you're not sharing across those and there's just you know Simplicity to do that in term there's simpli that can help in terms of um the architectures that you need to to dve and just sort of simplify some of the storage for this so it is you know these These are per node and then the other thing that I just wanted to clarify also is that when you get to the root later and you're making a decision over which of these actions to take I mentioned that what they do at the final end is that they're going to do something proportional to NSA at the root 1/ to so this is going to be the policy at the root and I just wanted to make sure to be clear about what that does so this is sort of prioritizing parts of actions that you've taken more that you've explored more in your neural network so let's just see a little bit about what this would look like so if to is equal to one what that would mean is that your probability of taking action a from the root would be equal to the number of times you've taken action a in the root divide by the sum of the times you've taken well just really the total number of roll outs you've done from the root so that would be strictly proportional in that case if you have a towel less than one that means that you are going to upway some of these things so then you would have like so let's say if to is equal to 0.5 then you would have na a root squared / the sum of N a root squ put a prime there so what that would mean is that if you make to go closer and closer to zero then this is basically going to do a Winner's takes all approach and you'll basically SE select whichever action you took most from the route and then as you go to more dos one it's sort of equally spread across all the times you've taken each of the actions and as you might imagine that's going to have different implications for sort of how exploratory you are now note that none of these things are doing it based on what the value is at the root all of these are just based on essentially how much of the time you've explored different parts of the tree so I just wanted to make sure to clarify those in those cases does anybody have any other questions about Alpha Zer and I do just want to say that I as I mentioned before there's a number of different other derivatives that have happened since this so there's muz which doesn't even need to know the rules of the game um and there are also a lot of sort of sophisticated approaches which have to do with hidden information games like poker so in this case you know there's full information you know exactly where all the white stones are you know exactly where all the black stones are there's no hidden information that either player has and there's only two players but there's been a lot of work at thinking about cases like poker and others where there's some cards that one agent doesn't see from the other and so then how do you play optimally in those games as well so H any questions before we move on to our guest lecture yeah with these other models like specifically zero that doesn't make of the game um do we just observe that it even though doesn't know the rules it like learns just as well or does it do better without knowing the rules like what's the consequences of doing that yeah that's a great question um I'd have to go back to the paper and remember the exact results it certainly can do just as well so it could quickly you know as you still have to give it some feedback it has to know whether or not it one but it doesn't have to know sort of all the individual rules um and so uh you could I just don't remember how much additional data you needed in that case as you guys might remember from last time we saw that there is a really substantial impact of architecture so depending on the architectures you're using and you know are using a convolutional neural net or some other different types of networks those also make a massive difference the amount of data you need and the quality of the result so I think that's something to keep in mind when we think of you know removing information like what the rules are that you could imagine that if you do that but then you have some other Innovations in terms of the architecture it might be that you need only the same amount of data or even less than what we're needed here so generally like they don't do full oblations over all the combinatorics of this sort of ways these systems are specified it's a good question but that's that works certainly sugested and they've also extended this to other games so things like chess and others just just show show that you can use very similar techniques to conquer those games as well all right with that let's switch over to Dan um so I'm really delighted to have Dan talking today um he is I guess I'll keep this here until he comes up um he is a post talk fellow here at Stanford he um he'll introduce his own background a little bit more but um has he has a lot of expertise in thinking about different Frameworks for uh how do we think about rewards and what are the implications of the different ways we're going to define those in terms of the subsequent type of systems we might [Applause] develop please please hold your applause until you see how it actually goes um all right I'm gonna need a sec to get this hooked up while I do that um I should not I am going to ask you uh at various points maybe to to talk to some of the folks next to you so if you're not in a good position to do that uh now might be uh a time to to move to get yourself in such a position okay is that good is that too loud just right love it okay and we go to 250 yes 250 y perfect or or we can at any R Great uh okay uh so uh yeah I'm Dan Dan Weber um here today to talk to you about value alignment um but before we do that maybe it is just worth saying a little bit about uh who is this guy uh why should we listen to him or care at all about what he has to say he's not the professor um so as uh as Emma mentioned I am a postdoc here at Stanford in Hai and Eis that's uh The Institute for human- centered artificial intelligence and the center for ethics and Society uh if you've taken a lot of Cs classes uh at Stanford you've probably seen somebody who has my job at some point or other um yeah big part of my job is embedding ethics into computer science courses like this one uh before I came here to Stanford I got my PhD in philosophy at the University of Pittsburgh uh where I wrote my dissertation on uh moral theory uh which basically means uh just trying really hard maybe too hard to think systematically about value uh which is what brings me to you today so before that even uh I got my bachelor in computer science at ammer did a couple of years in software development after that so uh I'm not completely new to CS uh I know this world a little bit uh I did take an introductory course on AI that was 10 years ago uh I think the field has changed immensely since then I don't even think we covered reinforcement learning at all uh so you all are going to know the reinforcement learning way better than I do I'm not here to be an expert uh about that uh what I am hoping to do is give you a bit of a window uh into how to think about value and how it might be more complicated than uh you think so we're not going to solve any deep problems about value in the next uh hour uh we're not going to be able to go very in depth on a lot of this stuff if you're interested in that I recommend courses in the philosophy Department uh but but just try to give you a quick maybe lay of the land uh sense of sort of the range of possibilities when we're talking about value and value alignment um so okay might help to start with an example of value alignment or uh maybe more accurately an example of value misalignment uh one of the classic examples in this literature is uh paperclip AI uh but this example from uh Nick Bostrom in 2016 maybe you're all used to this uh in reinforcement learning but tells you something about the state of this literature that a classic example could be from 2016 um so Boston describes an AI designed to manage production in a factory uh which is given the final goal of ma uh maximizing the manufacturer of paper clips uh do anyone have an idea maybe of how this example continues maybe you've seen it before anyone know this one no okay well uh in in bostrom's example at least uh this AI proceeds by first converting the Earth and then increasingly large chunks of the observable universe into paper clips uh okay now bostrum is thinking in particular about super intelligent AI that's what his uh book is about so he's got the destruction of the entire universe in view um but even a less powerful AI system uh might pursue a simple goal like this in surprising ways does anybody maybe have a more a more mundane example of what what could go wrong uh if an AI system were say in charge of a a paperclip Factory given no further instruction than to maximize the production of paper clips yeah be schedu for people for a lot of shifts like through the night and then fire workers to complain higher than you world yeah good good right yeah we could maximize production if only we trapped people inside the building and made them work around the clock right uh excellent any other yeah doesn't about the quality of theer right so they could all be really bad good yes exactly it might be that the the easiest way to maximize the number of paper clips I produce is to produce really terrible paper clips right that's not really what I was looking for probably uh great thank you anyone else yeah I mean if the price of like electricity changes like at different times a day it could be like trying to make paper clips but just like economically in efficiently yeah good right so it's maximized the number of paper clips but there's no there's no sense of sort of other goals that you might also want to pursue here like like efficiency or you know minimizing the amount of electricity you use or anything like that great yeah or you could imagine you know uh I mean depends what levers the AI has to pull right but you could imagine it recycling the Factor's Plumbing for raw materials right or locking out humans who who could interrupt its process right uh something like that so great so in general we might say the the problem of value alignment uh is this problem of how do we design AI agents that will do what we really want them to do um where what we really want is usually a lot more nuanced than what we say we want right uh humans work with a lot of background assumptions and these assumptions can be uh hard to formalize easy to take for granted right if I told you as the manager of the factory to maximize the production of paper clips uh you would realize that you should do that you know consistent with existing labor laws or uh that you should make paper clips that actually work uh or you know that you should be on the lookout for keeping your costs down things like that um but uh but because these can be hard to formalize they're easy for us to forget about uh it's hard to solve this problem just by giving better instructions uh to AI agents um and here I mean if anybody wants to give it a try what would be what would be the better how would you solve this problem maybe just by trying to give a better instruction to the AI anybody have a have what they think might be an improvement on just maximize paperclip production yeah good yeah so uh yeah specifying a that you want paper clips of a certain quality and giving a sample of what that looks like good that would help uh that could help address this problem potentially of can you maximize production just by making worse paper clips right might not go far enough to say and by the way you shouldn't uh you know work the the factory workers Around the Clock um but great start yeah maximize the long run profits of theer factory good good so yeah giving a broader goal uh right I I want to maximize the production of paper clips but that's that's something I want probably because I want to maximize The Profit that the factory generates good um is that going to be enough to avoid all of the problems that we've seen come up I mean yeah I mean it's most of them right like you need high quality paper clips you can't turn the universe into paper clips will be zero you can't be using too much electricity or like cotton like doing things economically and efficiently because it won't be proper I mean the labor laws are probably thing that you'd be violating yeah right I mean if there's yeah if there's enough people you know willing to work in this Factory maybe we were able to keep a lid on how poorly we treat people we could we could get away with maximizing profit while still um but good okay so that's that's getting a stum of the way there but still there there's a worry about uh yeah about essentially treating people well okay so uh I mean we could keep we could keep doing this all day but hopefully this is a little bit of an illustration uh you know even trying to think of better instructions you might just realize oh there's another thing I forgot there's another thing I forgot um I mean you can compare this maybe to the difficulty in manually specifying reward functions I mean in some sense this is the same problem right is uh okay I think I I think I know what the thing is that uh that I want okay it turns out to be much more complicated than that much harder to specify um especially uh when you're thinking about making a system that's going to take instructions from users maybe who are not experts in reinforcement learning right uh folks in this room are going to be relatively good at seeing foreseeing these kinds of problems with giving incomplete instructions uh if you're designing a system that's supposed to take instructions from non-expert users uh they might not be so good if you're seeing these these issues um okay maybe any I should I should say any questions now and in general going forward I mean if anybody has any questions at any time don't hesitate to raise your hand okay um so we have this problem how do we design AI agents that will do what we really want um but that's a little underspecified right I mean there are lots of things that we might mean by a phrase like what we really want uh so here's one of them you might think value alignment uh is the problem of Designing AI agents that do what we really intend for them to do right the problem with paperclip AI might be that it failed to derive the user's true intention right which is to let's say maximize production subject to certain constraints right uh maximize production without overworking the workers and you know while making sufficiently good paper clips and while keeping costs down and so on and so on and so on uh deriving that Nuance complicated intention from the UND specified instruction maximized production right if that's how we think about value alignment then of course the solution is going to be to design a systems that can successfully do this translation take under specified instructions uh figure out what the user's actual intention is that they're trying to express uh and then act on that instead um how how is this from a technical perspective here's uh Jason Gabriel a researcher in philosophy and ethics of AI so what he says about he says this is a significant Challenge and he means from a technical perspective to really grasp the intention behind instructions AI may require a complete model of human language and interaction including an understanding of the culture institutions and practices that allow people to understand the implied meaning of terms that's what he said in 2020 does how how do how do folks in this room feel about how this this quote has aged maybe in the last four years does this seem like a significant technical challenge does it seem less significant maybe than it might have seemed four years ago for any reasons seeing a shaking a head why not well you're probably trying to imply uh trying to allude to uh GPT but I don't think that's enough because GPT might like omit uh certain aspects of the world model that might still cause loopholes like that so I don't think the problem has really been solved good yeah so yes I am uh I am not a subtle man uh I was indeed I was indeed thinking yeah require a complete model of human language and interaction hm that maybe sounds like a model that uh a lot of folks have been hard at work uh developing but uh but yes I agree with you our uh yeah so you might you might think uh yeah could you use something like an llm to to affect this translation as part of the system um but uh yeah how complete do we think those models really are uh if if I give this under if I if I say you know if I give to chat GPT uh you know the user wants to maximize production in the paperclip Factory what do you think they really intend uh is it going to catch all of the nuances that are typically communicated uh you know when one human is talking to another uh yeah I I agree there's reason to doubt that um but you know see see what the future holds uh but that's the that's the technical challenge um there's a philosophical challenge here as well which is that you might think our intentions don't always track what it is that we really want um so classic cases of this uh might be cases of incomplete information or imperfect rational we've sort of already uh broached this one right I mean suppose that I intend for the AI to maximize paperclip production again subject to these constraints because what I want is to maximize return on my investment in the factory right if the AI knows that I would get a better return by producing something else or by selling the factory uh has it given me what I really want if it does what I intend which is for it to maximize paperclip production right well in one sense yes but in another sense no you might think that other sense uh is the more important one it's not giving me the thing that I really wanted uh because that thing is coming apart from from my plan about how to get it um okay so you might think the solution here is that uh what you really want is an AI agent that does what the user prefers uh what they actually prefer even if this isn't what they intend uh on this interpretation of the problem paperclip AI is misaligned because I prefer that it not destroy the world or I prefer that it not lock all the users in the factory uh users all the workers um okay now the problem here is that uh if you want to align to what the user actually prefers there's going to have to be some way for the agent to know what the user prefers when that differs from the intentions that the user expresses um how are you going to go about doing that um solution to this uh might be to work with the user's revealed preferences right preferences that uh are learned from observing the user's Behavior or feedback uh obviously you've learned some techniques for how to do this kind of thing but not every technique is going to be like this right you're going to have to do something like inverse reinforcement learning or reinforcement learning from Human feedback that allows the agent to train on observation of the user uh to try to determine what they prefer based on uh how they've behaved or what they've told it its preferences are um of course you're going to run into this problem that from a finite number of observations of the users Behavior or preferences uh there are at least in theory infinitely many sort of preference function that that could represent uh inferring that could be a challenge um and it might be especially hard to infer preferences about unexpected situations uh like emergencies where you don't have any direct you're unlikely to have directly observed the user's preferences about unusual emergency situations because they arise so rarely um but you might think it's precisely in unusual or emergency situations where it's so important for an AI agent to be aligned to our values um so those are some of the technical challenges but here again we have a philosophical problem uh which is that just as my intentions can diverge from my preferences it seems like my preferences can diverge from what's actually good for me uh or so some people might think so uh for instance a lot of people uh prefer to smoke uh but you might think it's not really good for them to do that uh or I might prefer to maximize profit on my paperclip Factory at all costs uh but maybe it would be better for me to be less focused on money and spend more time with my family right uh so uh the the thought here is that your preferences might actually in some cases come apart from what's really in your best interests uh objectively speaking and that this is something that you might try to align an AI to instead we want to do what's actually in the user's interests uh even when that's not what the user thems prefers to do right if you think this you're going to think paperclip AI is misaligned because it's objectively bad for me for the world to be destroyed uh or objectively bad for me for uh these things to uh you know for the pipes in my factory to be ripped out or what have you um here face sort of a sort of combined Technical and philosophical problem though which is that uh unlike the intended meaning of my instructions or my revealed preferences what's objectively good for me is uh not something that can be determined empirically right this is a philosophical question not a scientific one um so it's not just a matter of building the right model of human language or observing the user enough um to to figure out what what's actually in my best interest uh is on is not entirely an empirical Endeavor you've got to actually do some substantive uh moral philosophy to solve this now the bad news uh for solving this problem is that uh there's a lot of disagreement about what is objectively good for a person um I say philosophers disagree about this but I think non-philosophers also disagree about this as well um right is it is it just a person's own pleasure or happiness that's good for them uh or is it the satisfaction of that person's desires or preferences that could be different from pleasure or happiness I might have preferences that will be satisfied uh you know only after I'm dead or something I I'll never derive any pleasure from their satisfaction although they could still be satisfied um or do we want to say that things like health or safety knowledge Human Relationships these things are objectively good for us even if we don't enjoy them don't prefer them these are all sort of live live options in the theory uh of value and depending on how you answer this question you're going to be looking at a different kind of value uh even if you already know that what you want to do is align to what's in the user's best interest um okay the good news though is that behind this disagreement there there is quite a lot uh of agreement I would say uh these things like health safety Liberty knowledge dignity happiness almost everyone agrees that these things are at least usually good for the person who has them even if you think that really ultimately all that matters uh all that's good for a person is their own happiness well these things typically make the person who has them happy uh so uh you might think you don't really need to resolve this underlying philosophical dispute to have a good sense of what's in the user's best interest right I mean these are uh these are things that for the most part are are in a person's best interest uh sort of no matter what theory you endorse behind it [Music] um okay any any questions about any of that so far okay one complication about aligning to the user's best interest uh is that one thing that we normally take to be good for a person is autonomy right which is the ability to choose for yourself how to live your life even if you don't always make the best choice uh it might be good for you to have this kind of control uh over your own life right we want to avoid paternalism we want to avoid choosing what we think is best for someone rather than letting them choose for themselves so even in a case where you're aligning to to the user's own best interest uh you might still need to take their intentions or their preferences into account it might be that part of what's best for them is to have their own intentions fulfilled to have their own preferences honored um okay so uh this has all been pretty abstract I want to move into uh slightly more concrete case study but first maybe just to to recap what we've covered so far um value line is this problem of Designing AI agents to do what we really want them to do um but this could mean a lot of things it could mean doing what we really intend them to do what we really prefer that they do what it would be actually in our best interest for them to do uh and all of these things can come apart they're not necessarily the same thing and they might impose certain technical or philosophical constraints on your approach um okay let's talk about how this works or what kind of difference this could make in practice um think a little bit about uh llm chat bots so everyone who talks to chat GPT is talking to the same chatbot okay I there's there's different G there's GPT 3.5 there's there's GPT 4 ignore that right I mean the fundamentally it's the same same chatot for everyone um but plenty of chapot providers are now offering uh a wide range of different chatops with different personas some of these designed by by users themselves um so these examples are all from uh character. uh which which promises personal ized AI for every moment of your day uh so here this comes out maybe a little small but you can talk to the the creative writing helper you can talk to the are you feeling okay bot you can talk to the dating coach uh these are these are some of the relatively normal ones uh you can talk to depressed roommates uh you can talk to torbot I am torbot I believe in the free market uh you can you can chat with AOC you can chat with Donald Trump you can chat with feminist Fay I am a feminist that hates Donald Trump okay lots of lots of variety lots of options here um you could imagine yet stranger and stranger personas uh that you that you might uh build into a chat bot so or that your users might right none of this all of these are are designed by users none of these are coming top down from the provider of uh of the lln so um okay so think about this a little bit imagine you're building an llm chat bot to serve as a source of news for users I mean maybe this maybe this is going to strike you already as crazy but I think there's there are a lot of people out there who already treat Google as their primary source of news a lot of people who are replacing uh Google and other search engines with llms so there's there's de I think there's demand for this imagine you were you were wanting to fill it um and think a little bit about these questions how would you make in what ways would you make the chatbot personalizable if you were interested in aligning to the user's preferences right in what ways might you make it personalizable if you wanted to align to the users's best interests um and think a little bit about the pros and cons of this so I think take a minute to think about this and then maybe chat with somebody near you compare notes see what you're what you're thinking and uh and we'll come back in a couple minutes for uh for a larger discussion e it's clear AC all right I've been I've been hearing a lot of good conversations that I'm uh not eager to cut short but uh maybe there are conversations that we can uh now now bring back to to the whole room so anybody anybody have any thoughts from their discussions that that maybe they want to share um I know I know some of you have thoughts cuz I was hearing a lot of good ones out there so uh don't be shy you probably have thoughts than I do uh if you don't say anything then I'm just going to tell you what I think and uh then you're going to be stuck with that yeah I guess for the first point it would be I think it's pretty simple you'd probably be you'd use sort of a preference optimization approach and you offer 10 different questions or of of hey do you prefer this answer or that answer and then you would optimize the you would optimize the news that's being fed to that user accordingly yeah good if uh yeah like you say fairly simple if I want to uh align to the user's preferences I'm going to figure out what it is that the user prefers I'm going to give them news that fits that profile right um is that sort of what everybody was thinking about this first question anybody anybody have something they want to add to that yeah great okay I think yeah I think think that's exactly right um okay what about what about if you were trying to align to the users best interests their own their own good objectively considered uh yeah a thought on that one which is that you don't it's pretty it's pretty hard to know what someone's best interest is as well as avoiding sort of the tenant that was on the previous slide of don't internalistic so really the only way you could have any hope of doing this would be optimizing for best interests of like an entire population so you you basic like if it doesn't apply if the policy of best interest doesn't apply to everyone then I would argue that you can't actually you can't actually do it for an individual user so that's the way that's the way you would personalize it if it's wanted align to a user's best interest is you would you wouldn't ask that question to begin with you would just have it set already for the entire population okay yeah I mean um good I think uh well without I need to constantly resist the uh temptation to just turn every one of these lectures into a philosophy class so I I I love that answer I I'm I'm curious about uh I'm curious about why it might be less difficult to determine what would be in the objective best interest of a large group than maybe of one person but this is a question we'll come back to uh maybe later anybody else have have thoughts about this second one thing something different come out of your discussions yeah I think you know just take the movie her as an example right you got to know the person very well the person opening up a lot of data a lot of the uh uh the information um then you will be able to maybe prioritize on how you uh make the suggestion and also depend on the person using the tool right for example some tools are better on uh you know deling to the news trying to understand the sources some of them are better at you know you just want to take the most important thing and then just you don't have to spend time on the email on on random news and all that um so I'm talking about like two components at least one is you know the person better the other person the other thing is you know you know the behavior and how they would use the tools and just refrain I mean the tool should be refrained from like extending uh too much and just grabbing too much attention of the usering that up good thank you yeah and I think that the there there's something to this right that like it might be that just from observing someone's preferences for long enough getting that that much data about them you might be able to get a little bit of insight maybe into sort of what's in their best interests even when that diverges from what they want in the moment so um yeah great I see yeah we were similar we similarly had an idea about like maintaining some sort of uh State for the users like um best interest like maybe you could have like some sort of structure that would represent different aspects of the best interest and which could be personalizable to the user and with every like llm interaction it would uh reprompt the uh llm and then change this if appropriate and over and every time you are trying to get a like a output for for the user uh you could uh put this as part of the context and write the prompt accordingly alongside whatever the user is asking in order to fit that goal better good well that s that sounds to me a little bit more like maybe aligning to the users's preferences maybe I misunderstood this sounds like kind of trying to figure out what it is that the user is wants to get what they're looking to sort of get out of the the bot and then determining which return based on that maybe I misunderstood I don't think that's necessarily true I think you could write a prompt that would uh like the internal prompt for keeping up the state of the users best interest could be written such that and the field could be provided such that it would try to meta like it you could ask it to meta reason about what the users interests likely are B I see okay good um yeah great um yeah any anybody else I mean maybe there's there's a more there's in some sense maybe a more basic question behind this um which would be something like what what what might what is maybe in a a news seeking agents best interest what kind what kind of news would it be best to provide somebody yeah probably that like a variety of perspectives and you check that it's actually correct as well I think it's in the users's best interests that they're properly informed as opposed to maybe like only seeing news that puts them in a good mood or align with their existing opinions yeah good right yeah you might think uh yeah in contrast to the approach we discussed earlier of we're gonna we're going toig query the user about their preferences every time that we give them news we're going to say did you like that was that what you were looking for yes no we're going to adjust and give you the news you want based on that uh yeah you might think it's it's actually better for people to be exposed to uh to high quality news uh unbiased news to be exposed to a variety of opinions and arguments um rather than right what's the worry about aligning too heavily to the user's preferences is that you might be putting them in in a kind of echo chamber right where they're getting all of their news from uh talking to you know Donald Trump bot or talking to feminist bot and they're not getting uh other perspectives yeah good any does anybody else have a different answer to that question maybe what would sort of be in the user's best interest to receive as news or how you would approach that uh from from like a design perspective okay well good I think that's I think that's definitely right I mean in terms of Pros pros and cons did anybody have sort of get into this like which if you were if you were designing the the news chatbot which of these approaches would be better what would be the pros of one cons of another yeah I mean I think like optimizing for best interest is almost like paternalistic cuz you're assuming that you know the best Elric the user or you have a good approximation you might really not know at all so it's like uh like like is some like that user had like some sort of tragedy or whatever in their life recently and then you're and then like there some sort of recent news event has like a lot M St like maybe they don't want to be exposed to that even though maybe it's like a very important event you should know about this but like you don't have complete state of like the user psychic State like how how they actually feel so maybe just using the preferences that are already that you actually have like just from using the app what do they click on might be better good yeah so there's um yeah I think that's great so there there're like uh even if we can say things maybe at a very general level about what is in a person's interest right what is actually good for a person in general right that leaves a lot of room for variation from person to person right especially uh if you think that quite a lot of what's good for a person is built out of sort of uh subjective interests of theirs right or their desires or um what makes them happy what makes them unhappy um you might that's that's not a thing you might have full access to um so there is this problem that if you're trying to align to what's really good for the user you're your only real way to do that is by aligning to what it is that you think is good for the user right and you might be you might be good at figuring that out you might not be um and absolutely that's where you run this risk of paternalism so an advantage of of just aligning the user preferences giving them what what they've said that they want uh means that you you avoid that risk right you avoid trying to position yourself as saying no I I know what's really good for you when maybe you're not in a position to determine that um yeah anyone else on this point okay now yeah yeah I just thought that uh the Contra argument is that you know running a risk of uh being personalistic you actually give convenience but then giving them low quality choices you actually waste up a lot of time so you know ABS Evol yeah um yeah I think that's right um and and right I mean to to the earlier point it's uh yeah there might be some aspects of the user's best interests that are easier to determine than others right it might uh we might be reasonably confident that it's it would be in any user's best interest to be given high quality sources of of news to be exposed to a variety of opinions that might be you might want to align in part to that sort of General human interest while still allowing some room to align to the user preferences so these are not uh necessarily mutually exclusive goals in alignment right I mean it might be uh you know in in some ways it might be worth uh focusing more on the user's preferences in some ways in some cases context you might want to focus more on what do we think is actually good for the user because what they prefer might be you know junk information or uh convenient bias confirming information things like that okay great um well there's one thing I think that has been not completely absent maybe from our discussion but I hope noticeably absent from my lecture and for my slides so far is there a is there maybe a big piece of the puzzle that we're missing something that you would have thought this would be a this is what we're going to talk about with value alignment and why haven't we gotten there yet anybody at all we've talked about aligning to the users uh intentions we've talked about aligning to the user's preferences to the user's best interests yeah like a way to measure alignment a way a we measure alignment yes uh that has been absent um I just think yeah maybe aligning to a society's overall interest rather than just person yeah so you are correct but I was thinking yeah I mean that there are people other than the user um where's oh where's my text there's my text uh yeah there's uh there are other people whose interests are important maybe to take into account uh than just the person who is giving instructions to to the agent um so you might think there's there's really another possible interpretation of what we're after with with value alignment right which is that an AI agent is value aligned if it does what's morally right right uh I mean the main problem with paperclip AI isn't that it does what's bad for me it's it does what's bad for everyone if it destroys the world um right it does what's bad for uh the factory workers if it makes them work around the clock making paper clips and so on right so earlier we were sort of focusing on what what do we mean by what what we really want what does really want mean um this this would be to focus a little bit more on the on the wi what is it that we really want um because of course what the user intends prefers even what's in their individual interests um might be bad for others right right we probably don't want to say that paperclip AI is value aligned uh if it maximizes production by you know exploiting the workers in the factory even if I as the user have no calms about exploiting the workers right um okay that said I it wasn't a waste of time to start by focusing on the user um right even if we want to align to to morality or to the interests of more people than the user um we also do want to align to what the user wants when what the user wants uh is morally acceptable right so it still matters how we understand what it is that the user really wants even if we need to place that in a larger moral or societal context um but of course here too we have a philosophical problem right I mean which which things are really morally right uh there's a lot of disagreement on this one too uh not unlike the question of what is objectively good for a person right um is it right to lie to spare someone's feelings uh is it right to Pirate copyrighted material right is it right to buy luxuries when you could donate to charity instead is a right to kill one person to save five or a thousand or a million uh right these are uh at least some of them uh I hope you think difficult uh moral questions certainly they are moral questions that people disagree about um again philosophers and and non-philosophers alike um so how do we align to what's morally right in the face of this disagreement um this is you might think where my field of study comes in um you might turn to moral theory which is basically uh just a systematic attempt to answer questions like these right so um a moral theory you might have heard of it's called consequentialism says that an act is right if it produces the greatest net good of any act available or you might have heard of uh utilitarianism which is a kind of consequentialism says that you should uh produce the greatest total happiness uh that you can uh across all people um right if you have a theory like this this this can be used to answer some of these difficult questions people disagree about right is it is it right to lie to spare someone's feelings well if you're the consequentialist you'll say it might be if you can get away with it if no one discovers that it's a lie uh and it makes somebody feel better that might produce more good than not telling the LIE um so there's an idea here which is that we could align AI to morality to what's morally right uh if we align agents to the correct or best moral theory uh there's going to be there's going to be a philosophical problem with this does anybody think they know what it's going to be has a similar form to all of the philosophical problems we've encountered so far uh well there there's a lot of disagreement about what the correct moral theory is so there's disagreement not only at the order of uh sort of ground level uh moral facts about whether you should uh tell a lie to spare someone's feelings but also about the best theory for systematizing um this kind of stuff um right we already saw consequentialism um but there's a whole host of others and just to put a few of these on the table uh just to give you a sense of of the of the range that we're at um there's you can be a prioritarian and where you would think that really what you want to do is not to maximize the total good but to to produce the greatest weighted sum of good where the interest of those who are worse off is given more weight or you could take this to a sort of extreme u a kind of maximin or Minimax view where what's morally right is to make things as good as possible for the person who's left the worst off um by what you've done or to minimize the negative consequences for for the person who who suffers the most um so in uh you know cases where you have to think about a quantifiable good but if I have you know four people who I can how would I do this assign Goods to I have the option to say you know I have options to distribute Goods say these ways among different people right if I'm the consequentialist I'm going to say well I want the one that produces the most total good that's this first option um if I'm a prioritarian well I'm going to need some kind of way of waiting this say that uh you know the way to give more weight to the people who's worst off is that your uh you know we waight the good to you on a log scale or something right uh then uh well in base 10 or or uh at least this is going to be uh the prioritarian choice right we want to by giving more priority to those who have it worse um with uh you know the sorry I'm not explaining this very well trying to move too quickly if I was taking the log of each of these as the prioritarian I'd say here you know we have uh this is coming out to six this is coming out seven that's better if I want to make things as good as possible for the person who ends up worst off I might choose this last option even though in this option we're getting the Le the least total good right the person who ends up worst off is doing better than the person who ends up worse off in the other options um so all these options and more are available to you in moral theory uh right you might take a satisficing version of any of these views instead of trying to maximize the total good you might think an act is right if it just produces a sufficiently great uh sum of good or weighted sum of good um uh we haven't even yet touched deontological views which hold that even acts with the best consequences can be wrong if they violate certain moral rules or rights um often these rules will be rules like don't murder anyone don't steal lie keep your promises um right you might think that an act can't be an act can't be right if it involves stealing from someone even if it produces a lot of good this is something that uh VI like consequentialism might not capture um right although you might think that these rules or rights are themselves justified by their by their good consequences it would be best if if we accepted rules like this and followed them um okay returning to this this problem of paternalism that we encountered earlier um there is another problem here so one is there's just what is the best moral theory who knows that's yeah you know I've been uh I've been working on that for a decade and haven't gotten much closer to it uh but even if we knew what the best moral theory was it might be bad to design AI agents to act on moral values their users don't share right this could be because we want to avoid a kind of paternalism where we say no this is these are the correct moral values um could be for more practical reasons just the users won't trust AI agents if they uh disagree with them about moral matters okay so there's some difficulty trying to align to the best or the correct moral theory um but also like with the objective good where there's a lot of disagreement here there's also quite a lot of agreement uh about what is the morally right thing to do um right in in simple cases we all agree uh you shouldn't kill people you shouldn't lie to them you shouldn't steal from them um so another idea for aligning to morality would just be aligning AI agents to what we might call Common Sense or consensus morality right Common Sense moral ideas that most people agree on instead of trying to make AI morally perfect we should just aim to have it make moral decisions like a normal person would um right this this view probably ends up being pretty deontological and satisficing right most of us think you follow certain moral rules you respect other people's rights uh then you're not morally required to do the best you can um uh it's fine to do less to prioritize yourself uh in some cases things like that um right now one advantage of allying to something like Common Sense morality rather than to a particular moral theory is that moral theories often have surprising implications um I know we're just about out of time so I'll just I'll skip to the chase on these um I mean you can think about the consequentialist requirement to maximize net good I mean suppose we had suppose you had an AI agent that was a surgeon uh five five patients dying Each of which needs a different organ transplant to save their life well if you're thinking about just maximizing the net good subject to no constraints maybe maybe what you think is well that nurse walking by in the hall has all of the organs that I need maybe if I just maybe I just harvest the organs from the nurse put them in the five people save five lives the cost of one five is greater than one we just maximize the net good that's probably not what you wanted your surgeon surgeon AI to do um think about cases uh uh where you might want to break a deontological rule against lying as well right AI agents aligned to a particular moral theory might uh discover some of these surprising implications before we do and they might discover them in practice rather than in the philosophy seminar room which is where we prefer for them to come up um so uh by contrast aligning to Common Sense morality you might end up with an agent that behaves more predictably right making moral decisions like a regular human uh it might be predict unpredictable in some edge cases right where Common Sense arguably runs out right would an AI aligned to Common Sense morality kill one person to save a million I I don't know that's what we you know we got into moral theory to try to answer hard questions like this if we've just taught the agents to think about morality like we do it might be as unsure as we are about what to do in a case like this um I need to let you go so I'll just leave you with the thought um how bad would that be how bad would it be if if AI was as unsure about morally hard cases as we are um okay we've covered that uh I will let you go to enjoy your Wednesdays um if you are interested in talking more about any of this ethics in general feel free to reach out set up a meeting we can talk more um any any questions now before we depart or I can stick around for a few minutes if folks want to talk to me offline okay great well take care