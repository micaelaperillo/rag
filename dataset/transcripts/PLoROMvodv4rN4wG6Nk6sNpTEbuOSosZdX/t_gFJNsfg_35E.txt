hi everybody welcome back um we're going to start talking uh more about sta efficient reinforcement learning today but before we do that we're going to start with a check your understanding so this ask you to think back about what we were learning from multi-arm Bandits um I would probably do one and six first CU they're kind of warm-ups and then the rest of these just to clarify in terms of notation I'm using F of Delta here to be a function of Delta because I was slightly loose on exactly what the dependence is on Delta in terms of whether you know um it's like Delta over t or what we're going to choose for that function so I just wanted to be agnostic to that there and put it as a log of a function of Delta for and as usual feel free to look back at your notes from last week if you want to refresh your brain on the notation for all right one more minute to write down your initial answers and then I'll ask you to turn to a neighbor and compare for all right why don't you compare answers with someone that's nearby you different all right great let's come back together um so I think most people conversed on the same answer for the first one which is yes algorithms that minimize regret do also maximize reward o hold on pen is not working see if I can grab a different one so the first one is true I can get some here okay so the first one is true if you minimize regret you also maximize reward um for the second one is that one true somebody want to say why it's true you say it's false saying the second one is true hold on let see if I can get my thing to power up my pen isn't working so the second one is uh just double check that I I'll keep it back onto here in terms of the answers I move things around a little bit last minute which is always dangerous um but I wanted to include a couple additional ones okay let's see if we can make this do the right thing okay so the second one should be true um this is basically the UCB algorithm which is this is the empirical estimate of the performance of each arm so in the case where we just have a finite set of arms which we can also think of as a finite set of actions um we just look at what their average reward was n t of a was how many times have we pulled action a after T time steps and log of f of Delta was just the term that we had to try to express the dependence on Delta Delta was used to look at confidence intervals um that we were using for the upper confidence bound so this is true um the third one is also true true so in general with our confidence intervals um you will be selecting all arms an infinite number of times but it might be really slow late later on because uh let's say you have a really big gap between arms then um that log term and and you'll still you'll have a t dependence in there in general that will continue to grow a little bit so you'll sample another arm again which sort of helps with the fact that you might have been really unlucky and gotten a really weird estimate of the arm performance so far okay this one was a little bit subtle um and I realized it could be not quite clear here whether I was asking you to think about the T over Delta part or the 1 / < TK < TK and T of a I wanted you to focus on the first thing so what this is saying here is that instead of shrinking our confidence intervals by a rate of one over < TK n t we're shrinking them at a rate of n the minus 4th just rip that it's very squeaky okay all right so let me just give so we shrinking it so will that mean that our confidence intervals are wider or narrower for the same number of counts so let's say versus NT of a to Theus 12 so for example if NT of a is equal to 100 you've pulled this arm 100 times which of these two is going to be bigger one on the left or the one on the right the one on the right that's right so instead of it being um oh the other way around so this is going to be so if you have 100 to Theus 1/4 versus 100 1/2 this is going to be 1/10th this is going to be approximately 1 over 3 there's like several different inverses here I know what this means basically is that we're growing we're shrinking our confidence inals slower so another thing you might see often as a bonus term um Deep Mind often uses this in particular for some of their algorithms is to the minus one so that's a faster rate so then that one would be 1 over 100 and you can think of these as trading off different amounts of sort of essentially how much exploration you're going to get because this is saying sort of how quickly are you collapsing your confidence interval as you have more data now as somebody pointed was asking me about when I was going around they're like well we didn't just randomly pick this we pick this because of the uncertainty bounds that we derived from from hofing so hofing said if you have an empirical estimate of your mean how far away could that be from the true mean well you know under pretty mild conditions about your variable being bounded we could get this kind of uh one / Square n rate so someone was asking me very reasonbly was asking me like well why would you pick say this or something else you might pick this because you just don't want to explore as much so even though this holds for our Theory it's often somewhat conservative in practice so you might just pick something like a faster exploration rate because empirically you want to just you know explore less and you could think of that as being related back to what we saw with po that a lot of their theoretical derivations said this is what your step size should be but it was way too conservative for most realistic applications so they just changed it and they introduced the clipping thing on the other hand um there might be cases for which you might not be sure you could get this sort of rate and in those those cases or you might have other reasons to think you might need more exploration so for example maybe things are non-stationary and you think my customer preferences are actually changing over time and so I want to sort of Explore More over time than I would if I assume that I was stationary okay and we'll talk more about stationarity in just a second so given all of that this means that this expression would actually have wider confidence intervals and probably a higher upper confidence bound than our original algorithm which means that we would still expect that over time it will learn to pull the optimal arm bless you more than any other arms but it probably won't have as tight regret bounds because we may be exploring too much now this was an interesting one um will this if we add this particular bonus term make the algorithm optimistic with respect to the empirical Rewards I think I know somebody want to say if it's going to make if we add on a bonus term will it make it optimistic with respect to the empirical Rewards just the empirical rewards so like compared to your empirical mean I'm not trying to make it a trick question just yes yes exactly so if you just add 20 to your empirical estimate it will be optimistic with respect to your empirical estimate but is it guaranteed to be optimistic with respect to your true mean so imagine if I'd said B was like 0.001 it would still make it be optimistic with respect to your empirical rewards but would it necessarily be optimistic with respect to your true mean in general no right like so if you think back to the our um our Bandits which just had binary rewards let's say you know you have a coin that actually has um a 05 probability of getting a heads which we'll call a one if you flip it once and you get a Tails your empirical estimate will be zero if you add a bonus of 01 your empirical estimate will be 01 the true value of the mean is still 0.5 so one of the key ideas from using hting and explicit upper confidence bounds is that in general it's not easy to figure out a simple bonus term you can add in order to make things optimistic and so that's why uh you might in General want to be using these um hting or other sort of uh explicitly derived confidence intervals okay great and then the last one is true does anybody have any questions about these yet explain number three again dep where the is at it's inside of here so if you go back to the slides from last time let me see if I just have them up yeah we create these upper confidence bounds so in general we Define these upper confidence bounds we talked about how we need the the bounds to hold over all time steps T and so that in fact this was not a perfect Rec um expression that we're going to have some sort of T dependence inside of the log and in general we'll have like something like t or t^2 inside of the log and so that will introduce a dependence on the time either the time step so far or your total time Horizon ins TI of your per confidence bound any other questions about this part okay right I'll make sure that the solutions are aligned okay so last time we talked about basian we defi sorry we talked about Bandits we which were this single state version of markof decision processes your actions didn't make any difference to the next state cuz you're always in a single state um we talked about how people often use the word arms as an equivalent for actions and but there we were trying to be really explicit about uncertainty over the rewards H and what and we talked about an algorithm upper confidence bounds for trying to be optimistic with respect to that uncertainty what today we're going to focus on mostly is basy and bandits and we'll get there in a few minutes before we do that I think it's nice to think about um I think it's exciting to think about all the application areas where these come up and I wanted to go through this example which I think I mentioned briefly in lecture one um just again to think about all the complexities which come up when we want to try to use these in practice and where Bandit algorithms in particular might be used so this is a really beautiful paper by Hobson Bastin um uh which was in nature a few years ago and they were trying to tackle a really important problem which was at the time as everything was shutting down with covid all these countries had to decide on a quarantine protocol and who to test so before you know a number of countries basically almost entirely shut down travel but particularly in the beginning people were letting uh and even then often there might be you know exceptions so as people come into a border crossing um organiz or countries had to decide who to test now they couldn't necessarily test everybody because re resources are finite they're also having testing facilities they're using for testing all of their own individuals and tests are expensive in addition when someone is tested they were going to ask them to quarantine um depending on where you were in the world that might actually have been funded by the government so they you know you have to go to a quarantine hotel which also cost the government money so there are a lot of reasons in this case that your resources are limited and you don't necessarily want to test everyone also in general it may not be necessary to test everybody if you're trying to minimize the probability of letting in people that have covid um in terms of limiting spread so if there's someone that's from somewhere where there's no covid then you don't necess need to test them so this is the setting what happens is that when people were coming into Greece they would submit a form in advance um like you when you go to the airport and your before you go Etc and then what they would do is they had this um approach called Eva which tried to use the prior testing results to figure out who to actually test when they came so what would happen is that then when somebody comes like the next day either they would say we're not going to test you at all and then you leave the premises or for a subset of people based on the form based on where they were coming and based on prior results they had they would decide to test someone then after you got that test you would send it to a lab and it normally um would take 24 to 48 hours I don't remember exactly what kind of test they were using there maybe it was some sort of Rapid PCR I don't remember um um those would go to a central data database and then they would use those results so someone all these people would quarantine you know for 24 to 96 hours or so during this time period they would get the results back if you're clear you can go and um you know proceed otherwise you need to continue to quarantine and then they're going to use this information to go back to Eva and update their algorithm so this is really cool because this is an opportunity to try to be very careful about resources but really do so in a way that still preserves the safety of the individuals in the country as much as possible and the public health so I like that in the besty paper they describe this as a non-stationary contextual batch Bandit problem with delayed feedback and constraints okay so that's quite a mouthful um but I think it's really nice to think about sort of you know as we go from the simple setting of just thinking there are K arms we can think about all the Practical things that we might have to deal with in this setting so here in some ways the K is very small it's only two either you're going to test someone or you're not going to test them so it's a very small action space which is nice in this case compared to what we've seen so far but we'll we'll see this this case later we're going to have context context you can think of as just being like States so people will have a feature Vector that describes what country they're coming from you know a bunch of other details about them and that gives you a state that we're going to use to decide whether or not to test someone okay so that's why it's contextual it's non-stationary because covid was constantly evolving um and often a lot of the information we were getting was lagged so if you're in Greece you might be able to see information from Sweden and from China and from the US but all of that information is often likely probably at a population level those people may or may not be the same people that are traveling to Greece probably in general are different um and uh because of the lag it may or may not be informative and in fact in their paper they are a lot of that information was not as informative as this kind of real-time information it's batched what I mean by that is that um and we'll see with this more today you don't get to make a decision after um every test or not test you don't see the result immediately so what happens here is that say you know 200 people flying on a plane you have to decide for every single one of them whether or not you're going to test them and then you wait you know two days so it's this delayed feedback and you have to make a decision for everybody before you get to observe that feedback and so that makes it quite tricky um and we'll we'll talk more about why that might be tricky for some of the upper confidence bound algorithms we've seen so far I think this batching is really important for many many application areas so if you think back to our guest lecture and you think about uh direct preference optimization this is another area where in general you're going to have be able to get sort of a batch of data label it all and then continue um so and some of the work that my lab is doing and some other people's work when we're thinking about doing adaptive data collection for preference optimization we again need it to be able to handle this kind of much more realistic batch setting compared to getting information after each decision okay we also have con so the delayed feedback is this 24 to 48 hours and the final thing is constraints so there are lots of constraints in this setting um which also generally changed the setting from a lot of the ones we've thought about so far so one is that you might have resource constraints um you might say at most we can handle let's say 100 I forget exactly what was in the paper a 100 tests a day so you're going have constraints on that the second is politically you might have constraints too you know it might be tricky for Greece if they decide that they're not going to let in anyone from Sweden so there might be different quotas and there might be other reasons to say we have to um think about some broader types of risks and benefits in these cases so that's also challenging what way you can think about implementing this is this could essentially change your policy class that is reasonable so instead of your policy class saying you can make any decision for any individual you may now have sort of a population level constraint as well this is something that my lab has thought about some with um our partner Shard goel who's uh at the Harvard kity school and there we thought about cases where you might have resource constraints and fairness constraints that mean that you can't just make decisions for people individually you you need to think about overall um trade-offs in terms of your policy quality that happen at the up population level the reason that's important is because it often introduces a lot of challenges computationally when you can't just think of each individual separately all right so we won't be able to cover all of the ways that they're um that you know they handle this algorithmically but I encourage you to read the paper if you're interested in this space and I think it's a really beautiful example of using reinforcement learning particularly multi-arm Bandits um to tackle this problem one of the things that they had to do so this is a real system they really deployed it in Greece I think when I talked to hobma she said it came together in like a month it was a really amazing effort and then one of the interesting things they also had to do here is to understand how much of an impact it made because they weren't going to do a randomized control trial in Co to understand this so another interesting thing that this paper looks at is using offline methods like the batch methods you've been seen in the past to try to estimate the counterfactual of how much impact this had so I think it's a really nice example of a lot of the different ideas that we've been seeing in this class all right so that's one of the many many ways that Bandits are useful clinical trials is another one AB testing um you know ad placement there's many many others as well but I think this is a really nice example in public health okay so now let's continue we're going to talk about specifically some of the algorithms that could be relevant to this and in particular Thompson sampling which is particularly relevant to this kind of batch setting all right I'm going to do very quickly just notation remember regret is the opportunity loss per one step total regret is the total opportunity loss we're using Q to denote the expected reward for a particular arm so one thing I'm blanking on who suggested this last time forgive me but someone uh came up to me and said hey couldn't we have used just like a smarter optimistic initialization you know do we have to actually um have these upper confidence bounds and I think that's a very reasonable suggestion H and that was a great follow up to some of the stuff we're going to talk about today so one simple thing you can imagine you could do instead of worrying about these upper confidence bounds which you have to update all the time is you just optimize you just initialize your Q hat to some high value and then you just update that estimate over time and when you do that you know that eventually you're going to converge to the right thing like ASM totically with a law of large numbers as you know as you you're not changing that initialized value that initialized value may or may not have been right it'll be an upper bound and it'll just converge to it so this is an interesting thing you can do the challenge with that is that in general you don't know how high to make it and so if you make it really high um now often let me just just be clear here what I mean by really high often this might be much much larger than the actual range of possible rewards so maybe your arm rewards can be between 0 one and you initialize this to 70 so sometimes you know the the initialization might be far higher than what is actually practical um it does encourage a lot of exploration early on which might be really valuable but in general unless you get the value exactly right which you generally can't know because that's why you're trying to learn in the first place um then you can still lock on to a suboptimal action and what do I mean by lock on is that you converge to a suboptimal action and then you never try anything again which means you'd get linear regret the other thing that's bad is that um if you initialize Q um too high then you're also just not going to benefit from you're going to be making bad decisions for much longer than you actually need to do okay so even while even though you know in theory this could be a good thing to do or like sorry in principle you might imagine this is a good thing to do um in reality it's very hard to set now it's also a interesting question of how you might do this with function approximation so if you think back to I know you didn't Implement deep Q learning but if you think back to deep q q learning where we used a neural network to represent the Q function do you guys think it is easy to initialize that so the values are optimistic I see at least shaking his head why not um it's just Network output specific value yeah it's yeah it's hard right like maybe you could train it on fake data but then you'd have to know how big the queue is yeah in general this is really it's if it was a table it's at least easy to write down you know like 90 for all of those things and that's what you initialize in a deep neural network it's really unclear how you like initialize those parameters so that for all the states that you would reach you would have a even a good shot of it being optimistic so I think that's another challenge here is um you know and that's a challenge for a lot of the optimism algorithms we'll see in general is can we do it with function approximation now there's a lot of work on thinking about sort of how to do things with function approximation and we'll get into that soon okay so if you do carefully choose the initialization value you can get good performance um under new way of measuring what good performance actually means okay so let's go back to regret so in regret we sort of just try to think about how do we quantify the performance as we make lots of decisions so T here is the number of decisions we make and we're just trying to think in this case about how many decisions we make over time let me see if the pen is finally charged not today okay so um so we could either be making lots of little mistakes or frequent large ones and what you might imagine that we want to do is to think about a different form of loss and so or in particular another form of performance then we're going to that is going to be pack okay so let's draw it that quick okay so think I drew this last time but I'll it again so I make this time step T and this is Q of a q of the actual arm that you pulled and this is Q of a star okay so let's imagine that you have an algorithm that is pulling arms like the following okay all right which means that then maybe sometimes it's pulling the right arm hopefully okay so in this case sometimes the algorithm is doing something that's just a little bit suboptimal and sometimes it is making a really big mistake so what we can do here is we can quantify how big our mistakes are and you might have a situation where you say Optimal Performance is really hard you know it's really hard to learn um like what everyone's perfect like ad preferences are things like like that maybe I'm going to relax my criteria I'm not going to require Optimal Performance but I want pretty good performance I want Epsilon optimal so what we do in this case is we count every time we make a bad decision meaning something that is worse than Epsilon optimal and otherwise we think of all of those as basically being an equivalence class of optimal so that's going to be what we think about when we think about pack okay so I'll Define what that is so a pack algorithm and raise your hands if you've seen this a machine learning if you've taken machine learning you might have seen back okay yeah so at least one or two people have so often a machine learning particularly if it's a machine learning class that includes some Theory um we'll they'll talk about pack and and probably approximately correct algorithms and that's where this idea comes from so it was it came from the machine Learning Community um and then reinforcement learning B it so the idea in a pack algorithm is that on each time step a pack algorithm is going to choose an action whose value is Epsilon optimal meaning the value of the action that's taken is at least the value of the optimal action minus Epsilon so that means that we're in this region with high probability on all but a polinomial number of time steps so essentially it's saying that the majority of the time your algorithm is making good decisions good here didn't being defined as Epsilon optimal but sometimes we'll make bad decisions but we're going to say with high probability the total number of bad decisions we make is not too many what we mean by not too many here is something that's polinomial in your problem parameters so that generally means like the number of actions you have Epsilon Delta Etc as you might expect if Epsilon is smaller generally the number of samples you need will go up normally something like 1 Epsilon squ so if you care about being more optimal you're going to need more data um or in other words your algorithm might make bad decisions for longer if Delta is small smaller meaning that you want this to hold with higher probability you'll also need more data um and if there are a lot of actions to learn about in general you need more data so it gives us some notion of sort of the complexity of the problem to learn in so this is a different type of um uh a lot of algorithms you can get both pack guarantees for and regret guarantees but it is just a different notion of optimality okay most of the pack algorithms for reinforcement learning are based on either optimism like what we've seen from last lecture or Thompson sampling which we're going to see later in this lecture and there do exist pack algorithms that just initialize everything to a really high value I don't know of any practical algorithms that do that like ones that people use in practice um but there is theory in papers about that case so it is possible to do all right and we'll we'll see more stuff about pack shortly let me just give an example so remember back to our fake trying to learn how to um treat broken toes example from last time where we had surgery and taping um like buddy taping the toes together and nothing again this is not medical advice imagine that this is Epsilon to 0.05 so in this case before we thought about this is what the optimal sequence of actions you should take but of course you don't know that because you don't have data if you had this sequence of actions say under an optimistic algorithm this would be the regret you would get in each case but under optim um but under the pack case let me see if I can type these here under the pack case would be um okay so the important thing to notice here is that because the reward of A2 is within the Epsilon bound of A1 which is the optimal action this action would also be considered optimal so from the perspective of the pack algorithm definition this would be not denoted as a mistake the only mistakes would be when it the algorithm takes A3 so when we talk about this pack definition here of sort of counting up the number of time steps we don't make a really good decision the only decisions that would count for that in this setting is the A3 decisions in contrast to that when we talk about regret anything that suboptimal counts so you get penalized for all the A2 decisions I took you were allowed to make mistakes for a polinomial number of steps yes but I guess I yes you AR allowed and it still be packed that's exactly right but I'm just pointing out here that the only actions you're taking that count towards that polinomial is 83 here it's not A2 whereas A3 and A2 count towards your yeah does training become easier if I like gra to good question so normally in these cases um you fix Epsilon in advance um and it defines kind of a um it defines the number of samples you're going to need for each of the like actions or states and actions in the mdp case so it's kind of like an exploration term and you keep track of counts um there are algorithms um with me and my former PhD student part of the work that we did there was to talk about what if you want to have guarantees or many epsilons at once I'm think more along the lines of where because we gradually conver to the the same they can be yeah it's a little bit subtle it's a great question so in general the balance will depend something like one over Epsilon squ so if your Epsilon is going to zero that will say that you have to do an infinite amount of exploration um if you're interested know have one of our paper thinks about trying to have s simultaneous bounds over lots of epsilons but in general the the basic version of this you commit to an Epsilon in advance great questions all right so going back to sort of where we are and reminding ourselves in terms of algorithms and this relates to your Epsilon greedy constant e greedy decay greedy and optimistic initialization all have the problem in general of having um sublinear of having bad performance it's in theory possible to have sublinear regret but you often need to have stronger guarant um stronger knowledge than is known optimistic and initialization also can have the Pack guarantees that we just talked about um and I guess I'll just say to you can convert these results into so Epsilon greedy is not a pack algorithm um but you can think about different types of other sort of exploration strategies and whether or not their pack and we'll get back into those soon okay let's jump into Basi and bandits they're a pretty elegant idea so so far we've made almost no assumptions about our reward distribution so we maybe said they're bounded you know it could be between zero and one and that's basically all we needed for hting um we need them to be bounded we need them to be um we use the where they're independent and identically distributed but we haven't made any other assumptions so we haven't said it's Galan or it's a beri or something else we might know and when we're being basing about this we're actually going to leverage knowledge we have about the structure of the way the rewards are generated and what I mean by that is normally some particular statistical model so it's a gaussia model or it's a beri model things like that and the reason that that might be helpful is that often if we're doing these in a domain like public health or others people might know lots of information about um you know what the reward structure is bless you and could we leverage that to get better algorithms and better performance okay so before we do this it's probably helpful to do just a quick refresher on Basi and inference some of you guys might have done a lot of this some of you might have done very little we'll we'll go through sort of just a quick quick reminder about this okay because this is going to be used a lot for what we're going to see today okay so the idea is that we're going to start with a prior over the unknown parameters in our particular case that's going to be the unknown distribution over the rewards for each arm so it's like if we have a coin flip or like if we think about the toes example what's the probability that someone's going to heal if they're given surgery we don't know what that parameter Theta is and so we're going to have a prior over what that Theta could be once we're given some obser ation about that parameter for example if we observe when you do surgery that you someone was healed that is going to change our uncertainty over the unknown parameters so let's do a particular example so if the reward of arm I is a probability distribution depends on the parameter f i we have initial prior over that parameter pull an arm we observe a reward reward then we can use base o that should be B um B rule to update that okay and I think it's really helpful to visualize how the prior change over time so we'll see that in an example shortly just so you can kind of think about see what that might look like okay so what we're going to have here is base roll okay all right this is our prior probability over the parameter governing the reward uh distribution for this arm this is the likelihood of observing a particular reward given a specific parameter value and this is the probability of seeing that reward in general okay and when we do that this is BAS Rule and then we use it to update what our new probability is over the parameter that generates that reward so in the case of surgery it would be before we had some distribution over How likely how successful we think surgery is on average we give surgery to someone we update it uh we observe that they are healed and then that changes what we think about the underlying parameters so BR that out here so this is the prior probability this is the probability of reward given a particular parameter this is the probability of getting the reward in general and we can rewrite this by using the joint distribution of the reward and the parameter and then marginalize out the parameter okay all right okay so this is beautiful and really oh yeah um can we go back to slide I'm just kind of confused on the setup like if I imagine that f is like U the par for like a that variable um and I using like background knowledge I have some sort of Prior what this should be like what does it mean to have a like like a distribution over that yeah it's a great so it's in general it may not be obvious that we can compute this so for example um we're going to see in some cases this is analytic you can analytically update this which is super elegant what I mean in that case is like um as a simple so let's say we'll see an example shortly but five could be um you know is the probability of recovery I'll do this for surgery for surgery okay so this would be say like you know 90% of the time someone's recovered and let's say um or something like that right and this is this could be a particular prior okay so I could say I think my probability that your recover mostly from the surgery is .9 so I I'm pretty confident that the surgery is going to be highly effective on average but you know I think that there's some probability that the surgery is not so effective and then I would say well I think that you know maybe with 10% probability the surgery isn't as effective that on average people are going to recover at rate point4 with the surgery and we'll see some specific examples of this this is not the priors we're going to use but this just illustrates how you can have distributions over distributions which can get confusing pretty quickly but on the other hand is also super elegant and a place we can put in prior knowledge just like clinicians and others may have information that where they can actually directly specify these right and so so there's many questions you might have in this case of like where do these priors come from and even if we have these priors how do we do this calculation so in general This is complicated so you can see here you've got to have like a functional form for this this in our case was like flipping a coin and so if your coin has a bias of 0.9 you know what's the probability you'd get reward one it would be 0.9 so you have to probability distribution here probability distribution here you have to marginalize one out over here and when you do all of that you get your new posterior which is After You observe something now what is your new distribution so you might imagine that now I update this maybe I see that the surgery was successful and I'm like maybe I'll can update this to be 095 and okay so in general this is going to be computationally trickly to do exactly um without additional structure there's lots of ways to approximate it but the really cool thing is that in some cases you can do this analytically so this is this idea of these conjugate priors so this this beautiful idea of the exponential families and if you have a representation of your prior that is conjugate with this is often called your likelihood function then after you do all of this updating this new thing is in the same statistical family as what this was in before and we'll see some specific examples of this in a second so the really the highle really beautiful idea in this case is that it's analytic when you do all of this let's say this was initially like a gaussian this is still going to be a gaussian if you use conjugate priors okay so let's see how to do this for bruli so for Bui there is a conjugate prior which is really cool and the conjugate prior is called a beta distribution and it's going to have a really nice beautiful interpretation that we'll see in just a second so the equation looks terrible um the equation says the probability of a particular Theta remember this is like the bias of your coin given some Alpha and beta these are just two other parameters is Theta the alpha - 1 1 - Theta the beta - 1 * this the gamma function of alpha plus beta divid by gamma of alpha gpha of beta okay so this looks fairly terrible but it is conjugate which means that after we observe something our new posterior is also going to be the same but it turns out that it has a really simple explanation a really simple intuition which is imagine you start with your prior being a beta Alpha Beta And Then You observe a reward that's either Z or one because your variable is just Z one so newly then your new beta your posterior is just r+ Alpha comma 1 - r + Alpha what does this mean if you observed a one then you increment your first parameter it's like you increase the number of successes If You observe a zero you increase this number the second number like you increase the number of failures so you can think of what the beta is doing is essentially just keeping keeping track of how many heads did you get and how many Tails or how many ones did you get and how many zeros it's just keeping track of those and it can use those to explicitly update what the probability is of your Theta okay so it's really beautiful because you don't um computationally that's really easy to keep track of just going to add one if I know what you've seen and what you can think of this here as being is how confident are you in advance of sort of how many pseudo counts did you see of uccess versus failure so for example if I'm really confident that the surgery is going to be successful maybe I'm like yeah I'm so confident it's as if I've seen 100 successful surgeries and only two failures okay but if I'm really uncertain what I would do is I'd say well I'm going to treat like one successful one failure I really don't know and we'll see what this looks like in just a sec okay excuse me so um now when we have this we're going to this is basically giving us to distribution over the reward parameters and we can use this to actually make decisions all right so there's a couple different ways to do this and one of the ways to do this is by getting a confidence interval similar to what we've seen before but the other thing is called probability matching or Thompson sampling and let's go through Thompson sampling now and see an example okay all right so in probability matching we're going to assume we're in the Bas Bandit case and what prob mching does is says okay the way we might want to explore is by sampling actions according to the probability that they're optimal given everything I've seen so far okay so what it says is I given some history which is like the past things I've tried and whether I've gotten ones or zeros for them I want to select a new action based on the probability that its true mean is higher than the mean of all the other arms and I'm not going to tell you yet that that's you know formally a good thing to do in terms of regret but you might imagine that's a reasonable thing to do it s of says like oh well if I think that arm is likely to be the optimal one with 60% probability I'll try that with 60% probability and then if I think there's another arm that might be optimal I'll try that with you know 30% probability now in general it's not clear how you would compute this it seems like kind of an interesting idea it's not clear how you can compute it but it turns out there's a really simple algorithm to compute this so this is called Thompson sampling and I think it came it was first invented by Thompson maybe in 1919 maybe 1919 maybe 1919 so it was around forever I mean it's been around for like 100 years but at least from the machine learning perspective I think it was forgotten about for like the first I don't know 90 of those it really came back into prominence about 2010 2011 when some people discovered that it actually had some really nice empirical properties so unlike hting which has been used for a long time how does Topson sampling work we're going to have a prior over each arm then for each iteration what we're going to do is we're going to sample a reward distribution from the posterior we'll see an example of exactly what I mean by that we compute the action value function given that sample we take the arm that is maximum given those cubes we observe our reward and we update our posterior and then we're going to do this many many times and again this will all seem much more concrete when we do an example right and it's going to turn out that this exactly implements probability matching so let's come back to this in a second and let's first do a specific example because I think that'll make it a lot more concrete all right so let's go back to our broken toes example what we're going to do so we're going to place a prior over each arm's parameter and I'm going to choose beta 1 one what does a beta 1 one look like that looks like the following I'm sorry my pen isn't working today otherwise that would have been helpful but I'll draw it up here okay okay this is zero this is one this is Theta okay we know that for um a bruli variable the the value for a Theta has to be somewhere between Zer and one because you can either always get one or always get zero or somewhere in between what if what a beta 1 one looks like so this is going to be the probability of theta this is my prior what a beta 1 one looks like is this which is a uniform distribution what it says is I have no idea what Theta is it could be zero it could be 1 it could be 0.5 it could be 7 it could be 0.9 it just says someone is totally agnostic this is called often like an uninformative prior saying I have no idea what my probability is for surgery Etc but this is what that looks like okay so this is our prior and now what we're going to do is we're actually going to sample a bruli parameter given the prior of each arm for the three arms okay so what does that mean that means I'm going to sample something for surgery going to sample something for Buddy taping and I gu sample something for nothing for do nothing all of them have this particular prior for now so for the first one it's like I'm just sampling from a uniform distribution between Z and one so it could be anything between those zero and one let me just check which number I'm sure I use the numbers that I'm going to use for the next ones okay so let's say for example that I happen to sample 0.3 okay that's a totally reasonable thing that I could sample given this uniform distribution between 0 and 1 one then for Buddy taping let's say I sample 0.5 okay again a totally reasonable thing I could sample given this distribution and for do nothing I'm going to sample 0.6 so this is just the the distributions that I have over my prior over the parameters and this is a particular set of parameters I could sample given that I now what Thompson sampling says I should do is I should select the action that is maximal given the parameters I've sampled so under these three parameters if you want to maximize the probability that someone will recover from surgery or sorry recover from their from recover from in terms of their broken toe should I do surgery buddy taping or nothing which one has the highest chance in this case nothing in this case nothing right so as soon as you in our case it's pretty simple once you see the Theta because the Theta is exactly equal to the expected reward so what this would say is in this case you should do nothing so this is going to be okay so this will say do nothing all right we're going to observe the patient's outcome now in this case we're going to assume that doing nothing is actually not so effective and so we're going to observe a zero and now what we're going to do is we're going to update the posterior over doing nothing given that observation okay now the other two haven't the other two arms their prior hasn't changed because we haven't gotten any observations about surgery or budy taping the only thing we've got an observation about is doing nothing so what I said before so we have Alpha this is our Alpha Beta parameter this is our prior in particular it was beta 1 1 before and when I pull this arm and I get a reward of zero what I said we would do here is the first one you can think of as being the number of successes the second one is the number of failures so what this becomes is it becomes beta 1 2 okay and that is going to look different it's going to look like this this is a beta one 2 this is a beta 1 one okay does somebody want to tell me intuitively why it makes sense that this looks like this for doing nothing does this put where does this put weight in terms of parameters um since we receive yeah we should think that the parameter value is likely lower and so we've shifted our probability mass and so we're like okay for the things that we don't know we're still totally agnostic about whether they're effective or not for the thing that we just tried do nothing we got a zero so it is more likely that our actual Theta is lower because a lower Theta in general will generate more zeros and so we've changed our distribution let see what that looks like here so this is our new posterior we're using again remember this is conjugate so this is our new Beta And we haven't changed it for the other two now here's the next important thing what we're going to do now is we are so this is what that beta looks like just for beta 1 2 now we're going to do our next step at Thompson sampling so what we have to do now is we now have to resample so we are going to resample where we have two distributions these two ones are a beta 1 one and this one is a beta one two we're not we're going to throw away all of our old parameters from last time they were just samples we now have an updated distribution for one of the arms and that we have the old one for the other two so in this case we might resample and we would get this it's more likely now that we would sample a theta 3 which is lower because our beta puts more weight on the lower part okay so this is what this looks like here so under this we're going to pull arm one because it has the highest expected success and that one is going to give us a beta 21 cuz remember again that we increment the first one if in terms of the number of successes and number of failures so as you might expect is exactly symmetric to this one we have something looks like this I'm not being perfectly precise of the intersection okay so now now we can again we throw away all the old parameters that we've sampled so far so we're going to throw away the 7 the5 and the3 and we're going to resample okay so this time let's imagine resample like 7165 and 0.1 and we again observe that the outcome from surgery is successful this is what a beta 31 looks like so it stops looking like a straight line starts having curve so I really like these uh these graphs cuz CU I feel like it gives one a much better intuitive sense of how as you get information that translates to your posterior over what the what you think the Theta likely is so as you see more successes um it'll tend to go weighted to one way as you see more failures and we get it the other way and as you might expect so we're not seeing that right here but if you can see cases where it starts to concentrate in the middle or somewhere in between right just depends on what actual um observations you're getting okay so this is how Thompson sampling works and I think so let's say we did this then now we have this something that's even more peaked we get another one and you can see it just continues to curve okay yeah um could you use toson samping with like random variables that are just like have many more parameters like as opposed to just using bures yes absolutely yeah and one of the examples we'll see later today they're using it for advertising and they have a large number of features yeah you can extend all of these to the function approximation case good question okay so I think one of the things I mean obviously this is a small example what we saw in this particular example I just did is that we quickly started to converge to a one in this case now notice so far we've actually never pulled A2 we had some probability of pulling A2 because if we had sampled a really high high value for A2 then we would have pulled it but we haven't done that yet and A1 which actually does have you know high like generally higher probability in this case of having good outcomes is starting to be pulled so it's quite different than the optimism methods because in optimism we had to at least pull each arm once so we could even start to initialize our confidence bounds that's not the case here we already have a prior over what the values are all of these and we can immediately start using that to make decisions okay all right so what is Thompson sampling doing um when we're doing these pools and and what sort of uh results do we have in this case so what it is doing in this case is it's well actually let me just step back CU I wanted to get to the example and then um so I went through that part a little bit fast let me just go to how the POS the matching is working so let's just go back to here for second so what we can see in this case is what Thompson sampling is actually doing is at each time point it is trying to select actions according to this probability and it'll often end up being optimistic in the face of uncertainty because we're doing an argmax with respect to like our empirical estimates but it won't in general as you might imagine uncertain actions have a higher probability of being the max so so if you are really sure that your parameter is at 0.5 and you have another parameter you have very little information about like you have a beta 1 one then you're more likely to accidentally sample a much higher value for that parameter so the elegant thing here is that you can think of this as the following and this is really useful also for the theory so in posterior matching that's this first line that's sampling things of according to this what put what Thompson sampling does is it doesn't do that explicitly it just samples a reward for each of like a reward parameter for each of the different arms and then it picks the one that's argmax and so it's really elegant that that is in fact the same thing as doing probability matching that gives this um the fact that that ends up working in terms of these so the key IDE a in this case is that as you're Computing these with respect to the data that you have so far in fact the probability that Thompson sampling picks an arm is exactly equal to this true probability given all the data you've seen so far I'll do a pointer if we have time at the end maybe I'll to go through the proof briefly um for the basing regret case um but there's also a really nice explanation of this inside of T Lamar and chabab as spire's book um there's quite a lot there is a quite mathematical version of it um but it gives you some really nice nice background let's go back to here okay so how how let's first just talk about how do we evaluate performance so what we saw in frequentist regret like what we saw last time is that we're assuming like a particular unknown set of parameters like our arms are actually 9.76 we just don't know what they are and then our regret is always um evaluated with respect to the optimal arm given that fixed set of parameters basing regret assumes there's this prior over the parameters and so when we talk about regret we're actually taking an expectation with respect to that prior so it's still this looks like exactly the same as the frequent just regret but now we have this outer expectation over Theta one of the key ideas of this in terms of how one might prove things in this case is if we think back to how we proved some ideas around regret we didn't do the full proof I just tried to give some sketches one of the key ideas in the proof for um frequenti regret and upper confidence bounds is that we tried to construct these upper confid balance UT that we thought would be higher than the true value of the arm with high probability and we use that in order to figure out how many times we would pull suboptimal arms we leverage sort of this this fact so it turns out that you can do basing regret bounds under a pretty similar decomposition you can think about sort of computing an upper confidence bound and the likelihood that it'll hold we might come back to that later today but I want to First sort of get into extending these up to sort of higher level settings as well before we do that I just want to highlight that if you try to get standard bounds like what we saw last time for standard Thompson sampling and what I mean by that is the type of Thompson sampling I just showed you to my last check they don't actually match the best Bound for like upper confidence bound and frequen algorithms however often empirically they can be really effective algorithms and I'll just highlight here that in general you can't compare directly between basian regret bounds and frequentist because one of them is with respect to this prior bir parameters okay so let's look at that for a particular domain and why um Thompson sample might be particularly helpful for a lot of real world cases so this is a really nice paper by um Olivia Chappelle and leh Hong Lee which sort of re um initiated a huge amount of interest in Thompson sampl a little over a decade ago so I think they were both at Yahoo at the time if I remember right um they were thinking about a contextual Bandit case so they were thinking about making you know news article recommendations Etc and so there you would have a context like you'd have a bunch of features about an individual and also often you would have a bunch of features about the arms so like explaining like may be news articles and all the features or you know ads and stuff like that and but we're still going to assume the context is sampled I ID at each step so you know if I give a particular ad at this time point it doesn't impact what's going to happen to okay so there's it's still a bandit there's no like sequential dependencies there arms or articles reward is uh binary either you click on it or you don't in this case that you can model it using like logistic regression because you have this binary output so what are we seeing here so this is CTR which means it's a clickthrough rate it's normalized because they're not going to tell us exactly what they get on their real world data um the important thing to look at here is the x-axis which is DeLay So in many cases just like what we saw for the public health setting there'll be some form of delay even for online you know customer cases so Amazon will show you something and they don't find out for a little bit of whether or not you're clicking on it or whether you bought the thing and so what they're comparing their algorithms with here is the following so TS THS is Thompson sampling OTS is optimistic Thompson sampling you can try to add in a little bit of optimism in these UCB is upper confidence bound EG I think is Epsilon greedy and exploit is you just do whatever the mean looks like so far these are all hyperparameters um as often is the case the hyper parameters matter so it's useful to to to look at these I think the really interesting thing to look at in this case is to look across the time so if you this is um sort of the shortest delay and this is the longest delay and you can see for the blue algorithm it varies very little in terms of its performance even if things are delayed a lot but if you look at say UCB it's performance tends to drop a lot in terms of as you have longer delay okay and so that is one of the reasons why you might want to do Thompson sampling in these cases okay so let's think more about that okay and do a check our understanding so let's think about an online news website with lots of people logging in every second often someone will come online before you've seen the outcome of the previous person it asks you to select all of the things that you think are true as we think about Thompson sampling versus upper confidence bounds for for all right wait you compare your answer to some nearby e okay so let's come back together um so as we were just discussing point out that like Thompson sampling could cause much worse performance this one is true um than optimism if the prior is very misleading so this is true okay because if freak somewhat for example maybe surgery is really effective and someone starts off and thinks surgery isn't effective at all and so you put a lot of probability mess could have a really sharp prior on it over here then it could take a long time essentially for your data to overwhelm your prior so this one can be a problem um the first one is also true so if you think back to the algorithms that we saw last time for optimism there is no Randomness in there unless you have a tie so if your upper confidence Bound for arm one is higher than the upper confidence Bound for arm two and arm three you're just going to take arm one and that's fine but if you have a delay that means you can't update those upper confidence bounds so if like the next customer comes you're like oh or like the next patient comes you're like I still think surgery is best I still think surgery is best and you're not going to try anything different whereas Thompson sampling just has this P this you know prior or posterior and so if I have someone come I can just sample from all of my um my priors and then if another person comes I'll again sample for my priors and so because of this sort of distribution over parameters unless it's collapsed to a Delta function in which case you know what the right thing is to do anyway you'll get natural exploration so that's one of the really big benefits of Thompson sampling is that even if you don't get new data you naturally sort of will try out different things um and that can be really helpful it is true that optimism algorithms generally are better than Thompson sampling in terms of their regret bounds that may or may not translate to empirical benefits um but they don't actually necessarily have stronger bounds for this setting this is false and that's because all the bounds we've been talking about so far don't think about that batch setting they're being derived for the case where you get information you update your confidence bounds you incre you know you continue so this s of highlight some of the particular benefits and the potential weaknesses of Thompson sampling if your prior is reasonable and you've got got this sort of delay or batch setting it can be very helpful um if your prior is really bad that it can take a long time to sort of get past that so before we end today um I think it an interesting question to consider is whether or not Thompson sampling is optimal now we can get nice regret bounds for this case I know I didn't have a chance to go through that particular proof today but um but it's not optimal in general so it would be really cool if we could get something that was basically perfect you might imagine that if you have a prior and you have a known Horizon you could compute a decision policy that would maximize your expected rewards given that prior and the Horizon so I haven't at least in this class taught you all the tools you need to do that but at a high level you could think of it as kind of a Markoff decision process over parameters which is kind of wild so if any of you guys have taken Michael Cocker's group class actually who's taken Michael's class anybody here okay so you can think of like a pom DP your state is your parameters your actions are pulling things and then your belief state is your new probability of your parameters so it's really elegant in theory you can compute something that will exactly maximize your expected reward by doing PM DP planning the problem also for those of you who taken my class is that often pom DP planning is really intractable so it's often not clear that we could do this in a computationally reasonable way in general when one of the challenges here is that um if you wanted to do this it would have a decision policy that's a function of the history which means all the prior actions you've taken and all of the rewards you've observed and that's going to increase exponentially with the number of decisions youve made so there's this idea of an index policy and an index policy says we don't want have to think about this exponential sort of history or state and a decision an index policy is one a decision policy that computes a real valued index for each arm and it plays the arm with the highest index using statistics only from that arm in the Horizon so that means I don't have to pay attention to the sort of combinatorial exponential thing um I can just say for this particular arm maybe you know what were my rewards that I've observed so far um and then I can use that information to make decisions so for example a greedy algorithm which just relies on your empirical average of the performance for each arm is an index policy so as an confidence bound algorithm because it just relies on the upper confidence Bound for the rewards you've seen for each arm so there are a lot of index policies surprisingly there is an index policy that is optimal so gens proved that there exists an optimal policy for maximizing the expected discounted reward in a basian multiarm bandit that you can compute that only depends on these statistics separately free charms so that's really cool it means that it is possible in some settings to actually exactly optimize your expected sum of discounted rewards for these type of Basi and bandits Thompson sampling will not do this in general so Thompson sampling is generally not equal to what the G index would be but it can still be a very good thing to do all right so just to summarize some of the things that um are useful to understand uh from this part of the section and next time we're going to start talking about these sort of ideas for sequential decision processes like Markoff decision process you should be able to Define regret imp pack you should be able to prove or know why sort of the UCB banded algorithm has sublinear regret like up to the proof sketch we did in class um you should be able to give an example of why egedy and greedy and pess is can result in linear regret um I don't think you need to be able to do this for G rewards but you should be able to do Thompson sampling for the case that we've just talked about at least like sort of in pseudo codeland so if someone said Like You observe another count what would your beta parameter B um and then also that you should be able to understand the UCB banded algorithm as we've covered in class so I we've been building up all of these things to think about now how we can do exploration and data efficient learning um for sequential processes so next time we'll think about how to do this we in a standard decision process as well as thinking about what do we do when we're in really large State spaces or really large action spaces and how do we lift this all up for function approximation I'll see you on Wednesday