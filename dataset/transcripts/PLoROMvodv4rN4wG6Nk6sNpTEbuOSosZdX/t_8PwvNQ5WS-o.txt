hi everybody welcome back um we're going to be talking more about policy gradient methods today and we're going to start off with a quick refresh your understanding for all right let's go ahead and go through these so everybody said the last thing was false which is correct there not guaranteed to converge they're not guaranteed to converge to a global Optima they're just guaranteed to converge to a local Optima of the policy gradient space um the first one is true there are different ways to write down this down um but in general what we're doing is we're going to be trying to find take steps in the policy parameterization space we're parameterizing our policies by Theta um so that we're going to be trying to move in the direction of the log of the policy parameters times um their value the return you get from them um the second one is false there's a bit of disagreement over this so Theta because you can see from this first derivative we are going to look at the direction of um the derivative with respect to Theta of the log of the policy parameters but it's weighted by the return or waited by the Q function so whether we push it up or not will depend um whether or not we're getting high rewards when we go in that direction so this one is false and this one is also true that in general what we're trying to do is we're trying to find parts of the policy space such that when we follow that policy we visit states and actions which have um higher estimated Q function or higher estimated rewards do you have a question stretching okay great all right okay so last time we started talking about policy search which was this idea of saying we're going to be directly trying to search in the policy parameterized by some Theta this could be a gaussian policy class this could be softmax or this could be as it will often be a deep neural network and what we're going to talk about today is we're going to get to talk start talk um finish off that part and then talk about more advanced policy gradient methods and in particular today we're going to cover at least the majority of Po so this should be enough for you to be making significant progress on homework two so in particular what we're going to be covering is we've talked last time a lot about likelihood Ratio or score function policy gradients we're going to talk more about the notion of a Baseline and why introducing that um is not going to incur any bias in the estimate of our gradient we'll talk about alternative Target uh targets and then we're going to talk about Po and again just to remind ourselves PO is what they used in Chad GPT and a huge number of other application areas as well so it's a really really useful technique all right so let's just remind ourselves we talked about how we can take a derivative with respect to the value of a particular policy so this was the policy parameters and we showed that it could look like this and this was an unbiased estimate of the gradient but it could be very noisy um in part because it looks something like our sort of Monte Carlo estimates that we saw before because we're looking at these returns um and so we talked about a couple different fixes or we started to talk about fixes to make it tracable so one was to leverage the temporal structure meaning that your reward on time step three can't depend on your actions after time step three and so we could use that to reduce the variance of our estimator and now the next thing we're going to talk about we started talking about this last time is Baseline so as we talk about this I think it's just useful to keep in mind throughout this that we're always trying to sort of converge as quickly as possible so we want these estimates to be of the gradient to be as low variance as possible so we can try to be taking better steps in our uh policy space all right so let's look at the Baseline started talking about this before and we said well when we are thinking about how to move our policy inside of the policy space we want to think about not just what how much reward we're getting but really maybe how much reward we're getting relative to other things we could be doing so we want to know how much better this policy is compared to other stuff and I said you could introduce this Baseline B of St um which was only a function of the state so note not a function of a of theta or a now there's been other work including from my lab thinking about whether we can introduce baselines that may be a function of something beyond the state but for today we're just going to assume that it's only a function of the state and what we're going to prove now is that for any choice of the Baseline as long as it's only a function of the state this gradient estimator is unbiased which means that we could introduce this here and we're not changing on average um what the the gradient estimator is and a near optimal choice is going to be the expected return okay so now we're going to again just be trying to think about how much better is um taking the actions under this current policy compared to other things we could do so let's see we're going to step through why adding a baseline does not incur any bias in our estimate of degree so what we're going to do to do this is we're going to think about how our gradient comes together put this good okay so we can remember to here's our trajectories and then this is our gradient and what we want to so the goal is to show this is equal to zero why is that because if we think about what this term was this first term was an estimate unbiased estimate of the gradient and now we've subtracted off this term times this term and we want to show that in expectation subtracting off that term is zero which means that we didn't introduce any bias okay so let's just step through how that works so the goal is to show this is zero and we're just going to step through this so our expectation is over to TOS are our trajectory so let's just write it out you can write it out as the states we've seen up to a Time step t plus the states that we see from that time step onwards okay because that's like we're just writing out our full trajectory so we can just think of our trajectory here as being you know s0 to T and a0 to T ni One and that's just the full trajectory so that's TOA and so we're just going to decompose this expectation okay so we break this up and after we've done that we can notice I just make that we can notice that we can pull one of the terms out okay I'm going to pull this out because this is not a function of this future expectation okay and I could pull that out there because this is just a function of the current state and it doesn't depend on the future States and the future actions that I take all right so that's what I did and then next I'm going to notice that well this term here is only a function of the state and the action it's not again a function of all the future actions and future States so we can just rewrite that as expectation over the action t okay all right so what are we going to do next the next thing we're going to do is I'm just going to write this out more fully so I'll repeat this so we've got our Baseline here St and we're going to write out what this expectation is okay this is an expectation over the actions what actions are we taking we're exactly taking the actions according to our policy okay okay so I've just Rewritten what that expectation is the expectation we're taking over actions is exactly the probability we take each action according to our current policy okay but once we have that we can play the likelihood Ratio or we can think of you know what is this derivative this derivative is equal to of St Su over a the derivative of log is just going to [Music] be the derivative of the things inside divided by I should have added let me put a Theta in here to make it clear that all this is a function of my current Theta okay so I just took the derivative with respect to the log but when we see that we realize we can cross those out so we can cross off this we can cross off this because that's the same so now what do we have we have the expectation of our states B of St Su over a derivative remember taking the Der respect to Theta Pi of a St thet so this is what this looks like so far and now what I'm going to do is I'm going to switch the derivative and the sum okay so I'm going to say this is B of St derivative OFA sum a okay why was that important because now and let me just that we know here that the sum over all actions we could take at this time step has the sum to one because that's true for any policy so this has to equal one so which means we have B of St of the derivative of theta of 1 which is equal to zero because that's a constant let me just WR show it here because it's more neat okay so there's two sort of I guess there's two main insights for how we did this proof the first was we thought about our expectation over all trajectories and we broke it up to the the part of the trajectory that happened before sort of the state of Interest the St and the part that happened afterwards after we did that we showed that we could rewrite that expectation just in terms of a because we didn't care about all the future stuff this only depends on at and St and then we take the derivative and then we can see that we can switch these and then that just becomes one and it's a constant that doesn't depend on Theta and so the Der respect to Theta of it is zero and so that is why introducing a baseline that only depends on the state does not introduce any bias because an expectation its value is zero so that allows us to dve what we often call sort of a vanilla policy gradient method which incorporates both the temporal structure and a Baseline and so the idea in this case is that we're going to collect a set we're going to take our current policy which is parameterized by Theta going to roll it out um a number of times and then we go for sort of each um time step T in each trajectory we're going to compute the return so like our Monte Carlo estimate and then we can compute the advantage estimate which is we take that current um return from that state till the end of the episode minus our Baseline and then and someone asked me about this last time generally we're going to change refitting the B the Baseline each time so we can reestimate the Baseline again it doesn't matter what we pick for the Baseline it will always be unbiased but there will be better or worse choices so you can imagine if the Baseline is zero it will never make any difference the goal is to hopefully have a baseline that's pretty informative and has a value close to the value of your policy and so then we'll update the policy using our policy gradient estimate which is a sum of these terms okay so we're going to use all of these terms where we've got that derivative respect to the log of the policy parameters times our advantage and then repeat this is like a Vana policy gradient algorithm yeah there is no discount factor in the return is the intentional yeah good question so right now so question was there's no discount Factor there's no discount Factor right now because we're assuming we're in the fully episodic case so we don't have to have a discount Factor you could certainly include one if you want to yeah so for right now here we don't have a discount Factor now one thing that you might think about when you're starting to look at this is to say well a lot of this feels like the Monte Carlo estimation that we did earlier in the class we've been using these G estimators this return um to estimate what is the performance of the policy from this time step to the end of the episode but as you might imagine in this case um that generally is a pretty noisy estimate okay so then the question is going to be could we maybe do better so there are two places we could imagine I guess there's two things here that we could imagine plugging in other choices for there is what is the return of the color policy from a particular action till the end of the episode and what is my general estimate of the performance in that state so one thing you can imagine here is if we think back to Q functions and value functions maybe we could plug those in instead of using the return and using a generic Play sign so you could plug in instead of saying what is the return from this date in action to the end of the episode you can imagine plugging in the Q value for the current policy from the state and action to the end of the episode and you know we can either make gamma equal to one or not we're going to generally assume for [Music] now assume episodic so you can set gamma to equal to what and the state value function could be a good Baseline and just to remember here like on this slide you can think of G as kind of being like a q function and B is being a value function so this would be an alternative we could do okay so let's think about how we generally could sort of reduce variants so what we've seen so far is we're mostly using Monti Carlo like returns now let's see if we can do something better so one thing we can do now is we're going to try to plug in and use things like State action values and this this is where the idea of actor critic methods come in which are also really popular in reinforcement learning so the idea here is that we could do we could reduce the variance at this estimate of the value function at a single St from a single roll out by bootstrapping or doing function approximation at that point so you could think back to like deep Q learning or something like that as a way for us to approximate what the value might be or just general sort of deep learning for the for the value function so when we do this we end up with what is called actor critic methods the idea is that the actor is the policy so the actor is the policy Often parameterized by Theta and the value function or the sttion value function is the critic and it's representing a v or a q function so that's why they're called actor critic factor is our policy parameterization critic is our state action value and the great thing is that we can use both of those inside of of a policy grading algorithm so you are constantly updating an estimate of the state action value as well as having an explicit policy parameterization and you use them together to hopefully um increase the rate at which we learn to get a good policy now in this case normally what we're doing here is we're Gathering data using the policy and then we're using that data to fit a Critic okay and the reason we call it a Critic is because the critic is sort of trying to estimate the performance of the an explicit um representation of the performance of the policy so the actor makes decisions and the critic says that's how good it was that's why it's called actor critic a3c um is a pretty popular actor critic method there's quite a lot of others so many of the reinforcement learning algorithms will end up being essentially actor critic algorithms and so it'll be useful to have both representations so if you think of it you'd have a sort of a deep neural network to represent your policy and you'd have a deep neural network a separate one could be could you you could share parameters but you don't have to to represent your value function all right once we do that we can um think of rewriting our policy gradient formulas so this was what we had before we could approximate this now as saying well what if we just plugged in instead of that return G which is that sum over the rewards we plugged in a q function and we plugged in a parameterized q function with a parameter W so these were our weights so now just to highlight here we're going to have these two sets of parameters W and and Theta Theta for the policy W for the the value function and um if we let the Baseline be an estimate of the V then we can just directly write down sort of a state action Advantage function where we look at the difference between the Q and The V and so now V is serving as our our Baseline and I'll just highlight here that using the advantage function was one of the first things that I think there was um got best paper maybe in 2016 right after deep Q learning started coming out and people thought about these different um adaptations one of the things that was proposed is to think about trying to maximize with respect to advantages but here we're going to be using that within a policy gradient approach okay now one of the things you might wonder here is like okay well we've got these extremes on the one hand you could have this Monte Carlo return of what is the value of the state in action that you get from starting that state in action and rolling out to the end of the episode and the other is you could plug in a q function now there might be some sort of blending between these two so these are known as endep estimators so a Critic in general doesn't have to pick sort of a temporal difference temporal difference in the way that we've seen it so far is normally so this is I'll just write down here it's often we've seen it as td0 which means we have the immediate reward plus gamma * V of S Prime so TD Z you take you sing of your immediate reward and then you plug in or bootstrap immediately when you say like and I saw the next state and I'm going to plug in my value for that next state so if you think back to our tree representation it's like you see one actual observe return and then you plug in your estimate but in general you could trade off between taking one step or taking two steps or three steps and then play plugging in your estimate of the return so in particular here's a number of different types of um estimators you could look at so you could have let's call this R hat one which is you get your met reward this is what we've seen before plus gamma then you bootstrap a second one would be you take your next two again your gamma can be one or not um and then you plug in it and sort of R hat Infinity would be your normal Carlo return which is you don't do any bootstrapping you just sum up all your rewards and you can think of each of these as being estimates of your Q function and then you could just get Advantage estimators where you subtract off the V of your current state in each of those settings so those are all things you could do they're called nstep estimators where n is sort of the the number of time steps until you bootstrap and one of the important things to think about is where you might want to tradeoff between these ones so we'll do just a check your understanding now if we think about introducing these type of advant um Blended estimators how does bias and variance tradeoff so but we go ahead and um do that now for can you like more than one you should be able to that work okay good I'll give you one more minute to think about it and put in your answer and then there's a lot of um variability in what people are saying and so why don't you talk to your neighbor in a second all right turn to neighbor compare what we got what do you think about it yeah you are subtracting the B function but if you think back to monteo and TV method they have the same bias well this is the question is is are these both do these both have the same bias so if you ignore for second the first part of the estimate do you think the first one has higher high higher bias or the second one yeah I I don't know why if you think back the first one should be like the temporal difference method and the second one should be like Monte Carlo and remember I guess it should be clear that these V's are all estimates so they're not like converged so then the top exactly because exactly yeah and then what do you think that means for the variant um I guess higher variance also but close yeah so for the first one you're toally right that it's got higher bias because you're immediately boot strapping but in general it will have lower variance okay oh is that just generally a tradeoff yeah yeah that normally the Monte Carlo methods because you're summing them up they're normally totally unbiased but they have lots of terms so you could think of there's lots of stochasticity oh okay yeah and then the other way is normally I that makesense cool all right I'm going to ask if people Al turn this yeah I'm gonna ask H did ibody change their mind after talking to someone okay at least a few yep all right so um I I think one of the things too I just wanted to clarify here is that I find it easiest to I think it's often easiest to think about this without subtracting the V because the V is the same in both of these so you can just focus to understand which of them has high variance or high bias you can just focus on the first things and when you look at just the first things it should remind you of Monte Carlo methods versus temporal different methods um so the first one has low variance and high bias does somebody want to say why so the first one which looks kind of like a td0 update so this one has low variance so A1 low variance High bias so I want to share why that is which is it sorry wasn't it wasn't just yeah just just to make sure I'm going to explain what each of them have but yes um do you want to say in the back yeah and my I'm so I'm not entirely sure but I think the intuition at least when I was thinking about it was that um using the actual values of like for example RT +1 um is like more accurate versus like if you bootstrap very early um that's like more of an estimate like it's not it's not as accurate yeah that's exactly right so it's the same as in temporal difference meth in general it's a little misleading so to look at the maybe I should put hats over all these next year but um all of these V are just estimates they're like given finite amounts of data and however many backups we've done Etc V is an approximation so this is an estimate so this isn't true and if it's not true it's probably biased um uh so this is uh in general V will not be an unbiased estimator just um and so this means that we're only using one um one point R from the true policy and then we're immediately bootstrapping so in general this is going to be high bias or higher bias but it's generally going to be pretty low variance and the way to think about this is that each of the rewards in general are going to be from sort of a stochastic process um because you're like taking a series of State steps and then at each one you're going to sample a reward from there so you only have kind of one really random thing here and then one thing that is fixed it might be wrong but it's fixed um in contrast this one which looks like a Monte Carlo estimate is going to have high variance because it's got all of these different rewards that are all being sampled from like a stochastic process so if you think of it this way like um imagine it's something where like your robot can walk anywhere over the room and under your policy like your policy is it can go in all of these different directions okay but when you actually just execute one trajectory you're just going to get one of those and so its variance generally is enormous and this might be true even if like on average you know you kind of have a trajectory like this or something but so in general this one is going to be really high variance but it's generally going to be low or zero bias why is it lower Zer bias because this actually is a return from the policy that you're executing so um in expectation this really is equal to the value of that policy so generally low or zero bias um but it can be really high variance and so that should maybe give some intuition over um and we're just discussing this in general it's going to be unfortunately be a trade-off between low variance um and low bias and so often you're going to want things in terms of n so this is if that's the end step and you want to sort of minimize your mean squared error often you're going to end up wanting to do something where it's like a couple steps and then bootstrap to get a nice trade-off between bias and variance and I think we'll probably talk a little bit more about that next week okay so I'll just highlight this here that this has um low bias and high variance on this one so this one has low variance and high bias the other one is the opposite all right cool okay so just to think about this so when we are sort of thinking about these targets we can go between these different ones and then oops sorry right somehow these got copied okay so these are all things that you can plug in um you can make different choices over whether you plug in um these endstep methods or others and then you can use this all as part of like your actra critic policy gradient method so now what we're going to do and I'll just make a delete those slides so now what we're going to go into is more advanced policy gradient methods okay so those are the basic ones and they're kind of the backbone between all of the algorithms that we do now but there's been a lot of interest in these types of methods and how do we scale them up and how do we make them better okay and we'll talk about what we mean by better here so we'll talk so this is actually we'll probably talk about some of this next week because I wanted to make sure that we got through the algorithm today so that you guys can um have all the knowledge you need to be starting to do the implementation and then we'll do more on the theory next week okay so why so policy gradients so far we've been talking about them being great and we know that they're used in some really important application are is why do we have to go beyond the methods that we just saw well there's a couple different limitations to them one is that the sample efficiency is generally poor okay and I'm uh so that in general you have to do many roll outs with the same policy and in order to get a good estimate of the gradient because you want to good estim the gradient because otherwise when you take a step you might get somewhere that's you know not as good or not don't get to the the place you want to as quickly and the other and we're going to see an example of this shortly is that the distance in the parameter space doesn't necess say equal the distance in the policy space so this is a little bit weird of an idea but the idea is that if you have a policy parameterization whether it's like a deep neural network or others there's some parameters in there and when you change them like when you change your Theta you're going to get a different policy out but whether that is really smooth like oh if I change Theta let's say Theta is a scalar so let maybe Theta is like 7 if I change it to 75 does that smoothly change how much more I take a particular action or might it be really discontinuous like might it suddenly say like oh you know you were taking this action with 20% probability I changed the a little bit and now I'm like taking that action with 90% probability the main idea here and we'll see an example of this in a second is this part may not be smooth which means that even really small changes in your Theta in your policy parameterization might actually lead to really big differences in how you're making decisions okay so you could imagine your robot like picks up things one way and then you change your Theta a little bit and suddenly it like drives off a cliff okay not quite that extreme but but we it's not clear that sort of as we smoothly take gradient steps in Theta if that's going to smoothly change our policy parameterization our policy decisions okay so let's look at those both all right sample efficiency so what we've been seeing so far is the idea is that we take our policy We Roll It out one or more times and then we take a gradient step and we take one gradient step and then we roll out our policy again and in general it would be really nice to be able to take multiple gradient steps um but so far we have not seen that and it's called sort of on policy expectation so this is similar to kind of Sara and other methods we've seen before were like you're learning about a policy and its value by actually executing it so the problem is when we think about let me just go back to here when we think about doing these gradients we've assumed that we've gotten trajectories from the policy and the Theta that we're at right now and then we use that data to take a step so you know we're at some point here's our Theta we're at some point and we estimate the gradient from that point okay so we estimate it from trajectories that are generated under Theta and then we take a step now the problem is if we now now we might be here and what we would like to do is to take another step before getting more data but the problem is we don't have let's call this Theta Prime what we have is we have data from Theta we don't have any data from Theta Prime so a priority it shouldn't be clear that we could take more than one step we can take one step because we have data about Theta we estimate the gradient at that point now we would like to just be able to continue to take gradient steps before we actually go out in the real world and gather more data but it shouldn't be obvious how to do that yet and so when we talk about policy gradient right now we've been talking about sort of on policy methods where we just try to estimate the gradient for the policy we just executed so similar to sarsa now so what we would like to be able to do is so that's what we've been doing so far we collect sample trajectories from the policy then we form a sample estimate it's pretty stable we get our gradient we take a step we rinse and repeat another thing we could do like thinking about Q learning or others is like well what if we could use that old data to estimate the gradient at some other Theta some other policy could we do that this is known as off policy estimates this generally can start to be pretty unstable we're going to think about different ways we could even do that but we really would like that we would like to be able to use our old data to take multiple gradient steps before we actually have to gather data and you could imagine that might end up allowing us to be much more data efficient so that the total amount of times we have to gather more data is much less so we're going to see think about a way today and we'll talk more about this certainly over the next few weeks as well of like how do we use our old data to essentially move faster in our parameter space okay here's the second Big Challenge okay so in general we're going to be doing s stochastic gradient Ascent with some step size like we've repeatedly thought of there being some sort of learning weight or step size one of the challenges and this was important you know for deep Q learning we thought about it even for like TD learning and stuff what was the step size how much do we update our estimate every time we get new data turns out it's much harder here now we saw before that under some like you know pretty loose uh loose requirements on the learning rate we could guarant you know at least in tabular cases guaranteed to converge Etc policy gradient methods are a little bit different um here the step size really matters and if we take a step size that is really quite bad we can collapse in our performance somebody have an idea of why why does that happen why we why can we suddenly collapse the optimal Target go for yeah that is great great R so let's look at an example so remember the way that we're getting our data is from our policy if you overstep so let's say we're trying to get to this point you have a big learning rate so we took big steps you might now get to part of the space which is really bad like really really bad policies by really bad policies I mean they have really bad value functions if you have really bad value functions and trajectories which you're vising States and actions which all have really bad reward it's really hard to estimate a good gradient of where to go in general you might be in sort of like a really long Plateau place right so this could be really long and so the gradient here might be really hard to estimate of like how do I get back to the local Optima in general it's not going to be impossible unless you know it's completely flat but but it might be really close to completely flat and so that's a big problem is that you don't necess say know how large your step size should be on the other hand if you use really small step sizes that's bad too because then you're just you know each time your step size is basically determining how much you change your policy before you get new data and so you would like to take as big of Step sizes as possible that don't overstep um and that allow you to quickly get to the optimal local Optima now you know things like sort of atom style optimizers Etc help but it won't necessarily solve the problem and one of the challenges here is you know the we're only getting information about the the states and actions that are visited under our policy and you just might get to Regions where there's very little information to estimate those gradients here's another challenge and this has to this relates to sort of that we're taking steps in the Theta space not directly in terms of the actions we take when we update our policy okay so let's think about sort of a parameterization which is a pretty simple parameterization so this is um logistic function this is like one one okay so in this case this is the parameterization it's kind of you like kind of like a softmax you just have some probability of going to one action and the other the rest of the probably goes to the other and you've parameterized it with Theta so if Theta is equal to zero it's 50/50 like you take a a 1 or A2 if Theta is equal to 2 you suddenly take A2 with much less and if Theta is equal to 4 you basically never take A2 and that's just because of how our relationship goes from Theta 2 pi of a like in this parameterization it's pretty extreme so as we make sort of small relatively small changes to Theta like I didn't make Theta a million or anything I've basically shift even with sort of 0 to 2 to 4 I've radically shifted how much of my probability Mass goes on to A1 and that's just sort of just to illustrate this issue of smoothness that is I make what might be considered relatively small changes in my Theta space that might have that might make my policy near deterministic and we know that if our policy is deterministic we can't learn anything about other actions so let me just make this a little smaller so you can see the question so so the challenge in this case is that step size can matter a lot in terms of efficiency we don't necessarily know what the right step size is and it may be hard for us to know how small changes in our parameter space relate to changes in the action distributions we actually follow and so what we'd really like to do here is to actually come up with an update rule that doesn't sort of over change the policy too quickly but still allows us to make rapid progress so we'd like to sort of move as far as we can in a way that we think is is really ideally is going to just directly increase the value of our policy okay and I guess I'll say well we'll see a bit more of that I mean and also ideally we would like this all to be monotonic we' like it so that if we think back to the policy Improvement algorithms that we've seen before policy Improvement algorithms for the tabular case where we knew how the world worked had this great property that every time we updated our policy we got a better policy or we were done now the world is much more complicated now we've got you know these sort of continuous parameterizations we not guaranteed to get to the optimal policy but it would be really cool if we could still guarantee that we're going to just get sort of monotonic improvement unless we get to a local Optima and the things that I've shown you so far um you know don't necess have that property because they'll still converge to a local Optima but you might sort of overstep like we see here so you might go over you might be having monotonic Improvement and then crash and then you have to go back and forth so we have not guaranteed monotonic Improvement so far but that would be really nice and that could be important in a lot of real world domains like you could imagine if you're using this for like healthcare applications you would really like to have monotonic Improvement and not suddenly kind of performance crash all right so let's think about how we might be able to get here and you can think of a lot of this lecture as motivating the things that you're going to be doing in homework too all right including the theory so in general what we'd like to have is we'd like to have an update step that uses all the data that we just got as officient ly as possible and that take steps that sort of respect this distance in the policy like the decision space um as opposed to just smoothness in the parameter space and and in order to do that we need to sort of understand how does the performance of two policies relate so we have data from one policy and we're considering trying to move to a new policy and we'd really like to know okay given the data that I have from policy one what does it tell me about how good policy 2 might be CU ideally it would allow us to policy 1's data would allow us to tell us which policy 2 I should move to next right so this is what you're proving in homework 2 you're going to prove the performance difference Lema and in the performance difference LMA it allows us to relate the performance of one policy um give the performance of one policy to um the performance of another policy given data from one of the policies let me just State this out so what does this say this is you know the value of one policy policy one policy Pi Prime I'm just using J here um but this is value B is what you can think of this is just V so what is that equal to okay that is equal to the expectation over trajectories that are sampled using pi Prime of and again you know if it's a if this is if finite canol can use Gam equal to 1 okay we sum over the distribution of trajectories you could get if you followed policy Pi Pi 1 times the advantage under policy Pi okay so this part here is just equal to the difference between if you took an action minus okay but note here because this expectation here is over Pi Prime the way we're selecting these actions just write it out a little bit more imagine we have deterministic policies so it's like we're thinking about the Q value if we first take an action according to policy Pi Prime and then follow policy Pi for the rest of Time Versus what we would have gotten if we just followed policy Pi the whole time so you can think of it as sort of like breaking down the difference in the value between two policies into a series of small differences of like well how much game would I have gotten at this date if I had taken Pi Prime action instead of the one I actually took okay what about here and then you kind of like want to sum up all those additions like every day I'm happier because I went to Stamford instead of Harvard and I just add up all of those and that tells me over the course of my whole career how much happier I will have been hypothetically Okay so this here is over trajectories now we're going to make a transformation and move it into State action distributions okay cuz um what what this is going to be here so now this was over trajectories we're going to rewrite this just in terms of State action distributions okay what we're going to say is all right as we think about adding up all these advantages what I'm going to do is I'm going to Cluster together all the advantages that have to do with the same state so I think of there as being a distribution Over States I might reach and actions I might take under policy Pi Prime so if I just follow this policy I'm going to visit some states and I'm just going to think about what is the advantage in each of those States um weighed by how frequently I visit them so we're transforming things from thinking of it as being like trajectories and thinking about waiting over time steps to waiting over a finite set or you know a a space of states and actions and so we'll have a distribution you know it might be like I visit State 1 half the time I visit state 7even only you know one in 10,000 times so this allows us to to reight the advantages how what does this distribution look like this looks like the following essentially you just think of what is my so here we're allowing us to have discount factors because we're looking for the infinite case but you can adjust this what this is just saying is that well my sort of weighted distribution for State s is equal to well How likely was I to be in state s s on time step one under that policy well How likely was I to be in State uh s in time step two under that policy what if I was in time step three and so you just sum up all of those and as you could imagine in the infinite Horizon case you're going to you know those could easily go to Infinity particularly if you have States you can visit a lot um and so the discount Factor here makes sure this becomes a distribution so we want to still to be normalized and then similarly this is also with respect to taking the under P Prime so you'll be you'll be proving this in the homework um but we'll see how this can be helpful okay so why why are we making you do this so the nice thing is is it's going to define the performance of Pi Prime in terms of advantages from PI okay so that seems good because we're like well we have um we have an inis policy but the problem is is this still requires trajectory sampled from PI Prime so you could think of pi as potential being the policy we have right now and maybe we can estimate the Q function for it we can estimate its advantages and now we want to figure out how good would be this new policy we might take but the problem is we don't have any trajectories from the new policy we only have data from the old policy okay so we really want to get to something where we can estimate how good is pi prime using data only from PI that's our goal so like our our goal estimate J Pi Prime only from data from PI so you think of Pi Prime is our the new policy so what we really want to do is sort of take a step so that like our new policy is the best one we could get to in terms of the its improvement over the previous policy and we want to be able to do that by only using an estimate from our old data and it shouldn't be clear yet how we could do that like this is looking promising but still seem like we need data so this is still data for the new policy so let's look at it from a different angle so this is this thing d d Pi of s is a distribution Over States it's a discounted future State distribution and we're going to use that to sort of rewrite the relative performance a policy performance identity so why is this relative it's because it's with respect to the per performance of our current policy so that's why we have a subtraction there we're going to rewrite that there see if I okay yeah so I'm going to step through this so what we can do at this point is we can rewrite it as follows so right now remember this is in kind of like the time or the trajectory notation okay so I'm going to again rewrite this so that I'm going to move it into the state action representation so instead of thinking about as trajectories I'm going to think about it as what's the distribution Over States and actions that I'm visiting okay so I'm going to write it as follows one 1 gamma Su over that as right as the expectation expectation over S Prime according to okay under let me actually write it this way okay what have I done there so this should look pretty similar to the previous slide what I've said is okay this is the discounted future State distribution I saw on the previous slide that I could rewrite this expression as 1/ 1 - gamma e * s * this D * a so that's what I'm doing now so I've Rewritten it in terms of this state action distribution okay and this is where I'm in this problematic case that I've got um the wrong let me make sure I put a quote there so this is just to be really clear this is over Pi Prime okay so this is all with respect to the new policy so now I'm going to note the following okay so what does this look like this looks like it's 1 1us gamma the expectation over S Prime sampled according to D Pi Prime and then what is this expectation so this is going to be sum over a pi of a given S Prime it's horrible notation let's try this again okay that's what this means like I'm taking an expectation for each of the states I'm taking an expectation um with respect here let me just try to make this neat there we go okay so I'm saying imagine I sample States from my D Pi Prime distribution how do I do this expectation over a sampled from PI Prime well I just sum of all the actions look at the probability of me taking that action for that State under Pi Prime Times my advantage okay so now the key thing we're going to do is we're going to try to change this so that we're using um more we're going to try to get to a point where we only need data from PI okay so the first thing we're going to do here is we're just going to rewrite this and I'm going to multiply and divide by the same thing okay so I'm going to say this is pi Prime of a given s / Pi a given s * pi a given s okay I've not done anything at except for I've multiplied and divided by the same thing why did I do that well the good thing is is I know how to get samples from this I have samples from this this is from my old data this is from the actual policy that I took before okay so what this says I can write this as I've got 1 over 1us gamma e of s according to D Pi Prime and then I have this expectation over a sampled according to Pi not Pi Prime of my re weighted advantages okay and what do I reway them by I reway them exactly by the probability I take that action under the new policy versus the old policy and that's okay because I'm going to assume that I have access to the policy parameterization of the new policy I'm just trying to figure out how good it is I don't have samples from it but if you tell me hey this is the action you took in that state I can say oh okay well that's how much How likely I would have taken that under my new policy so I can do that reting and this is an instance of something called important sampling and we're going to see a lot more about that soon but so this is the first step I can do so this is great right because now I don't need to have samples of from actions taken by my new policy I can just reweight the data I already have so that's super cool but there's a problem okay this is still Pi Prime so I still don't have any data over States from my Pi Prime okay all right so this is that just written out more neatly we'll see a lot more on that in the future okay but we still have this big problem because we don't have any States from PI Prime we have data from PI okay so what are we going to do about that well we're just going to ignore it always an option so um we're just going to ignore that and proceed and this is what is going to happen in in the rest of the class for the rest of this lecture we're just going to pretend that those states are the same now as you might imagine that is going to slightly induce some error in my estimate when might that be bad well it might be really bad right if the two policies would actually visit totally different parts of the state spacee but if they visit things that are really close maybe it's not going to be that bad okay so let's imagine that you have you know policy one and it goes this is your real SP and it goes to most of this part of the space and then you change your policy and maybe it also goes to that part of the space and it goes a little over here too but there's quite a bit of overlap okay the places where it would be bad is something like you know if your policy goes like this your new policy and then there's like no overlap in your state space so it's going to turn out that if pi and Pi Prime are close and we're going to find what we mean by close then this is actually not a bad approximation it's not perfect it's not too bad so in the paper they prove that we can bound how bad this approximation is okay so in general so this is we're going to Define this to be L um Mass script L of Pi with respect to Pi Prime if this was perfect this thing would be zero because this would exactly equal this difference so this minus this would be the these two things would exactly be zero but what it turns out is that um how far off this appro excuse me how off how far off this approximation is depends bless you on the kale Divergence in the policies and and I'll Define what kale Divergence is in a second so in particular it depends on the kale Divergence with respect to uh let me just undo that so I can just do this part with respect to States if you were sampling them according to dpy now dpy is good because dpy is the actual discounted states that we visit under our current policy that means we actually have data about it so I always like to work with my in my lab we always try to instantiate our theoretical bounds because I feel like it's super informative to be like oh is this like you know 10 to the 10 or is this like 05 um and what and one of the big questions often that comes up when we try to do this is that um sometimes you can't instantiate your Bounds at all because it'll depend on constants you don't know so like it's a beautiful theorem but like you can't even check how big it is the nice thing about this is like it's checkable um at least this part is right because you can get you can look at your actual trajectories from your current policy and if you have a new policy Pi Prime you can see and evaluate what your K Divergence will be so this is actually evaluatable so we'll see okay and I'll what c ah so C is a constant any information about its value um no I will um I we're not going to need it for now but um in the paper you can read about sort of exactly what that is yes good question though okay let's see what kale d es so what this says is just at a high level is that this approximation is not totally insane if the policies are close um in fact this is going to be a pretty good approximation so as we're going to see this is tight if the policies are identical which is exactly where you'd expect this to be tied so if your two policies are identical their difference should be zero and this bound would tell you it's zero all right what is K Divergence some of you guys might have seen this before but in case you know some people might be the new so what K Divergence allows us to do is to compare two probability distributions so in our case what this is going to be is over actions we will take so like you know Pi of a given s versus Pi of a s so both of these are uh probability distributions that sum to one for a particular State and so what what in our case what we would be summing over here x would be a so' be summing over all of these if you have the same um probability distribution the K Divergence is zero otherwise it's strictly positive it's good to know it's not symmetric because we've made a choice here in the ordering so just these are good properties to know about it comes up all the time um in reinforcement learning so in our case we can look at for a particular State s what is the K Divergence in what the policies would do for that particular state so that's what we've got there so it says essentially like how different are the actions you would take now why is this good well we've been spending some time saying hey we really don't want to think just about what the how close we are in Theta space we really want to get to thinking about how different are our actual policies like how different are the actual decisions we make in particular States and the nice thing is is what this bound says and is that the difference between two policies is not just about how close you are in some parameters base it's really about how different are the decisions you'd make in all the states you'd reach under your current state distribution and so if you'd make all the same decisions in the in the states you're already reaching your policy value is going to be really similar if you would make very different decisions then your policy might be value might be really different because you sort of go off and explore really different parts of the state space okay so this is really elegant um and now we have something where we can just use our old data so we have our old data we can use it to estimate what what the performance Improvement would be if we try to get to a new policy and so what you might imagine in this case is you could use deserve search over or decide which new policy to try Okay this allows you to compute it for lots of different Pi Prime doesn't just have to be the pi Prime for one gradient step it says in general even outside of policy gradient methods you can evaluate the um value of of changing your policy to Pi Prime with respect to your current performance by the expression and this will be more or less tight depending on how different your policy is at making decisions in states that you would currently reach okay and we'll talk more about this we haven't talked about this yet so we will talk about okay this also relates to some really nice literatures from kind of the last 20 years of thinking about how do we do monotonic policy Improvement ment in sort of policy gradient methods and policy search methods it also relates to the notion of a trust region which is the sort of this idea of how um when you're changing your policy how far can you go and still sort of trust the performance of it and trust you can get Improvement so there's a bunch of different nice papers related to this okay let's talk about the algorithm proximal policy optimization is going to be inspired by all the things that we just talked about so what we want to do is it wants to be able to take multiple gradient steps and it wants to be able to do this in a way so that we don't overstep we try to focus on um policy parameterization uh in terms of the actual decisions that we make so there are two different variants one is it solves an unconstrained optimization problem where it uses this approximation so that's the approximation we had on the previous slides I'll just write down from prior slides where we use data from the current policy and we kind of add up these advant these weighted Advantage functions so what it says is well the thing you want to do is you want to pick the policy um that maximizes our estimated difference subject to a constraint on the kale Divergence because it's realizing that that that L approximation is going to get worse and worse as the kale Divergence gets really large so it's sort of directly incorporating this bound so it's thinking like okay I want to think about what this is but then I also have to consider the fact that my estimate might be off by as much of this sort of square root K so you really want to sort of improve with respect to something that considers both of those right so this is what this is one version of policy this is not the way most people do po we'll see the other really common one but it's it's a nice Baseline to know about and here when we think about what that KL is as you might have noticed KL is defined for a single state so for a single state we can say what is the distribution over actions i' take in one policy versus another but of course we have lots of states and so what we can do here is we can take an expectation over the kale over all the states we'd visit and that was part of the the theoretical bound too another really important thing you can see here is this waiting waiting between trying to optimize kind of this policy Improvement while respecting this kind of K penalty and you can change this over each iteration to kind of approximately satisfy the kale Divergence constraint so this does not guarantee that you will this does not guarantee that you're going to get monotonic Improvement but it's trying to get towards that so let's see how that works so this is the algorithm what it does is you can compute the advantages using any advantage estimation algorithm you comp compute your policy update and you can do K steps with that so the nice thing is is that you know you can use your old data here and you can take multiple gradient steps after you do this you can also check if your K Divergence for your new resulting policy is large if it is then you may increase the penalty if it's small um you can decrease the penalty and that just allows us to sort of trade-off between how much we paying attention to this kale Divergence constraint versus not and as as I noted here you know you might violate the K constraints but most of them they don't empirically okay this is one reasonable thing to do based on everything we've seen now we're going to see something else which is inspired by that it's a much more common thing to do um which is uh well let me just highlight here multiple gradient steps is really good so this is you know the one of the benefits is that we're not just taking a single gradient step we're taking multiple so just to really highlight that all right what is the other thing we want to do here um we haven't talked about natural gradients um just but for any of you that are familiar with these this is not they another way to try to think about taking gradient steps um we're not going to talk about that for now so the other thing we do is a clipped objective so what we're going to look at in this case is remember how we talked about we had this kind of ratio between this is what we're using to weight our advantage function was the the difference between How likely you were to take that action under our old policy versus our new and we're using it to wait our advantage function what the clipping does is it says well I don't want this to get too high or too low okay this could become really high or really low when my policies are really different if my policy is really you can imagine that um if my policy puts really low probability on something that the current policy puts high probability on this ratio here is going to go towards this R is going to go to about Z zero and if this puts very high let's say this is one and this puts very low probability on that this could be extremely large this could be like 10 the 6 this could be very very large and both of those are being used to weight the advantage function right so your advantage function could be getting shrunk towards zero or it could be getting blown up by a factor of say 10 the 6 if you have a big difference in the actions you would take under one policy versus another and in general we don't like where like you know we're thinking of policy gradients where we might have terms that are exploding or Vanishing and that's part of the point of the kale Divergence constraint is to say you want your policies to stay close so the clipping is sort of inspired by this general idea but says well maybe something similar we can do is we're just going to clip we're just going to say you can't have weighted Advantage terms that are going towards um you know infinity or minus infinity or zero and so if this ratio is too extreme I'm just going to clip it I'm going to not allow it to be less than um 1 minus Epsilon or greater than 1 plus Epsilon and and Epsilon is just a hyperparameter and essentially it's sort of meaning that you know your policy might change further than that but that's not going to benefit your loss function so this again is going to sort of constrain your policy class to stay within this code of region for which it's making similar decisions so we're still really F focusing on what actions are we actually taking are we taking similar actions in these states as we would be normally regardless of how much my Theta is changing okay and then you just do your policy update by taking an argmax over this so this is your clipped objective function all right so let's see how this works let's think about what it's doing so we'll do um a quick check your understanding so this shows you what L clip does this is L clip as well and what I'd like you to think about is what does this look like depending on the advantage function so L let me just write down LP okay so this is R so on the x- axis is r and then on the y axis is L clip and what this is asking you to think about this is a from their paper um is to think about what does clipping do in terms of your sort of loss and so I'd like you to think about in this case which of these two if either um match within the advantage function is positive or the advantage function is negative so a here is the advantage let me just make that clear a is equal to the advantage so just think of this for a single term consider for one term okay so just just this part okay so just for one single um RTA what is happening here and what and just to be clear here what we're doing in this cases we're taking the minimum between the normal thing we do which is this reweighted Advantage function times a clip of the r time the advantage function and feel free to like you know flip back and forth or play with numbers just to sort of get some intuition of what is this doing to our um you know our loss function or I shouldn't say loss um our objective function in this case we're trying to take the arcmax of it so thinking of this is sort of an approximation of how much is our policy going to improve when we change our Theta so we're going to want to take a Max of this over with respect to a new policy Theta and we want to think about this is sort of bounding what that new performance benefit could be and how does that vary with respect to the advantage for all right nobody thinks it um depends on the value of e which is correct so this is um does not depend on the value of e want you turn to your neighbor and see if you got the same thing Val e e oh cool it looks like talking converged most people which is great um so does someone want so the first one is correct so this is a greater than zero this is a less than zero does someone want to explain why those of you that voted got it right well it is quite simple because the simply like a qu efficient to the uh value so it has a positive slope and it is positive and negative slope and negative yeah so if we just focus on this for a being equal to greater than zero um what will happen is as that ratio R gets higher and higher and higher um you'll just linearly go up because it's just um you know something between 0 and one that's getting larger and larger um R can never be negative so it's just useful to see in this case so as it getting larger and larger just going to increase your L clip value but at some point you're going to run up against this part and so at this part you're going to clip it and you can't get higher anymore remember in general we're always trying to maximize L clip in this case when the advantage is negative you're trying to sort of reduce the amount of um probability Mass you put on that action because you don't want to take that anymore like oh I got a negative Advantage so I need to stop doing that so essentially you want to be sweeping changing your policy in the opposite direction you want to you'd really like to be able to push R all away to zero and say I never want to do that action again like it gave me a negative Advantage but you can't do that because that might change you know radically change your policy and so once you get to one minus Epsilon you cap it and you can't further shrink it to zero great okay so and another way to see this is from the paper is like you can think about sort of these different types of constraints and different clipping and essentially again it's sort of like making the objective pessimistic as you get really far from Theta now just in the last couple minutes I want to make sure to show some plots so this is just the same algorithm um but where we're doing clipping I will just note here that next week so next time we'll discuss next time we're going to discuss um the choice of a further just like what we saw earlier today you can do endep estimators Etc you can do what's called generalized Advantage estimation um you don't need to know that for this we we'll cover that for today but we'll cover it more next week so just to note there's some additional choices here but let's just look at sort of what the performance is so at this point trpo and some other algorithms were out there um they have the PO clipping in purple this is a number of different mojoko domains similar to mojoko domains you're going to be working with what you can see here is that in general so trpo um has this trust region idea and it's similar in some ways to what we're what they're doing in po but trust region policy optimization is quite a bit more complicated and what you can see here is that like this light purple which is po um this is the number of steps uh is generally doing just much better than these other ones so it is they're not they're going to get to a better better Optima they're just saying we're going to be able to get there much faster with much less data and in some of these cases this is really you know an enormous performance Improvement for the same amount of data so so this is one of the reasons why it became extremely popular is it is a pretty simple algorithm to implement and it has extremely good performance in many cases now I do just want to so um you can read you can go to the original paper proximal policy optimization alth or go to the blog post from uh 2017 I do think one thing that's good to know and really throughout much of ourl recent history is to also understand what are the implementation details so this is optional you don't have to read it but there was a nice paper that followed up from um this work because again po has been hugely influential uh from some colleagues at MIT that said well really which of these things are most important because in general in these algorithms there will be these things but then there's also some hyperparameters or you know architecture choices Etc and so knowing how these choices are made often do make a big difference in in reality and so that's always good to know it's whether or not like is it an algorithmic improvement or are there additional things that we're not treating as part of the algorithm but actually are really important for practical performance so you should know everything now that um you need to for making good progress on homework 2 and we'll continue to discuss this next week thanks