all right they should be up now m all right just take a second and then compare your anwers to someone near you the reason I'm asking you about these particular algorithms is because some of the ideas today even though we're going to be talking about alphago and Monte Carlo treesearch will be related to some of the things that helped make those advances possible so just check a good chance to refresh your understanding of how upper confidence bound algorithms work and the one I thought might be somewhat controversial in particular is the third one of whether or not if you have a reward model and it's known whether there's still any benefit to using an outp confidence bound algorithm e all right let's come back together so the looks like there was good agreement on the first couple so the first one is true which is you can think of upper confidence bounds as being a way to balance between our uncertainty over outcomes when we have limited amounts of data um and yet use that information to still try to acquire High reward and these algorithms can be used both in Bandits and markof decision processes um the third one is a little bit tricky uh actually either answer would be fine depending on which setting you're looking at so does somebody want to argue why if the reward model is known there is no benefit to using upper confidence bound algorithms so in some settings there would not be someone told me a setting where if you need the reward model you should not use an upper competence found algorithm something that we saw over the last couple weeks that was different than the reinforcement learning framework the MTI Bandits that's right yeah so in the multi-arm Bandit case where there's like no State there's no Dynamics the decisions that you make don't influence the next state at all um then exactly what said if you knew what the reward model is you'd know how to act like if I knew whether a customer liked add a or add B better I would just show them at a so in a multi-arm bandit setting so in a map set this is true in general it's not true in RL thatting generally false somebody want to tell me why in general it's false even if you know the reward model in reinforcement learning you might still want to use an upper confidence bound based algorithm because what we want to know is the value function rather than just the immedate rec that's right so assuming that you don't know what said which is I'm assuming you don't know the Dynamics model um so you don't know how to compute what your optimal value function is it's still often helpful to use upper confidence bounds and in fact in many cases you might know the reward function for when you reach estate like you know when a customer clicks on something that's good um but the hard thing is maybe to drive them into states where they are going to click on something or they are going to make a purchase so in RL um this is generally false and we'll see some other examples today where it's helpful to use an upper confidence bound algorithm even though we know quite a lot about the world so what we're going to be talking about today is Monty Carlo research in alphago um and before we get into that I'll just remind us a little bit about where we are in the course so we have uh just a few more weeks left you should have all gotten feedback on your projects um if I encourage you to come talk to me he anybody else um about any questions you have I have two office hours this week because I was traveling late last week for a conference um so you're welcome to come to my office hours today which are right after this class or on Thursday there's also a lot of other office hours in addition to that a week from this Wednesday we're going to have a quiz the quiz will send more details out about on Ed but the main idea is that it's going to be multiple choice it is designed to be easier than the midterm but we'll give you the full amount of time people generally take full amount of time just to check their answers um and it'll cover the entire course so everything up through um you know the day before somebody have any Logistics questions before we get going all right so we're going to talk about Monte Carlo treesearch um and Alpha zero so as many of you may know there was this amazing series of results from Deep Mind in kind of like the 2016 to 2019 time period um around showing how you could use reinforcement learning and AI to conquer the board game go and this happened about a decade earlier than people expected and this was really considered one of the huge achievements in AI um so people really thought it was going to take a lot longer to do this chess had already been mastered um Checkers longer before that um but there was a lot of different innovations that came out of a long history of work that deine used to make this possible and I also think that it incorporates a lot of interesting different ideas that one thinks might think could be helpful to try to solve other problems the other thing that I think is interesting about this is it's quite a different form of reinforcement learning than we've seen before it's really reinforcement learning for computation um and we'll see a lot more about that okay so what we're going to start with is thinking about simulation based search um and I in simulation based search is going to sound quite familiar because we've been seeing ideas around this with Monte Carlo search um Carlo methods but then we're going to think about combining these with using different parts of the sort of stochastic decision-mak process all right so in particular one of the major ideas that we're going to be looking at today is the idea that we're going to be mostly focusing on how to make figure out what we should do in the current state only so in general in class whenever we've been Computing a policy or a value function we've been Computing it for the entire State space so um we might have a policy and if anybody gave us a state we could immediately tell you what action or action distribution we should use or we compute a q function for the whole Space one of the key ideas today is to say well maybe particularly if we've got an enormous space that we don't really care about trying to compute a optimal policy for everything in the space maybe we just want to use our current our use our computation to really focus on a good decision for right now or for whatever state that you might end up in and there are lots of reasons to think that that might be important particularly in really large domains so you can imagine like you know if you're the fed and you're trying to make p sort of federal monetary policy you probably don't care about doing this for all the scenarios which the US is not in you really want to figure it out for the current scenario um in the case of the board game go as we'll see there's just an enormous space of potential States you could end up in and it may not be important to have a perfect way of acting in all of those so one big idea here is that we're going to be mostly focusing on computation to figure out what's the right thing to do in the current space so one thing you might do in this case given all the ideas we've seen in class is you might simulate so imagine that someone gives you a policy and what you want to do is try to do at least as good as that as that policy and maybe a little bit better so one thing you could do is say well I'm in a current state like a current real State say St and what I'm going to do is I'm going to say think about all the different actions I could take next and then I'm going to roll out using my default policy from those States so this is just like for K episodes from the current real estate I'm going to roll out in my brain what might happen now this means that I need some access to a Dynamics model so I can only do this if I have access to a model and what I'm going to mean here by model is Dynamics and reward model so you might imagine that you actually know how the world works or that someone you've learned some sort of model from the past this could be estimated or true and so then what we can do is we can just do sort of Monte Carlo evaluation and what we're getting here is an estimate of the Q function that says if I start in this state and I take this action and I roll out under my simulation policy what is my expected return so that just gives me an estimate of the Q function we've seen this before with Monte Carlo me and then you could just pick whatever the real action is like this is what you're going to take in the real world with a maximum value and what you can think of what this is doing and I'll just going to augment this with pi to make that even more clear is what I'm essentially Computing is for from the current state what is the Q value of my simulation policy and so I could roll out under those policies and then I'm going to do sort of one step of policy Improvement given that and so if someone gave you a budget of computation this would be one reasonable thing you could do with it and then we're going to see a lot of things that you can do that are much better than this but this is one thing you could do that would be viewed sort of a simulation based search which would allow you to do better than the current policy you have all right I think it's helpful to think about this in terms of sort of what the tree structure is so the idea in this setting is that we start in some State and we're going to roll out under we're going to take a certain action so that's going to be our action a it here that's our action a and we will take and then after that we're going to get to another next state S Prime and then from then onwards we're going to follow our policy Pi so this here is policy of S Prime I'm going to sample an action according to that so at the root for my current state I'm going to consider all possible actions but then after I take that action and transition to next state I'm just going to roll out by following my policy Pi okay and then I do that all the way out till I hit a terminal so T here equals a terminal State and so one thing you could do is just do the simulation and then average over the the root nodes but you also now have a whole bunch of data so you could do other forms of reinforcement learning given that data do the P to be optimal for this gu good question so um it depends how we think of what this is Computing so if this is just Computing Q Pi of sa it's just just doing one step of policy evaluation so this will work whether Pi is a good policy or a bad policy but exactly I think point if Pi isn't very good then you probably aren't going to get a very good um like you're just doing one step of policy Improvement here you're not necess going to get qar unless Pi is really CL optimal okay so this is one thing you could do but I think the nice thing of visualizing the tree in this case is it starts to make it really obvious that you could do other things that could be better than just follow following whatever your current policy is okay or whatever policy you might have access to and I'll just make this CLE with the model and uh default so we might instead if we have limited amounts of computation instead of just doing roll outs we might want to try to get something that's closer to qar and one way we could try to do this is by trying to construct an expax tree so raise your hand if you've seen either Minimax trees or expecting Max trees before okay like a few people but not everybody okay so um this be a quick introduction to those but the idea is if we think about what this forward searches really what we are doing when we construct this tree so this is the action so this could be like say A2 and this is A1 and this is a next state imagine that you just have a few States in these examples so the black nodes are all actions and the white nodes are all the states you can think of this and we've seen similar graphs to this a while ago think of this as just sort of rolling out your Bellman backups okay so you can think of what happens in the world is I take an action and I transition to some State and then I take another action and I transition to some states and sometimes I terminate and what I would do normally in this case is then I would back up along this tree so whenever I have States I would take um like an average or an expectation and this is really just representing the probability of S Prime given sa so it's representing that sum and then every time I have actions I would take a Max and this is just representing inside of the bman backup that I take the max over the actions so you could think of this as just approximating the max over Max over a RS a plus gamma sum over S Prime probability of S Prime given sa a v of S Prime except for instead of having B of S Prime then you would just expand this out all the way until you hit the terminal State okay and this would require us also to keep track of the rewards we obtain as we go down this tree so for example here you might get reward of S Prime a soes that makes sense so if you have access to a markof decision process and its Dynamics model and its reward model one way you could use that to figure out what's the optimal thing to do in your current state is you build this tree build this tree into till at a leaf you reach a terminal node or for a fixed Horizon H and then you back up by doing Wherever You See It branching according to States you take an average weighted by the probability of each state and whenever you get to an a set of action nodes you take the max have any questions about that um we might get to this later but like if we're considering like complex games like go like the state space is like massive right it's very unlikely that you're going to run into the same same um like composition of the board twice like how do you deal with that great question hold on to that we'll get to it yes yeah absolutely so right now well in fact on the next slide we'll talk about how big this tree is okay but this at least conceptually should be something that you think yeah we could do this I could imagine doing this so why might this be better than before well this might be better than before because you're not actually solving the whole mdp you're only doing sort of bellman backup starting from the current state you're in and so you might imagine that if the space is enormous even though you're sort of rolling this out in terms of this kind of exponentially growing tree um it still might be smaller than your whole state space okay but as was saying um this is huge in general so if you want to actually expand the whole Tree in general the size of the tree is going to scale by the size of your state space times the size of Your Action space to the H H here would be her Horizon and so as you could imagine this is going to be terrible really quickly right like we don't want to if you think about go you think about Mountain car or other games where you might be or other environments where you might be having sort of a 100 or to A Thousand Steps this is going to be completely intractable right but as you might notice when we're looking at this here when we wrote out we sort of thought about all the next States we could reach but that if that's a really large set we know that we don't necessar actually have to sample all of them and compute that exactly in order to get a good estimate of the expectation we we know that in fact we can just sample so if you sample what's the next date 100 times an average overall of their values that's a pretty good approximation what the average value is even if there are 10 billion States okay because you can approximate um an expectation by an average and that tends to concentrate really quickly so that's going to be one of the really big ideas of using Monte Carlo tree search is that we're not going to have to expand all the next States we're just going to sample them so let's see how that might work so this is where we get into Monte Carlo tree search and note I highlighted in tree here because we're not doing Monte Carlo search anymore we're not just rolling out with a policy we're essentially going to try to sample parts of that tree okay but we're not going to um just do single pie roll outs so we're going to still we're going to build a search tree rooted at the current state we're going to sample actions of next States and we are going to explore different parts of that tree we're not going to always follow the same simulation policy Pi okay and then after the search is finished we're going to take an action in the real world by whatever has the highest value as we estimate at the root at least that's one way we could do things we'll see some other ways to do it and you know well let's let me just explain a give a little bit of intuition of why does this work this works because what we're doing in this case is we are approximating expectations with averages okay so we're not actually trying to expand all the next state we're just going to approximate it with averages and that will turn out to concentrate pretty quickly and that's going to be really helpful okay so let's do a quick check your understanding so oops well there you go it's okay you can think about whether or not you agree with this um mon Carlo research involves deciding on an action to take by doing research and so think about whether it's a good choice for short Horizon problems and Y long Horizon and large state in action space and actually the middle of this is slightly debatable so take as I can and think about this this so that when I uploaded later people can so why might the first part be false why would we not want to do this for well first of all does anybody have any questions on what Monte car research is doing um in terms of how it's different than the other things that we could do so then tell me why it's not probably a good choice for short Horizon problems with small state and action spaces what would you do instead in those cases yeah we better do what maybe I guess what I was think more is in that case maybe just do dynamic programming yeah if the state space and the action Space is really small you can just do value iteration yeah my Monarch could work too um but in particular if things are really small if you think back it's been a long time I know but um in uh in Monty in sort of standard dynamic programming it's only like s squ * a for each backup and then you're just doing that if you're only just doing the H times that's nice you don't have any exponential dependence in that case so if it's really small just do bman backups and the Order of that is roughly s^2 a Time the Horizon H roughly okay so at least it avoids the exponential um it will be a good choice for long Horizon problems with a large State space and actions small um action space because what we're doing in this case is we're approximating that expectation by samples so we approximate so this is true this is false approximating expectation by samples and so that means instead of us having to get that like enormous State space that we're multiplying by whether s squ or such we're just sampling from that and so we can have something that's more like a constant with respect to how much we're sampling now the middle one is actually a little bit controversial um and we're going to see different ways to tackle this why should this be somewhat controversial well in Monte Carlo research the initial way we're getting a big gain is we're sampling next States instead of enumerating them but it shouldn't be obvious that like for actions we want to maximize for actions we want to take the best over all the actions and so Monte Carlo research a priority still has to just sample the whole action space and so it's not clear yet that unless we do something special that Monte Carlo research is necessarily going to help us when we've got really big action spaces because in general we've replaced the expectation by a set of samples but it hasn't told us yet how to do anything smart in terms of the action space okay so this one is sort of debatable um maybe false depends how you think about it but of course there are a lot of algorithms that combin with Monte Carlo research to show us how we might be able to tackle this problem okay so the the what we really want to be able to do is solve long Horizon problems with enormous action spaces and enormous State spaces and so we're going to need ideas Beyond Monte Carlo research to tackle that okay an upper confidence Tre search is one idea for how to do this and I think UCT came out in around maybe like 2007 2008 people started using it for go around then and the idea in this case is that in addition to doing the sampling over next States let's be strategic over what action we take when we're expanding in our tree so when we decide to sample a next action doesn't have to be from a default policy Pi let's think carefully about essentially where do we want to fill in our our search tree and this is one of those other really big Ideas because this is really where we're going to start to think about ideas from reinforcement learning essentially to optimize computation because right now we're still assuming that we know the mdp like we know what the Dynamics model is and we know what the reward model is so in theory if computation was no issue we could just do value backups the challenge is this is going to be completely enormous and that's totally intractable so the ideas here is to say well maybe if we have access to those we can still think of trying to approximate sort of like Bellman backups or approximate Maxes but we don't actually have to want to enumerate all the actions as much and we want to really focus where using our computation and deep mind's been really a Pioneer in thinking about using reinforcement learning to prioritize computation to solve a lot of really important problems and I'll try to come back to that at the end okay so how does UCT work um the idea is and this is why I asked you guys about this and refresh your understanding is we're going to treat each node we're each node that was sort of like a state node inside of our research as a bandit and so it's like we have many many many many band up problems inside of our search tree and we're going to then maintain an upper competence bound over the reward of each arm inside of a node okay so the first node you would have is your root node okay right and so it would have say A1 A2 A3 and we would think of that as a MB as a multi- arm bandit okay and then when you get further down in the tree tree so let's say we this goes to next S Prime this would be another A1 A2 A3 and this would be another multiarm Bandit okay and you would have you'd have to store in memory lots and lots and lots of different multi arm Bandits okay so you're maintaining huge numbers of multi arm Bandits and just like what we normally do in Upper confidence bounds we're going to maintain a number of confidence down over each arm but what we're going to be thinking of that is is essentially what would happen if I take this action and then act um optimally till the end now one big challenges of course we don't know what the reward would be of acting optimally so there's going to be a lot of different sort of um policies that are moving at once but let's see what that might look like okay so here's the idea okay so let's say what we're going to call we're going to say we have a node I so this could be our root node or it could be any other node the way we're going to I'm just going to call this AI we're going to try to maintain an upper confidence bound over what is the potential expected discounted sum of rewards we'd get starting in this node and taking this action as the following let's say that we've been in that particular node and we have rolled out from it using some strategy that we haven't really talked about yet ni a time so this is the number of times we've been to this node before and we've happened to expand the a action what we do is we look at all the returns we've gotten under those cases so what's a return again so a return would be go back to here so let's say you done this okay what you would do what your return in this case would be is it would be a sum of all of these rewards you've gotten along the way okay so G we're going to use to denote the return so this would be reward from starting in state s taking A1 and getting the rewards out to the terminal State and maybe next time you go down this action you actually get to here and you get a different return so those are just like your Monty Carlo returns from before and it's just for all the other times youve went through that action okay so that's part of it so that's just an average and it's kind of a weird average right because it might be that your which nodes you visited and which actions you took have changed so we're not committing it to it to be a particular policy it's just like we've taken some action and we followed some you know we've made a series of decisions till we got to a terminal State we added up the rewards um and we keep track of that here so that's one thing and that's sort of we probably look at and think this is a very loose approximation of what the optimal Q value is for that state in action the second term looks like upper confidence bound which is you have some constant C you have some log term which depends on the the number of times you visited this node divided by niia number of times we've been in that node and taken that particular action okay so this just looks like a bandit term it's an upper confidence bound over the reward that we can get and so what upper confidence tree does is that the way it picks the action the next action to take from the current node is it picks whichever one of these has this higher upper confidence bound now it should seem slightly suspicious that this works because in Bandits when we took an action we knew that this really was an unbiased estimate of the reward of that action because we just would to see that one action and then we knew from hofing that this really was an upper confidence bound on the true value of that arm but now we're in a much more weird case right where we are thinking of this for a sequence of actions we're going to take we're trying to do expectations Over States and the actual actions we're taking from this um node onwards may not be optimal you know one time we might go through guess I'll draw it on board like one time we might go through this zigzag another time we might go through this zigzag another time we might come back to here and then we take a different action okay so it's not like we're not doing one step of policy Improvement here we just have lots of different things that we're trying and we're averaging over them okay so you might should be slightly suspicious whether or not this is going to be doing a reasonable thing okay but it's certainely something you could do something you could imagine coating and then we'll do this many many times and then at the very end so this will expand essentially different parts of your tree okay and when you're following this in particular you're going to start to expand parts of the tree which look promising more okay so if this one happens to have been getting you know this one gets like plus 100 and this gets plus 100 and this gets Plus 90 whereas let's say one other time when you took this action you went down here and you got - 10 well then when the next time you get to your root node you're probably going to be more likely to keep going down this path so it's going to sort of selectively expand parts of your tree it's not going to H and you'll sort of have these like unbalanced trees where uh you'll often see like parts of things are getting filled in and then maybe if something else becomes more promising you'll switch to an another part of the tree and fill in things there okay so it's sort of this unbalanced construction of your forward search tree and the way that it's unbalanced is that using the Monte Carlo aspect to approximate all the expectations and you're using this upper confidence bound to sort of selectively prioritize across your actions that's what's going to help with our sort of enormous action space now you still might be concerned that when there's really like a really enormous number of actions like we're going to see in go in other cas cases that this still isn't going to be enough right because if the number of actions you have is like you know like a million um these things generally you'll have zero for this part right before you have taken any actions and your counts will all be the same so it should still be concerning because like what should you do if you have these a thousand different actions and like you might not be able to do anything essentially until you visited everything once because before then as long as you've defined something that's a reasonable upper confidence bound everything is going to look awesome it's like oh action 99 will be awesome action 100 will be awesome and so you'll have to sample all of them at least once and that generally will be completely intractable so I'll see ways to further um reduce this but what you can think of this part is doing is saying well if you can at least sample every action once you can at least mean that you're not going to have to focus on unpromising actions later because you're going to quickly use a separate compid isbound so these sort of monticolo tre searches were starting to look really promising um you know a lot of people have used tree search-based algorithms as some of as you might have seen other machine learning algorith or sorry other AI classes probably in particular um but what this Monte Carlo and UCT based approaches is there sort of this highly selective best first search but with simulations as well and they're using sampling to Cur to break the curs of dimensionality and UCT and UCT to help with large action spaces and the other really nice benefit of these ones is they're paralyzable so when you're sampling things you could certainly imagine trying to expand do do these sort of roll outs many many times and then sort of collect the results so you can start to paralyze these methods as well and that's going to be really helpful okay so that's the background between Monte Carlo tree search um but now of course the really big breakthrough that this allowed or people built on these ideas is to achieve Alpha go Alpha go and then Alpha Z and then mu0 so there's a whole sequence of them and let's just get up a movie for that for a second and who he's played go okay a few people I think it could be fun to have us all play it so we can see that it's really quite hard but that another time go is the world's oldest continuously played board game it is one of the simplest and also most abstract beating a professional player at go is a longstanding challenge of artificial intelligence everything we've ever tried in AI just Falls over when you try the game of Go the number of possible configurations of the board is more than the number of atoms in the universe Alp go found a way to learn how to play [Music] Go building suspense we'll see how the network goes this is a documentary of deep mind's efforts to try to beat the world class people and go let me see if I can make it work think it's probably decided it doesn't like the internet right now just double check if I can get that to work so what they ended up doing is they are they're going to use reinforcement learning um to help solve this problem we'll see whether or not the technical difficulties resolve and then what they did is they started playing against grand grand Masters and they tried to then they played against Lisa doll who was one of the best people in the world ago and I think one of the really interesting things about this is that it really shows that it's now possible to use AI to beat the world best people in the world at go but also the types of strategies that it built were very different than what people were doing before and so I think this is a a pretty important aspect for AI because we've Al often thought of AI as sort of automating things that people already know how to do and I think this Illustrated that there are really starting to be places where computers go beyond even the best humans and what we know how to do and since then there's been a recent paper I think maybe a year or two ago by Bean Kim trying to look whether or not you can teach Grand Masters using the strategies that um alphao and its descendants in invented and so then there's this really interesting opportunity and question to think about can we actually learn from computers in these new ways and try to sort of exceed both human level performance and computer level performance so I will post this later you guys can look at it uh um let's go back to there okay all right so how does go work well it's a really really old game um it's considered sort of one of the classic hardest board games and it was considered a grand challenge for AI for many many decades the sort of game tree search that we saw before is something like a forward search um now it couldn't be expecting Max in this case cuz it's a two-player game go is what's considered um a zero sum game meaning that someone either wins and loses and in this case whenever we sort of think of a next state rather than it be an expectation it's really a Mini Max problem because each opponent is playing to win so in this case um it's good to think about sort of you know what is actually uncertain in this case um when we're playing go the rules of the game are known they actually have another descendant now where you didn't have to know the the rules of the game but certainly for the first few the rules of the game were known um so what's unknown go if we wanted to think about sort of building a tree or trying to learn in this case like is the are the D if we know the rules yeah well you might expect your adversary to play the best move that might not always be true they might be seeking different strategy so you wouldn't know that good point so it might be as saying that you might not know exactly what the best strategy is or you don't might not know what whether someone's going to play the best strategy I think the other thing that I think of is that we don't always know what the best strategy is it's just incredibly hard to compute this in this case and so in this case that sort of next state if that next state is really from an adversary it's not clear you've got stochasticity in that because you don't know what the optimal game is um now of course once someone picks a move everything is deterministic so in some ways it's all deterministic it's all known the key thing is that because it's sort of this adversarial game it's not clear what the optimal strategy is so that's kind of one of the really hard parts all right just a couple basics of the rules of ghost so normally it's played on a 19 by9 board but when people first started to well kids and also when few AI researchers started tackling this game in Earnest starting like the late 2000s or 2000 David Sor who's one of the authors of of this and an amazing researcher I think as part of his PhD in the late like 2008 2009 he was doing things on sort of a 9x9 board um and just as a couple Basics there's two different players uh either someone playing the black stones or the white stones and you're trying to surround stones that are captured and then you win the there's as I said it's a 01 game 01 game which means a winner takes all one of the interesting things about go is that in general there's no intermediate reward so you have to play till the end of the game see if he's actually winning and so there's just a single reward at the end which also makes it very hard for credit assignment and to understand what moves caused you know the resulting game so alphao and Alpha zero alphago was the first one that was used then um they developed a number of variants they then played against leas all and then there was Alpha zero and what they exhibit in this case is a number of different really interesting features so they have self-play strategic computation highly selective best for search they use the power of averaging they leverage local computation and then they learn and update heris for those of you that have seen tree search based methods before you've often probably seen ideas around heris which are other ways to sort of think about how do you expand the tree one of the interesting ideas in these in these papers is that they're going to learn those heris and update them over time it's another important aspect so let's see how it works so how does selfplay work um so the key idea in this case is that we're going to have the agent play it itself so there's going to you can think of it is there being two copies of the same agent right now um and what will happen when they're playing a game is they compute the best move at the current state and then the opponent does the same and they have access essentially to same the same policy or the same sort of algorithm but they're both just using it in an adversarial way and so that means like the only bottleneck in this case is computation we have no humans involved um and selfplay also provides a well match player so take a second and think about like what are the benefits that will happen with selfplay and what's going to be like the reward density there going to be lots of rewards when you do self play are there going to be very little rewards let's just take a second and I'll check and see whether or not I can make the networks work maybe talk to our neighbor and see if you guys both have the same idea of with their self play will be helpful or not all right what does this do to policy training what happens when you do selfplay like do you have high reward density do you have low reward density what happens here raise your hand if you think you have high reward density raise your hand if you think you have low reward density okay so all right well somebody who think we have high reward density want to explain why that's right we do have pretty high reward density why do we get that when we do self play what happens or I think it's easy maybe easiest to think of like if you play against someone that's much much better than you what happens kind of lose lose all the time right you know I mean everyone's probably done this before you play against like a friend of yours is maybe much better at a board game than you or something like that right or your friend of yours much better than you at tennis or something you go and play with them and it's not normally that fun because you just lose all the time um and when you lose all the time you may not get very much signal about what things are even doing better or worse at because you always lose okay and so that would be a case where the reward density is very low because the players are really mismatch matched and it means that most of the time the agent is not winning now the same thing is true if the agent is much better than the other agent but self-play means you're sort of matched you know you're like matched at the same level someone who plays tennis the same level as you or you're matched with someone who has the same ELO score as you and um in chess or go which is sort of a way to quantify the player skill and the nice thing about that is that you would expect that roughly if you play someone that's exactly the same level now here you're an RL agent so you're going to play someone that's actually you just on the other side so they're exactly the same level as you and that means you'd expect you'd win about half the time like on average so that's really good density for something that is um a01 game because you're not just like every you know 3,000 games getting a zero or a one um here about half the time you'd expect to get a one and half the time you'd expect to get a zero and the reason that might be beneficial is hopefully that's going to give you a lot more signal of how you should change your policy in order to figure out how to get better so I think that selfplay is a really interesting one because you could think of it in some ways as kind of providing an automatic curriculum on the next one so the rewards are going to be pretty dense and for those of you that have seen curriculum learning before another machine learning stuff just like in classes where you often sort of build up with math over time and you don't start with Calculus you start with like addition or what a number is and then you slowly build up so you're always trying to be on roughly the right level similarly here the agent should do that automatically because they're going to start off and they're going to both be terrible at go but they're still going to get pretty high density of reward because they're both terrible to go and then over time the agents are going to get better and then now they're automatically always playing an agent that's roughly the same level as them now we'll have to see why the algorithm will help them get better but intuitively as we saw even with like the Monte Carlo simulation not even tree search there it was doing like one step of policy Improvement so you can imagine that if even if each round we just doing kind of like one step of policy improvement over time we would hope that we're going to get better and better okay so this idea of selfplay I think is a really interesting one it works really well in games and it's been exploited a lot um I've often thought like it would be really interesting to see are there other places you can set up to essentially be like a game because what you could think of here is what selfplay is leveraging is that sort of for the um for the Dynamics part of your environment you now have a simulator you can plug in which is the agent itself now in general cases you can't do that like if I'm going to simulate patient Dynamics I can't just plug in like you know I don't I can't do self-play for that right like an action is how the patient responds to some treatment and I can't like play against two a you know two patients that doesn't make sense but I think in in some cases here it's really a very reasonable thing to do to use selfplay and it can be really efficient because you can think of it in a a way as like it's changing the strategies in a way that sort of iteratively um updating the complexity of the environment you're trying to solve soone I think I have a good idea but ex respect ah good question so here I just mean about um mean lots of things here what I mean by reward density is how often you're going to get a um you're going to win okay um so so and here rewards only happen at the end so it would just be of the games you play are you going to get a lot of reward you're going to get zero if the agents are really mismatch in general their W density is going to be either saturated which means you always win or um near zero because you're never going to win and neither of those are very informative and the idea is that if you're getting re about half the time that might be really informative because you can just you get lots of signal of like that thing work that didn't work that thing worked that didn't work you know and so you're going to have a lot of stuff done estimate kind of a gradient or an improvement for your decision policy yeah um with Sal play can we say that um like if some if you play against someone who has like a completely new strategy might not be able to generalize well enough because I was always playing against myself and always using the same kind of strategies great question so yeah that so that's a really great question which is okay well so self play might be good but then what if you suddenly play against someone really different um so what we're going to have to see in this case is whether or not over time you do get to something that's essentially like a Mini Max policy so if you get to the optimal policy you could hope that you really are at you know like Grandmaster or Beyond and and one of the exciting things here is that um they will get to that so as this ratches up and ratches up after lots and lots of trading and after using very very complicated complicated networks you can get to that level um does does that work well does that work for like games where uh moves are not like deterministic like uh I don't know like the gambling games like poker or something where there is some sort of probability yeah inter so there's also been a lot of work there's um there are really really good AI agents for poker now I think it was 2019 that um gome Brown a paper in science showing that you could beat um I don't know if you could beat but it was certainly sort of competitive with top humans I believe um so Thomas sandome and noome brown who did his PhD at CMU had have got an agent to do well at poker the algorithms are slightly different um but yes you can you this but it's a good question here we're assuming that it's also going to leverage the deterministic nature yeah all right okay so how does this work let's go through sort of um what it's doing because it relates to Upper confidence research but there are many changes um so there many improvements that were needed for it to get much better but it is going to be similar in the sense that it's going to try to compute um first we're going to start with it's going to simulate many many many games and it's going to iteratively try to learn better strategies one of the things that is going to be different compared to sort of naive upper confidence tree um is that we're going to actually maintain a neural network so let me just get back to there okay so what we're going to do in this case is we're going to have um a neural network that given estate can produce both an estimate of V of s and a policy distribution for that state over actions so we're going to maintain a single neural network this is what Alpha zero does maintains a single neural network that given an input state will output both an estimate of the value of that state and um a policy for that state add distribution over our actions okay um and we're going to talk about how we train that shortly but for now just to assume to start that we've already trained that or that we have access to that and we're going to use that now when we're going to do um a number we're going to play a number of games okay and um in particular let's first think about how we're going to compute the first move in a single game so we're going to do some self we're going to do selfplay in this case between two agents all use the same what we're going to do is we're going to do an upper confidence bound based thing okay what is these oper confidence bounds going to be based on um and then we're and then we're going to decide the max between them so this is going to look like UCT but slightly different so what U is going to be equal to in this case so U of I a so let's say this is node I it's going to be proportional to the following it's going to be proportional to PSA it is / 1 plus okay this is from our policy I'll write it as this so this is from our policy Network this means that our upper confidence bound is going to include in it a bias towards some actions versus others so our policy network is going to say if you give me a state I will give you a distribution over actions okay and that's this and that is going to help us with the fact that we have an enormous number of actions and so this is going to prioritize some actions that we think in general might be better for these types of States so this will be a deep neural like that neural network up there is going to be a huge crazy deep neural network um and it's going to try to leverage similar types of states to suggest which actions might be useful in this particular State the other thing you can see here is that this is going to Decay as we visit a state in action more um here in this case so I'll just be a little careful where this is all going to be operating I believe this part it's really I so this is I think at the node level I'll double check that but um the PSA has to be at the state level so remember you'll be in some State at this point and you could feed this into like a convolutional neural network it's an image of the board or you know some other deep neural network so that part has to generalize but I'm pretty sure I'll double check this at the the count here is actually specific to this particular node now why is this U interesting it's interesting both because it incorporate sort of a priority function over actions you might say some actions are better or worse and that's going to change which ones we expand the other is that we are decaying faster than normal upper competence bounds so recall like UCT U is proportional to 1 / square root so this is going to Decay a lot faster this means we're being a lot more aggressive in our upper confidence bound we're shrinking fast and so what that means is that we're going to do a lot less exploration of things that we think are not so good okay so that's one really important part of how we're going to pick what to expand the other part is this notion of Q so how are we defining Q for this node Q is going to be equal to 1 / n by a and again I'll double check this is no rather than States Su over S Prime B of S Prime okay what this means here is that this is going to be an empirical estimate of what the value is over the states that we've reached by following this particular action in this node okay and we're going to see where that comes from shortly okay it's going to be a little bit different than what we saw before but these are the two components that we're going to use to decide which action to expand so yeah by to the node here we're talking about the identity of the node in The Matrix or the state of the node and the state the state so you can think of sort of like what what a what a node here is this case is it is a particular board game configuration so it's like saying the white pieces are here and the black pieces are here so it's like you could think of as just like an image an image of the board okay um and the earlier work in fact was using convolutional neural networks to take in essentially images and features yeah is there a meaningful difference between nodes and States yes so that's a great question so in general there may be a difference between nodes and States because um well this is I I'm not a go expert so I don't know but in general for these type of algorithms you could reach the same state as at different parts of the tree and if if you can do that then you would have different bonuses there now I don't know enough about go to know whether that's always possible and it's certainly possible in some cases that you know it would be isomorphic that like the the nodes and States would be identical but in general these sorts of algorithms can work in cases where you consider Imaging for like Checkers or chess and stuff you could end up in the same board game State later on but it would be a different part of the the tree okay all right so this is just the start this is just starting at the roote trying to figure out which action we're going to take from the root and then what we do is we repeatedly expand so in this case we would follow the right hand side now what we would do with this case which is pretty interesting is so this would deterministically you know I put down say a piece on the board in this case I decided to put down this piece okay and then what I would do is I would flip over and pretend to be the opponent and it would do the same thing using its q and u okay now it's q and u are going to use the same neural network approximation so this is just selfplay but it's just useful to know in this case that they're going to be optimizing for the opposite you know one's trying to optimize that the black pieces are going to dominate the other one is going to try to optimize so the white pieces dominate so now we're going to have that the opponent selects the max q+ U so just useful to think of like you know you're sort of repeatedly flipping back and forth between these two but you're using exactly the same Neal Network parameters when you do that okay so this is going to continue going all the way down until we hit a leaf node so we just WR this is again we haven't even selected a single action to take all of this is going to help us finally take a real action in one game so right now we're just going to do a whole bunch of computation to figure out what that action is okay and just to note again here so we're assuming that we have access to this parameterized deep neural network and whenever we do this expansion we are using our P function because that's what was going into our upper confidence bound so our our U was a function of P okay so it's a function of these probabilities and so we could wait different actions more so we keep going all the way down until we hit a leaf node and at that point we plug in V of s so we hit a leaf node so if this is terminal we're going to do V of s we're going to use our Cal Network to plug in V of s so this is different than what we saw before because before we were thinking we'd actually get the rewards along our trajectory until we get to the final end or if we didn't have any rewards we just get whether we sort of thought we were in a winning or losing State at that point we not doing that anymore we're plugging in an estimate of the value of the the final State according to our value netw work and that means also that we can either go all the way out till we win or lose a game or we can terminate we can you know say after 700 steps plug in our V of s which would give us an estimate of How likely we were to win the game at that point so once you have that we're going to propagate all this stuff back up okay so if we're going to select that you know once we go all the way down and we get to some V this is going to go back up and remember what this is going to do is we're going to update our Q function so our Q was equal to 1 over n i a sum over all of our V of s Prim so we're going to update our value all the way back up so we used our P function when we're expanding out to figure out which uh actions to take as well as our upper confidence bound and then we use our V prediction to do the backups so the way that it would work is we go all the way out to to a leaf node and then we go all the way back up along the ancestors to the root node and then we do the whole thing again we do that many many many many times I'd have to remind myself I think it's like say for example it might be 160,000 times for example just to give you a sense of the scale so it could be something like 160,000 times and that means you're going to fill in parts of the tree and then after all of that we have to decide what actually to do okay so that's just to compute a tree to decide the current move okay so we do this many many many times okay so we do this many times and then at the end we are going to decide what to do with our root node by the following and this again is a little bit different than what we've seen before we are going to compute a policy for the root node by figuring out which actions did we mostly visit underneath it so we're going to look at NSA so sort of which actions how many times do we take each of the actions from the root node to one over a towel which I think this should be minus let me just double check yeah I guess it just depends how you set toel so to is just going to be a temperature parameter okay so if to for example was minus one in this case then it would be 1/ NSA You' be proportional to that or if n was one you would be sort of proportional you would take things according to that divided by the total so if n is one sorry if to is one to is equal to one then it would be NSA / n of s and as you increase or decrease this then you get things closer to taking a Max or just averaging so this allows you to sort of like have a stochastic policy at the root node instead of necessarily just taking the ARX so this is quite interesting so this is what they're going to end up doing after you do all of this you're going to actually take an action and then you're going to so that gives you um a policy and then you are going to sample from that policy to actually make a decision okay so this is how a game works you do an enormous amount of computation at at the end you get this policy according to the number of times you've taken each action from the root node and then you sample from that policy you reach a new state so like let's say you put down that board that um thing then the opponent does exactly the same thing and they put down something and you repeat this all the way out until the game ends now even if you're deep mind you care about computation and so in some cases they will truncate games um if they think there's definitely going to be one outcome or the other okay but in general you would just keep going this all out and Z here would be who won or lost the game yeah so you said they will truncate games but like uh how do do they actually set behind the computer and watch games being played or like no no it's absolutely all automated so this is going on you know C billions of of times and what they will do is if like um I think it's after 700 moves if they're not sure if they think either it's going to end in a draw or it's definitely going to be a lose and then they try to bound like um false positives and stuff like that but it was interesting to me that they included that just indicates that it probably save them a substantial amount of computation time but yeah know everything is totally automated okay so what they do is now at this point so this is like a single game as you could imagine this is an enormous amount of computation after a single game but a lot of this could be paralyzed we now going to train our neural networks so remember that we used a neural network to both give us an estimate of the probabilities like give us a policy for each state as well as a value and what we do is now we have so from that game that one game we have one observation so this is our Z no who won who won the game and we had from each step we had these policies that we computed and we're going to use those as targets to train a neural network so what we do is we go back and we say okay well in that time when you were in state s and you computed a policy and eventually you got a value of Z you either won or lost the game we are now going to train our crazy Big D deep neural network to predict for this state this is the policy and for this state that is the value and this is just a supervis learning problem and then they do the same thing for every state that was reached in that particular game all using the same final state which is either you won or lost okay and so this is just an enormous Network I can't remember I think it's maybe like let's say 40 layers and um they try and we'll see shortly like the influence of architecture too the architecture matters but um and so again just this neural network goes directly from states to both predict it's got two output heads both predict policies and values in it in their earlier work they had separateur neural networks but one for policy and one for values here they just combined it all right so so that that is sort of like how it works in a nutshell um in terms of what they're doing and then they do this for an absolutely enormous amount of time um the final thing I think was trained for 40 days over like many tpus Etc yeah does this mean like if you think about kind of like a loss function with respect to the value the policy is not actually a component of that loss function is that yeah it's a great point so what is a really good point so these are just two different heads um and you can think of it as what sort of assuming in this case is that the representation you're learning is going to be helpful for both but this value may or may not relate to this policy and this is just saying like we think that like the features we're going to learn about this like that's a way that we're encoding um the game States and also just to note here it's not just the current board that they're using the states they use tend to use history as well because again I'm not an expert in go but there are various rules in go which mean like I think you can't repeat a move and stuff so because of that they have to maintain a short um history of the previous game States so you can think of s really as being like multiple games game board states of the past and I think the intuition for this is that you're going to learn feature representations from that they're going to be helpful for predicting both of these now ultimately you would hope that this sort of there is some relationship between these two but they're not constraining it so just a recap what are the key features that they're using so so I guess also to specify in this case they're going to do this across many tpus over many many many days um and what they're doing when they do this is that they're constantly retraining um these neural networks and at the end of all of this when they act play kind of test games say against other human players or against other neur um other AI agents is they're going to still do the Monte Carlo research so going to take sort of their their final neural networks and then they're still going to do the um monticolo research method that we've just seen before they make decisions and so we we'll see in a second sort of whether that's important or not so in particular some of the important questions that they consider in this paper is you know what is the influence of architecture does it matter which architecture you use in these cases um what is the impact of using MCTS obviously they're still learning a policy and their learning a value function and my question is how much additional gain do you get even after 40 days of training this um on um by doing monteola research and how does it compare to human play or using human players so the first way that they did this is they instead of having this neural network that was predicting a policy and a value is they'd actually did supervised learning on human play and that gave you a way to prioritize actions so that's what they done when they did Alpha go to start and I think that's what they did also for when they won Le against leasy doll and then what they've been trying to do in this paper and um others is to kind of remove some of those assumptions to see if you could learn without even human knowledge now here I'll just specify that you know they they still know the game rules and then they have later paper where they want to not even need that but here are the algorithms so the first thing to note is that um hire is better this is talking about the performance of um the resulting approach under different architectures okay so what they do is they actually have the same training data that they use and they just use different architectures they use data in this case from um one of some of the runs of alpha Zer which is the algorithm we've been talking about so all of these have the same data and then they look at what the performance is if you train um the neural networks with that data so same data just differences architecture and there is a huge difference okay this is like from 3,000 to 4,500 so they're current on so this is a com a convolutional neural network which is separate meaning that you have a different policy network from um a value Network whereas this is a reset and they're using a dual representation so you can see that you get a significant benefit by leveraging representational strength across both of these targets and also that this is better than using convolutional neural networks so I mean I think this is a good reminder that like when we're doing reinforcement learning we're doing decision decision making we still want to build on all the amazing advances that are happening in deep learning in general and the the complexity of the neural networks that we use and the functions they can represent really matters okay so that's sort of the take home from this part this is a huge difference in performance um the second is the impact of Monte Carlo tree search so I think this is important to know this is if you use the raw Network so you take the network I think this is after those 40 days of these crazy numbers of tpus and you don't do Monte Carlo research on top in kind of your evaluation games and again this is much much much worse okay so this is alphao zero the algorithm we've been talking about alphao master was another one they developed shortly before this this is the one that beat leasy doll um alphago what they call fan is sort of the first big alphao paper um and these are some of the other approaches that happened before their methods and again you can see that even though they now have all this beautiful different architecture Etc um if you don't do monol research on top of that you miss a lot so it really is important to do this last mile of additional computation even after you have these really really good neural networks this kind of local computation matters um this gives you a sense of sort of the training times involved so this is the lisad do paper or ladol method um I don't think they published this before this is sort of one of their Master methods they had and this is showing for a particular size approach how long it took of training before you got something that exceeded all of those so it gets there but it also just highlights of the enormous amount of computation needed and um the importance of of uh the architecture so I know we're almost out of time but I just want to highlight two things so again in this case it didn't need any human data no supervised learning and they noted though that it was less good at predicting human play than some of the other prior methods so that again just highlights that these methods really are helping um agents to discover strategies that are not necessarily the ones that are used by humans they're just going different ways of solving these sort of incredibly complex optimization tasks and I think that's really interesting in terms of sort of the future of human AI collaboration we're almost out of time for today um I'll just highlight as well that these sorts of ideas of how to use RL to optimize computation and solve really really big search problems have also been used by Deep Mind to solve things like Alpha tensor and these other ways of sort of trying to start automatically searching for new algorithms um which I think is really exciting because you can think of sort of the space of algorithms or the space of sort of different search algorithms Etc and those are enormous and so you could think of using these types of strategies to prioritize which things are most effective all right so I'll leave this here because we're out of time you're welcome to look at this to think a little bit more about the aspects of UCT search and then on Wednesday we're going to think more about rewards in RL and what are the implications of which ones we're choosing I'll see you then