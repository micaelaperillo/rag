hi everybody welcome back we're going to start to talk about fast or data efficient reinforcement learning before we do that we're going to start with a refresher knowledge one of the things there's fairly good evidence about in terms of learning is that space repetition is helpful so I'll try to periodically bring up ideas that came up earlier in the quarter when we do the refresh your understandings for all right qu you turn to a neighbor and see if you got the same answer I [Music] or all right so we'll go ahead and get started for those of you that just came in feel free to vote um the first one is that important sampling does not leverage the mark off assumption do not need this so this is false um important sampling can be work with non-m markup systems which is one of the benefits of it it makes very little assumptions over the data distribution generating process um so it can be used in just similar to how we could use Monte Carlo methods to estimate the value of a policy through rollouts this also is very general um let's go through the next one as well so let not use the mark for this one um the first one is true so we can think of using the advantage function of one policy samples from the other um the second is that we can sort of importance weight between the two policies and get the samples from policy one so it's not really an exact bound but it turns out we can bound how off that is the reason it's not exact is because we're using samples of States from one policy whereas in reality the other the other policy might visit different types of States um and po uses these types of ideas um and the approximation error is bounded by the average over the states visited by one policy between the two policies so this is trying to say EXA sort of how bad is this approximation when we use just samples from one policy okay and so that was one of the really nice insights of that um prior work is that to show you actually could bound what is the error in the like the approximation error that we induce by pretending that we'd get to the same States under policy 2 compared to policy one awesome so last time we talked a bit about learning from prior data and in really the last few lectures we've been talking about how to learn from Human feedback or from PR demonstrations of people or sort of historical data we have and now we're going to switch and we're going to think more about well what if we can actually gather that data and of course that's where we started at the beginning we thought about if we are quite early on we thought about how to evaluate policies if we could gather data but we didn't think a lot about how that was gathered we talked about Epsilon greedy and we'll talk more about Epsilon greedy today but we didn't think um super strategically over the influence of the way we were gathering data and so for the next few lectures we're going to talk about that a lot and that's really a critical part particularly for online reinforcement learning it's like how do we actually gather the data we need in order to learn to make good decisions and can we do this are there better or worse ways to do this so one of the things I want to emphasize when we start thinking about this part part of the course is a lot of reinforcement learning particularly if you have simulated environments focuses on computational efficiency um so we think about things like any place where you have a simulator so if we want to do like Atari or if we want to do go in these sort of cases computational time is essentially the same as data because you can either be using that additional computational time to sample from your simulator or to actually spend more time Computing a function or policy and so to some extent um simulators sort of blend the difference between computational efficiency and data efficiency because it's all just computation like you have a you have a simulator and it can either give you data or you can use it to you know do bman backups or whatever else you want but you could just count how much total resources you're using essentially in terms of computation there are a lot of other domains where computation is really separate from samples like from actual data so this is data and these are a lot of the application areas that I tend to think about and a number of other people think about as well so if you think about something like um uh using mobile phones for health interventions or if you think about consumer marketing like which ad to show to people or you think about educational technology or you think think about climate oh do like s like environmental in all of these cases there's sort of a real world that's happening out there there's like real students or there's real patients or you know there's like where you're trying to decide um uh say policies to encourage like wildlife conservation or others and so you have a computers you can use to compute that policy and then you have real World data and the real world data I'll call samples or You Know sample efficiency here and you care often a lot about that real world data and how sort of squeezing the most you can out of it so in particular you might imagine that like if you have say data from um you know 500,000 patients or something like that that's quite a lot bless you but it's not nearly as large as what you would normally have in the case of say Atari where you can just run the simulator forever um or in the case of things like Alpha go where again you could just play um the board game go against each you know again simulated agents forever so a lot of the things we're going to be talking about in the next few lectures just going to assume that we care about this because we can't get infinite data so we have to be we're thinking about cases where like these are coming from patients or they're coming from students and so we want to be much more careful with the data we're Gathering and think about how we could s maximize the information we get out of those to try to make good decisions so when we start to do that there's a number of different things we want to consider in terms of um how good are the different algorithms we're going to consider so one thing might be that if it converges um at all and we've seen before that from the deadly Triad we're not always guaranteed to converge so we've seen that for some settings where where this is not guaranteed or it hasn't been proven yet so you're not even necessarily guaranteed to converge to anything it might chatter it might oscillate it might not go to anything stable second question you might ask is if you're going to be guaranteed to converge to the optimal policy and then a third thing that might be really important is how quickly so in this case it's going to be how much data and we're going to see different types of measures to evaluate different reinforcement learning algorithms so let me just give you sort of an illustration to of like why these things might look quite different so imagine that you have something like this where this is time and this is reward okay so you could have really different algorithms you could have algorithms that look like this that might be one algorithm or you could have an algorithm that looks like this like really smooth and you could have algorithms that in general maybe you know most of the time do great but periodically make terrible mistakes versus you could have another algorithm which never does awesome but is always pretty good and those are really different types of behavior so if you think about that in terms of say um an AI clinician you could have an AI clinician that on average um you know helps get like let's say 80% of your desired outcomes like it helps you manage your blood pressure with 80% accurate you know um Fidelity or it could be that for 80% of the population it helps them completely manage their blood pressure and for 20% of them it fails so those are really different types of um performance guarantees and we'll think about whether you know trading off between those and what sort of algorithms guarantee us to have different sorts of performance so we'll start to introduce sort of different types of settings and ways to evaluate the quality of algorithms and we're going to start with Bandits um and we've talked very briefly about Bandits um in the context of s chat gbt and preference learning we'll talk a lot more about them now and then we'll move back into the Markov decision process case a lot of the ideas from Bandits will turn out to exactly for quite quite easily translate over to the um the RL setting okay all right so let's dive in so what is a bandit so a bandit is like a really really simple RL problem um they've been studied since I think at least like around the 1920s there's a very long history um of research on multiarm Bandits um it's been used for all sorts of application areas so let's describe what it is so the idea in this case is that there's no States there's just a finite set of arms and arms are the same as what we've been calling actions before so as a concrete example you might think of there being like 20 different ads you could show customers and we're going to assume that there's a probability distribution over rewards for each arm so maybe you know like on average um this gives you you know 90% click-through rate for this particular ad and this other AD gives you 20% click-through rate but that's not known that's you know not observed and what will happen is each time step you get to select one the actions and then the environment will sample a reward from that stochastic variable so if the clickthrough rate is 90% for that particular arms most of the time you'll get a one and sometimes people won't click on it and the goal is to maximize your cumulative reward so overall time steps that you get the most amount of say clicks and this is a very simple setting but it's been used extensively in a lot of areas you could think about this for something like how could I if I was doing something like a clinical trial how might I randomize the next person over what treatment to get you know treatment or control for example um for ads for many many different types of application areas so I'm going to go through I'm going to have some running examples for this part of the course and we're going to have a sort of a silly one that's going to be illustrative so let's imagine that we're trying to treat patients with broken toes this has nothing to do with medical stuff so this is not medical advice um imagine you have three different options you could do surgery you could buddy tape the the broken toe to another toe or you could do nothing and your outcome measure is a binary variable about whether or not that toe is healed or not healed after 6 weeks so that's our setting we've got broken toes we want to figure out what's the best strategy for healing them and we're not going to do a clinical trial instead we're just going to say well sometimes people come in and they've broken toes and I'm going to try to figure out over time which thing is best okay all right so in this case we're going to model it as a multi-arm bandit with three arms the arms are the and we're we're going to model each arm as a newly variable with an unknown parameter Theta so let's just do a quick check your understanding about the framework of Bandits excuse me okay great I think most people are converging on this already Yes um pulling an arm or taking an action um is just the action we're actually doing the second one this is a better fit to the problem than an mdp because we're only going to make one decision per patient and we're also going to assume that whatever decision you know whether um uh whether Mofo's toe heals after we do this is independent of whether or not when Sophie shows up what we do so these are totally independent processes um you know the next person to show up so we don't have any sort of sequential dependence even though we're making a sequence of decisions it's like each time Point there's a new person we're just going to decide what to do for them um and yes this is right so if your Theta I is between zero and one meaning your outcomes are not deterministic sometimes you'll heal sometimes you won't okay so one thing that we could do to solve this would be to use yeah so there confirm there is no like time Point dependency of the probability distribution it has to be the same in every single rate question we're going to assume for now that everything's stationary meaning that that reward probability distribution is the same at every time step so there's lots of really interesting questions around non-stationarity our Labs don't work on that there's lots of other really interesting work on this like with time point you know detection and change points for now we're going to assume it's stationary and that would include the fact that we don't suddenly get a new distribution of people for whom different things work good question all right so one thing you could imagine doing is just to be greedy so what we're going to do in this case we're going to use Q today not to denote like a state action or discounted sum of future Awards or you can think of it like that except for there's no State there's a single state um and um it's only over actions and it's only the immediate reward so what Q here would denote is what is just the expected reward of R and we can just estimate that by counting we can just look up every other time you know we we did surgery what were the outcomes for that individual and we can average and what the greedy algorithm does is they just selects the action with the highest value and um takes that action observes the outcome and repeats so let's think about what happens when we do that so if you have this setting um imagine that this really is the true set of param so surgery in this case in our fake example is actually the most effective body taping is second and doing nothing is not very effective so imagine this so you start off and this is pretty common with a lot of Bandit algorithms if you have a small finite set of actions often you'll just start off and you'll sample everything once now when you start to get into like really large action spaces like all of the ads you could recommend to people will have to do something smarter but in this case you can just sample all the actions once and let's see what you would would observe so in this case imagine that you get the first observation here is zero for arm one it's one for arm two and zero for arm three so which arm this is not meant to be tricky which arm would you select next under the greedy algorithm which of them has the highest right exactly so you would just um there would be you deterministically the probability of taking A2 would be equal to one you just deterministically take whichever one looks best um so would that be good or bad and in particular would you ever select the optimal action no so you actually couldn't exact um so it will never find it because you have a really low estimate of the true value of these two your average for A2 can never drop down to zero because you've got at least one one and so even if you get zeros forever which you're unlikely to get to for A2 you're never going to sample A1 again so what we would say in this case is that this means that you will not converge the optimal action and this algorithm is is not very good and and we'll formalize what we mean find not very good in a second so this just is to illustrate why you should not just be greedy um that you can lock onto the suboptimal action Forever This highlights why you need to do some form of exploration um because you can in fact make you know an infinite number of bad decisions so how do we quantify what it means to make an infinite number of good or bad decisions um we're going to use the word regret uh and we mean regret in the case of sequential decision- making okay so the idea in this case is that we're going to think formally about what is the difference between the decisions that our algorithm makes and the optimal decisions um and then we're going to score the algorithm based on what the Gap is so in particular the optimal value just like what we've seen in the past is the maximum overall the Q value so which whichever arm has the best highest expected reward and the regret is the opportunity loss you could also think of this as the difference of the advantage the advantage of the optimal action compared to the action that's taken and so you're regret just like we often use it colloquially is the gap between what the agent could have achieved and what it actually got um we're going to focus here of looking at these in expectation of course due to stochasticity there could be times where the particular reward you get for a suboptimal action might be higher than the action the reward you'd get for the optimal action because of stochasticity but we're just going to focus here on expectations so we're always comparing the expected reward of the optimal arm to the expected reward of the suboptimal arm all right so that's regret so how do we compute it um we're going to think about sort of comparing this over all time steps and we're going to maximize cumulative reward which is equivalent to minimizing total regret because remember this is unknown but it's fixed so we really want to you know maximize our total reward and we can either think of that as your maximizing the queue you got overall time steps or we're minimizing the total regret and normally in Bandits we talk about minimizing total regret instead of maximizing total reward all right let's see how we can think about how big the regret will be so um let's let NTA be the number of times action a has been selected at time step T so that means that if your agent has made T decisions you count up and see how many times did I take action A1 how many times I action A2 how many times I take action A3 the gap for a particular arm is essentially you know its advantage of a star over a so it's just the difference between what is the expected reward the optimal action would have gotten minus the expected reward you get under this alternative action and we often call this the Gap I think the the literature developed somewhat independently and so I think that's why you know people don't commonly call it the advantage in the case of bits they typically call it the Gap and the Gap will turn out to be pretty important because as you might start to think about intuitively depending on the size of the Gap it's going to be easier or harder to learn which of two actions is better so if the gaps are really large between like you know action one and action two which means they have really different expected rewards you're going to need less samples to figure that out if the gaps are really really small generally need a lot more data okay so G is going to just be a function of the gaps and the counts so we can just think of the number of times that you took each action and the difference between the you know and the and this Gap the difference between the optimal action you should have taken and the reward you actually got and so the um our expected regret here is just going to be the sum of times you take each action times the Gap and so what that means intuitively is that we want to don't we do not want to take actions which have large gaps very much and it's more okay if we take more of actions that are close to the optimal action and a lot of algorithms um for a lot of algorithms what we try to do is we try to bound this quantity so we try to say in advance in general this is something that we can't know because this requires access to what is ever is the optimal action and its value and we don't know either of those things but what we can do is we can have algorithms where we can prove something about how the regret grows all right let's first just see sort of what I mean just to instantiate that okay so just so again we can't do this in the real world but we can do this um in in you know for a toy example um let's just think about what the regret would look like in this case so this would be a series if you were running your greedy algorithm so this is the actions this is like time this is like one two three four five so we first take all of our actions in each of those cases the true optimal action was A1 and our observe reward was on and our regret was as follows so a one really is the optimal action so we had zero regret there the second one our regret was this and for the third one I regret was this so this just shows you how what the size would be and so this this here is actually the Gap it's the gap between the optimal arm and and the arm that you're taking so it's just shows you sort of how regret can grow and as you might expect if you make bad decisions forever you're going to get linear regret so for example here in the greedy case if we now take A3 forever our regret is going to be the total number of time steps T * 85 because that's how much we're losing for every single decision and then we sum them all up okay all right so in general it can be linear in the number of decisions and so part of the the main thing we're going to be trying to do is ideally you would have constant regret or zero regret what I mean by constant regret would mean that um you make a finite number of bad decisions so if you can figure out what the optimal arm is and then take that forever then you'll have constant regret because it just is going to be say I make 10 decisions then I learn the optimal arm and then I make the optimal thing forever that's generally pretty hard to do um if you in the worst case you'll be linear you'll make a mistake on every single AR you know decision forever and typically what we're hoping to find is we're hoping to have sublinear regret so it still might grow with a number of time steps the number of decisions you're making but it's not going to be linear okay and we'll see a lot more about that okay all right so what we're going to think of next is we've seen these before the Epsilon greedy algorithms so let's think about what sort of regret Epsilon greedy will have we've seen that greedy can be linear now let's see if there's some better things we can do okay so in this case we're going to do just to refresh our memories um with probability 1us Epsilon we're going to select we're going to be greedy we're going to select whichever action is the argx and otherwise we're going to select a random action and that means that Epsilon amount of the time we're going to be making some suboptimal decision because unless all of your arms are optimal and your gaps are zero in which case it doesn't matter what arm you're picking um if you select things at random you're always going to be making some bad decision um at each time Point okay so what does this look like okay so what this would look like in this case is Imagine again we sample all three arms to start this is our Epsilon I'm just going to work out what it will look like so with this case we're going to 90% probability we're going to be greedy and in that case we will take action A1 and a two each with probability assume that you T split ties 45% probability and then with 10% probability we will take all of the other actions so we'll have like 3.3% A1 A2 and A3 so that's just to conc you know to be concrete about what that would look like in this case I'll skip through this um so the question here is what will this regret look like so now we want to try to compute this for Epsilon greedy to think about whether will have sublinear regret for Epsilon greedy okay all right so let's assume that we're in a setting where there always exists at least one action such that the Gap is non zero that means that not all arms are tied if all arms are tied again and doesn't really matter what you do because everything is the same and so it doesn't matter what action you take so this makes it a non-trivial decision-making problem so let's think about in terms of our thing whether or not Epsilon equals 0.1 can have linear regret and whether Epsilon equals z can have linear regret as in this is generally trying to think about are there settings of Epsilon for which you could get linear gr and maybe set of Epsilon where you couldn't I don't know if this is actually on the there are three and we three you don't know what I don't know if this one's actually on the post we all three I wonder if that was missed all right well if it's not on feel free just to I have it oh okay I can check and see I wonder if some things got missed in last one oh I think okay hold on I'll put I'll post those in but feel free just to think for a second and then I'll ask you to talk to a neighbor let me see I think something's got mangled which ones were mangled this one okay I think this should be the there now you can check I just updated Ed tell if that didn't work do that work looks like great I think most people agree on this but maybe let's just do one minute and check with the neighbor and just check you got the same thing e all right I'm going to interrupt you for a second um so I think one way that's useful to think about this is when we think about how many times we sample things all of the arms are going to have a lower bound on the number of times we sample them which is at least Epsilon divided by the number of actions time T where T is the total number of decisions we make and so I think that can be a helpful way to think about this that you see there's a t here times some constant because there's a big T here times a constant that means you're going to have at least linear regret so if Epsilon is greater than zero you will have linear regret and if Epsilon is equal to zero you're greedy and we just saw that that can have linear regret so in either of these two cases unfortunately both of these both are true anybody have any questions about that now it turns out there there are certainly better and worse ways of setting Epsilon but if you just set Epsilon in a static way um it can be pretty bad and as you might remember from a long time ago sometimes we talked about decaying Epsilon over time and so that can matter a lot too but static Epsilon is not great all right so let's look at what this can look like if you think about how regret is growing over time steps these are very common plots when you look at like Bandits or some some other approaches we'll see if we consider what total regret is you'd like regret to be zero um if you make it greedy it can be linear if you make it Epsilon greedy it's normally a little bit better but it's still linear um if you Decay it it can get a lot closer uh and it is going to be possible to be sublinear for good choices of algorithms one of the challenges for this is that it can turn out that there can be some pretty good choices of Epsilon but it often depends on problem dependent properties that we don't know in advance so we need to have an algorithm which before knowing how you know anything about the problem in terms of the gaps or anything like that can be guaranteed to have sublinear regret so first of all um let's think about sort of what type of regret bance we might get and is there reasons for hope so a problem independent bound talks about how does the regret grow as a function of t for any possible problem um that you might be given so what this might say is I might give you an algorithm which is guaranteed to be sublinear in Te no matter what Bandit problem you put me in so it's just an algorithm that will work well for any po potential domain you put in and it'll make a bound on its performance instance dependent or problem dependent bounds um bound things uh as a function of the Gap now one of the really elegant things of problem dependent bound is that it doesn't mean the algorithm has to know the gaps it just means that if it turns out the problem's easy like the there are really large gaps you will have a much better regret and so some of my and my lab's work and a number of other people too were often very interested in this and I think at a high level what this means is you have an algorithm that's adaptive to the problem so it means that your algorithm will be guaranteed to do really well on the problem if the problem is easier to learn in and if it's harder well then you can't do well anyway it it'll do as well as it can so we'll talk about bounds the the both types of these in general is the gap usually less we're considering only rewards we usually consider only rewards between Z one great question totally depends on the domain um uh so if you're looking like beri then it's naturally between zero and one um uh other domains might be very different you can always normalize it I think whether the domain has really big gaps really depends so if you think about something like C through rates for ads you know clickthrough rates are are really really hard to optimize it's often like you know 01 versus 011 like um you know nobody likes ads so in those cases the differences the gaps we're looking at could often be really tiny and so you'll generally need a lot of data and having smart data efficient algorithms will matter a lot um there might be other cases where there's really big gaps um if the problem has really big gaps it's really easy um and so tends to not matter too much what you do there because you can quickly estimate them great question okay all right so here's a Reason for Hope um so there's a nice lower Bound by lean Robbins I think this was around 1950s it's been a long time um which tries to think about what's the minimum regret you're going to get um as a function of the problem and so this means that any algorithm is going to suffer at least this much in terms of regret so it says um you're going to at least be log T like the number of time steps number of decisions you've made and for any arm for which it is suboptimal you're going to suffer this in terms of a kale uh difference between your distribution of um rewards you get on your arm on that arm versus the optimal arm um with the gap on the numerator but this should be promising because it's sublinear it's log it's not linear which means that the lower bound says um according to this it is not yet impossible to try to have sublinear regret okay and this would be considered an a problem dependent or instant dependent bound because this holds based on the unknown gaps okay so now we're going to see one of my favorite ideas in the course um which is optimism under uncertainty which uh gives us I think it's a lovely principle because it shows why it's provably optimal to be optimistic about things which is kind of beautiful um and it's going to be one of the first things we're going to see that's going to allow us to have sublinear regret okay so why is optimism good and what do we mean by optimism in this case okay what we mean is we're going to choose actions or arms typ of there um that might have a high value well what happens when we choose things that are good so one thing that can happen is we actually get high reward so that's good because that's our goal is we want to get high reward we want to maximize reward SL minimize cost what's the other thing that can happen if we pick something that might be good might have high reward lower reward lower reward exactly what okay so yeah those are the things you can e get higher reward you can get lower reward what happens if there's lower reward I mean of course we're that but like aside from that what happens do you think probably to our estimates like those Q estimates if we get low reward yeah improve prision exactly yeah exactly remind me your name yeah so what said is exactly right so basically either you get high reward or you learn something okay so the other alternative is you get low reward and you learn something and you're going to improve your estimates and from the point of view of a reinforcement learning algorithm or banded algorithm both of these are really valuable right because either you're actually achieving your goal or you are learning something so that in the future you won't make make bad decisions in the future okay and so that is why optimism is we're going to see provably optimal okay all right now of course that means that we have to have an algorithm leverages the information we get when we see low rewards okay so we're going to have to be formal about what it means to be might you know we're going to we're going to formalize this as quantifying our uncertainty so we're going to need to be precise over our confidence intervals or uncertainty bounds and then use that to make decisions okay so in particular what we're going to do is we are going to estimate an upper confidence Bound for each action value such that that confidence bound uh that upper confidence bounds holds with high probability so we're going to make sure we're going to be frequentist today we're not going to be basian and don't worry if you haven't done a lot on E on either of those but we're going to focus today on just high probability bounds so we're going to need a UT of a where that holds with high probability um and we're going to want this to be dependent on how many times we've SL to the arm there are lots of ways to quantify uncertainty we're going to focus today on a frequentist view and just thinking about counts and then the way we're going to behave the way that our agent is going to take actions is just going to pick whichever action has the highest upper confidence bound and there's a whole Suite of algorithms that are called UCB algorithms there are many algorithms that are variants of this notion there's also ones that are called optimism in the face of uncertainty ofu okay so it's a really simple idea and now the question is going to be how well does this perform and how do we quantify the uncertainty so let's go through hopings inequality we're going to use it in homework three but I'm curious who has seen it in previous classes okay maybe a couple people but most people I wouldn't expect you us to do okay so hting inequality is a really useful inequality um the idea of it is we're just going to think about how different can our observed average from the true true mean so let's say we have n samples that are somewhere between Z and one and this is our true expectation this is their true mean which we don't know what it is and this is our sample mean just over the end samples what hopings inequality says is that the difference between your empirical estimate and the true estimate um if it if they're off by U then the probability of that happening is going down exponentially which essentially means that um uh as you have more data the chance that your empirical estimate is really different than your true mean is going down exponentially fast like you can't have your empirical average be you know 30 and the real thing is 2,000 if you have a lot of data you're going to converge at you know on the true mean which is of course what you would hope but um that this says a formal thing about what the rate is so let's just look for second and think a little bit about what this can imply okay so let's look at this part let's say I'm going to do it for the absolute value probability of e of x minus xn so this is again just our EMP our empirical mean the probability this is greater than me so that this gap between our empirical average and the true one is good and so why just to back up why are we doing all this we're going to want to figure out a way to get an upper bound on what the real mean is of this and so what this equation is going to allow us to do is to try to figure out how big do we need to set u to be in order for us to get an upper bound on what the true expected reward might be for a particular arm okay all right so let's see how we can do that all right so we're going to say this is less than I've gotten an absolute value here so we're going to use this version and we're going to set this to Delta so this is going to be the confidence with which we want this um this Pro uh confidence interval to hold so this is going to be want the CI to hold with this probability with 1us Delta probility so we're going to try to construct an upper competence bound that holds with least probability 1us Delta okay so let's just do this now we're going to focus on this hand this side so we're just going to do some algebra equal to Delta over 2 that means u^2 is equal to 1 n log 2 Delta which means you is equal to square root okay so this gives us a range and it says if we want the probability that our empirical estimate differs from the true mean by no more than U um then it is sufficient to set u equal to this okay so that means that we can say that X nus mu is less than or equal to expected value of x Which is less than or equal to xn plus u with probability greater than equal to 1 Delta so that just created our upper confidence bound so they said with high probability I can take my empirical estimate I add in my mu my mu here note just depends on the number of samples that I have and that gives me my upper upper confidence bound and so we can use this we can use this given our data it just requires us to count how many times we've sampled things compute the average and then add on this additional bonus we often call these like bonus terms in these cases so this is going to create the ucb1 algorithm which is at every time step we're just going to compute this is again remember the qad is this the empirical average and then we add on this bonus term okay and this is again just the number of samples of a after T time STS and for those of you familiar with things like Union bounds and stuff we'll come to that shortly so this is we haven't really fully made sure that all these competence intervals are going to hold overall time steps um so we'll be a little bit more careful about what Delta needs to be soon yeah it's called ucb1 like why is it one so there's a lot of different variants um of the UCB algorithm I think this is one of the first ones it was I think our UE 2002 I think it's the one they named first in their paper but this notion of kind of optimism under uncertainty is certainly around before the 2000s um but I think this is the paper where they first did some of these nice proofs okay all right okay so let's think about what that how different that algorithm would look like in our types of settings okay so we're going to use optimism under uncertainty and what we're going to do in this case is we're first going to sample each arm once so same as before and this is what we're going to get and now what we do is we're going to compute those upper competence bounds okay so what we want to do is compute this upper competence bounds for each of the arms so UCB of A1 2 A3 okay and so this would be 1 + < TK of 2 log or Delta over 1 same for this one and then 0 + < TK 2 log l 1 / Delta okay so in this case you would pick A1 or A2 with equal probability because the upper confidence bound is identical okay so we select the argmax let's say that we pick okay and now we're going to again compute the upper confidence bound so in this case what would happen is you would still have you would have the following you would have UCB of A1 is equal to 1 + < TK 2 log 1 / Delta 2 UCB A2 is = to 1 + < TK 2 log 1 / Delta over 1 and UCB A3 is equal to 0+ < TK 2 log 1/ Delta 1 so what you can see here um is that we've now reduced our upper competence bound because we've learned something and this case we happen to have atin gotten high reward but either way we learned something we could shrink our competence intervals because we have additional accounts just make sure understanding correctly the Delta is something that we would select to kind of figure out or to choose our confidence B yeah great question so yes we haven't talked a lot about how we set Delta there're going to be a couple criteria for it in general um we're going to need all of these confidence bounds to hold for all time steps for all arms so we're going to need to do some Union bounding to make sure all of them simultaneously hold um because we want to have it with high probability that all of these things are valid at the same time um we also in the simple setting we know how many total decisions we're making and so we need to use that information as well and then you can um use those two things toe to bound the regret as we'll see so you can see this is why it's a bit different than greedy because we are still using our empirical averages but then these confidence intervals are going to change so that over time these will sort of alternate often depending on which um uh rewards you're getting and you may periodically take A3 because with that little data there is some probability that A3 is just as good as A1 and A2 particularly after you get additional data so sort of alternate between the arms based on these upper confidence bounds [Applause] okay let's go ahead I'll skip through those here let's go to here okay so this is as were just asking it's a little bit subtle um if you have a fixed number of time steps like you know that total you're going to make like Big T decisions you can set T to be roughly you probably want this by a um this is because you can use a union bound so why are we doing this we want these upper confidence bounds to be valid and we need them to be valid at every single time step because we are using them to make decisions uh so this is also related to false Discovery and other things like that if you've heard about them in machine learning so what we're going to use here is we're going to think about all of these as being events that these confidence bounds are hold and what we mean by that is that they really do contain the true um the True Value the true unknown value with high probability so what we're going to say is the probability that all of these events hold which means that all of our confidence intervals are valid for all of the arms for all of the time steps we're just going to use a union bound which says we're just going to sum over the probability of each of them over all of those events so that would be roughly the number of arms times Big T and so that's why you can then just divide your confidence interval your Delta sorry you can just divide your Delta into Delta divide T * the size of your a and that generally is sufficient and just to think about what that will do in terms of your bounds so remember we had a log 1/ Delta term so that means you would get something like this log ta a / Delta so generally the union bounding sort of blows up your log term um there's various approaches including law of iterated logarithms and others to try to get this term to be smaller so you can do tighter things than this okay so let's think about I I promised you that we're going to be able to use this type of idea to get sublinear regret so let's um go through a proof sketch to think about how this actually enables us to have to get much better performance than what we've seen before all right so what this statement says and I'll just put a pointer in so it's in the um it's in the references for under the website but there's a great book on B I think it's just called banded algorithms by Tor ladimore and Chaba sasari which I think maybe came out in 2019 or or 2000 I'm trying to remember um but they have a great book so it came out of a series of blog posts they were doing on multiarm Bandits and then they turned it into a book um and so there's a really nice one and if you go there it's I think approximately chapter 7 they're going to do a much more rigorous version of this proof compared to what I'm doing today um what I'm going to try to do today is just to give you a flavor of um types of bounds that you might want to uh prove in these sorts of cases and how we end up making getting a sublinear regret so what this result says is the following if you think back what we said before is we could bound um the expected regret by how many times we make we choose an arm and how much gap or loss we have whenever we choose it and so one thing that we could do is then try to just think about well we you know we don't know what the gaps are but the gaps we can just write down as the difference between the the expected reward of that arm versus the true reward of that arm that's not something we can influence the thing that we can influence is how many times we're selecting bad arms so what this says is that if an arm is suboptimal the number of times that we pull it number of times we take that action in Upper confidence bounds scales as a constant C Prime not going to tell you what that is um often in the algorithms they don't tell you what that is either I mean it'll be somewhere in the print um the point is that constant can't depend on parts of the the domain so it can't depend on the number of arms or the gaps or things like that it could be like 37 for example so a constant time log of 1/ Delta Delta 2 + < ^ 2 3 + okay so why is this interesting before we get into how do we prove this this is interesting um because it says if the Gap is large we're going to take it many less times so if the Gap is really small then it means that we're going to um we might sample that action a lot more and if the Gap is large we're going to take it less and then we can combine that with this equation and what happens in that case is so just I'll go through that part before we actually s think about so what we're going to focus on doing a proof sketch of for today is to focus on this part but let's just think if we could prove that why that would show the second well what we would get in this case is we would say we get this term plugged into here and the main thing that would happen there is this would become Delta because we multiplied it by a Delta on top and then here if you assume that everything is bounded between 0 and one then the Deltas are at most one too so you can get this is just the number of actions * 1 plus s three this term so this just shows how your you know what your total regret would be in this case your total expected regret as I said there's quite a bit more subtleties to the formal proof but this just gives sort of a rough idea so we have any questions of that before we dig into how we show the first part which is the total number of times we're going to take arms we're going to pull a particular arm scales with one over the size of the Gap squared all right let's go through it so this is going to heavily rely on um the hting inequality and upper confidence bounds so remember um what we saw before is let's imagine that we've got this so we're going to say this was our our upper confidence bound so we have this upper confidence bound and again I'm going to be loose with the Deltas okay you'd have to be a little bit more formal of it in general but let's look at this so this is going to be the True Value and this is our empirical estimate okay so what hting inequality had told us is to say the difference between the true expected value for an arm and your empirical average is greater than this quantity upper confidence bound with probability no more than one uh Delta over T okay so now let's think about the following let's think about the times we pull a which is not equal to a star and Delta a is not equal to zero these are kind of the only things we care about in terms of regret if we're pulling a star we have zero regret if we are pulling an arm that that is has Delta a equals z that also means that it has zero regret because it means it's tied with an optimal arm so the only things that we care about bounding here is to think about for that NT of a how many times are we pulling arms that are not optimal okay all right so what we're going to do is kind of observed a couple so if the confidence interval holds so we can think this if this holds then we have the following we have the Q a minus C log 1 / Delta / NT so here I'll say if one holds okay which is less than or equal to QT hat of a Which is less than or equal to Q of a plus < TK C log 1 / Delta / n T of a this just says if your confidence intervals holds what it means for it to hold is that that confidence interval is wide enough that it contains your true value and the upper confidence part is higher than that and the lower confidence bound is lower than your true value so this is just holds if our confidence intervals hold okay now if we pull a instead of a star so under UCB algorithm we have the following we know that the upper confidence bound of a was greater than the upper because that's why we picked its alternative action so in this case if we pull this arm a that means that its upper competence bound was different than the op competence bound of the optimal action and it was more preferred so that's the only time we ever take the wrong action is if its upper confidence bound is higher than the other actions so let's write down what that means in terms of its upper bounds so the definition of upper bounds here is that QT of a plus s < TK C log 1 / Delta / NT of a is greater than QT of a star plus C log 1 / Delta by n t of a star because that's just the definition of our two upper confidence bound so it says okay I'm only going to take this other non-optimal action because its upper confidence bound was actually higher than the upper confidence bound with the optimal action okay and then we notice so let's just label them so we're going to call this two call this three so now we're going to subtitute in from two okay all right so we know that this is greater than QT of a star from equation two because we know that the upper confidence bound on the optimal action also holds so its upper confidence bound has to be higher than its true value okay all right so now what do I have I have that q and let me write one more thing here so similarly just check that I get that right right one two three good I just want to make sure I got that one right yes okay so that means that QT oh hold on all right so this is going to mean that Q of a plus I'm confusing myself but we'll figure it out in a second of n n t a * 2 is greater than Q of oops I should have WR this here okay so let me just make sure I did that correctly because I want that to end up going in this case let just make sure that I did that in the right way feel like I'm off by a constant all right I'll double check the constants afterwards I'll just write a note um so I'll check the constants okay but the the main formula is going to be fine even if you drop the two here that would um here so what we're going to have in this case something's bothering I'll see if I can figure out in a second um so what we want to argue in this case is that the Q of a that we have plus two of the competence intervals is going to be greater than Q of a star and I'm confusing myself slightly now and I'll check into it later but what this would mean in this case is let's assume this holds for a sec I'll make sure I get the explanation for next week or I'll just put on Ed um what we'd have in this case is we're going to have that 2 < TK C log 1 Delta over NT of a is greater than Q of a star minus Q of a which is equal to Delta a let's go to the next slide okay so if we have this in this case what we can then argue is that in this situation what we have here is that we can rearrange this to the other side so let me just do the algebra for that part so what we're going to have is we're going to say that 4 * C log 1 / Delta / NT of a is greater than equal to Delta a 2 which means that if we rearrange this here we have NT of a is less than or equal to 4 C log 1 / Delta squ okay and that looks really like this so what is this saying intuitively intuitively this is saying if your confidence bounds hold and you use them to make decisions then if those confidence bounds are holding then the only time that you make a decision that is wrong is where these confidence bounds is large enough that it overwhelms the gap and the number of times that that can occur is finite because the Gap is non zero and since we know from hopings inequality that the size of the confidence intervals are going down over time eventually they will get smaller than the Gap so you're going to sort of take these suboptimal actions less and less often according to sort of how quickly your competence intervals are Contracting relative to the Gap in these cases anybody have any questions about that okay all right so what this means is then when we look at this we end up getting that it achieves logarithmic ASM totic regret as a function of log T so because we had the log T here inside of the number of times we're taking these sub optimal actions and what you can see in these cases is that over time so this is a a previous um result where we look at sort of the amount of data that we have and sort of what is the best performance that we have over time if you tune Epsilon grey it can definitely get better but also UCB ones definitely have this nice logarithmic shape if you have the right um you know if you set the constants correctly now empirically often it will end up being that the constants matter a lot and so if you set the constants wrong or if you set the constants off into the theoretically prescribed value it'll often explore for a long time so you can often be more aggressive than that in terms of the resulting bounce so an alternative we could have done to UCB is to always select the arm with the highest lower bound this can yield um linear regret so I think that's um a useful thing to think about um this is optional but you couldn't do the check your understanding to think about why can this lead l lead to um linear negative regret it's helpful to think about the upper confidence bound case and why that one works and why this wouldn't for so in particular I guess imagine this was on an exam what I would be looking for in this case is for you to construct a multi-arm bandit case for which selecting um based on this criteria would give you linear regret so if you think back to the example I showed you for greedy where we considered a particular sequence of armps such that you would never recover and you'd get linear regret in that case think about this sort of setting to where based on some confidence intervals if you select whichever one looks like it's better in terms of its lower bound that you would never recover and select the optimal action so I had a question about um the slides before where we were kind of assuming that the that condition was met um then I'm assuming like the other parts came from where the condition isn't met that's right yeah so in those cases if you set the Delta correctly you can say um uh so with high probability you're going to want this to hold for all time steps and then there's going to be the small amount of probability that it doesn't hold and then you can argue in that case that um the the regret is going to be bounded from those time points so you split the it's a good question you split sort of the expectation into the high probability event and the low probability event so why don't we why don't you talk to neighbor and see if you got the same thing at least one person already has the right answer oh good by but okay I'm going to interrupt you for a sec for interrupt you now where would I have to put the mean and the upper Bound for A2 so that beam pessimistic fails so according to the algorithm here if we select the arm with the highest lower bound we would select A1 because A2 has a lower lower bound but where would I have to put its upper Bound in its mean for it truly to be for us to have linear regret so here I put reward on the Y AIS at least one person said the right thing in in there so I know one of you guys know this yeah should be really high yeah that's right so for example could have this okay so it's you could be really uncertain about it its lower bound is lower once you pick a one the the lower bound here e an expectation is only going to get closer like the lower bound this is these are valid confidence intervals this lower bound really is smaller than the mean of A1 which means on average when weever we sample A1 this is really just going to shrink okay which means we'll never pull A2 A2 is upper confidence bound is higher than A1 so under UCB we would learn this but if you're pessimistic in some if you think about for um upper confidence bounds if you're optimistic either you're correct or you learn something the problem with being pessimistic is that you may not learn anything because you're not updating your other other bounds okay I realized why I was being confused so let me go back and just correct that here um okay so how did I get these so let me just clarify it was this step I was confusing myself okay so we have this this particular equation right that the empirical average plus its upper confidence bound was bigger than the optimal arms empirical average plus its uper bound what I did from equation two is I reminded is that remind ourselves that the empirical average is always less than equal to the True Value Plus the upper confidence bound so we substitute that in for QT to get the QA plus 2 * the bound okay so that's why this this works out so you just substitute this with this upper bound into here so then it gets another it gets Q of a plus this upper bound plus this upper bound which means this bound becomes two so that's where that came from okay so this is the first algorithm we've seen which has provably sublinear regret which is really nice it's also really easy to implement certainly when you have counts but all of this stuff can be extended to much more complicated settings um and so there's a lot of work of thinking about for function approximation and RL and we'll see all of those of um ways to sort of think about formalizing this optimism under uncertainty principle in order to make decisions when we don't know um uh what the outcomes will be in order to reduce our regret over time so what we're going to see next time is we're going to see more fast learning but we're also going to think about it from a very different perspective called um a like basian Bandits where we think of it not being this sort of just fixed upper and lower rectangular confidence intervals but we think of having a prior over what the distribution is going to be of the rewards for each arm and then in that case we can also introduce algorithms of that end up being somewhat similar to optimism in certain ways um as ways to use those prior informations to figure out how to quickly gather data um and start to make good decisions so we'll see that next week thanks