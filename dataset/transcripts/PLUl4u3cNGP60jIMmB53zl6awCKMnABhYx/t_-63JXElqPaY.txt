the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencare at ocw.mit.edu okay so let's uh let's start on the material where we're following the V model and you know the empty boxes are getting fewer and fewer every week here so we're on the right side uh moving up to um toward life cycle management and operations and today's topic is uh VNV verification validation and I also want to discuss the f frr as one of the key milestones and that's the flight Readiness review and if you're not building something that flies this is sort of your launch to market right launch to market review you are you ready to launch your pro project or product to market the outline is we have quite a few things to cover so first of all I want to drill into verification and validation what's the difference what is their role what's their position in the life cycle then we'll spend quite a bit of time on the issue of testing what kind of testing is done why is it done we'll talk about aircraft testing flight testing we'll talk about spacecraft testing but also some caveats test is not always you know the it's not free of of challenges and difficulties then uh I want to talk about technical risk management which is often covered in classes on project management but I think it's essential here as well as a system engineer to have a good grasp on technical risk management so I'll cover the risk Matrix the Iron Triangle uh cost schedule scope and risk and then I added a small section on system safety which is a very important important topic as well and we'll finish up with discussing the F frr the flight Readiness review so the readings related to this lecture are sections 53 and 54 in the handbook the system engineering handbook and then there's a couple of appendices there appendix e and I that are very very helpful and at least one of these I'll mention in the lecture plus uh one of the papers and this is a paper about a decade old and I know a lot of work has been done on this since then but this is a Prof a lecture or a paper by Professor leveson Nancy Levon who's a colleague of mine here at MIT who's really an expert in system safety so the system safety model that uh she's developed um is is the subject of that paper Okay so let's talk about V&V verification and validation and how they fit together you've seen this diagram before but I just want to talk through it in in um in some detail again so the idea is you start your your project your undertaking in the upper left you do your stakeholder analysis who are the stakeholders the customers beneficiaries The Regulators the suppliers you know the uh the partners on the project so you really have to do a good job doing your stakeholder analysis then in order to meet Set uh write your requirements and I I do have to say I've been very pleased especially with your assignment 82 you know you really dug into those 40 seven requirements for can set you grouped them you scrubbed them you uh you did a great job and the idea is that for each of these requirements you also have Target values there are certain thresholds or Target values that have to be achieved and then you actually do the development you do the conceptual design the detailed design and that's written here as functional deployment in other words these especially for the functional and the performance requirements how will you actually Implement those embody those in technology Hardware software and so forth so that's your intended function your concept and then your implemented design solution now the question is now the question is do you actually satisfy a the requirements and do you satisfy your stakeholders and it is possible that you satisfy the requirements but not the stakeholders so the way to think about this is we're going to close the loop in fact we're going to close two Loops an inner loop and I put testing here as testing is really one of the ways to verify whether you meet requirements that there's other ways too but testing is often the most important and so we close this inner loop so what we ask is we we test our implemented solution our implemented design and ask the question did did we deliver do we actually satisfy the requirements as they were written do we satisfy the requirements as they they were written and are these attainable were these requirements attainable so this Loop here this inter Loop is the verification Loop you verify whether your design as implemented satisfies the requirements as written that's what verification is and then there's an outer loop where you take your implemented design solution and you essentially bring it to the bring it all the way back to the stakeholders and usually that also means your your deploying it in a realistic environment like in the environment that the stakeholders will actually use the system not in a pristine lab environment and you have the stakeholders uh try out your system and see whether they're satisfied whether this meets their original intent you remember the conops concept of operations can you actually do the conops the way you had envisioned it in a realistic environment and that's what we call validation right and that's the outer loop you see the difference so a lot of people who don't know system engineering who've never been exposed to this when they hear verification and validation they think it's basically the two different words for the same thing it is it is different it's not the same thing so and then if you successfully verify and validate you end the SE process and you deliver which is good so this is something I pulled out from the handbook which which is the differences between verification and validation and I'm just summarizing this here so one way to ask is was the end product realized right meaning like did you do the right thing or did you implement it correctly so verification is often done during development so you verify components subsystems you check if the requirements are met typically verification is done in a laboratory environment or on a test stand you know or some some um environment that allows you to very carefully control the the conditions the test conditions and verification tends to be component at subsystem Centric okay and then validation is the question was the right end product realized did you actually build the right thing did you deploy the right solution this is often done so validation focuses more on during or after system integration it's typically done in a real or simulated Mission environment you check if your stakeholder intent is met and it's often done using the full up system it's it's difficult to do validation with on a subsystem basis alone typically validation implies you got to use the whole system to do it uh or you basically use dummy subsystems you basically replace the actual subsystems you're going to have with something something temporary so that you can go back to the stakeholder and give them as close to the real experience as they'll have with the actual system okay so that's that's essentially the distinction here so uh I want to do a quick concept question on this to see whether this uh this point this distinction came across so here's a link se9 VV these are all caps and what I'm listing here is different test activities or different type of activities and I'd like you to check the box here whether you think this is verification whether this is validation or you're not sure all right uh test testing and handling of a new car in snow conditions in Alaska 90% of you said um this is uh validation and I would agree with this so um you know many car companies I think all car companies once the vehicle has been uh finished essentially the design uh it doesn't go to market right away there's a very extensive you know usually it's at least 6 months of field testing of a new vehicle and you know you go to the desert you go you know where it's very Sandy and hot you know test your air conditioning systems right at the limit and then you go to the really cold climates either you know in Europe they go up to Sweden and Norway uh and here we tend to go up to Michigan Minnesota and so the idea is that you really utilize the vehicle in a kind of extreme environment but that's realistic of of actual operation so I agree with this frontal uh crash test in the lab uh most of you said it's verification and I would agree with this so those very standardized crash tests that you've you've seen in some of the commercials right the the vehicle is prepared instrumented you have the crash test dummies and then you have uh typically there's at least three different kinds of crash tests there's frontal there's side impact right like the the te kind of Crash and then there's also rollover tests that are more more and more standardized and the test conditions in these are they're um they're highly stylized tests they're very prescribed exactly the speeds the angles everything is prescribed and you know real accidents uh the V variability of conditions is much much bigger than in these tests so I agree because the test conditions are so tightly uh defined and constrained this is verification not validation um testing of a new toy in a kindergarten okay so here you're you know you're and and this is you know the the toy companies and the they they essentially uh you know they they before again they make a big million or billion dollar decision to you know Mass manufacture toys they will actually have kids play with them in sort of realistic environments and so uh I agree with that that this is validation a vehicle emission testing obviously this was the big Crux with the Volkswagen Scandal that we had um so basically what this this this cheating that happened is essentially software that um was embedded in the vehicle such that when the vehicle experienced exactly the test conditions of these drive cycles that are very well known very well defined the vehicle would internally switch or reconfigure to a verification mode right and really emphasize low emissions uh at the expense of fuel economy right and as soon as the vehicle would detect that it's in a more General driving conditions it would essentially switch that mode off right so verification again is uh on a Dynamo in a lab um satellite vibration testing on a shake table uh we'll talk about this this is often we refer to this as shaken bake right in the in the spacecraft business the Spectra the load Spectra that are put into these shaking tables are again very stylized and different for each launch vehicle so here here we can we can debate a little bit whether you know is it closer to validation because the the actual test conditions are are so much adapted to each each launch vehicle but I agree this is primarily a verification activity and um then the field testing of the Google Glasses so you basically produce an initial batch of your product and then you give it to like lead users you have them try it and give you feedback this is um this is much closer to validation so I I I think by and large uh your answers here are very good and the real distinguishing factor is you know whether this activity happens in a lab in a very controlled environment under stylized conditions or whether you're actually going out in the field in a realistic Mission environment with real users or real potential users you know that are not especially knowledgeable are not especially trained about the system so very good job I think I think U most of you really understand that distinction okay so um let me talk briefly about the uh yes please Veronica are there any got it today okay um are there any tests that really bridge the gap that really could be seen as both so I'm thinking about products in particular that are are to be used in kind of a Lab setting where you have a very specific kind of user where meeting the requirements is is kind of more about how how the tool is employed and and I I see the user in that sense is kind of part of the system so I'm wondering if in a more clinical sense there's there's an action that is both validation and verification yeah and so you said clinical so I I I I think there are situations where the distinction is not as sharp not as clear and you said clinical so I think in hospital you know for medical equipment like if you think about surgical equipment and things like this where it's it's very hard to really it's very hard to do verification in a in a kind of stylized way the only way to really check it is to have the equipment embedded and used in a in a pilot study right for example in a hospital and and in that case because the human is so involved and it's not a general consumer product but it's really a tool for for Specialists the only way to really get uh check your requirements is to actually embedded in a in a realistic environment to begin with so whenever it's it's very difficult to design very specific isolated tests where you can check for each of these requirements one by one um you almost have to move straight into validation and I think in in in medical equipment that's often the case yeah go ahead uh would you say that for spacecraft really true validation isn't possible like you have to recreate the conditions in some way in some kind of a laboratory environment um I think it's depends on the novelty of the spacecraft if you're launching you know something like a standard communication satellite where you've launched dozens before and you know you know the actual pitfalls and the operating conditions you've experienced failure modes in the past and eradicated most of them um I do think that um that you can do a lot in verification but then I'll show you one example of a spacecraft we've actually talked about before where there's going to be a lot of residual risk and the first time it's deployed people are going to sweat because you know there's still a lot of unknowns to be resolved okay so uh let's look at the product verification process in particular this is from the um so this is the product verification process for the from the NASA system engineering handbook so what are the inputs the end product to be verified so you have to have the artifact fact that you're going to verify the specified requirements Baseline you need this as a reference what are you going to check against the product verification plan which is essentially you know your test plan what test cases are you going to run how long are you going to run them how many repetitions will you do and then product verification enabling products which would be test software test equipment things that are not part of the product itself but are enabling of the verification process you then essentially go through this process and what are the outputs the verified end product the uh product verification results so these would be you know test protocols things like this the product verification report and then product verification any other work Products that come out of it so you could have for example discrepancy reports you failed some tests well that would be an important output and then the question is is this significant enough that you have to go redesign or retest or is it a minor issue that you can wave essentially to move on to the next stage um so let me let me just give you a quick example here from my own experience about this verified end product one of the things on the Swiss F-18 program that we did is not just by airplanes and Equipment but also models of the airplane itself in particular uh finite element models very detailed finite element models of the structure and um these models were very expensive expensive like some of these models were millions of dollars and so I got a phone call I was a liaison engineer at the time in St Louis I got a phone call from Switzerland saying this is crazy you know how how can we be charged millions of dollars for this particular set of models and I said yeah that seems pretty expensive so I'm going to go negotiate this and so I started negotiating and I guess either I'm a bad negotiator or it was really uh really clear why these were so expensive the reason these models were so expensive because they were on the right side not on on the left side so every one of these models that we were purchasing had been verified using um actual physical tests so every location was guaranteed under the load conditions to produce a stress and strain prediction at that location that was guaranteed to be correct within plus or minus 5% so the model had been very carefully calibrated and tuned against physical reality as opposed to a finite element model that's just um you know anybody can make a model and put some load cases and boundary conditions on and you don't know you know how closely does this Mir so there's a huge difference in value between a product a model that has gone through verification where at the end of it there's actually a report there's a protocol there's data that says all these features all these requirements that you had against it have actually been checked this is a certified product and that's the main reason for uh for the price because the actual process of verification is very very resource intensive so even though when you look at it physically you might say I can't tell the difference between prever verification and post verification CU physically it's the same but in actuality there's a huge difference because once it's been once it's been verified and certified uh against a set of requirements it's a much more valuable asset does that make sense so keep that in mind uh when you when you think about these products on the right side now what are the types of verification so test we'll talk about so you're physically testing but there's other ways to do it through analysis through demonstration and through inspection so analysis essentially means you're um you're doing a cal calculation with a mathematical calculation or simulation that satisfies you that this requirement is met and you're doing this with the input parameters into the simulation are as accurate as possible based on the physical reality of the system you have but for whatever reason either because you don't have the funds for it or you can't simulate the operating conditions well enough you have to do it through analysis um demonstration essentially means you're you're operating the system you're demonstrating the functions that you need but you don't necessarily uh have a lot of instrumentation on the system and you don't certainly you don't do destructive testing in other words a demonstration simply means you're operating the system as intended and demonstrating physically that it that it performs its purpose inspection essentially means you are physically inspecting the artifact uh either visually inspecting it there's also a lot of techniques called ndi non-destructive inspection you know through with with x-rays or Eddie current sensors you know you're checking for uh the lack of um manufacturing flaws or parities whatever it is but inspection essentially is U you're not physically operating the system but you're inspecting the artifact to make sure that it satisfies a certain set of requirements and then testing typically means that you're you're putting a stimulus into the system you're operating the system under some test conditions you're recording data which you then analyze in terms of comparing that to your prediction or expected behaviors so these are analysis demonstration inspection and test they're all different ways of verification yeah uh how do I know when I'm supposed to use more than one type at the same time I mean in and or or yeah that's that's a good point there's no real general rule of this but in general I would say the more crucial the more critical a particular requirement is to the operation of the system uh the more intense the verification will be whether that's just using one of these types you know you just run more tests or more different tests or you're doing a combination of you know inspection and testing uh there's no there's no General uh rule in terms of you know two out of three or two out of four but um the purpose of the VMV plan the verification and validation plan is and you did a little bit of this in A2 right you did a little bit of thinking into how would we actually verify this requirement the purpose of a VNV plan is to say for each requirement which of these four um methods are we going to use for verification and then actually write down each test that you're going to perform what kind of equipment you'll use what kind of test conditions it's a lot of work in fact um I think it's fair to say that the people that do this kind of work verification validation are typically different people than the people that do the writing of the requirements or that do the actual design work this is a pretty specialized activity and the people are a little different um if you've met people who do uh testing or Quality Inspection they're quite different different it's a different Minds go ahead um according to what's happening in Issa actually this uh Adit will be imposed in the specification prior to your proposal and they will give you the minimum requirements that you have to test against and they will give you a rough Matrix for every requirement line whether it's Adit and then you have to answer with a with a validation and test plan usually unless your agency and you are defining the specification then you have to do it and it's mostly based on experience um and it's some people that have really lots of years of knowledge that then make these specification but uh I think for all of you Engineers here in the next 10 years you'll be just hoping there's not so many of these adits uh in the specs you will get because you would have to answer it's part of the specification actually yeah and out demonstrated that's different well and the point you're making vulker is that this is a contractual requirement right this is not optional yeah it's not optional and it's it's it has to be part of the pricing right up in front of before phase eight yes very good points so you know the outputs of all of this are uh discrepancy reports if there's any discrepancy reports um waivers the verified product itself and then the compliance documentation which is essentially your test protocols etc etc and as you can imagine imagine now if you have you know you had 47 requirements with canet and then by the end Yan what would you say the number of requirements that we ended up with at the end in A2 we're closer to 100 right sort of most people were around 80 90 okay now imagine now you can the good thing is if you can do tests that actually check multiple requirements at once right that's a good thing uh if you can do tests that that help you verify multiple requirements through through the same test you can save some money but the whole testing strategy uh the contractual requirements that uh vulker was mentioning it's it's a big big big deal it's a really critical part of system engineering okay so in terms of the life cycle phases where this uh where this fits in um most of this activity happens during phase D okay so you remember this was the the life cycle the the NASA life cycle model and so phase D is System Assembly integration test and launch so much of the testing that we talked about happens during phase D so the system has been fully designed it's been assembled it's been integrated the way we talked about last week and now you're really putting the system through its paces and so this phase D is is uh is intense it's expensive uh and if something goes wrong it it sort of sends you back to to the drawing board often you have to do either you have to figure out whether a failed test a failed verification is a showstopper right you have to if it is then you have to redesign the system you have to retest um but if it's a minor thing then you might be able to request a waiver and you can say okay we didn't we didn't achieve this requirement uh or we failed this test but we think it's a minor issue and uh instead of holding up the program we're going to get a waiver for it which means you get an exemption essentially and you can move on and that whether or not a waiver can or cannot be granted is a big deal and that goes under risk management which we'll talk about in a few minutes yes I have a question on so if like you're a system integrator and you've you know created some statements of works for you know other people to know say procure a large optic or something um you know they go and build that that M your require ments and then they do all their verification and testing and then they provide all that documentation then like for my experience um with anything like procurement and stuff that comes you know back into your you know um inhouse and you go to assemble it and you essentially do a lot of that verification and and testing again to kind of double check that Supply out is there I mean there's a little bit of a conflict of interest obviously with you know that Supply you know doing the work and also verifying their own work is there any good way to get around that so it seems like a very expensive kind of kind of process so my my experience uh basically you're talking about um separation of powers right and making sure that the bet the good suppliers they will have internally separation of powers in other words the people that are doing the testing and the Q&A they're usually people who really enjoy finding mistakes and faults and that's why when I'm saying I was trying to be diplomatic when I said it but people even in software you know people who do software verification they love to find bugs right they love to find problems because that's their job right and um so in the good suppliers they uh it's in their own self-interest not to do shortcuts right now if you if you don't trust that and you do all your same testing and Q&A again that's a duplication of effort the way I've seen it done effectively is that you as a customer say you're going to buy the subsystem or engine for example from a what you do is you send liazon people you send Representatives who are knowledgeable people to the supplier while the testing is being done and so they're present when the testing has been done they're very involved with it and therefore you don't have to do it twice so there are ways around this um okay so I just what I did here is just um search for the word test in the list of milestones and where does it come up so the first time it really comes up in a major fashion is at the CDR okay so let me just read this to you so the CDR demonstrates the M maturity of the design and is appropriate to support proceeding with fullscale fabrication assembly integration and tests so the other words in other words even even though at the CDR you're blessing the final design you should say something at the CDR about how the testing will be done in fact test planning often is way before the CDR but at the CDR you should you should really talk about the testing then we have a so-called trr which is a test Readiness review and so for each major test you would have a separate trr the TR ensures that the test article Hardware software the facilities the support P Personnel the test procedures are ready for testing data acquisition reduction meaning data post processing and control and then at the system acceptance review the S um that's when you essentially transfer the ownership of the asset right and it's at the S at the system acceptance review that you're going to review not just the product and its documentation but all the test data the analyses that support verification so at CDR you say this is our this is the testing will do at the test re uh review itself you say everything is ready for the tests to happen and then you do them and at the system requirements Review You Look Backwards and you say what tests actually happened what's the documentation what were the results are we ready to own the asset now does that does that make sense okay okay so with that in mind uh let's talk about uh testing itself what kind of testing there are and so testing is one of the four methods of verification and it's the one that um we often uh spend the most money on so uh this is from the handbook section 53 this is um basically an alphabetic list of the kind of testing that we typically do and I'll just sort of highlight a few here and then I have a a a group exercise for you so aerodynamic testing Burnin testing which is often done with electronics make sure that you um you you burn in your electronics you you get them running at the right conditions drop testing uh pressure testing pressure limits thermal testing g-loading um human factors testing thermal testing um manufacturing random defects that's when you do like non-destructive inspection um thermal cycling vibration testing and so forth so this is you know 20 or 30 types of testing and then within each there's even subtypes so there's a there's a lot of different and there's a whole industry actually that is primarily focused on you know providing test equipment sensors data logging equipment it's it's a big industry not just in Aerospace but but throughout okay so I'd like to do a little turn to your partner exercise and uh the question is I want to ask you what kind of testing have you been involved in in the past and you know if this was like in product design product development that's fine if it was for an internship but even at the University itself if you did some experimental work and experimental testing as part of research that's fine too you can talk about that too so what kind of testing have you been involved in in the past what was the purpose of the testing what were the challenges what went well what were the results maybe if it didn't go well talk about that too all right good so let's see we're going to go back back and forth so who wants to start here at MIT who has a good story to tell go ahead so I worked on the um the ground station side of the lunar laser communication demonstration program that recently flew um so I was involved in assembling integration but also doing verification testing in the lab and uh at our field site but then we did validation when we moved out to uh the field site in New Mexico so I was involved in sort of the whole the whole process and it was you know it's neat to see and and we had to go into the cleaning room a few times to sort of adjust the Optics because we saw that they weren't meeting requirements so is there the reflector that was left by the astronauts you using the reflector that was left on the surface no This was um we were using uh just you know like zygo like Optical alignment stuff in the lab uh and then when we were out at uh at the field site um we were utilizing uh just like guide stars to align Optics so so what was uh you know what went well was there a big difference between indoor and outdoor what was sort of what surprised you in these tests yeah so you know once you can do the alignment of the individual telescopes which were you know 20 Ines in diameter uh you can do that well in the laboratory but then every time that you assemble and disassemble the system you change the alignment of them relative to each other so there was a lot of attention paid to making sure that we could uh replicate um you know the alignment to a certain extent so that was that was very difficult in sort of the laboratory setting to get done but once we did that we were able to uh have Fair confidence that we were out in the field that we could match that so very good very good um uh what about epfl well okay I can I did an internship in a aluminum World products Factory and but so basically I was doing Material Science there and there was a whole bunch of tests to do and once we have done this well all the tests about heat treatments and different um tempering and actually the alloy that was already produced in the factory was not the best of what we can have it have of it and it implied to change a little bit the heat treatment for a few seconds and actually on the line of production um adding this this amount of time was totally critical because the it was continuous and the the rolled aluminum if it spend a bit of much time more time in the oven could melt and that's that's a bit like the for the plane the swis plane actually we like I discuss with my boss saying that we should maybe change a really small parameter but at the end it was really critical to change something on the line because it it could have cut a lot um like in modification of the OV or the the gener machine so where were those tests successful were these heat treatment changes eventually implemented on the line or did the test reveal that it would be too difficult unfortunately I don't know because I finished my internship before okay well you should find out find out whether it worked out in the end very good uh back to MIT any other examples people want to mention test experiences yes please go ahead uh we bought the Kaza 295 it's a small cargo aircraft and we get we got involved in the development of its simulator it was pretty different the simulator because as the aircraft is has no fly by wire and it is pretty light so lots of hydraulics to to the motion and they brought the flight model from the factory and we applied the flight model to the simulator but it was not real enough so we had to go for flying like 60 test flying points and we have to to go back to the simulator to apply this points to to tailor the simulator to meet re it I see so the the per the purpose of this testing cuz the airplane itself had already been certified it sounds like is a Spanish right Spanish airplane yes you you tested it uh specifically to get flight Dynamics and other data to then tune the simulator to be more reflective of reality exactly because the flight model from the factory was not close to reality at all very very cool very interesting um so different uh different purposes not so testing for certification of the the first airplane in because it had already been certified to get the simulator to bech it was development for the simulator because it was sold afterwards as a type Delta simulator so it was like the development of the simulator okay um great thank you for that example anybody involved in tests that didn't this all sounds pretty good uh anybody involved in test failures you know things that didn't go well yes Nar it was an interesting experience we were designing a wind turbine that we were 3D printing in in undergrad and we had certain requirements on the wind turbine and we were supposed to test it in the Wind Tunnel afterwards um what happened was uh that the wind turban matched our performance prediction fairly closely but the generator and the electrical power system that the test apparatus consisted of uh wasn't designed to handle the current that we were outputting so we caused a small fire okay so so this was but so this was the uh the test equipment itself not the artifact you testing failed but the test equipment around it is overload the the the interesting point was that we had no control over the test equipment it was managed by the university so the within the requirements that they gave us the power output possible was didn't match what what they had so okay great great example and so to the test equipment the test artifact need to be matched right to the test conditions so excellent good um I do hope that you uh those of you that have not had a lot of test experience that you get to um to experience it it's it's a lot of work it's slow tedious but it's um you know in many cases despite modeling and simulation there's still a big role to play for actual testing okay so let's talk about aircraft testing um typically we distinguish between ground test testing and flight testing weights and balance I had some experience with this on the F-18 program you think this is the most most trivial testing there could possibly be you just put an airplane on a scale and that's it well it turns out it's actually more involved than than you think first of all airplanes are very big they're heavy right multi- tons and uh typically it's not just one scale but you have several scales you put the on the landing gears so you have to figure out the scale sces need to be properly calibrated right if you have differences in calibration of the different scales you have an issue you need to determine the mass uh not just the mass but the CG and then the most difficult thing to experimentally determine at least in you know in a 1G field is the inertia Matrix if you need to experimentally get the inertia Matrix you remember your iixx ixy i y y the inertia Matrix is tricky because you typically then have to suspend the air plane and um and just the presence of the cables in the suspension will kind of pollute the real inertia Matrix and you have to subtract out the effect of the suspension system so something that seems super trivial weights imbalances just S stand on the scale right in the morning there it is uh is actually very tricky and there are people that's all they do they do weights and balance testing for spacecraft aircraft and and it's it's basically a science engine testing I'll show you some pictures this is done in What's called the hush house so Hush Hush house is heavily insulated uh you run an engine through all of its test conditions it's operational conditions and then you integrate it into the airplane and you run It Outdoors fatigue testing this has been a big uh issue on the Swiss F-18 but in general making sure that the airplane can uh satisfy all the static and dynamic structural load conditions avionics checkout this is very very involved as we get more more uh displays Mission Control computers um all of The avionic Suite needs to be checked out essentially every function every button every menu item needs to be tested and the tricky thing is interactions among different pieces of avionics so you can't just test each each box in isolation you also have to look at at um at the interactions of different pieces of ionics um the the flight Control software or the flight software that is loaded in each of these avionics boxes needs to be in the right configuration it's a very big combinatorial challenge to do avionics checkout these days and then finally pre-flight testing so this is everything you can do on the ground uh run the engines taxi with the airplanes uh basically turn turn all the equipment on turn it off do the cycling um you could do a lot of testing before you actually fly flight testing itself falls into different categories so flight performance testing uh rate of climb range uh can you meet the each point in your prescribed uh performance envelope stability and control this is where test pilots typically earn their living uh putting airplanes into stall uh conditions recovering from stalls uh trimming uh flutter testing is a big deal so flutter is a phenomenon whereby at high speeds you have a coupling between the structural deformations of the airplane and the actual exitation of for example the wings flutter can be very dangerous if you hit a resonance at high speed uh you can actually destroy the airplane because of an instability so flutter testing is is also very very um interesting and very tricky and then finally this is primarily for military airplanes weapons testing uh both guns missiles bombs uh life fire testing uh sometimes uh also using airplanes that are towed um simulated targets and then Lo stands for a low observability so this is essentially all the new generation of military airplanes have measures to reduce their radar signature or even make them invisible or quasa invisible to radar and uh you know a lot of the stuff is classified uh but actually checking that an airplane is invisible on radar or has truly low observability there's a lot of testing involved in that and that's also uh quite expensive and very involved so let me show you just some pictures um that I've collected over the years this is a a wind tunnel test model this is a um model that was developed uh as part of the F-18 program this is about 1995 vintage um this model has um it's a subsonic wind tunnel model and you can see in yellow you have all these probes and and uh rate OHS and things like this so it's it's basically to check whether any modifications you make to the airplane will affect its performance and airf flow this is for wind tunnel testing this model by the way just builds this model is about half a million dollars uh it's very accurate it's very precise uh this is a picture yes go ahead is that model full scale or half scale no it's I want to say like 1/8 scale something like this yeah okay here's the hush house that I was talking about this is in St Louis so you can see that the airplane is not painted yet and uh only one engine at a time so the engine is being in this case with full After Burner you can see the airplane itself is secured with these uh chokes here and there's load cells in these chokes so as you fire up the engine you can measure the thrust by the load cells that are uh attached to these um to these chokes here um the engine you also see that uh there's these cables running in and out of the airplane so that all the sensors everything is being and the engine is put through its full different uh operating profiles and uh a lot of lot of sensor data is recorded to make sure that the engine responds appropriately it has the right thrust for the right throttle setting the fuel consumption all the temperatures in the engine uh that everything is is nominal essentially life fire testing uh this is a Maverick missile being fired air ground missile um as you can imagine there are special ranges and test sites for doing this kind of work so in the US one of the most wellknown is China Lake out in California um you have to reserve uh months and sometimes years ahead so if you want to do like a live fire test campaign you have to reserve the range uh at least 18 months to 2 years ahead of time because a lot of other uh Services a lot of other programs are using the same facilities uh in Europe it's a little harder uh definitely in Switzerland because the country is so small and dense and highly populated um you can't test live missiles in Switzerland you can do uh guns air to ground but uh in order to do missile testing typically that's done here in the US or in a more limited fashion in Scandinavia like in Sweden and Northern Sweden there are some test ranges up there uh this is the most expensive kind of testing you can do so a single test like the one shown here a single test like this will probably cost several million dollars just because of the not just the airplane and the weapon itself but all the test procedures the protocols airplanes that observe it from all kinds of angles uh it's very very involved and so because it's so expensive you will typically only do it for something new that you haven't done before either a new weapon or a new weapon integrated on a new platform and so forth and uh obviously it's it's very interesting but it's um it's very involved yes are they just testing accuracy or that the two things work together or like what what are they looking for really the first the first thing you look for is does the weapon fire so do you have you know do you have all the electronics all the signals the wire bundles did you get it right is there an endtoend functionality that's number one number number two safety does the weapon separate properly from the aircraft the worst thing that can happen to you is if you release the weapon and it collides with the airplane and so you can see the the various angles and release conditions are very tightly prescribed and so there's a separate separation has to be proper and then the third of course is you know accuracy uh so there's within each of these tests there are multiple subobjectives that you would test for but safety always comes first okay any any questions about this was sort of a little bit military Aviation heavy uh if you're testing a new you know whether it's a Kaza airplane or new Airbus or Boeing commercial airplane uh many many months of testing they actually fly the routes you know you'll Fly New York to Singapore to London you would actually fly the real routes you would record fuel consumption uh lot of param you know some of this testing is not very uh exciting you know it's many many many many hours in the air but the key is that you have a lot of instruments and sensors during these tests that you may not have during regular flight operations to really make sure there's no surprises the airplane Pro uh flies at least as good as what the requirements that you promised your customers and uh even then you know when you think about what happened to the Dreamliner the 787 had a lot of battery problems you know because they they Ed a lot of lithium ion batteries there were overheating issues uh some of these problems didn't show up in testing they only showed up in early you know early operations once you had a fleet going so it's not a guarantee because you're doing a lot of testing that you're going to catch all the problems um but you want to catch as many as you can yes so my question was about the uh risk posture uh for larger airliners you know for Boe the air buses uh so for military aircraft you know there is an escape method for the pilot um but for these larger aircraft you know how much analysis do they do before they decide to go ahead and put a person inside um do they do fly by wire befor hand or is that possible such large aircraft so that's where that's where the um you know this this um ground testing pre-flight testing becomes very important so you basically you taxi for many many hours all the flight control surfaces all the engine you the hush house testing you essentially try to do as much as you can on the ground before you do the the maiden flight all right let's move to spacecraft and it's kind of a similar thing you can distinguish the ground testing versus on orbit testing so the ground testing is really not that different weights and balance you know the biggest thing is if your satellite is heavier than the launch capability of the launcher you have a real problem so the mass constraint is even tighter in in spacecraft then a lot of testing on antenna and Communications this is typically done in anaco Chambers in the near field and then uh later in the far field uh vibration testing that's the shake part thermal and vacuum chamber testing that's the bake part and then uh you also have pre-launch testing so off- pad and on pad off- pad testing is the satellite or the spacecraft has already been shipped to the launch site and it's hooked up to you know like a patient in the hospital it's put it's hooked up to a lot of cables and power and Cooling and so forth and then when it's on the pad it's pretty limited so on the pad means the satellite or the spacecraft is already integrated into the launch vehicle it's on the Launchpad and then uh the amount of testing you can do is very limited uh so that's when we say off- pad on pad is you know is the spacecraft already been integrated on the launcher or not once you launch to orbit you know you got your 8 minutes of Terror and hopefully the launch goes well and the spacecraft is released into its Target orbit initial Target orbit and then you do a lot of other tests like Thruster testing can you do station keeping can you turn on and off the thrusters you deploy all of your mechanisms your your antennas your scientific instruments uh and then your Communications communication and instruments and we'll talk more next week but this uh this typically is called commissioning you're commissioning a spacecraft uh before you actually turn it over to the users and that commissioning phase could be you know anywhere from a few days to several weeks or even a couple months and again you don't want to randomly randomly put in commands into your spacecraft these test sequences deployment sequences are very very very carefully worked out every command the order in which you send the commands have been worked out ahead of time they've been simulated uh and all you want to see here is confirmation that the spacecraft behaves as planned some pictures uh so this is what typical spacecraft integration testing looks like um so this is an a clean room environment um you have you know people in bunny suits um and and the idea is to not um you know not damage the spacecraft while you're doing the testing um um this is a picture of the the Clementine spacecraft um this is a radio frequency anac chamber testing so you see these funny cones here this is these are essentially foam cones and the idea is to prevent uh VI uh multipath to prevent uh echoes in the test chamber to test all of the antennas uh Emi electromagnetic interference and compatibility um charging and discharging of the spacecraft this is one of the failure modes is that you have high electrostatic charges that build up on a spacecraft create a l large voltage potential across the spacecraft um some spacecraft has have failed because of that um so all of these things you want to test in a in a in a um in a very controlled environment um James web Space Telescope I'll send you a link um I'll send you a link through email about uh there's a there's a simulation of this deplo on orbit deployment it truly is amazing um th this spacecraft uh will be launched in a box essentially and the deployment sequence is very carefully choreographed uh first typically you deploy your solar panels right because you need power because you're only running on battery initially so if your B battery runs out before you've had a chance to deploy your solar panels and get fresh power into it you're in big trouble so typically solar panels first then Communications uh and then you start deploying the other subsystems so for James web uh also very tricky is this this is called the sun shield it's essentially thin layers uh of of insulation and uh the geometry is very important the primary mirror the secondary mirror all these things need to be deployed with very very high Precision um and it's even to the point where this particular spacecraft is so lightweight that it cannot support its own weight in a one 1G gravity field so there's no way to to test end to end the full deployment sequence on Earth the first time it will happen is in Orbit now they've tested subsequences or scaled models for example the sun shield has actually been deployed at a smaller scale in a 1G field but never the full thing so this will be kind of scary after you know an8 billion investment so let's hope for the best 2018 so testing is good uh testing testing testing but testing uh also has its caveats so caveat um means um limitations essentially so testing is critical but it's very expensive uh so you know think about test rigs test Chambers sensors daq is data acquisition equipment all this stuff is very expensive and if you can reuse things between different programs that helps but still how much testing should you do of components so one of the comments um who mentioned the the the vendor the supplier one of you you you you talked about it and this is a key question do you trust the parts that come for your vendors or do you retest everything yourself calibration of sensors and Equipment if you've done some testing and you forgot to calibrate right your displacement sensors your thrust sensors your you didn't calibrate them against a or they're out of calibration that's a big problem that's a big problem so before you start your test make sure that all your sensors are properly calibrated or you can get the wrong conclusions this is a mantra that's well known test as you fly fly as you test fundamentally this means that the configuration of your item spacecraft aircraft medical device uh the one that you test should be the same configuration as what you're actually going to fly and it's often failures occur when the test went well but then somebody tinkered with it and modified it before it actually flew and that change actually caused a big problem so um make sure that your test conditions reflect the actual operations as closely as possible simulated tests what do we mean by this so simulate a test use use dummy components maybe your full spacecraft or aircraft isn't ready yet you don't have all the pieces so you can still start testing but you have to replace the missing pieces with dummy components at least they should reflect the right Mass distribution but maybe you can do more uh simulated operations so Zerg versus 1G is it representative and then what's often true is that you you you pass all your tests and then you still have failures in practice and the failures often happen outside of the test scenarios that you have tested so you have to be ready for that uh but try to avoid that so um here's from appendix e this is called a validation requirements Matrix essentially what this is is you know an organized way to organize your V andv activities in terms of what's the activity what's the objective which facility or lab will you use will you do it in what phase who's in charge and what are the expected results it's pretty straightforward it's just a a table to organize these activities and then appendix I is your more formal VNV plan this is a suggested outline for it and I'll just say this the degree to which you take verification and validation seriously and the resource you make available for it are critical for Success so how many people uh how many dedicated Q&A Personnel what is the interaction and working with suppliers uh are you planning ahead for these tests are you doing how close are you getting to actual endtoend functional testing uh can you piggyback on existing facilities and Equipment how well do you document all the outcomes and follow up with discrepancies you know and and my last comment here is this work is often not glamorous and except for you know some of the very cool flight testing that I showed you uh most of this work is really hard work it's very detailed oriented it's not glamorous but it's essential if you cut corners uh you often pay the price for it so uh any any comments or questions uh we'll take a short break like a five minute break uh but any questions about testing verification validation yes go ahead it's not really a question but I wanted to say that also flight testing is not so fancy I mean many people think it is but it's actually a lot more well the ACT fly maybe it's fun but there are not that many you need to prepare them weeks and weeks ahead and it's actually very very boring because you need to make sure that you don't waste time at all well least you can waste a little bit more time in a lab so it's very stressing and not so funny did you uh do you have experience with this or I wasn't flying because you need to be certified but I Was preparing some and that's very I think it's even worse than tasting in the lab yeah yeah which which uh airplane or which system were you involved with A330 I was in with the power PL system okay FedEx in particularly and the AR was an A330 A330 okay yeah great um but you know I think it's healthy to have this experience it really makes you humble and you also see you know for the things in design you know did you design well did you design for testability really involveed being in I I highly recommend for every one of you to try to get on some kind of test campaign at least once in your career because it's eye opening so thank you for that comment any any comments at epfl uh anybody any of the students vulker did you want to add something Kata go ahead yeah I guess the the comment is seems like sometimes you can meet all the requirements in the verification process um but when you get to the validation part for example maybe the customer had some expectation that you know the range for the time of flight would have been know on the maximum Edge Ed and you were on the minimum Edge then how do you actually seems like sometimes you can meet all the requirements but they're still not going to be happy when do you find that middle round and it's going to be costly to continue iterating this over and over again if you try to involve them earlier on during the testing process too like how do you handle that that difference yeah it's It's Tricky you know so that's when you need you do need contractual agreements in place right you need to have the requirements Baseline you need to have the contractual agreements and hopefully uh any problems that occur will not lead to some kind of legal dispute uh but sometimes that's unavoidable but if you don't as a designer as a manufacturer unless you have agreements in place and clear Baseline what how do you decide in the end is it successful or is it n successful and if there's problems try to isolate these problems and say okay here by and large the testing went well but we have like three four five issues that uh that need to be addressed and you can tackle these issues one by one but if you don't have a contract in place that's really a good contract if you don't have a clear requirements Baseline and then if you don't have a good relationship right with your customer you're setting yourself up for big big problems fulker go ahead yeah um there's also the one big difference between commercial operators and is or commercial customers that are becoming more and more free compared to the institutional ones and often I remember for some of the global star aridum series the customer was only accepting the hardware 6 months after after it had been commissioned in orbit so there you have a validation that is still your responsibility in orbit you can't go and fix it but it still has to work and he was retaining up to 10% of the full contract value even up to the N minus 2 two tier level suppliers until he was satisfied that it was working in orbit so these considerations it's it's really uh I mean uh the the proof of the pudding when you have to test this up there and can't fix it so there's no recall of a satellite constellation like uh sorry we messed up with the software it's not possible there yeah and of course those you know those terms and conditions you've probably negotiated years before so you got to be careful that's where that's where sort of risk management which is actually it's a great thank you vulker for that comment uh let's take a short break and then we'll talk about risk management which is really what what this ends up being so let me talk about risk management and this is actually quite prominent in the uh system engineering handbook this is right in the middle here of your system engineering engine technical risk management section 13 um why is it is it important um so first of all what is risk so risk is uh the probability that a program or project will experience some undesired effect or event and then the consequences or impact or severity of that undesired event should occur and so think of risk as the product of probability times impact and uh the undesired events could come from a number of things technical programmatic so cost overrun schedule slippage safety mishaps health problems malicious activities right cyber security is a big thing these days environmental impact um failure to achieve the scientific or technological OB objectives or success criteria and uh so technical risk management is therefore an organized systematic risk-informed activity centered around decisionmaking to proactively identify analyze plan track control communicate risks and the uh to increase the likelihood of success of a program and so what risk really does is measure the future uncertainties of achieving your goals your program goals technical cost schedule goals and think of risks in a sort of holistic way all aspects of the technical effort uh technology maturity supplier capabilities performing against plan and so forth and so the idea of risks is that they have risks have some root cause there's some something that gives rise to risks and then the actual quantification of risks happens in terms of likelihood and consequences which are kept separate separate Dimensions um so the first thing to think about is where do risks come from right where where's the source of risks and I want to show you a couple models for thinking about this the first one is um is this idea of layer layers of risk that there are layers of risk and I want to credit one of my colleagues here at MIT Don lard from the Salo school who really sort of developed this layer of risk model and applied it to different Industries so there's a version of this for the oil and gas industry you could make a version for medical you know medical Technologies so this is the uh version for Mars missions okay so if you designing a new Mars mission a new Mars Rover so you have in the bullseye here the narrow inter interpretation is technical or project risk so the airback technology if you're using airbags for deployment will it work the over Motor Performance are you going to have software bugs those are those are the risks we typically think about um and the idea is you have high influence over these risks as a as a system engineer as a project manager then you have uh a layer around it which we call industry or competitive risks will your contractors perform will you have budget stability and then there's sort of more country and fiscal risks so in the US we have uh a budget cycle we have four-year administrations will you get your budget uh what is the priorities between human and robotic space exploration and then working with International partners and then there's another layer of risk which are called Market risks so if you think in Mars missions who's your Market well the science community and maybe the public right so will will uh these missions hold their attention are there new science requirements you know we discovered there's water May probably flowing water on Mars maybe with a lot of perchlorates in it you know it's not pristine water but you know that could change soort of the priorities for your mission and then finally the the most outer risk outer uh is what we call Natural risk so this would be things like Cosmic radiation micrometeorites you know uncertainties in the atmospheric density of Mars as you're doing entry descend and landing and you have very low influence right the uh that doesn't mean you can't you can't protect yourself or take measures to deal with these risks but fundamentally the occurrence or the probability is something you can't really do much about so that's one way to think about risks and the seeds of risks is in these layers I think I find this to be I know this is very high level but I find this to be a pretty useful model yeah go ahead so this is just kind of references the in influence you have not necessarily the amount that each of these uh are a risk to the program that's correct that will be program specific this is just you know the stuff that's in the bullseye here you can do a lot about it um and perhaps both in terms of probability and impact and then as you move further out um you know there's less and less influence you have as a system engineer as a project manager here's 's another another uh way to organize your thinking around risks and this is around the Iron Triangle in project management we talk about the Iron Triangle of cost schedule and risk and we call it iron because the idea is that if you if you constrain all three uh too tightly uh it's it can be it can be very difficult and um and these are it's also referred to as the triple constraint in project management so the three dimensions here are techn techical risks cost risks and schedule risk and then in the center we have programmatic risk which means it's kind of the combination of all three and the idea that even if you do a great job on keeping your budget under control schedule and you're meeting your technical objectives you can still fail because the program as a whole isn't doing the right thing or the market that you had been targeting is no longer really attractive by the time you launch the the key idea here is that these risk categories are not independent of each other so let me mention a couple of examples um so cost risk might limit your funds and that could in itself induce technical problems which cause you further cost risk so one of the big initiatives at Nasa in the 90s was the faster better cheaper program right we're going to launch more missions cheaper and you know out of 10 missions maybe two or three will fail and then seven will succeed but get more value out of this as a portfolio unfortunately it didn't work very well because you know when the one two or three missions fail out of your portfolio you know the media and the public focuses on the failures rather than the aggregate value of the whole portfolio and eventually that's probably the main reason why faster better cheaper was abandoned but here's so for example we just talked about testing if you have a very limited budget what is the first thing you would people typically cut out what's the first thing to go testing testing is very important Sam asked me during the break what's your typical budget for testing and V&V activities and in many programs it's very substantial you know 40% of the budget maybe 30 40% of the budget uh easily and so you start cutting out tests well what you do is you you introduce technical risk and if if you have failures because you didn't test that could cause you uh additional rework and more cost um similar uh schedule and schedule slips can induce cost risk so as you slow down you have the what's known as the standing army cost right people people are going to charge to your program even if it's at a reduced level um and that will also increase your cost so lots of coupling here between risk categories this is a very useful I think risk management framework it's essentially a controls framework work and the idea is you start in the upper right you anticipate what can go wrong in your program so that's risk identification you then analyze these risks uh in terms of prioritizing them which of these are important you plan to take action this is often called risk mitigation you track these actions and then you correct any deviations from your plan and you you communicate throughout and you cycle through this so typically risk management will happen on a weekly basis a monthly basis at least quarterly basis for for big programs um now how do you actually do this uh first of all the risk ID and the assessment so the the the risks are typically brainstormed so you think about risks based on you know you have to imagine all the bad stuff that could happen to you and your program uh the pro the probability that that these things will happen and then the impact or consequence if they do happen based on you know the requirements the cost the schedule uh the product in its environment and and this is where actually having a mix of younger engineers and more experienced Engineers really comes comes in handy the experienced Engineers they will have been through several programs they will have seen failures in the past they will really um be able to uh point to potential risks that less experienced people may have uh May ignore or just not be uh not understand how important they could be so the next step then is to aggregate these into categories uh typically not more than 20 or so categories or risk items um Pro projects often keep so-called risk registers which is just you know a database or a list of risks if you have hundreds and hundreds of risks in the risk register it's too much you know people will just it's just a long list and nobody really it's just a check the box exercise to really take risk management seriously you have to focus on on few of the risks that you think are important you score them based on a combination of opinions and data and you try to involve all the stakeholders in the risk management and eventually risks are placed on this uh Matrix uh of uncertainty and consequence so let me zoom in on The Matrix there are different many many different versions of the risk Matrix this is one that uh NASA typically uses and I I like this this particular version for several reasons that I'll explain but basically the way it works is you have the two Dimensions impact and probability so probability is You Know How likely is this to occur and then impact if it does occur what what will happen what's the consequence of that and what the so of the two things that I really like about it the first one is that each of these levels there's actually some definition behind it you know you not just guessing at the level but there's some criteria so for probability um a level three means it's about equally likely that that it will happen and not happen so level three means it's about 50/50 whether this will happen in your program and then uh four is very likely so maybe that's you know I don't know 75% and then near certainty is like 90% or more improbable is like 10% unlikely is 20 20 to 30% something like this um and then more importantly the impact so a level one impact is you know negligible it has almost no impact a level two means your Miss Mission performance margins are reduced on the technical side it means um your reserves you remember margins I asked you to assign margins in in assignment A2 so it means you're eating into your reserves but you should still be able to meet all your performance there should be no visible impact you're just your safety cushion is less that's what level two means uh number three means your mission is degraded so you can still do the mission but you're not going to hit all your targets uh four is you lose the mission but the asset is still recoverable so maybe you could try again in the future and then level five is a catastrophic failure uh that involves you know loss of mission and or loss of crew um on the cost side uh you have some thresholds for cost obviously these numbers have to be adjusted for different programs you know a $10 million loss is a huge thing in some program and and almost like pocket change and other programs and then uh schedule so a a level one Milestone would for example be launch right if you miss good example of this was the Mars science laboratory the Curiosity Mission originally it was supposed to launch in 2009 they missed that deadline mainly due to problems with with cryogenic actuators they took a lot of the blames the actuators but there were a lot of problems across the board they they missed that launch window and they had to launch in 2011 so that was considered from a programmatic standpoint a level five failure because you missed your main launch window and you had to wait 26 months for the next one so so that's good because because now you know when you assign a probability and impact you can really look at these criteria and it's it's easier to do that in a sort of repeatable fashion the other thing is if you look at the colors on the Matrix you can see it goes from one blue which means low risk to 12 which is the highest risk so there's 12 risk levels but when you look at the Matrix that's there's something peculiar about it so look closely at the colors and you'll see something um special about this Matrix anybody notice what I'm talking about let's see at epfl do you guys when you look at those colors at the Matrix do you notice something go ahead and the BL so it goes from Blue to Red which is the light light spectrum with the blue the lowest radi wave and red the highest one right frequency right the most power so yes go ahead yeah it's huge so impact is more serious than probability right so it's asymmetric you see that it's asymmetric so the high impact low probability corner is weighted more heavily than the low impact high probability and that's intentional because it's been shown in the past that you know things that are not likely to happen but if they happen they're really bad uh in the past people have sort of pushed that away and ignored those so the the purpose of this asymmetry in this Matrix is to elevate the low probability high impact events to be higher in the risk level so that people pay more attention to it okay that's the so that's most uh risk matrices don't don't have that asymmetry in them but this one does and and I like it because of that so bless you so the question then is is what what do you do with this so the the idea is you you do your risk management you identify your risks you place them on this Matrix and then you track each of these risk items over time so here's your 12 risk levels right between 1 and 12 that's the Y AIS and then on your x- axis there's time and for each of your risk items people might disagree you know some people on your team might say hey look this is not a big deal we've we've seen this before the impact is not big we have a quick fix for this we we know how to deal with this and other people disagree and say no this is this is very very serious you have to take it seriously so the idea is that um for each risk item you have this optimistic expected and pessimistic estimate of what is the true level of risk that's what these bars are and then you track it over time okay and you know uh depending on whether you're in in P before PDR or CDR you could have very substantial risks and and that's I guess okay still as long as you find ways to reduce the level of risk for example by doing extra testing by changing your design to put in extra you know power margins bigger solar panels you know redundancy for example there's a lot of things you can do to affect both the probability and the impact right radiation extra shielding uh and so the idea is is that over time you're going to reduce these risks gradually below some threshold so this red line here is like the acceptable threshold of risks at that point in the program and then as you get closer to launch uh things should should be below the the threshold and if it's below the this watch domain then you you even don't track it you don't pay much attention to it if it's above this red line you have a big problem you might have to uh stop the program or do a major redesign or repeat you know a major Milestone and some programs have been cancelled because they just couldn't get these risks under control so the idea is that gradually you transition and you do this by actually doing risk mitigation around that risk management cycle now the last thing I will say here is that um every mission that is uh you know uh worthwhile doing is still going to have some residual risk at the end risks will not the requirement is not that all the risks are at zero in the lower left corner you will launch and you're going to have residual risks and you just have to accept those but you have be to be cognizant of this and this is really no different in the automotive industry for example right if if when you're developing a new car or a medical device and you're going to launch it to the market uh if your requirement is zero risk you will never sell anything you will never launch anything because you will always think of something bad that could happen and there'll always be people saying it's too risky we can't do it so knowing how much residual risk you should be willing to carry is is a big part of being a leader being a system engineer really understanding things and then the automotive industry there are people whose the primary job they have is to do this work and they're called quality Engineers or warranty Engineers so the the warranty Engineers their job is twofold before you launch a vehicle to Market is actually ranking you know what are the on this particular vehicle or program what are the top 10 things that could cause warranty claims and problems in the future we don't know that they will but they might right and so and then once a vehicle goes to Market and you know reports are coming back from from users and from the fleet actually tracking what these issues are and then knowing when has it you know hit a threshold where you do need to do a recall you do need to do a retrofit this is a big part of it and it's it's really a big deal I mean the the amount of money that Automotive companies spend on recalls every year is about the same as what their profits are so if you could eliminate recalls and warranty claims Al together you basically double your profit right and so depending on what industry uh you're in whether it's you know Automotive medical spacecraft uh how much risk and safety is involved this is more or less emphasized in the industry but it's a big part I think of system engineering job is to understand this here's the um you know this is again a flow diagram for how to do this uh uh risk management properly you have a risk management plan your technical risk issues that are placed on the Matrix uh the any measure ments or data you have and then how do you report this and then out of it comes a mitigation plan a set of actions technical risk reports and then any work products from the technical risk management and the idea is that you repeat this process on a regular basis it's a big part of your your Milestone reviews as well okay so um I'd like to spend a few minutes on system safety I am not going to do this Justice because there's a whole class here at MIT on on this taught by Professor Nancy Levon by the way who's taken that class or who's been thinking about taking it about three four of you so I'm just going to give you a very quick exposure to this um so um this is a book that Professor leveson wrote several years ago and uh she basically distinguishes two kinds of failures uh component failures which most people think about you know a an axle broke or uh there was a you know the battery uh caught fire and clearly component failures are real and they happen single or multiple component failures and usually there's some Randomness to them so most of the classic accident investigation techniques and safety techniques focus on component failures but there's also component interaction accidents or failures which are trickier in a sense because you could have a system that has no single component has failed and yet you had a system failure and so it's it's the interactions among components and this could be related to inter uh interactive complexity and coupling uh more and more computers and software and then the role of humans in systems and this is really what what um a lot of this is about so the traditional safety safety thinking is that component failures you need to worry about the component failures only right so here's a classic example of a uh a kind of event sequence of events this is for a tank failure so in this case we have a tank and uh there's um moisture that builds up in the tank and then corrosion essentially uh is a result the metal gets weakened and under the operating pressure of the tank the the tank itself has been weakened due to corrosion the operating pressure causes a tank rupture and the tank rupture then uh causes a essentially an explosion and fragments or shrapnel from the tank will be projected and then cause equipment damage or Personnel injury and so in this kind of linear chain of events model that the way you think about safety is putting in barriers this is often also referred as the Swiss Cheese model right if you take layers of Swiss cheese and I guess it has to be emmentaler right you guys know the Emler at epfl right Emer the one with the big it's got the big holes so you take these slices of cheese and if you can look at the cheese from the and there's actually a hole right through then the accident can happen but if if you put another barrier in between you can't see through the cheese and the accident is prevented that's that's sort of the classical thinking around system safety is chain of events and then put barriers between these um and you know this is I think valid uh for a very particular kind of accidents which are these component accidents what professor leveson says and her stamp and stpa framework is a little different uh this is based on uh essentially thinking about safety as a lack of control of a system lack of lack of controlability of a system and so if you think about it this way I'm showing you here a control Loop and you have in this case you have your uh the actual system the actual process that you're executing the controlled process is here you're sensing things about that process so temperature pressure uh proper alignment um and so one problem could be your sensors are inadequate you're sensing the wrong information and then here's your controller model how should you control the system so you have the wrong controller or the wrong process model and then here's your actuators um are you issuing commands at the right time um are you issuing the right commands or are you not issuing commands when you should be to the system and that feeds back into the proc into the Control process itself and so uh process inputs could be wrong or missing you could have disturbances into the process that are unidentified or out of range and then ually if this process goes unstable then you have a failure or an accident so it's quite different it's essentially thinking of this as a control problem instead of a chain of events problem and the argument here is that for um safety or or failures that involve a combination of Hardware software and humans often this model is able to be more complete in terms of identifying hazards and potential mitigation actions so I think we're out of time but I want I want to give this to you as a thinking as a homework for thinking about this uh this is an accident that happened uh earlier this year I guess in July and um this is the Virgin Virgin Galactic crash that happened uh Virgin Galactic is one of the space tourism companies and during a test flight uh the airplane crashed because the co-pilot unlocked the brake system so it has a kind of Feathering mechanism uh and the pilot unlocked it too early during a kind of high-speed flight phase and so what I'd like you to do the link is here just read read the story there's a a more lengthy accident report that's come out I'd like you to just read this quickly and then think about how does this relate to risks how does it relate to this particular model of system safety okay so the system's theoretic view of safety is then that safety is an emergent system property accidents arise from interactions among system components physical Human Social constraint violation uh losses are the result of complex processes not simple chain of events and that most accidents arise from you know you could have a system that's quite safe when you start operating it but over time it migrates to an unsafe State because people you know sensors fail people start bypassing safety procedures and sort of uh gradually it migrates to high risk okay so the last thing I want to talk about just for a minute or two is the F frr the flight Readiness review which is one of the later milestones and what happens at the flight RR uh F frr it's essentially this is your last chance to raise a red flag right this is the last Milestone before for launch have all the VNV activities been passed successfully are there any waivers that need to be granted what are the residual risks we just talked about and then you know after the f frr is passed you actually start the countdown right X day T minus X days Y hours zcs to an actual launch or a product launch whatever it is and then here's from the handbook the entrance in success criteria for f frr so this is you know you should have everything should have been done at this point your your design your integration your testing uh your operating procedure your people should be trained this is your last chance to raise a red flag after the F frr you're essentially go for launch so the stakes are high okay so quick summary uh verification and validation are critical uh there's a distinction between the two right verification is against the require requirements as written validation is you go back to your customer and you test in a real environment testing many different kinds of testing it's a fundamentally a Q&A activity and it's really expensive but it needs to be done right risk management we have different tools like the risk Matrix risk identification mitigation and that's really where the rubber meets the road in terms of the tension between cost scope schedule and risk in projects system safety think about not just the chain of events model but this this controls view as well uh stamp stpa is a particular framework for this and if you're interested there's a whole you know class there's a whole set of things you can learn about just safety and then finally frr is your last chance to raise the red flag it's sort of the the big milestone before you go live with your system okay so any any last questions or comments epfl yes please go ahead um so after the the frr you should have finished your test right it's the the limit uh yes you should have finished your test except for the ones that you're going to do say on orbit right because I was wondering in in the list there is the go no go Fest and I'm wondering if it's not related to the launch actually yeah so the the actual launch itself of course has you know the actual launch countdown and you can stop the launch if certain things don't so this is a a review the frr is more like a CDR so people the the countdown hasn't actually started yet but if you successfully pass the F frr that's when you begin the countdown the official countdown starts uh and then you still have a possibility of course of stopping the launch but in terms of a formal programmatic review this is your last chance I think this the point here is more about the what is your system boundary if you consider the system boundary being the whole mission including the launch uh then obviously uh the the system will will only be finished when the mission is finished phase EF right and this is more this uh this flight R review is linked to the to the payload or to the whole satellite uh specifically that you're allowed to go forward and now start the countdown issues and then all the other now if you extend the mission boundaries to the whole Space Program or to the whole colonization of Mars It Will Be obviously later