the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or to view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu let's talk about the uh V model here we are interesting we have a little bit of a lag here so the um life cycle management essentially is our last topic for today and that sort of completes the the V and then next manufacturing so what I'd like to cover today is essentially a definition of management what does it include what do we mean by that and then focus on the life cycle properties also known as the IL um discuss a little bit where they come from how much we know about them then the bulk of the discussion is going to be a case study called reconfiguration communication satellite constellations and this sort of explains the idea that a system once you brought a system into operation that's not the end of the story that system is going to continue to live and change and be modified over its life uh I'll try to summarize some of the key concepts of the whole semester and then I have like one slide on career and study recommendations if you want to go further into systems engineering um so life cycle management um so this is my definition of what life cycle management is it's essentially the active engagement of it's the active engagement of all the stakeholders with the system between the time when it starts to operates you've done the commissioning you've done the operations until you decide to decommission retire the system so it's the during the whole operational life and what you want to do is you want to maximize the value that you gain from the system's existence and so uh life cycle management starts from the very very very start and so I listed here some activities that are included in life cycle management so obviously daily operations you know monitoring is the system working uh as expected uh training and certification of The Operators sometimes you know in long lived systems the people that were the early operators generation one are not necessarily the people who are going to operate for the whole life so you have to you know you have to have new people operate the maybe one of the most extreme examples of that is an airplane known as the B-52 have you heard of this this airplane might be the first airplane that actually has a 100-year operational life and there are apparently maybe you uh Air Force guys at MIT you can confirm this that there's actually like you know the the grandfather was the first you know generation and there's like third generation Pilots now flying on the B-52 that's what I've been told is that have you did you hear this as well yeah that's that's correct do you know some of these people no I've never actually worked on uh that airframe okay but anyway so that's what I mean by training and certification not just of the initial operators but you know over multiple Generations then the third thing here is servicing and servicing comes in two flavors are preventive maintenance you know regular maintenance uh and then corrective maintenance which we also call repair uh dealing with small and large failures recalls anomalies big this is a big deal in the automotive industry as we know uh increasingly protecting the system from either random or targeted attacks cyber physical right physical security but also cyber security is becoming a huge topic uh sharing And archiving the data that's being produced by the system more and more systems produce large amounts of data well what do you do with it how do you store that data um upgrading and retrofitting the system as needed so retrofitting means we're physically changing the configuration of the system right over its life to add new capabilities um and uh upgrading upgrading can be done sometimes without having to change the hardware right so you're doing like a software upgrade or software update software patching all of this would would go under upgrades um cross strapping the system with other systems in a Federation of systems so what I mean by this is a system May uh have been designed just to operate on its own but now you're connecting it with another system this happened in for example the electrical grid right the the early electrical grids were local Regional power grids and now we've connected them in the US we we have three major grids the Eastern the Western and then Texas has its own grid Texas does a lot of its own stuff it's called aircut and then here in Europe uh you know the European electrical grid is now Continental wide and it used to be and of course that that has big big implications for operations um reducing the resource consumption and environmental burden of the system over time and then finally decommissioning the system when it's time to to do so and you know in many systems this is a big uh challenge is to finding when is the right time to uh decommission the system and that's often the case when you know the operating cost exceeds the value that the system delivers so this is a pretty long list here and um a lot of a lot of tasks a lot of decisions to be made um any questions about this list it's I don't think it's complete but um this this is this is a lot of things you need to take care of during operations here's a a more a more graphical view of this so this is my sketch of the systems engineering life cycle so this is part one right con conceptual design con conception design and implementation you start the life cycle you do something like a system requirements review understand the mission the requirements the constraints you you do conceptual design that's where you do creativity architecting trade studies at the PDR you choose your so in this case you know we're going to go for this triangle concept and then you design all the details within it that's where we do modeling simulation experiments uh mdo uh and we iterate uh sometimes you have to iterate this this dashed iteration means abandoning the concept you chose and go for a different concept that's usually undesirable at the after the CDR we implement the system we turn information to matter and this all happens in a technological economic social context so then the second the second part of it you know which in many systems you know if this if this is 10 years this could be 30 years or more uh is the actual operational phase so the system has now been tested validated verified and deployed and now uh you know we we operate the system things break we need to service it and then I have this um you see there's two versions of the system one is solid and the other one is kind of faint that's a virtual version of the system so one of the big Concepts that's being pushed now in certainly in the US uh by the dod but also other places is the idea of a digital twin right the the idea of a digital twin is that there's the physical system right that exists and then there's a digital version a digital twin of that system somewhere on a on a computer that mirrors exactly what the real system is doing so if something breaks on the physical system well that same component fails in the digital twin and then before you actually do a repair or any actions you execute those actions on the digital twin to see whether the system will operate properly and um and so this is this is kind of the the the latest this thinking is that for any system we should have a digital twin uh during operation upgrading the system so in this case we're we're adding things to it we're connecting it you know and at some point the system does degrade because for example you know materials age they get brittle uh technology becomes obsolete you know it gets harder and harder to maintain spare parts are uh the the suppliers of the original spare parts went out of business so these really old Legacy systems can be very expensive to operate and then at some point you liquidate and uh and it's the end of the life cycle so I've already told you about one example for this which is uh the space shuttle and uh so I'm not going to belabor this again but I think it's a it's a great opportunity to learn the space shuttle had a 10-year design life uh roughly from 1971 until 81 first flight and then we operated the shuttle for 30 years and we had a total of 135 launches during that time we spent 1992 billion uh dollars on the shuttle program and if you average that it's 1.5 billion per flight which is a lot more than uh than was planned but just yesterday here at the epfl we had the uh annual meeting of the um Swiss Space Center and one of the uh astronauts here Cloe nicolier gave and and his colleague uh professor meron gave a great talk about uh the Hubble and the servicing of the Hubble and you know the the amazing things we've learned through it and uh uh you guys at MIT Jeff Hoffman was part of uh at least the first servicing Mission as well so there's you know incredible things that were done by the shuttle and uh and so I think we we need to acknowledge that not just the fact that uh it was very expensive it accomplished great things during its life but you know if we had a chance to do it again would we would we come up with the exact same system would we make all the same design decisions you know we lost two of them probably not we would probably do something something different um so here's what we wanted and then finally we got a much more complex system in the end and particularly uh my sense is that a lot of the things that made the shuttle expensive to operate during its life were things related to U maintainability right reliability and so forth so the life cycle properties which I want to talk about next so the I I just going to repeat the question so you guys can hear as well so the question was about uh reusability so the the shuttle was re the Orbiter was fully reusable uh the external fuel tank was not reusable right and then the solid rocket boosters were partially reusable because they had to be stripped down you know and and and rebuilt essentially for every launch the idea of you know blue origin and then SpaceX stage one you know flying it back to the to the launch site is absolutely the fact that if you can make it reusable then you should you can amortize the capex in that element over multiple flights and it should drop you know the cost by you know a factor of five or 10 or more now the devils in the details right so if you start and restart an engine multiple times you have a big transient every time so did was the system in fact designed to withstand these transients can it handle multiple starts uh what does it do to the materials and so forth and and so to really model this ahead of time and then test it over these Cycles is the key so in the shuttle the big problems the two major subsystems in the shuttle in the Orbiter in particular that that made it expensive uh and the difference between the top picture and the bottom picture was a the TPS the thermal protection system you know every uh every tile has a had a different curvature and geometry so really inspecting every tile uh replacing tiles making sure that the TPS was CU just one weakness in the TPS could you know basically be fatal on re-entry and so TPS was very difficult and uh you know as opposed to an ablative heat shield that you just sacrific the heat shield uh completely and then the other was the main engine the shuttle main engine originally the idea was to only do very deep inspections and disassembly disassembly of the shuttle main engine after the first few test flights and then fly multiple times without having to really reinspect or you know disassemble the engines so they they did the disassembly of the engines after the test flights and then they just kept doing it for every flight where the or original intent was to only do it for the test flights and so the main shuttle main engines were the second big cost driver in operations so in in reusability is is a great idea but to the degree to which reusability actually happens in reality that's you know that's really about detailed design decisions you make okay so uh let me move on then um I want to mention is you know when it comes to really understanding um life cycle proes I do want to point you to the 15 28 the 15288 standard which is um the iso 15288 standard which has a fairly complete list of system life cycle processes so this is essentially the system and software engineering system life cycle process standard that has a whole range of processes that are that are described in quite some detail and they're not just the technical processes shown here on the right side uh we focused a lot on those technical processes in this class the stakeholders the requirements the architectural design which is essentially the conceptual design but there's also the project processes right executing the projects agreement processes so this would be negotiating uh contracts you know Supply contracts acquisition contracts and then all the organizational things you need to do to create the right organization to execute these projects so what's nice about the uh 15288 standard is that it's it's a pretty broad standard okay so let me talk about the Iles of the life cycle properties in particular uh what they are and how uh we can describe them and how especially how they relate to each other and so the paper there two papers that underly this lecture that you can read to get some more detail and this is one of them so this paper is called investigating the relationships and semantic sets among life cycle properties and we we published this about 3 years ago in Del at the uh seisan conference so um the the background here is the following is that you know as as you've seen complex Engineering Systems live for decades or some of them even for centuries and the the IL by Ides we mean properties of systems that are that are not the primary functional properties in software engineering they're often called nonfunctional properties and the the thing that's tricky about the Ides is that you often only can observe them during operations so you know you can test systems in the short term you can you can see does it work does it fulfill its function but whether certain ilities are present it often only shows itself over time um and and so um most of the research that's been done in sort of quantifying the ilities has been looking at these properties one at a time so the questions we wanted wanted to go here uh wanted to answer is you know is which of these life cycle properties are more prevalent than others you know the top 20 and then especially what's the relationship among life cycle properties do they form what we call semantic sets and then you know how could you use this information and so here we're going to do this using two different methods the first method is what I call prevalence analysis we're going to look in the literature and on the internet how frequently these life cycle properties show up are mentioned how much we know about them and then the second method is a cognitive method where we ask people to uh give their understanding of these life cycle properties and put them in into a hierarchy and then we'll compare the results so um here are essentially the results from the prevalence analysis and this is ranked according to uh the number of Journal articles uh this is scientific Papers written where this particular keyw like quality or reliability shows up in the title or in the abstract of the paper and you can see this is in units of thousands okay so quality is number one right quality is number one the most most scientific Papers written about quality and you can see it's almost a million Journal articles this is I'll show this to you over time but since 1884 is the first year okay and the databases that were used for this are called it's compendex and inspec uh these are actually combined if you go to a website called engineering Village . this is you know sort of a master database for scientific papers in engineering U that that was the basis for this uh number two reliability number three safety then flexibility robustness and so forth and the other bar the gray bar is if you Google essentially for uh this particular keyword and you know keep in mind this was done about 5 years ago this is the number the millions of hits that you will get and um so you know there's a factor of a thousand difference here between Journal articles and hits on the internet but still you can sort of compare uh the two and so in some sense the black bar is you know the the the totality of scientific knowledge about this life cycle property and then the gray bar would be you know the U amount of information or uses usage of that knowledge at least as far as it's represented on the internet and what's interesting is you know in some cases the the the black bar is smaller than uh the gray bar which means that common usage is leading so sustainability would be an example everybody talks about sustainability you know companies have you know they say our system is sustainable because you know it uses less resources it produces less emissions but the actual amount of uh scientific knowledge the actual amount of research as to what really is sustainability and how do you design for it is actually smaller than the usage and then there's areas where it's the opposite like for example modularity right you see that in modularity um so in a relative sense academic interest is leading there's quite a literature on modularity in mathematics you know modularity in software modularity and design but most people you know modularity per se is not so interesting and so exciting to you know the general public so keep in mind that for some ilities uh there's an imbalance essentially between our our understanding of the scientific knowledge our understanding to be design how to design for it and you know how frequently at least that keyword is used um now here this is a little bit more uh this is a little bit harder to see this is essentially looking at these life cycle properties over over time so what this graph shows you is starting here in cumulatively uh the number of Journal articles published about each life cycle property and uh roughly the way you can think about this is that there's some life cycle property the top four five that we've been actually working on for a long time even safety you know there's some interesting articles in the 1890s about safety for example in mines you know coal mines there's an article about how does the impact of lighting better Lighting on safety and productivity in mines 1890 okay and so they showed that just providing better lighting actually has a dual benefit pure accidents pure fatalities and better production output and so you know that that's and this was a scientific study that was done by you know comparing data in different minds and and actually performing some experiments um then you have a group of life cycle properties that we only started really thinking about and Publishing about I would say around World War II such as usability uh maintainability and my interpretation of this is that especially during World War II a big difference made Logistics you know how how easy was it was it to use uh different weapons uh how easy was it to maintain equipment uh it became a huge determining factor in the outcome of the war you know for example in North Africa and the North African theater uh you know tanks uh trucks uh being exposed to you know the sand and and so forth so people really started thinking about the military started thinking heavily About You Know M the importance of maintainability in the design of these systems and after the war then a lot of these Concepts like usability and maintain ility started spreading into General civilian life and products and so forth and then the third uh the third uh period or the third type of illes are the the newer ones that we've done research on just since the 70s in the last 30 years so here in this category I would put things like sustainability uh recyclability evolvability even interoperability which means the ability of systems to connect together and work across system boundaries those are pretty recent uh life cycle properties and we're still actively researching them so another way to show this is by essentially making a network of these life cycle properties and so the way this is still all using this prevalence data essentially what you see here is a network diagram that has these life cycle properties in a network relationship let me just explain um the so the size of the nodes relates to uh how many you know how much knowledge we have essentially the the height of the bars that we saw earlier and then the strength the thickness of the line relates to the strength of Rel how closely are these two ilities related and the way this is calculated is using a so-called Tut tupal correlation so you take essentially articles that have you look at articles that have for example reliability and maintainability in the same article right and then you divide that by the total number of articles on reliability and or maintainability and that's a ratio between zero and one and then this graph was produced with a cutof strength of 0.1 right so if there's more than 10% of of Articles list these two properties together then there will be a line here and the stronger the line the thicker the line is the more closely the more often these concepts are co- mentioned in the same in the same article or the same piece of work so it reflects the strength of relationship now what's interesting is you know when you first look at this you you know you don't see much but after you look at this for a while you start to realize a few things first of all in the center of the graph we have the classic ilities of engineering quality safety reliability and I would argue flexibility as well those are the top four that we saw before and then around the periphery of the graph we have newer life cycle properties that are more recent we haven't really uh thought about them too much we don't fully know how to design for them yet and if you look at them in groups I will argue that there are three major groups here so the first group in the upper left is things like maintainability durability reliability quality and so forth so this is all about is the system made well with high quality particularly early in its life right then we have a a group here on the right and this is about is the system easy to change is it easy to change the system configuration flexibility extensibility uh modularity and scalability uh interoperability those are all different subf flavers if you want of being able to modify the system over its life and so that's group abilities that that goes together and then the third one is you know resilience robustness quality um safety um and so this is really related to Performance of the system under different types of uncertainty you know either uh environmental variability or failures in the system and the ability of the system to withstand or at least perform you know have good residual performance even in the face of failures so that's I think an important way to think about it and when you're writing requirements for systems like the system should be resilient then this helps you understand well what are the other properties that are linked to it and that maybe support that okay any any questions uh here or at MIT about uh this is sort of method one is get data about these life cycle properties and put them in relation to each other any any questions Yanna are there any questions there yeah I have a question so is this chart that you have up now actually used in the initial design and like conception conops to like try to tease out the interdependencies or is it too abstract and just more of a educational tool it's really at this point you know this is fairly recent this was just done in the last few years so this I don't think this has fully penetrated system engineering practice yet but the point the point that I I want to make here is that there's a huge there's a huge gap right now between when people in the you know in briefings you know either at the the Pentagon or you know at a corporate headquarters say you know we want a sustainable product or we want a you know we're a software company we're do we do Optical networking and we want we want to have the most resilient design of the IND history you know they'll put that as a goal for the project the question then is well okay that's fine but what does that really mean what does it really mean resilient you have to operationalize that definition such that you can derive from it lower level requirements that you can actually go and designed to that you can test for so um what what what this helps you to do is understand what are what are maybe supporting that what are supporting um elements that will that will will be related to resilience you know what are what are what are supporting Concepts supporting life cycle properties that that this property that you're looking for uh is linked to right so it's it's an evolution you know we we know how to design now for you know speed and Energy Efficiency and you know the sort of things that were really hard to do 20 30 50 years ago it's pretty standard practice now you know how do you design for optimal interoperability we don't quite know yet but we're we're finding our way and so uh believe is that you know those especially the life cycle properties on the outer periphery of this chart the those are the ones that that need more work and those are the ones that were really learning how to operationalize does that make sense uh yeah thanks that makes sense okay good so uh let me talk about method two so this was basically uh trying to get to how how do people how do humans there sem semantics means the meaning of words right semantics is the science of the meaning of words how do they what how do they interpret these life cycle properties and um and so here humans have a deep and possibly varied understanding of the semantics of these life cycle properties so what was done here was that a list of 15 life cycle properties many of them are overlapping with the ones you saw earlier were presented um and then the challenge was to the the question that these uh four groups 12 12 participants four groups had to say is there a hierarchy here you know are some higher level life cycle properties are some of them lower level properties that support these so there was round one find the parent child relationships described these interviews and then a second round so here's essentially um what was what was the results of the first round so four different groups so each of them basically uh came up with its own version of a hierarchy they didn't talk to each other they were firewalled from each other and in all cases this notion of value robustness came out on top so value robustness means the system should deliver value right to the stakeholders despite uh failures you know environmental changes so value delivery of the system is the top the most important thing then what's different here is you know how do you achieve that uh so for example you see group one had uh robustness uh and changeability so robustness typically means even if you don't make any changes to the system it should continue you know it should be survivable it should be versatile uh changeable means to actually modify the system over its life and then we have lower level properties like modularity or reconfigurability and you know there were differences between the groups but um not not as big as you might think so in the second round as a result of the second round then this um so-called means to ends hierarchy was constructed so means means you know these are the enabling life cycle properties and ends means this is the final soort of this is really what you want to achieve and so this was the result of that uh so again at the top we have this notion of value robustness and this is achieved by a combination of survivability robustness and changeability and then these in turn are achieved by lower level life cycle properties and then at the lowest level we have things like interoperability modularity reconfigurability and so forth okay and the the difference between the solid line and the dash lines here is that if it's a solid line and these are directed arrows that means that three three or four out of four groups right so the majority of groups had this as a particular CH parent child relationship and if it's a dash line it means only two out of four and if it's only one out of four it's not shown here okay so this is kind of a combined results across the four groups so so um you know what do we take from this so first of all life cycle properties are absolutely critical you you'll find this in in in mission statements you know you'll find it in even in advertising right a lot of companies say we have we have a robust solution we have a sustainable system we have resilient networks so it it really is a huge huge selling point it's very critical what I encourage you to do as system Engineers is take a critical look at this and say well what does that how resilient is resilient you know how many subsequent failures you know of the system can you tolerate uh sustainability what does it mean in terms of you know kilowatt hours per hour of usage you have to get down to the details to start quantifying what these life cycle properties mean and compare them among systems uh despite differ differences this these two methods that we just looked at so one is the prevalence analysis and the other one is the um you know the the human semantic exercise despite differences the high level conclusions were similar so some ilities are closely related to each other and form semantic sets meaning they're they're tied together by both synonymy or polyi relationships synonymy means they essentially mean the same they're synonymous they essentially mean the same thing polyi means it is one word but it means can mean have possible sub meanings right so so uh the idea of groups of of semantic sets and um those uh those groups essentially are robustness so this is the ability of the system to perform its job despite either internal or exogenous disturbances right that's robustness flexible or changeable which means that um you can modify the system easily you know if you operate the system for a while and then you realize ah I I need the system to do something else or I need to ad adapt it or make it bigger or make it smaller or add some functions so you would modify the system and that's flexibility or changeability and then resilient and survivable is very specifically the ability of the system to continue performing despite uh failures or attacks right that's what resilience really means and so those seem to be the big three semantic clusters that we see in life cycle properties and then the the third Point here is there appears to be a hierarchy of life cycle properties with two or three levels where we have lower level properties of systems like modularity for example really your customer probably does not care about modularity right if you advertise the modularity of the product you know some of the more educated uh you know some of the more technically Savvy customers they may understand what that means but most of your customers really won't appreciate modularity because it's kind of a low-level form oriented technical property of the system but what what they will appreciate is the ability of the system to be reconfigured or adapted for different uses right so interoperability modularity Etc are lower level life cycle properties that act as enablers of the higher level life cycle properties okay and so future work uh future work in this area is to both um to you know to apply the these methods to a broader set abilities larger group of test subjects you know more data but also from a practical standpoint to to operationalize better how do we write requirements how do we actually design for these life cycle properties like resilience you know flexibility changeability how do you really design it so go from the keyword to real engineering by operationalizing the the the subattributes or factor of these systems okay so that's uh quick summary of life you know I could go on for hours about this uh this is I'm very passionate about this topic and I will say that um you know people who have dealt with large complex systems you know whether you're operating the transportation system of a city or you know in Airline operations or you know you're running the it infrastructure of a major corporation these words uh these words are real these words are you know real dollars real challenges this is really where the action is in in a lot of these large complex systems okay any any comments or questions about uh life cycle properties how what they are how they relate to each other why they're important bulker did you want to maybe say something okay all right so let me talk about uh try to make this a little more real uh let me talk about a case study very specific case study about uh communication satellite constellations and this is the second paper uh that underlies today's lecture and so let me give you a little context for this first uh so this this uh work here was originally published uh about a decade ago in March 2004 and that was a few years after um idium and Global star these are the two well-known communication satellite constellations had had been launched and I have to give you a little context when I first came to MIT in the mid 1990s there was a huge interest in this area of satellite constellations you know commercially scientifically um you know in fact you know the the impression was there were so many applications for satellite constellations filed that you know we're going to have so many satellites up there you won't even see the sun anymore right it was just like thousands and thousands of satellites and irium and Global star were really the first two constellations that were fully developed launched and both of them uh failed commercially within a really short time a couple years and so after this happened the whole market and interest in Satellite constellations collapsed uh for a long time uh until about you know few years ago two or 3 years ago now you know people talk about these constellations again you know uh constellations of cube sets uh uh iridium next you know there's sort of a new enthusiasm new wave of enthusiasm for constellation so um so this this paper and this case that I want to tell you about is really about the first wave and in terms of the Ides what I the the one that I'm that I'm um that I like to explain to you about here is flexibility and scalability rather than thinking about a system something that you design and build all at once how do you design a system such that you can gradually deploy it we call that stage deployment approach and what's the benefit of that okay so here's some uh pictures uh you see on the right side this is what the original aridium satellites look like they actually use uh phased array antennas uh they actually both use phased array antennas so you have individual elements here and by differentially phasing the signal you can actually steer the beam so this was a very new technology at the time and then the global Star Satellite uh shown at the lower um the lower picture here's a little bit of data uh both of these were launched in the late 1990s uh idium has 66 satellites Global star 48 uh idium is a polar constellation so the satellites go almost directly over the poles um global star is a walker constellation so it's inclined doesn't quite give you full Global coverage it's about plus orus 70Â° so you can't use Global star at the poles uh the altitudes are a bit different too idium is at 780 km while uh Global star is at 1,400 which um um who's I know at least one or two of you are working on the Van Allen the Van Allen belt mission right the cube sets to measure the Van Allen belts somebody mentioned that today who was that no did I hear that wrong it was mentioned right so who is that oh no and he's not here huh you see so 1400 kilomet you're actually pushing you're starting to push the lower edges of the Van Allen belt so one of the big you know in in in some sense it's easier to be higher because you need fewer satellites to cover the whole Earth Earth but the higher you go the more exposed you are to the radiation environment you get closer to the Van Allen belt so that's the big tradeoff there you know the mass of these satellites they're between you know 450 and 700 kg transmitter power around 400 watts you know which is not that much if you think about it right that's four very strong light bulbs uh the old style light buls um um and then what's very different again is the multi- access scheme so idium used uh time Division multiplexing and Global star which was supported by Qualcomm used essentially CDMA so you don't chop your frequency band into separate channels you use the whole the whole frequency band and then you then you use U pseudo random access codes to essentially uh deconvolve the signal uh for each Channel um the number of channels about 72,000 about 120,000 duplex channels so that you can actually duplex means you can carry on a two-way conversation as opposed to just a asynchronous and then you can see the data rates quite low right per Channel 4.8 uh kilobits per second 2.4 4.8 9.6 for Global star it's enough for uh having a conversation and total system cost idium was about uh $5.7 billion and glas star about 3.3 three not including the cost of the ground stations both went bankrupt relatively quickly after they launched however they've been operating really since then right since this time idium next is currently under development and is scheduled to launch in 2017 uh Global star is publicly traded and actually valued as a company at $ 1.9 billion so they're actually both you you could almost argue that they they were at a decade ahead of their time and um and I want to tell you a little bit about the story of especially idium so here's a couple of um these are press releases or things that have been written in the Press so look at this one 26th of June 1990 Motorola unveils New Concept for Global personal communication base is a constellation of lower low earth orbit cellular satellites August 18th 1999 9 years later last week rum LLC filed bankruptcy court protection lost Investments are estimated at 5 billion okay so question is why did it happen because the technology actually worked quite well it was not a technological failure it was a business failure but I would argue a systems failure to think about the problem differently so the the fundamental challenge is to to properly size the capacity of a lart system so if you're designing you know a car factory a new power system a you know and it's for a future uner demand it's very difficult to do this because uncert demand is uncertain so is it better to oversize the system or are you conservative and you make it smaller uh Market assumptions can change right in those seven to eight years and uh between so essentially the V right getting back to the V each of those two systems idium and and global are it took them essentially a decade almost 10 years right to go across the whole V and when you make you know your requirements your stakeholders all the stuff in the upper left portion of the V when there's there's so many years that elapse between when you make those assumptions you write those requirements and when you actually go to market a lot of things can change and that's fundamentally the challenge here uh just to illustrate this for you um showing some data this is uh cellular this is not space-based this is on the ground cellular subscribers for mobile phones it it's I know this is hard for your generation to sort of understand this but we actually didn't have mobile phones or they were these clunky bricks you know in your car you know it's it's really um remarkable what's happened so look at this data in 1991 right which is when when the system was just being developed there were less than 10 million mobile phone users in the US it just wasn't it was very expensive it was just not not widely uh that technology hadn't been the networks weren't there you know um so the the the green Bars were the forecast that was done in 1991 as to how quickly the mobile user Market would evolve in the US so their prediction was that by 2000 a decade later there'd be just shy of 40 million users okay now the dark blue bar is the actual Evolution so actually by 2000 you know the US now has 310 or so million inhabitants um so by 2000 there were 120 million users so the forecast that was done 10 years earlier was off by a factor of three the ground the terrestrial mobile networks uh veloped three times faster than had been predicted now that's great for the terrestrial people right the AT&T and Comcast not not Comcast Qualcomm Etc but the problem is of course that because you know groundbased Communications was so much easier a lot of the market that had been anticipated for the satellite based Communications was essentially uh eaten Away by uh you know by this competitor by the groundbased competitor go ahead may I intervene here just so that uh yeah okay uh can you still hear it are you still with us yes yes you are okay vulker wants to say something actually the figures that you showed o about American Statics and for once Americans did not look over the pond because at that time in Europe the GSM systems came really strong and very quickly in the early '90s you could already have your sim card and your telephone yourself and back in America then you could only buy the telephone with all the SIM card in you had to buy the whole system and so as the system was controlled by the large companies and not by the users the large companies Motorola thought that they could impose their satellite system on the market and in this case contrary to the uh uh video recorders Hi-Fi systems earlier uh Europe went much f faster in America and actually the European GSM Market cell phones as you know them went much quicker and then Nokia which is actually European overtook and it took probably the next decade for Americans to catch up so finally here this was then a combination that people had a product and they just didn't get in the US so they were limited to their own market and not to sell their products to the rest of the world as they had done in the past yeah good point so and in in fact you know one of the one of the things that um that um I think both Global star and aridium had to do and this was a very late decision like just in the last couple years here before launch 97 98 is to make their their handset dual use right that if you were in an in an area where there was a cellular network the phone would switch to that because that would be cheaper and if if you didn't have cell your network access it would automatically try to communicate to the satellite so there was a lot of there were a lot lot of uh issues that came up by essentially the markets I think in in in in Europe your point wker about the GSM and in the US just developing quite differently than had been anticipated okay so I want to give you a little bit about sort of Economics of in the end a lot of this is driven by uh by money by economics and so this is you know satellite economics 101 um the key question here is how much how expensive is it what's what's the cost and then what is the price that you can charge for one minute of service right one minute of one unit of service and so look at this equation here here this is cpf stands for cost per function um and so in the numerator we have the life cycle cost which is your initial investment uh and then we uh we essentially capitalize that with some interest rate K plus then for each year of of operation you have the operational cost for that year that you have to add so that's your initial you know that's your development cost your manufacturing cost for the satellites including the launch cost and then the Ops cost would be operating your ground stations your networks any replenishment cost they would be in your Ops cost and then you divide this by what's called here the number of billable minutes that doesn't mean you're actually going to build people it means just the are potentially billable minutes so that's the capacity of the system C Subs times you know 3 365 days * 24 hours * 60 Minutes times uh what's the load Factor so the load factor is essentially the the capacity utilization of the system that you anticipate okay so that's that's the basic equation for calculating uh cpf cost per function plug in some numbers here these are the numbers that had been assumed you know a $3 billion investment 5% interest rate $300 million per year of Ops cost 15year life cycle this is the capital T over 15 years 100,000 channels that's your capacity number of users in this case uh so the the the load factor is simply the number of users times the average activity per user in this case assumed to be 100 uh 1,00 minutes per year but 100 minutes per month okay uh so that gets you um that gets you a cpf of 20 cents per minute and based on that you can you can uh you know the business case was made made based on these kind of numbers um and and so 3 million users these are 3 million subscribers right not 3 million users at the same time that number would be much smaller that number can't be bigger than 100,000 because that's your capacity so for example if you if you run if if you run a fitness club right uh at any given time in your Fitness club you can maybe have I don't know 50 or 100 people actually working out right but your number of customers or subscribers should be you know a th000 or 2,000 and if they all show up at the same time you're in big trouble right so that's the big that's the difference between number of users or number of subscribers and number of active users at any given time that's that's a big part of um man in these kind of systems however uh what actually happened is the number of actual users grew much much slower so if you plug in some different num let's keep all the numbers the same except for the subscriber base the number of users in this case we're going to assume 50,000 users instead of the 3 million this is closer to what they had after you know about a year of operation now your cpf goes to $12 per minute which is non-competitive is right except for some extremely you know like military applications or you know making some emergency phone calls on an oil rig you know on the ocean you know most people at the time now and at the time would never pay this uh uh for for one minute of service and so um that was the fundamental problem is that the user base did not materialize as fast as planned therefore this cost per function was way higher and they did charge you know $3 to5 per minute of usage of the system which uh you know as you try to squeeze your existing users more uh you're not going to get the the ramp up and the scale up in the system that you need that was the fundamental problem with with the economics of these systems so let me talk a little bit about you know the design decisions the conceptual design uh of what this design space looks like so fundamental um oh the other thing I should mention to you what was interesting is both after the bankruptcy both of radium and Global Star both of the chief Engineers for both of these systems uh took Refuge at MIT essentially they came like uh Joel shandal who was the uh chief engineer for Global star great guy both very competent people uh came to MIT still there he's still a professor there and then Ray Leopold was one of the three architects for idium also came to MIT during this time so I had extensive discussions with them and what you see here on slide 25 is one result of those discussions which is you know the the key design decisions that they had to make uh fundamentally there's uh you won't be surprised to see this this is the magic number seven right seven key design decisions when you design a satellite constellation for communication purposes constellation type polar or Walker orbital altitude minimum elevation angle above the Horizon that you can communicate satellite transmit power the size of your primary antenna your multiax scheme so this is time division or uh code Division multiplexing and then the last one is about the network architecture do you have inters satellite links yes or no inters satellite links means satellites can talk to each other uh and iridium chose that um Global star did not so in global star the global star satellites cannot talk to each other directly in space they can only talk it's a bent pipe system through a ground station okay so if you this is like the morphological Matrix that you learned about you just pick uh design decisions in this morphological Matrix and you come up with an architecture in fact a full factorial search of the space base would reveal 1440 different satellite architectures um so what's your that's on the input side what's the output Vector what do you care about in terms of output of the system well first of all performance um so the performance of this is for voice Communications it in a sense it applies to data Communications as well is your data rate per channel right 4.8 uh kbps your bit error rate like what's the average number of bits that are wrong 10 Theus 3 that's actually a pretty uh not a very that's not a very stringent requirement and the reason is you're just this is for voice communication this is not for sending you know commands to a spacecraft going to Mars if you send commands it would have to be you know 10 theus1 or 10- 9 much much better bit error rate but this is okay for for voice and then the link fading margin 16 uh DB this is the strength of the signal uh which will which will dictate whether or not you can use the phone under trees or in buildings uh so you want this number to be higher but the higher it is you know given the power you have on the satellites the fewer channels you have so there's tradeoffs so what was done here is to keep the performance fixed and so that you can compare architectures you know compare apples to apples then we have capacity which is the number of simultaneous uh duplex channels and then finally the life cycle cost of the system which includes research development test and evaluation satellite construction and tests launch and orbital insertion and then the operations and replenishment so if a satellite fails in orbit either you have to already have prepositioned the spare or you're going to launch you're going to launch a replacement and this actually happen uh in both cases for both constellations so um in order to then you know connect the input to the output you need to build a simulator or a model of the system and this is a high level view of what that model looks like so we take our input our design decisions and certain Constance that we assume a constant vector and Cascade this information you know for the constellation module takes the altitude and the minimum elevation angle and produces uh T is the number of satellites and ke is the number of orbital planes so this is sort of orbital Dynamics the spacecraft module calculates the satellite Mass um the satellite Network um you know builds essentially the communication communication Pathways the link budget will calculate the capacity of the system you know given given those performance constraints the launch module will determine uh how many launches do you need from where and then the cost module basically calculates the total cost of the system the life cycle cost and finally you get essentially a tradeoff of life cycle cost versus capacity of the system um let me just show you some you know you say well that's tying together a lot of information into a multidisciplinary model so what kind of information do you have you have a mix really of physics-based models so this is a very well-known equation the EB knot this is the energy bit over noise ratio this is a closed form you know physics based equation that tells you how much energy is there per bit uh what's the signal to noise ratio on a per bit basis and this is a function of you know transmitter power receiver and transmission gains various losses in the system and then some of the equations are some of the information is empirical for example the relationship between spacecraft wet mass and payo power you know if you wanted to have a closed form or you wanted to have more detail you would have to almost build like a cad model right a separate model you'd have for each of these 1440 architectures you'd have to manually construct an individual um detailed design and that's not feasible so what you do instead is you use some prior data and you can see here uh a a scaling relationship it's not perfect but we we can um you know we have air bars we know how good it is satellite Mass satellite uh wet mass as a function of transmitter power and in this case propellant Mass okay so two kinds of equations bench marking uh so once you have this model you need to ask the question how can I trust this model does it give me reasonable reasonable answers and in this case uh benchmarking is the process of validating simulation by comparing the predicted response against reality so just quickly here showing you four kinds of data so one of the simultaneous channels of the constellation this is the prediction of capacity uh you can see it's pretty good uh in this case the model is actually a little conservative so the blue bars are the actual or planned capacities the red ones or magenta is simulated and you can see that the simulation underpredicts slightly the true capacity of the system um life cycle cost you saw that idium was just a bit more than 5 billion uh global star was between three and four and so uh here we're in the right Ballpark and then in terms of satellite mass and the number of satellites in the constellation required we match that very closely and the reason for this is fundamentally this is just geometry right if if I tell you the altitude and the minimum elevation angle and I'm telling you I need I need Global coverage and I want dual redundancy so you can always see at least two satellites it's just geometry to figure out how many satellites and how many orbital planes you need and that's why the model or the simulation and reality match very very closely so this gives you know some confidence that that that these results that this model is is reasonable so what you can do with it is now what we call Trade space exploration which is in a sense what you did uh what you did for the canat competition so this this picture here this graph shows you the life cyle cost right over those 15 years versus global capacity of the system each of these blue dots represents one of those 1440 architectures and you can even see on this uh where iridium actual versus simulated Falls and where Global star actual versus simulated Falls so both of them were actually off the Pito Frontier which was interesting and led to quite some discussions and uh you can the pero Frontier in itself is a of course very useful uh one of the reasons that idium is not on the Pito Frontier is that fundamentally polar constellations are inefficient and if you think about it when the satellites are crossing over the PO uh they're very close to each other and they're actually not Crossing exactly over the poles or you you could actually Collide right so you you offset them slightly to avoid collisions but um uh some of the satellites actually get get turned off when they cost the PO so you're not utilizing your assets super efficiently in a polar constellation which is one of the reasons why they're not on Theo front now the way you would use this in a traditional system engineering approach which is traditional system engineering approach means give me the requirement right write down the requirement and then find the minimum cost system for that requirement so a requirement could be you know we need a capacity of of you know 50,000 for example and then you find this intersection with the pedo front and that's the system you that's the minimum life cycle cost design that gives you that capacity right and that's the system you take and that's what you go and build and that's essentially what they did the problem with this if there's High uncertainty is the true requirement is kind of unknown right the market will tell us in the future what the true capacity should be so how do you deal with this well you could be oh we we're going to be on the safe side right we're going to be um uh we're going to oversize so if the true demand the true capacity that you would need is higher than what you've had your your system is going to be under capacity right your system is going to be undersized for what it should be and so what happens in practice uh what do you think happens in an underc capacity situation what would you say your system is too small relative to the market demand you're not capturing as much as you can meaning a competitor will probably capture it I mean I guess you can you can change your price policy but fundamentally you're missing out on Market opportunity right the other situation which is actually what happened is that the demand is much less than you anticipated your system is oversized and remember once you launch a satellite constellation it's not like a fleet of taxis right where you just park them in a in a in a in a parking lot uh you've already the fixed cost is very high so um if if if demand is below capacity you have this all this investment here life cycle cost has been wasted right because you're oversiz the system and and the challenge is that the true requirement is kind of like this there's this probability density function right there's a it's a probabilistic requirement essentially and and there's no set of experiments because it's aloric uncertainty it's not epistemic uncertainty there's no set of experiments you could do today that will help you refine the requirement now they did do Market studies uh and but again those Market studies were years ahead of when the actual system was launched so they were unreliable fundamentally so um what I'm arguing here is in this kind of system where you have large uncertainties as to the true requirement you shouldn't just guess and then put billions of dollars on a guess this is like playing in the casino right essentially what you should do is think about the problem differently and in this case the answer is stage deployment or one of the aners is stage deployment so you build the system such that it can uh adapt itself to the uncertain demand so uh you build initially a smaller more affordable system but and then the system has already built into it the flexibility or scalability to expand if needed but only if needed and there are two major uh economic advantages to that one is that you don't need to spend all the capital up front right so you're deferring capital investment and you retain the decision freedom to reconfig or not and that's typically what we call a real option You' created a real option uh for the future so the question then is well how valuable is that you know is it worthwhile doing this stage deployment approach so um there's probably different ways of doing this and I just want to share with you and this is from the paper how this was done so step one is you partition Your Design Vector so you basically decide which which part of the system is flexible cuz you can't make everything flexible typically what part is flexible and what part is fixed is the base you can't change it so in this case uh excuse me the idea is that the satellites themselves the design of the satellites their transmitter power their protocols their antenna size the network architecture should be fixed right because those things are difficult to change uh but we're going to allow the astrodynamics the actual shape of the constellation to be flexible so keep the satellites the same only change their arrangement in space so what you do is you partition the design Vector into the flexible and inflexible part and when you do this in step two what's nice about this is you can then actually find families of designs right you can find families of designs where the we do have a little lag here the the you can find families of designs that share the constant part right that have the common so what I'm what's shown in this graph here this is again our design space kind of zoomed in a little bit more every one of these points that's connected with these lines uses the same type of satellite same transmitter power same antenna diameter and all of them have inner satellite links so so what effectively you're partitioning the design space into sub sets that share common features and then we call this a family of designs and the idea is you start small so on the left side and then you grow the system over time right but only if needed so when you do this your 1440 um um strong design space turns out to be decomposable into 40 different paths now which path would you choose here well want to be as close to the Pito front as possible so this is an example of a of a path in this in this path if we started on the lower left we would start with 24 satellites and then uh we would actually move them uh to a lower altitude and add uh 30 more in step two and then you you would gradually grow the constellation over time you see that this is actually not too different from how GPS was done right the GPS constellation was was deployed in phases but it was not Market driven it was just sort of phasing the development for for risk reduction regions and spreading out the capital um the other thing that's interesting here is you see as you grow the constellation it moves further away see it moves further away from the front so when it's small this particular path it's close to being optimal and as you scale it up it becomes more suboptimal and there's other paths that have the opposite Behavior where when it's small it's kind of suboptimal and as you make it bigger it gets closer to optimality pretty interesting so that's step two find the paths step three you now have to model the actual uncertainty and in this case uh there are different ways to model uncertainties uh in this case what we used was a GBM model geometric brownan motion this is well known in physics uh it's been applied to statistics physics this has been applied to the stock market right and and the idea here is that uh in this case demand uh behaves like a particle in a fluid right that's that's kind of moving in an unpredictable fashion so this is the basic equation for this is the discrete version of of geometric Brown and motion you have um some Trend mu mu * delta T so the this is the delta in demand the change in demand divided by De demand so this is normalized change in demand is Mu * delta T so your your Trend time delta T plus uh Sigma * Epsilon * square root of delta T Epsilon is a standard normally distributed random variable between 0 and one and uh Epsilon uh and and sigma is your volatility essentially right so here's here's some examples if you start with with an initial demand of 50, ,000 which is what they actually saw you know as a as an initial demand early on you have a growth of about 8% per year this is your Trend Plus in this case a volatility of 40% per year which seems high but you know this kind of very new technology new systems is actually not that far-fetched you get three different these are there three examples of how demand might evolve these are just three scenarios so demand can go up it can go down and GBM is very nice but one of the downsides of GBM is you know there's infinitely many scenarios that you can generate because it's fundamentally even though it's discretized in time it's not discretized in in demand so a simpler version of this a more discret version is a so-called binomial lattice model so the way this works is again you start with some initial demand here on the left and then uh you've disc IED it such that um moving through time you can look at different scenarios right these scen scenarios could be you know the best scenario is things just keep growing right grow grow grow grow grow here uh time the 15 years have been subdivided into five three threeyear periods so here's a sample scenario demand goes up goes up and then it goes down twice and then it goes up again in the fifth period And so this is is a discretized random walk and you can choose the numbers so the the sigma the volatility the amount of the up and down movement and the probability P of going up and then the probability of one minus P of going down are chosen such that they're consistent with the GBM model so this model is statistically equivalent to the GBM model it's just discretized so the beauty of this is that you can now simplify this to 32 different scenarios instead of infinitely many and each of those scenarios is not equally probable the pro there actually probability weighted scenarios depending on what mu and sigma are that are underlying right so now we have 40 paths right we have 40 Evolution Paths of the system and we have 32 different future scenarios so what we do now in the next step four is to put the two together and calculate the cost of each path right calculate the cost of each path with respect to each of the demand scenarios look at the weighted average of all the possible paths and uh the one thing the trick the one tricky thing about this is you need to build in a decision rule such that if the system if demand exceeds capacity the system you're going to expand and move to the next stage you're going to Stage deploy the next phase uh and and there's many different ways of doing this decision rule so the the simplest one was chosen here and of course costs are discounted so let let me explain to you how this works so you know we we start um this simulation so the first uh initial deployment this is our initial stage one right for the constellation and then we start operating for uh two years demand in this period goes up you see the Ops cost in this case goes slightly down this is due to the discounting this is a discounting effect we arrive at the end of our first threeyear period and then the question is uh here's our capacity of the system what would you do in this situation let me ask somebody at MIT make sure you guys are still are still with me so what would you do in this situation you you're now at the end of year three anybody we lost you for a second halfway through your statement could you repeat yeah so you've you um you've done your initial 3 years you've deployed your initial constellation and you now have the situation shown on this chart what's the what's the right decision according to the decision rule what's the right decision okay I would keep it the same because Haven exceeded Demand yet but prepare to exactly you wait right because your is below Your Capacity so you keep it the same you don't do anything you just keep operating it's exactly right so um you have another three years right um during these three years uh demand keeps growing and you now at at year six arrive at this point here so what's the right decision now can I ask a question about the decision to stay the same would you stay the same what what's happening to the demand line Veronica what's happening to the demand can I ask a qu a quick question first about the the decision to stay the same yes I feel like by waiting until we've exceeded demand to choose to expand the constellation we're introducing a lag between demand and capability that creates an inefficiency that may actually drive users away from the system and I see this kind of oscillatory effect where you would expand and then people would meet and then the system would be insufficient and then people would move to a different system and You' kind of have a yo-yo around the the maximum carrying capacity and that doesn't seem efficient to me so I'm wondering if you could speak to that yeah so what you're what you're ESS essentially you're arguing for is what I would call an anticipatory decision rule so you would actually so the decision here is to deploy right you got to deploy your second stage because you've saturated the system what you're arguing for um is to before this occurs you know like at 80% saturation that's when you trigger the next stage right in order to anticipate the um the saturation and so absolutely you could do this you know there there are many different decision rules that you can try when you design a flexible you know Deployable scalable system that's part of the decision space so in this in this particular example uh you know we just implemented the simplest possible decision rule which was at when saturation has occurred you do deploy the next stage but not before okay thank you is that is that clear yes thank you okay so we we deploy the second stage now and you can see there's another Spike here right of of capital that's needed it's not as high as the first initial stage but it is substantial um so now as as we watched the as we've deployed the next stage our capacity went uh to the higher limit so we're now at capacity level two uh we're now operating the system uh demand keeps growing that's good now we're at year nine uh you can see but we haven't yet saturated our new capacity so again the optimal decision is is to or I shouldn't say the optimal decision but the decision according to the rule is you just wait right you keep operating ah now from year 9 to year 12 demand starts to go down right and this happens in some systems right you they growing and then they Peak and then things go down so in this case we wait and then in this scenario it goes down again you see how that works so what's nice about this is you can now in in this in this sense all the 32 scenarios of possible Futures you can run them against the 40 different families of designs or Evolution paths and out of that you can find the best path right this is the path the path that on average I should point out on average will satisfy your demand at minimum life cycle cost given the uncertainty model that you've imposed yes why did it go down this this here no the capacity stays the same you don't go down in capacity the reason is this is a good question um this particular system a satellite constellation is dominated by fixed costs right so it you wouldn't retire half your satellites because then you would lose coverage also potentially right because you move you move move them to orbits that that give you the right coverage you just increase your capacity this is different from a system that is dominated by variable costs like for example if you operate a fleet of taxis you know and and for whatever reason demand goes way down you can go and park half your taxis and just not operate them you may still have to pay a lease on them but essentially you adap you can downward adapt the capacity of VI system you can't do that here it doesn't make sense right so that's the big difference between systems that are fundamentally um fixed costs dominated versus variable cost dominated so in this case we you know we just don't use it as efficiently in the last six years so the answer here is um the answer here is that there's an optimal path and that's shown here that will um for a given targeted capacity you you can then you know you can compare essentially this Loop path against the traditional fixed design and in this case you know the traditional architecture fixed architecture would cost about $2 billion to build and on average the red point is the average life cycle cost of the evolvable system is 1.36 Right life cycle cost of the rid design versus the expected life cycle cost of the best best deployment strategy or stage deployment strategy which is um essentially the way that's calculated it's the probability weighted life cycle cost of each of the scenarios right against this path and the difference between those two numbers is about a third $650 million and that is the value of the real option that's the value of Designing the system with scalability with flexibility okay yeah yes uh that's true uh so the lifetime the question was is the life expectancy similar of course so the 15-year uh life of the whole system isn't is the same for both of course in the stage deployment strategy um some of the satellites are going to be younger right at the end of the 15 years and so they may have longer residual life left which is actually not included that's not even included in this uh real real option value [Music] y right that that's right so you could you could sort ofine this model to include a more a stage decommissioning right or stage transition to a Next Generation system so in this case you know in this model after 15 years boom you know everything you just finish uh no more revenues you just decommission a hard end a hard stop after 15 years but you could actually build you know transition and de commissioning models as well good point so so that's that's essentially the the case study I wanted to show you and you know you'd say oh well that that's a you know that's kind of like uh in in the US we we talk about Monday morning quarterbacking you know you you you comment on all the football games that happened and the M mistakes that the coach has made and I would have done this or I would have done that so yes you know this is sort of like looking at this problem in hindsight but the reality is that that um that uh this really has had a big impact and a big dampening effect and that the the new generation of systems I think are built much more intelligently you know with this kind of evolution in mind um I will also mention to you that idium is actually the system that went bankrupt first and the reason for the bankruptcy well one of the immediate reasons for the bankruptcy is because the way that the project was financed so about onethird of the funds those $5 billion onethird of the funds for idium came from um Equity uh from mola so that was their own money that they lost about onethird was uh IPO initial public offering you know shares sold to the public and about onethird was bank loans and these bank loans were relatively expensive and the banks expect you know that they get paid back at a certain speed depending on you know the market Evolution and and it's essentially the inability to pay for and service their loans that caused the aradian bankruptcy and it turns out that the loans were about onethird of the total capitalization of the project so what I would argue I would argue that if they had done this more flexible staged approach they could have saved onethird which is about the value of the F option and capitalization and just built the system only with equity and with the money from the IPO and that would would have given them a lot more time you know they wouldn't have had to service those loans and it would have given a lot more time to wait for the market to develop which it eventually did um so there was there was the whole financial architecture was poorly done as well which is a whole another that's a whole another question uh Professor quick question there situations where yes yes uh so the question I'm just going to repeat it for you guys at MIT the question was are there are there cases where the flexible approach is not the best and the answer is yes so for example if you're in you know I know that you you're starting the energy energy markets if you're in a situation where you have guaranteed a guaranteed customer you know so the the government or one of the big utilities is entering into a long-term agreement with you that they will buy the power that you produce at a certain price for the next 20 years right um and so you're not subject to that market volatility but you have a long-term agreement including Clauses that say well if if the government doesn't do this then they'll they'll compensate you for any losses you know this is the financial AR contraction if you basically have eliminated demand uncertainty because of the particular contractual Arrangement then there's no reason you shouldn't build the system for a fixed capacity and um because you're then you can build something right on the Pito front because that particular uncertainty has been eliminated for for policy or contractual re reasons so good question okay um so we're almost almost there let me just try to uh summarize some of the please go ahead um on the side before 37 um it seemed like um the probability waiting was for um kind of a a general capacity like uh something that could be adapted over the life cycle but here we're we're pinning down a a specific capacity so why is it the sum of probabilities then haven't kind of the probabilities been realized don't you know exactly what Your Capacity is that you're operating at yeah so so just to be clear on this so the probabilities those P subi they're they come from the lattice model they are determined by the lattice model so what you do is for those 32 scenarios of the future this is the pi right the I is the index is the is the uh for this particular future scenario for each future scenario you start with your you always start with A1 which is your initial configuration right of the constellation you always start with A1 and for each of those future scenarios you then simulate that future and you know if you know for those Futures where the demand does never really takes off and materializes you never actually trigger that next expansion stage and it's actually that asymmetry that gives you the advantage and um it's essentially not the deploying capacity when you really don't need it and um and so you do this for all of the end scenarios of the future and the pi is the probability waiting of each future scenario comes directly out of the lattice model that's not something that you have to manually determine right no I I understand that but here since we've kind of pinned down a capacity already we know what endpoint we're at I'm just wondering how the how the probabilities come into that or is it kind of working backwards now of all the out there on this specific plot I see what you're saying okay so the the left point the A1 point so the A1 point is driven by you need you need to make some assumptions about initial capacity and and then the end point the A4 is driven by how large you made the trade space so you if you could make the trade space you could make the design space even larger right this is a discretized uh full factorial design space you could make it even larger and then A4 would not be the end point this is just sort of a finite size effect due to the size of the trade space okay thanks okay all right so let me um let's go to the you know so the way I want to do the summary here is just go back to the learning objectives you know I always do this in every class you know let's close the loop what did I promise you in lecture one that you would learn in this class and you know each of you you're going to have to decide for yourself you know did I actually learn this did did we actually do this so this is sort of due diligence on the learning objectives and I have one slide on each of them so the first objective was to for you to really understand the system engineering standards and best practices as well as newer approaches number two is understand the key steps in the system Engineering Process number three was uh analyze and understand the role of humans in in the system as beneficiaries designers operators and so forth number four was um being honest and characterizing the limitations of system engineering as we as we practice it today and then uh objective five learning objective five was applying these methods and tools to a real even as not so complex cyber electromechanical system so uh se1 um essentially hopefully know we we didn't check the readings but I'm I'm hoping that you did your readings that you really feel like you understand the NASA system engineering handbook which is our our quasi textbook for this class but there's other uh other standards as well such as the enclos St the enclos handbook which is actually the basis for uh certification so several of you here at epfl mentioned to me that you're interested and certification so I I encourage that most of that is based on the on this uh number two uh I did mention the iso standard 15288 that's probably the best known standard you know ISO is a very located right here in Geneva ISO is a very important organization you know I know ISO standards are not the most exciting things to read but they have huge impact um and so 15288 is is is something that's well worldwide known and and across all all Industries really and then um I want to mention also the European systems engineering standard and fulker and I had a discussion about this earlier today uh this is uh issued by the U European space agency and it is a little there's some subtle differences between the ISA approach and the NASA approach and so for those of you on this side of the Atlantic uh I do encourage you to take a look at that ecss standard as well and then we we augmented this with you know different uh papers and readings and so forth so I want to do a very quick um this is our last concept question for the class so uh here's the link uh tiny.cc Capital SE standards and it's essentially uh I want to get some feedback uh what you think about these standards all outdated and surpassed by the digital Revolution one of you think that thinks that's true uh 90% of you think they're still they're useful codification of best practice um 15 14% think they can be dangerous if you adhere to them too closely um about 60% of you think they're essential reading for any serious system engineer and uh 3 10% said you would never use them as a daily reference book okay so that's that's good I think that's what do you think buker that's a pretty reasonable outcome you think okay but what is true is uh they're going to be referenced in contracts you know if you're if you're um if you're doing you know it depends again on on the industry that you're in but um you know you you're going to adhere to some of these things in contracts and if things don't go well people will actually check whether uh did you do this step did you do that step um so it does have you know real consequences okay great thank you very much that was that was good um step two you know the key steps the V model I think I'm not going to explain it again you know hopefully this is something that you will not forget and just one one point about the V model is you know it does not imply that that system engineering is always sequential you know there's it's possible to iterate between steps and across the whole v as well um se3 stakeholders and value Network so this is the uh the role of humans uh we talked about this very early on uh several of you talked about it during the oral exams here today you know the Hub and spoke versus the um the stakeholder value Network the method underlying this is critical but I do want to mention uh very quickly and this is something we didn't spend a lot of time on as human factors right how Design Systems so that humans can effectively and safely use them you know interfaces procedures um and and so this is a couple slides from one of my colleagues Missy Cummings this is actually some uh in a nuclear plant Kata you're going to like this uh lots of these dials and this is actually a Russian nuclear plant so it has it has a very uh specific layout and so the important questions in human factors are how do you best display status information what tasks do humans do what tasks what level of automation what are the training requirements and this is these are also very important questions and so you can apply you know the human systems Engineering Process very much uh anal analogous to what we covered in class so far so you do Mission and scenario analysis function function allocation and then in human factors you talk more about task analysis but essentially eventually leaves to leads to system design like what is the you know what are the buttons that you push what does the the layout uh look like or the user interface and so forth so you know I'm not I'm not presenting human factors as a separate topic but it the human factors requirements should be built in right from the start and uh essentially functional allocation is sort of the key question here how much is done by automation how much is done by humans and how do you split between the two and one of the most important things to consider here is this Human Performance curve and and we know this pretty well now that when humans are extremely highly loaded you know you have a huge uh workload you your performance goes down but interestingly if you're if you're under challenged your performance goes down as well and you know so for example people that are in power stations or you know mission control centers where nothing is happening for days and weeks you know they their attention goes down and they don't perform at their high level humans perform best at a moderate workload and and this is well known and it should influence how you design the human interface and the split among Automation and humans um se4 system complexity last time I will mention the magic number seven plus or minus two the the real limitations come in when we operate at levels you know three four five and six and the main reason for this is because now a single human cannot understand cannot remember cannot deal with all the detailed components and you need to split the work among teams among organiz ganizations and that creates extra interfaces and complexities okay and just to show you you know iridium we talked about iridium this is a very recent news story this is from October 29th the new iridium next has again been pushed back by four months uh this gentleman here is the uh CEO of the Iridium chief executive for iridium next Mr Dash and you know he's talking here about a particular particular Talis alenia and then ViaSat is one of the contractors there's a particular component that's been giving trouble uh and that's sort of in level three or four in the in the hierarchy and that's essentially delaying the whole system so there's there's an example of how you know the large complexity of the system is is uh is causing issues but they are they are actually a plier to Talis alenia through the through the the whole chain right oh you're saying there's more going on than meets the eye yeah who knows who knows um okay and then finally here I want to mention uh application to a case study you know we used the canat 2016 competition as a quote endquote safe case study uh and I have to say I'm really pleased with with uh how this worked out you had the 47 requirements as a starter uh you improved them you group them uh and my comments about the pdrs that we had a week ago are you know all P all teams passed successfully the PDR you know if this was a real PDR we would have issued a couple rids I think there were some couple of teams were over budget or needed to work out their aerodynamics in more detail but by and large I thought this was an excellent application of system engineering concept went beyond my expectations um several of you mentioned the importance of concept generation you know hybrid use of structured and unstructured creativity techniques uh I know at least one team applied to actually go to the competition and this shows you a couple examples so we had one team here at epfl using a bio inspirate design uh this is an actual seed uh air foil that actually occurs in nature and then uh here's an example from MIT the ralo wing team 7 so um I'm sorry I didn't mention all the teams here but it was really great to see um the application of this in the canet um competition case so uh the last thing I want to do and I only have one minute left actually I'm already slightly over time but I just want to give you some career and study recommendations so first of all you know uh I want to make sure that you knew and I said this in the first lecture this is there's a lot more to system engineering um and systems research than we were able to cover in this class so this class is really what I call a door opener to the world of system engineering if you want to go deeper there are deeper subjects um you know modelbased system engineering is a big Trend system safety and then in the spring I'm teaching a class uh called multi-disciplinary system optimization and it there is actually a WebEx uh access to it as well so it's not going to be officially offered as an epfl class but if individual students here are interested please contact me self-study you know there's system engineering Journal there's i e journals there's also you know for some of you this is maybe a little too soon but uh there's there's professional degrees in system engineering um for example at MIT we have the SDM program system design and management the average age of the students in that program is 33 uh but it is um you know they're sort of coming back to get their masters in system design and management uh and there quite a bit of slown content as well so Finance you know understanding the financial how to build a business case around systems as well uh professional experience you know getting experience on actual projects like at MIT for example rexus you know you got the clean space one project we heard about octanus one solar impulse there are a lot of opportunities or and we also had a couple of uh people here mention that you're starting your own company your own Venture and you know when you're doing that you you have to be the system system engineer you have to understand all of these things interfaces suppliers requirements markets all that has to be integrated finally um you did hear about en cozy we had a quick dial in with them at an earlier session so if you're interested you can join either as a student or professional member and also certification this class was not spefically a for that but if you're interested in this either have enough number of years of experience at the cep level and then finally please keep in touch and last but not least I want to thank all of you um the students at MIT at epfl our Tas uh le le Lou here at epfl Yana at MIT uh the technical staff who's been helping um you know run the technology uh vulker uh thank you and um and that's it so I want to thank all of you uh next week we do have a voluntary seminar on sort of future Trends and Manufacturing uh but again it's not mandatory uh it's going to be in the same place but it's like I said not part of the official class so with that sorry for running a little bit over time but um it's been really a joy to to teach this class in this kind of new format e