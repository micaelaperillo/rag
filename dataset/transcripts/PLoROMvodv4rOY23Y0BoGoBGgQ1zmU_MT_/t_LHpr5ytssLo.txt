This is week two of the systems lecture where we try to leverage the most out of the hardware we have to make uh models train faster. And last week we talked about parallelism within a single GPU and this week we're talking about parallelism across multiple GPUs. So this is a picture you should have in your head. So we have a bunch of nodes. These are basically you know computers that have each have a number of GPUs usually eight and within each GPU there's a bunch of uh streaming multipprocessors or SMS and which actually do the work and you see that in green here are essentially the memory and the communication. So within each SM you have an very small L1 cache um with on a GPU you have u high bandwidth memory HPM which is bigger and then you have these u links that connect the different GPUs. So the way to think about it is that compute has to happen within the SM on these ALUS right and compute needs inputs and uh needs to write outputs and generally the the inputs and outputs are can be relatively far if you're lucky they're on the L1 cache if you're say less unlucky they're in HPM and now this week we're talking about multi- uh GPU and multi- node training where the data that you might need might be across on another GPU you right? So the name of the game is how do you structure all your computation to avoid data transfer bottlenecks because we want to remember keep the arithmetic intensity high. We want to saturate our GPUs make them go um humalong and generally data transfer is going to be a lot slower. So we have to that's going to be the bottleneck. So last week we saw a bunch of different techniques to try to do that within a GPU including fusion and tiling. So the idea basically is that instead of reading and writing from HPM you can load into L1 cache or or I guess uh you know shared memory which is using the same type of um you know uh you know has the same speed um and just work there on your local scratchpad and then write out to HPM um only judiciously. Um and this week we started looking at communication across GPUs and nodes where we have to replicate and shard our models and parameters and optimize our states. And there it's uh the the way we do that will determine the the cost. So here's a kind of a I'm taking a little bit of liberty to put everything in kind of one hierarchy you can think from small fast to big slow. So the smallest and fastest is on a single node single GPU you have L1 cache that's extremely fast but very small and then you have uh HPM on a single GPU and then between um uh GPUs on the same same node we have MVL link and then finally we have you know MV switch and of course this is all in the Nvidia ecosystem. Um so so the idea is that many of the core concepts of minimizing data transfer are really the same but now uh the mechanics are a bit different because um L1 is behaves differently than these kind of envy um switches. Um so this lecture is going to be mostly about concretizing the concepts from the last lecture in in code. There's going to be a few new things, but um Tatsu did an excellent job of giving you an overview of all the different types of parallelism. I'm going to try to anchor it in the code so we can more deeply understand what's going on. Um and then u we're going to have uh I'm going to refer to this standard out file here which is the output of running this this lecture. Um there were some minor issues I'll spare you of where if you have multi- um processing then this uh this framework doesn't quite work. Um okay so this lecture has uh two parts. one. In part one, we're going to look at the building blocks um collective operations which we discussed last time, how this is implemented in nickel and pietorrch and then we're going to do some benchmarking and then in part two we're going to look at actually distributed training data tensor and par pipeline parallelism. Okay, so let's start with collective operations. So collective operations are these primitives that are used generally for distributed programming and collective means that you have um you know many nodes. These are actually quite old from the at least the the 80s in the parallel programming uh literature. Um and generally they provide a better abstraction than trying to manage the pointto-point communication um yourself. So these are really tried and true um um you know primitives that have stood the test of time. So a bit of uh terminology. So world size refers essentially the number of devices for example four and the rank um sort of confusingly if you're used to kind of linear algebra is is actually just refers to device. So we have rank zero rank one rank two and rank three if you have four devices. Okay. So um the collective operations are as as follows. Um so um starting from you know broadcast the idea is you have t0 on one of the ranks and you just want to put it on all the other ranks or all ranks. Okay so that's very straightforward. Scatter is similar but you have four values and you want to put each of the values on different ranks. So each of the ranks get different values, not the same um value. Um gather is the sort of the inverse of scatter where you have each rank having a different value and then you bring them all together onto one rank. Um you know reduce is the same as gather except for instead of concatenating you add them. Um all gather is the same as gather except for you just do it for all the destinations. um gather was just uh rank zero or you know rank one or rank two or any individual rank all gather as you do it for all of them and then finally reduce scatter I couldn't um you know find a a good picture of this so I'm reusing the one from um from last time is um like reduce um where you take a bunch of different values and you um you know add them or perform other commutive operation on them and uh put it on one rank. But like scatter um you're going to be um putting different pieces of uh the vector or tensor on different ranks. Okay. And remember that all reduce is equivalent to reduce uh plus all gather. Um so the way to remember this terminology is as follows. um because it can get kind of confusing like which one's all gather, which one's reduce scatter is that reduce just means you're performing some um you know associative and commutive operation like sum or min and max or average. Um broadcast scatter is the inverse of gather and all just means all destination is all you know devices. Okay, so hopefully this is a review from from last time. Um, so actually any questions before I move on since we're going to build on these primitives? So it's it's useful if everyone understands. Okay. So now let's see how this is actually implemented in um starting with the hardware. Okay. So um here's a classically what uh you know hardware for GPUs looks like. So um this is kind of in the home you have a computer I guess um and you have your CPUs and generally you have your GPUs um on one node that communicate via a PCIe bus. Um and if you have to go connect uh communicate between different um nodes then this is all connected to Ethernet. So this is kind of typically how you know machines were built. If you buy a GPU and you're uh for you know gaming or something um this is kind of probably what your your setup looks like. Um as we'll see this is uh kind of suboptimal because um there's a lot of overhead when the data gets needs to get shipped from GPU to GPU. it has to go through you know the kernel get copied into buffers and then go through this kind of a you know transport over Ethernet and that introduces a lot of overhead. So what has happened in modern times uh with scientific computing and you know deep learning is that u if you know that you're going to just string a bunch of GPUs together and do something uh together then we're just going to hook the GPUs up you know directly basically um so in the Nvidia ecosystem um we have MVLink that directly connects the the GPUs um therefore bypassing the CPU you don't need to go through kind of the you know um the kernel of the of the of the host machine. Um, and across even across nodes, uh, we can connect the GPUs directly via MV switch. So therefore, we're bypassing Ethernet and because Ethernet was developed a long time ago, uh, clearly not for for for uh, these type of applications. So MV switch just and MVLink kind of skip all that and just optimize directly for um, the type of workloads that we're interested in. So if you look at H100's um each um node has or sorry each GPU has 18 MV you know links um generation 4 coming out so that's uh gives you a total bandwidth of 900 gigabytes um if you compare to um these um it's certainly a lot faster than PCIe and it's certainly way faster than you know Ethernet um and um in comparison. Um, if you think about the cost of just going from the SM to, you know, reading from high bandwidth memory, um, that's still quite a bit faster, uh, by a factor of, you know, four or so. Um, and of course, these numbers are constantly changing with a new black wells. Uh, this number is like, you know, a like two or three times um, more, I believe. Um, okay. Yeah. for the PC like does it go to the uh CPU and then like with another GPU or it's directly used with the GPU. So the question is for the PCIe where how does the data get uh you know transferred? I think it has to still go through the CPU. Was there another question? And the PI was I mean it's developed for you know things like other things are connected to it as well like your sound card or your SSD hard drive. So it's not really it's sort of like a general purpose uh you know uh bus for communication of devices. Yeah. MV link also has connection with CPU. Yeah. So the question is MVL link also connects to the uh CPU. We're going to see a bit later how um I think maybe just in the slide how things are connected. Yeah. So you still need to talk to your CPU of course. Yeah. Okay. So there's this command uh that you can run um and this produces some you know output which allows you to see um how the GPUs are actually connected. So, I ran this on, you know, our our cluster. There's um eight GPUs. I guess you won't be able to get eight GPUs, but um but uh I guess if you could, this is what it would look like. And you see that between every pair of GPUs, there's MV18 um you know, connecting. Um there's also um these kind of network u you know, cards and and other things. Okay. Um, oh yeah. So then network cards are basically what gives you the PCIe connection and and the the CPUs. So um, okay. So that's the hardware. So how do you use the hardware? So Nvidia has spent a lot of, you know, time developing really good software on top of uh, their I guess really good hardware. Um, and there's a collective communication library by Nvidia called Nickel. Um and this essentially translates the collective operations which we looked at before like all reduce um into low-level packets that need to be sent between GPUs. So this library actually does a lot of work because it allows the programmer just to operate the level of I need this tensor to appear on all the machines and it just happens. Okay. Um so you know just a little bit of what uh what happens is when you configure um setup nickel you bring up a bunch of um you know devices and there's some communication that happens to figure out the topology of the hardware it optimizes the path between the GPUs and then uh when you actually call these collective communication operations and then launch CUDA kernels to send and receive data. Okay, so that's nickel. It's provided as a as a library. But nickel is still a bit too um low level to us because most of what we're doing is um you know in Python. So there's a pietorch has this torch distributed library which essentially provides a clean interface for these collective operations. Now from the comfort of your PyTorch pro pytorch program you can just write all gather into tensor on a tensor and it will appear on all the uh on different ranks. Um it also has this uh nice uh useful feature that it supports multiple backends for different hardware. So in particular nickel remember was for GPU but you can also run collective operations. Remember this is not GPU specific. it's just for any set of um you know devices. So you can also do it for CPU using this back end called uh glue. Um so if you're debugging stuff on your laptop for your assignment for example, you can use glue and still be able to run things without even a a GPU. Um so anyway that that's another advantage of having these highle primitives is that they're much more portable than having to uh you know only having something that's very GPU uh specific. Um of course the performance is going to really depend on the hardware but at least logically you can make sure your code um runs. Um, Pyro distributed also supports other highle things like uh, FSTP which Tatsu talked about last lecture, but we're not going to use this in this class because in the spirit of developing things from scratch, um, that's just what we're going to do. Okay, so uh let's look at some examples of how to distributed collective operations work. Okay, so there's this uh utility function I wrote um which you can take a look at it in the code if you want which takes a function and just um runs this uh basically it's a wrapper around um Python multipprocessing where it it just runs four processes that execute this function. So when you're in this function, you should think about it as there's actually worldsiz number of uh processes running this identical function where the rank indexes from zero one all the way to world size minus one. Okay, so right now I'm stepping through uh just one of the ranks um uh because lectures are not parallel. Um and so generally what you do is you the first thing the process uh needs to initialize itself and you essentially the they need to kind of find each other right because you're multi-processor running a lot of processes they need to connect to a uh a single host so that they can you know figure know that each other exist. Um so note that this is not where all of the data goes. The data goes through nickel but this is just for kind of coordination. Um and since we have a a GPU we can use nickel. Otherwise you would use uh glue. Um okay so after you set up um so now we're going to do some stuff. There's this useful um function called barrier which um basically waits for all the processes in your process group to get to this uh point right remember everything's running asynchronously um and in some cases you just want to have a synchronization point so barrier does that um the reason I put it here is actually sort of for trivial reasons because I want all these print statements to kind of be grouped together um but there's other reasons why you might unused barrier as we'll get to later. So I'm going to for each of these um groups construct a tensor. So the tensor is 01 23 U plus you know the rank. Um so I'm going to print out um for each rank before the all reduce. What does it look like? Okay, so here's what it looks like. Um can people read that in the back? Yes. Okay, good. All right. So on rank zero it's 0 1 2 3. Rank one 1 2 3 4 and so on. And notice that because it's async the orders uh it's just out of order in whatever order it happens to print. Okay. So each um um rank has a different tensor and then then you all reduce. So all reduce you pass in that tensor. You say I want to sum it. Um in this case I'm not going to do async but you can do async. um to uh which is useful for overlapping communication and um in computation. Um and then you know afterwards what happens after all reduce as advertised basically for the first component you you add them up you get six this you get 10 14 and 18. Okay. So after all reduce the um basically this tensor gets overwritten with um the the corresponding um sum. So it's very very kind of you know nice and simple to use. Okay. Um so so let's do reduce scatter. So reduce scatter um I'm going to create an an input um which is uh has dimension you know world size in which case this is four um and um I'm going to allocate an output because reduce scatter is not going to operate in in place this is just going to be a scalar um so before the reduce scatter um this is you know what it looks like um I have my you know input as before output um you know happens to be zeros but it could be any value um since I didn't initialize it and then after the reduce scatter I pass in the input and the output and I'm going to sum um then I get um essentially what happens is that for the first component I sum and that goes on rank zero for the se uh second component I sum and it goes on rank one and so on. Okay, so as you notice it is producing the same operation as all reduce except for the the output is sort of scattered across all the different um ranks. Okay, so now let's do all gather. So um I'm going to just directly use the output of reduce scatter which is um this um as the input and then I'm going to allocate an empty array for the the output um and then so so before um the all gather um the input is this and the output um I guess are just arbitrary values um and uh after I do the all gather um you know what happens is I get the all these uh tensors to to show up in on all the devices. Okay, so this is just a kind of also an example. Uh hopefully now you're very convinced that reduce scatter plus all gather is just all reduce because I computed exactly the same quantity as I did for all reduce. Okay, questions this clear? Yeah. In reduce scatter are we keeping track of which GPU? So the question is uh in reduced scatter do you keep track of which index goes to which GPU? So by convention um the the dimensionality has to be the basically the world's I mean it could be a general tensor but one of the dimensions is the world size and it just you know infers that um basically what you want to do is um the output is the let's say the um sorry um the input has to be basically world size and then it knows that uh basically the corresponding you know computations go to each uh of the outputs. Yeah, you have to be a bit careful with the making sure the dimensionality uh align. So you know going through this uh you know smaller examples can be helpful. Is there another question? Okay. So finally we're now in this um process that's uh running and when you're done you just you know clean up. Okay. So um so so far we've talked about these collective operations um bit about how they're implemented in you know pietorch and it's uh nickel and then pietorch um let's do a bit of um you know benchmarking um in the spirit of what we did in um assignment uh or the first lecture or rather the second lecture um we're going to focus on one node uh you know for now. So let's do all reduce. So I'm going to have this uh tensor of 100 million elements um and a world size of four. Okay. So um I'm going to just allocate a tensor. Um and generally as I I think um as you hopefully are uh can appreciate now that when you benchmark you have to really be careful to kind of clean your pallet in some sense like you um in this case I'm going to warm up basically com run the operation um once um and then synchronize and do barrier. Some of this is I think probably a bit defensive but but just to be but just to be safe so that all the kernels get you know loaded and whatever needs to be kind of computed gets computed and then I'm going to start the clock all reduce and then synchronize again and and and stop the clock. Okay. So um now I can look at the how long that took. Okay. So if I scroll down to here, um I guess this is not that informal. I should have printed in microsconds probably. Um it was I guess very quick um some number of uh seconds. Um and now let's measure the the bandwidth which is the number of gigabytes that were actually transferred in aggregate per second. Okay. So the way we do that is we have to think about what actually gets transferred here. So there's a tensor with that element size and the size of each element is um I guess this I think this is float um 32. So that would be uh you know uh two or sorry four four bytes um and and um and so that's the size in bytes. Okay. So now this is a little bit uh you know subtle. So um how many bytes are actually sent or or transferred sent received um so each tensor sitting on a rank has size bytes okay and it needs to send it to world size minus one you know other you know machines or or not or ranks rather so there but there's a factor of two so why is there a factor of too because you're doing an all reduce remember. So you need to send all the distinct um you know elements into basically one place. It needs to get uh summed up and then that needs to go back to everyone. Okay. So a rank needs to kind of send the input out and then receive the output. So that's why there's a there's a factor of two there. And so the total duration is um the world size times the the actual duration that passed. So I guess we're just kind of assuming that every we're we're you know if there's four processors that's sort of like four times as much wall clock time that happened. And the bandwidth is just the bytes over the duration. Okay. So what do we get here? um is about 277 gigabytes you know per second. Okay. So you know I think um for H100 um be above I think I claimed that there was something like 900 gigabytes uh per second. Now of course as we know your mileage varies depending on the size of the tensors and the exact number of uh devices and the weather and what no not the weather but uh but you know various factors. So your um your mileage might vary. So it's always good to benchmark to see what is actually the number of uh gigabytes per second you're you're getting. Okay. So so reduce scatter is going to be very very similar. So let's just go through this very quickly. So we create a input um which is world size times number of elements. So each rank is going to have this um this the matrix um and and so we're going to warm up and then um start the clock reduce uh scatter um stop the clock and then see how long it took. Well, okay, that's not helpful. Um and then let's look at the uh the bandwidth. So the number of scent bytes is no factor of two here because in reduce scatter remember all you're doing is you're you're sending um you know your inputs into you know one place. If you just think about uh reduce right all the elements just go into one place and that's it. And scatter just means that different components of your tensor are going to different places but it's effectively it's it's like a you know reduce. Okay. So if you do the same calculation um you'll see that it's I guess I get 70 in this case. Um so I don't exactly know why it's exactly 70 as opposed to some other number. Um I guess one could speculate that all reduce generally there's more traffic that you know happens and um all reduces are you know potentially more optimized. I think that Nvidia hardware has this kind of sharp acceleration that actually does sort of um some of these um computations in you know in the actual network um which shaves off a factor of two but I don't know if that completely accounts for a difference here. Um there's a lot of stuff that happens in nickel that it's a little bit hard to kind of reason about the performance exactly. Hence um benchmarking. Yeah. Another question about the set bytes and or the data bytes and how that was calculated specifically. It looks like it calculates just like the uh data that's being sent to the output. But what about like the input? So the reduction step. I was wondering how it gets the inputs to do the reduction. So the the question is it seems like this is just the uh the bytes for the output and what about the input. So to be clear I am assuming that the inputs just are already on the device. So I'm not counting that time and just uh I'm just counting what needs to happen to do the reduce scatter. Is this just a scatter operation? This is a reduce scatter operation. So, so you need reduction step. Um, so this function does reduce scatter. So, it's one operation. Okay. I mean like we count it twice in the previous uh because we were doing reduction for half two by half. So you're saying that for all reduce um there were there was a 2x because you needed to reduce and then you needed to you know uh spread out again. Um for reduce scatter I mean it's just a name. It's called reduce scatter but it's really just um a reduction. Okay. And you can also see uh based on this that um if you do reduce scatter and you do all gather each of those is doesn't have the factor of two. So when you add them up you get a factor of two which is another way to see that all reduces um twice. Okay. Um and there's some references you can go read about how to benchmark and um these collective operations. Okay. So let's now talk about the distributed um training uh piece. So our general approach here is going to be I'm going to walk through a barebones implementation of each strategy on deep um MLPS essentially. So recall that you generally are in the regime where MLPS are the compute bottleneck and transformers um not the attention. So in some ways even though this is a very simple architecture it's fairly representative of the type of um you know workloads that you'll see. Um okay so let's start with data parallelism. Um actually just one note is that data tensor and pipeline parism are you can just think about them as different ways of cutting up uh your um your either your model or your your data which hopefully I'll uh depict visually here. Okay. So in data parallelism um here's your model assume it has four layers. Each layer of the MLP is just a matrix multiply where this is the dimen hidden dimension. Um and so the data is also a matrix which is um there's the batch dimension and then the hidden dimension and data parallel just cuts along the batch dimension into um you know essentially smaller um you know pieces. Okay. So now each rank is going to get a different slice of the data. So let's uh do an example here. Um so I'm going to generate some sample data. So let's say I have batch size of 128, hidden dimension of 1,024. Um and then just generate some random data. Okay. So I have batch size by number of dimension and I'm going to run this data parallel algorithm um or DDP. So um so here um I'm going to so I got past this this data. There's a batch size and the dimension as as uh claimed from before. Now I uh divide the batch size by the world size. So I get the local batch size. That's how many um uh you know how big the batch size is on a a given rank. Um and then I'm going to based on the rank just um figure out which um starting and ending indices um of size local batch size I need to um access and then get the corresponding data from that. So basically I'm just like reaching in and grabbing some subset of the rows based on the on the rank. Okay. So now I'm setting up the the MLP here. Um and this is done very um sort of bare bones you could say. So here I am creating the MLP parameters. So each layer has essentially a matrix which is num demand mentioned by num dimension and remember num dimension is um and I'm going to create the optimizer. So remember this um uh function is running asynchronously on all the different on each rank. So each of the four ranks is going to be running this with rank equals 0123. Um and now I'm going to start training. So for a number of steps I'm going to do a forward pass um through the layers um matrix multiply nonlinearity matrix multiply nonlinearity. There's four layers here. Going to compute some loss. I don't really care what the loss is. It's just made up. Something made up. And I'm going to do the backward pass. So, so far this just looks like I'm implementing uh you SGD, right? Um and that's the kind of the point. The only difference is now to implement DDP is that you just like inject this line here which syncs synchronizes the gradients across workers. So what you do is for each of the layers you call it all reduce um where you're averaging and the thing you're averaging is pram.grad. Okay. So it's just like you've kind of hijacked this um someone's sd code and you're saying wait um I'm actually going to just mix all the gradients after um the backward pass. And then after you do that, you just um update the parameters as uh usual. So from the SGD perspective, it seems like nothing is happening. I'm just running SGD, but you know, someone has just uh um you know, mixed my um gradients. Okay. So um so I guess just print out some some things. Um so data parallel um I'm printing out the loss. So one thing to note is that the the losses are different between all the different ranks because they have different datas. Um but after the all reduce all the parameters are you know the same. Okay. So this is a kind of your textbook uh application of all reduce in ML setup. Um yeah. when each rank runs this all how do they ensure that they're all sort of at the same so the question is how do you ensure if all of these uh processes are just running asynchronously how do you make sure that each of them is actually for example on the same step um this is because um all reduce is a is a synchronization point it'll pop everyone and um and do the all reduce. So you have to be careful because if one of your um you know ranks has a missing all reduce then it'll just you know hang. Yeah. Yeah. Yeah. Oh why does getting the initial parameters depend on the rank? The question is why does getting initial parameters depend on the rank? The same they should be. They're the same. The reason is just because I guess I don't um the code for this basically puts it on the uh the appropriate GPU. Okay. Any other questions? So DDP is something you implement in assignment 2 which maybe some of you have uh um you know look at or maybe not. Um it will be done in the context of a transformer but this is sort of the sort of the most bare bones version so you can see um very clearly what's happening. Okay. So that's DDP. Um um losses are different across ranks. Um but the gradients are reduced to be all the same. So therefore the parameters um are of all the ranks are the same. Right? So actually you're doing worldsiz number of SGD runs but because they're synchronized they're doing the same thing. So you can think about this as sort of an instantiation of you know analog of activation you know checkpointing where sometimes you just do extra compute because you don't want to store things. In this case you know we could have for example ship the optimizer state around but that would be a bad idea because you know it's much faster just to run the to update the optimizer um state than to actually move the the optimizer parameters around. Okay. So last year I did try to do FSDP that but that was a sort of a hairball so I'm gonna skip skip that um and do a tensor parallel. So here the the picture is we leave the data the same and now what we're going to do is we're going to cut um the model along the hidden dimension. Okay. So um so each rank is going to get every layer but it's going to get only part of each layer and um what we're going to end up doing is transfer all the data and the activations around. Um okay so so we're generating the same sample data um and let's look at tensor parallel um okay so so I have um the batch size and number of dimension as before and now I'm going to not before I was cutting um batch size but now I'm cutting num dim so I have local num dim equals 124 uh 1 024 4 um divided by world size and that's 256. So each model essentially uh sorry each uh rank gets a part of the model which is one over the world size fraction of the parameters. Okay. And remember the whole why we're doing parallelism at all is because the model won't be able to fit into a single GPU. So we're going to shard it across multiple um GPUs. Um so so the parameter matrices are now um num dim by local num dim. And now each rank is going to I'm only going to implement the for forward pass uh here um not the whole training loop. Um so I'm going to start going through all the layers. So um I'm going to compute the activations first. So this looks pretty normal except for remember the activations are actually batch sized by local num dim rather than num dim because I only each rank only has a fraction of the activations now. But now once I get the the activations um I need to you know communicate um and here I what I have to do is um I'm going to allocate memory for all the activations. So at this point every one has a um as a X but that X um represents a different um part of the activations. Okay. So now I'm going to just allocate um batch size I'm local num dim but world size number. So basically each rank is going to basically have enough uh I'm going to just get the um basically have worldsized number of batch size by local num um you know matrices and then I'm going to do an all gather. Okay. So, um I'm going to send all the activations. Um and this I mean it's you know fairly uh simple. So X remember is um batch size times local num dim but x is different for every rank. So when I do that all together, I'm going to put it in activations which has essentially a world-sized number of uh you know the same shape as X. Okay. So now every um rank has the same activations now has activations of all the models of the of the whole model. Okay. And then just like just to concatenate them together to get you know X Okay. So now X is um now again batch size by um num dim. Okay. And I you know repeat. So as you can see this is um you know there's a quite a bit of communication that happens which is why you know remember Tatsu said that for tensor parallel you need pretty high um interconnects otherwise you'll be passing a lot of these activations you know around. Okay and then you do it for the next layer and the next layer and you get the idea. Um and just to print out some um output. Uh so tensor parallel um let's see here um forward pass produces activations of basically the you know the full size and everyone has the same um activations at the end. Okay. So backward pass I'm going to skip because that's kind of a annoying uh to do. Um all right. Any questions about that? Yeah. I was just wondering why it's hard to do. Um so why is it hard to do the backward pass? I I don't think it's necessarily hard, but in I guess in in the constrained, you know, time and space, it's it's um it's not hard. It's just uh you know, requires a bit more work. Okay. So now let's go to pipeline parallelism. So in this case, we're cutting um the model by layers. So all the ranks get all the data. Um and um all the ranks each rank gets all of one layer but they get different layers. Okay. So sample the data and run this program of this function for all the the ranks. Um okay. So here I'm going to uh figure out how many layers go in each um you know rank um which is two here. So I have a four layer network. I have two um you know two ranks. So each rank gets two of the layers um just like this picture actually. Um and here I'm going to um just allocate the the parameters just for the layers um that I need. Okay. So I'm going to do the forward pass. Um remember there's a further optimization that you can you do which is um you know if you just you know do it naively you get these pipeline bubbles that Tatsu talked about um before um one way to sort of mitigate that is to break up the batch into microbatches. So here I'm going to divide um this this batch into um you know uh batches of size 32. So four batches of size 32. Um and then now the idea is that every rank is going to essentially wait for the previous rank to pass it the activations. It's going to apply those layers and then it's going to forward it to the next rank. So starting at the base case, we have rank equals zero. That's uh just the data. Um so I'm just chunking the the data into a bunch of microbatches. Um and going through each of micro batches. Um I um first I receive the tensor. So I'm using these um pointto-point primitives now um instead of the collective primitives. um and I uh essentially you know ba basically receive the tensor X um and then I'm going to compute the layers that are assigned to this rank. So in this case there's only two of them and then um I'm going to send it to the the next rank. Um and then again send is a pointto-point um you know operation and then the next uh batch I'm going to do the same thing. So okay so I'm going to skip that. Okay. So that's basically it. So pipeline parallel at least the very naive version of it is relatively conceptually simple. As Satu mentioned last time, um there's many things that are missing from this basic implementation. Um overlapping the communication and computation is something we're not um you know doing at all here. Um for example, receive and send are synchronous, but you should really make them async. And also the the order in which you do the forward. Um actually this is just the forward even the not the backward but once you have the backward then um you have to figure out how to interle the forward and the backward uh steps. Yeah wonder I guess like maybe what you just mentioned about like the async not being shown here it's I guess in actuality like the GP will be sort of listening like whether another one passes something to it and it's kind of this kind of kind of in like a event driven sort of like it only starts processing once the like layer before it passes through it and then it starts processing. So the question is is this kind of like event driven programming where you're just waiting for things to happen. Um, in I think in event- driven programming, you basically write these handlers and then whenever stuff happens, maybe you get a mouse click, maybe you get, you know, a file ready event, then a piece of code runs. That's quite different, I think, from this style of coding where um everything has to work in lock step. Um it is true that you're sort of waiting for the previous um um your rank to send you the information but at least in this implementation there's no flexibility of where it's getting from. It's not like it's waiting for arbitrary data come from anywhere. Um I think there are ways to do asynchronous training which was you know uh I think quite popular you know 10 more than 10 years ago where um there is more event driven where um you have a you know server that sends data and whenever the gradients are ready it just like uploads and then the gradients get accumulated and if workers die then you know that's um then you know that's sort of handled more robustly. But in modern training um despite scaling up quite a bit um you know everything seems to be kind of in a synchronous paradigm. Yeah. So it is true that when I say the the workers are and the ranks are operating asynchronous that that's just because it's different processes but you're still putting quite rigid synchronization on how everything is uh working in lock step. Yeah. How would you change this program to handle to overap the configuration? Um so the question is how would you change this to overlap communication and computation? So um for example when you send this there's no reason to just wait for the data to be sent. You just basically fire off the send. Um remember the the send actually gets hap happens on the GPU via some kernel launch. So that's sort of independent. Um and it can just go and process another microbatch you know right away. So um the way I think you would do this is there's another uh function called send which is um uh asynchronous um actually this should be as synchronous um asynchronous which returns a handle and so you basically do all the the send and then at the end you basically wait for all the um the sends to complete. And then for overlapping the um when you actually have the backwards step then you basically have to you know schedule that um in here. Yeah. Send and receive the same if you have multiple sends multiple receives how does it know which one is which? So the question is if you have multiple sends and multiple receives how do you know which is which? So here you're spec the the tensor name doesn't matter it's just uh whatever variable is there and what you're specifying is the the source. So if I'm at a node and I'm receiving then whatever the next message coming from that rank I'm just going to you know put in this uh x and move continue executing. What if I want to do two sends from the same rank? If you want to do two cents from the same rank uh to the same destination. So, so I'm not quite sure about this, but I think if you have two sends, it's sort of put in a stream. So, the order of the sends still is preserved. It's just that other stuff can happen at the same time like you know you can send to um like I think if you have a a pair you do two sends then that order is preserved but um the order in which um you know you send some other rank is sending to another rank it can happen at any time. Yeah. What would happen if you just did like this send but then no one's receiving it would just get stopped there or like so what happens if you send and no one's uh receives it I think it would just stop it just wait because there's no yeah I mean because I mean the the process could just be running and you don't know whether it will it's just I mean it's just code executing so you don't know if it's never going to get there or if it's just gonna be a matter of time. Yeah. So the question is what happens to the last rank? So at the end the last rank has all the activation. So that has basically the results of a full uh forward pass. And then you know if you implement the backward pass then you would be actually now computing the gradient with respect to loss and then you would uh go back down and send um to from rank to rank minus one and so on. Okay. I guess maybe um I was afraid I was going to run out of time, but it looks like I had actually have time. Maybe next year I should do the backward pass. Um okay, so actually I'm going to finish quite early today, but um so if you have any other questions, you should ask. Um so so far we've gone through three simple examples of data tensor pipeline parallel. Um of course this is for simple MLPS. um you would actually want to do this with your own um you know fancier model um like a transformer. Um I did argue that at least at the the core ideas you can sort of understand through the MLP. Um I think the but of course when you want to train you want to train transformer not a deep MLP. Um so you still have to implement the the full complexity. Um what's also missing is the communication and uh computation overlap which is not really handled very uh carefully here. Um and there is generally a more complex code with bookkeeping. I you know encourage you to check out like Megatron LM or um PyTorch's FSTP. It gets uh you know fairly um hairy. And one of the things that I think makes some of the bookkeeping at least for let's say FSTP and you'll be exposed to this in uh a A2 a bit is that um if you want something that handles arbitrary architectures then you have to you know figure out the parameters and do a book a bunch of bookkeeping to and you know figure out what their layers are and and so on. Whereas in the MLP case, it's just I've sort of made the decision that I'm going to split the model in this, you know, particularly simple way. Um, one other thing I'll just mention as an aside is that all of what we're doing in this course is is PyTorch, but it is useful um to be aware of this whole other ecosystem around jacks and TPUs. Um, which is actually kind of uh nice in some way. Um and the idea here is um Jax has allows you to just define the model. It define the sharding strategy and then the Jax uh compiler handles the rest. So there's this um toolkit that we uh developed called Lavanter based on Jax. Um and I'll just show you a snippet of um what it happened. So this is FSTP and 10 lines of code and uh basically you have a um you know uh model and then you just say shard with this partic I mean I don't expect you to kind of read this exactly but um basically you define which dimension you're going to shard by um and then you know that's it and similarly for tensor parallel um you're just saying I'm going to um shard um the model along the you know you can shard by the on the head dimension um for uh for attention and also you can shard based on um the the model dimension. So in some sense you know this gives you a sort of com you know comp a conceptual simplicity of what you're trying to do is you have this basically um um computation graph but it has these kind of dimensions you know the the model dimensions the embedding dimension the attention sequence dimension and Jax allows you to basically uh just specify which dimensions you want to cut by and also define a mapping from that onto the actual uh TPUs and then the JAX compiler magically just you know figures out how to compile that down into the the primitives that you know shuffle things around. So this is much more higher level than um you know doing the the operating with the collective communication. Um, but you know, we're sticking with PyTorch um because it's it allows you to see kind of underneath the hood what's actually um happening. But if you're actually doing this in the in the real world, um obviously you don't need a you and you probably shouldn't implement all of this from scratch. Okay, so that's the end of the Jack's digression. Um so just just summarize we've um seen many ways to parallelize so far. Um and each of these ways of parallelizing is you can think about just like splitting either the model or the um data along some dimension either the data the batch dimension the width dimension or depth dimension or the um the context length dimension. Um we also see these this kind of recurring theme of you know recomputation. you can um you can kind of recomputee something um from scratch or you can store in memory and suffer the the data transfer cost or in now in the multi-GPU multi-node setting you can actually store on another GPU's memory and then you know communicate which is you know even slower um so there's kind of these these tradeoffs um you know here And you know often recomputation is actually um you know can be you know better but obviously you can't you know you can't repro compute the whole thing and often you're either communication or memory limited. Um a final word is that um it is the case that hardware is getting better. So you might think that well maybe none of this is really necessary because in five years everything will fit in you know L1 HPM. So this is not going to be the case because um those might uh grow quite a bit um although there are still physical limits um we'll always be ending up with bigger models that sort of are at the limit of what the hardware can do. So this hierarchical structure um ever since system computer systems was a a thing has always been um with us and it will always uh be there. Okay, that's all I have for you uh today. So I'm can take any questions. Yeah, increase with the same set of parameters the cost might be different because like your normalization might be a function of the whole data set. uh for example in so the question is in data parallel um you're saying that even though the parameters are all kind of synchronized there could be other things that depend on the the data like in batchorm um so I don't actually know how you batch norm is always kind of annoying Um, so I don't know exactly how you would do that off the top of my head. Um, and I guess at least in the LM world that doesn't really show up. Uh, because layer norm is is is used. Um, and as long as you initialize all the parameters and they're using the same random seed, you'll be fine. I mean there could be like non-determinism issues um on on the GPU but hopefully those are you know minor uh yeah so the question is uh is does PyTorch have um some nicities as well kind of like what Jax offers is Yeah. So I mean PyTorch does have the FSTP library which you should absolutely use if you're not taking this class. Um which basically is a wrapper. You define any model and it just does FFTDP on it. Um, I think that now if you're asking how well it can more custom allow you to more do custom charting, I I think there are some things that are coming but it's not as I think as developed. I mean I think there's sort of this I think spectrum between the Jax world where you sort of declarity define things and I think the Google infrastructure if you stay within the Jack's TPU system is pretty well developed and but then if you look at kind of deepseek which is a kind of opposite end where you have um these uh GPUs with actually really bad inter you know connect which means that they have to go in and hack, you know, they actually go to the kind of nickel level and actually do a bunch of things which I don't quite understand to ek out the performance. Whereas if you're writing a jack, you just kind of from on high declare your model and then you know stuff stuff happens. So it it's kind of uh the ways that you leverage hardware I think really depends on what what ecosystem you're operating in. Yeah. the the amount of recreation of the applications they can rec some of the activations is there API which may yeah so the question is activation uh checkpointing what's there is an API that's basically allows you to uh in I mean I guess in PyTorch Njax to uh specify which parts you want to uh recomputee because clearly you don't want to recomputee you know everything and or nothing. um probably every few layers probably right after like big Matt malls where um for example if you have let's say um mammal and then pointwise um linearity I don't think you need to store like two copies of um basically if you have two things where it's sort of trivial to uh comp um get to then you might as well just store you know one version um yeah over Okay. GPUs are replaced by specific hardware or like more specialized. So the question is are GPUs going to ever be replaced by transformer specific hardware? Um so you're seeing this in the inference space uh uh quite a bit already with um um like Grock and Cerebras have specialized hardware that can do um inference and also I guess training. severe resist training. Um so basically those hardwares um essentially give you just a lot more kind of onchip memory. I mean that's basically the the name of the game. I think cerebrus has like a huge um you know essentially effectively a L1 cache so you don't have to move things off and I think a lot of simplifications can happen because GPUs were there's a lot of baggage actually if you think about because um they were designed in an era where you had to do a lot of branching and like you know various types of ad hoc computations which are not really needed in the deep learning regime. So I think there are quite a few opportunities to um improve the hardware as well. Um I think there was a hand back there and I'll um I don't know if I this is like the right question that I'm thinking about but um in the context of the lecture it's basically a model um that's being trained in one go that's been optimizing but I'm wondering if any of the techniques that we're talking about can be used to incrementally train a model for example as you get new training data um not just to like fine tune but actually to kind of recapulate everything without having to recalculate them. Yeah. So the question is um can these techniques be used to essentially do continued um training? Um yeah absolutely. So if you think about the the unit of what we're working with is just doing gradient uh steps, right? So if you take a halftra, you know, checkpoint, you can just like continue doing what this is. There's nothing specific about starting from scratch. Um here um I think there was a question there. Yeah. So on the like the model specific hardware end you know the previous question um like presumably there's like a physical technical reason you can't make nodes much larger than they are currently. Like what's the change that you're talking about? Like so if you could just make GPU nodes like infinitely like as big as you wanted people would do that. So presumably there's a tech like a hardware reason that's not possible. So what's the actual advancement being done for like the rock specific hardware you mentioned? Yeah. So the question is there are physical limits for sure uh for a um you know for a GPU. Let me just go. So the um so you can't make GPUs obviously infinitely large um or infinitely dense. I mean there's also like you know power um issues uh you know you need to get rid of all the the the heat and um you know there's only so much kind of bandwidth that can um you know fit um so I don't know the the exact um you know details but at least in some of the cerebrous case I mean they um they sort of have this way of you know manufacturing you know basically the the chips so that the um the memory is kind of on on on the chip. So So I guess it's just a way of putting it on there. And I think that there are obviously um you know trade-offs because it comes at a um cost of not having as much you know flexibility. Um but in but in general I think the way to maybe think about this more broadly is that you know GPUs were still developed in kind of the CPU era where it's much more control focused like I have code that I'm executing that's the sort of first class citizen and then data needs to be moved to you know um execute the to to handle the code but um the big difference with deep learning workloads is that it's all sort of data flow like the the computation graph if you look at these sum is like static. You know from the beginning exactly all the computations that are going to be done until essentially the end of training, right? So using that knowledge, you should be able to kind of lay out your computation in a much smarter way than having to deal with the the flexibility uncertainty over ad hoc uh computation. Okay, maybe a few more questions and I'll end there. Yeah. Is the computational graph usually stored in the CPU or in the GPU? So the question is where is the computation graph uh stored? Well, the the code is um all I mean all this code is running on on on this you know CPU. Um but but when you call something like um the PyTorch function it it that's needs to run on GPU then it launches kernels under the hood and the kernels are code that runs on the GPU. Um yeah, I'm not sure if that so so I guess maybe another answer is that the computation of graph is more of a I guess a conceptual um you know it's not like there's a graph literally that's being you know uh I mean I guess there sort of is but it's uh it's uh it's not like the graph gets put on the GPU if that makes sense. Okay. So, so these communication primitives that we have like they are actually CPU instructions or like are these programs using the GPU? So the question is the communication primitives are they CPU or GPU? So these collective operations are in some sense um abstract specification of what types of operations need to happen um which can happen if you remember um this PyTorch distributed um has different backends. So it could happen on GPU or happen on um CPU. But but when they're happening on CPU, is it like is the CPU sort of scheduling them or or is it like kernels which are independently? Yeah. So well the CPU sort of drives basically is the sort of the master still and then when you do a collective operation it calls the nickel library which uh launches which is you know it's still CPU and then it launches some kernels that move data around. Yeah. Okay. Maybe this is a good place to end. All right. I will see you uh next Monday or Tuesday. [Applause]