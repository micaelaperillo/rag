Okay. Uh so let's get started. Um today's the the second and last of the scaling laws lectures. Um and today it's going to be a little bit more of a a case study and details oriented lecture. Um I'm going to cover two separate kinds of things. The first one is I'm going to go through a couple of papers um where people have done careful scaling loss studies as part of their model building. Um and I'm going to use that as a way to to convey to you how modern large um language model builders use scaling laws as part of their design process. Right? So um motivation from last time and today is what's the best practice for scaling a large model. Um we want to have um large language models with nice hyperparameters and good architecture choices. Um, and you know, I've already told you about chinchilla and using scaling laws to validate some of this, but I think you know, you should have rightfully skeptical questions about scaling laws, right? Like it's curve fitting on a log log plot. Is it really as good as as you know, um, I said it was last lecture. So, does Chinchilla's approach to scaling laws actually work? Um, you're finding this out in your assignments. Um, you know, if you fit an ISO flop, is that really telling you about the right token trade-offs? um can you use this stuff to really set optimal learning rates um and should we be picking sort of particular architectures or parameterizations to scale um nicely. So um the last paper or the newest paper we talked about with like lots of detailed scaling studies in last lecture uh was the deep mind chinchilla paper right and after that um chat GPT happened and kind of the competitive landscape of large language model building really changed and people just stopped publishing anything about you know data and scaling and all these things right it it was sort of very secretive um I've talked to you know people at some of the frontier labs before and ask them, oh, you know, like what are you guys doing for scaling? And they're like, no, we will not tell you anything about what we do for scaling. And so, um, we have to sort of rely on other sources for how scaling happens in practice. And there have been, um, several, you know, competently executed largecale models that have done scaling. Um, and so, you know, last year, um, in this lecture, I covered Siri, GPT, u, Deepseek LLM and mini CPM. Um, and as a as a nice side note, you know, last year I had to really strongly justify why I was covering these like Chinese models, so to speak. But this year, thankfully, hopefully you're all already excited to hear about DeepSeek rather than me trying to convince you that this is the right thing to listen to. Um, in the years since then, um, I've looked at a lot of the the models that have come out. Um, actually the the hall in terms of new scaling law insights and papers is actually much sparser. Um I'll briefly mention some results from um Llama 3 which came out at the later end of last year. Uh Hunan Large um which is thee model from China. And then Minimax 01 which is a linear time sort of hybrid attention model um from or long context model um that came out this year. And all three of those have some scaling studies, but really nothing quite as extensive as DeepSeek or MiniCPM, which have really been the gold standard, I think, for uh modern scaling law studies. So, that's one part of what I want to talk about today. I want to make sure you guys have an understanding of, you know, what scaling looks like in a real, you know, semi-production model. Um and the other thing I want to talk about which is an important deep dive I I think is uh the MUP uh method that I mentioned last time. So, MUP is this approach. Just as a recap of um last last lecture is you know when we train these models as we make them bigger we need to change certain hyperparameters right um on the left hand side of this plot here you see that as you make models wider um in this case like a MLP you make them wider the optimum learning rate sort of shifts downward so you need smaller learning rates for these bigger models and that's a really big problem potentially because then you need to hyperparameter tune your learning rates at the very large scale right and that's going to be very computationally expensive. It's going to be a huge problem. Um, if on the other hand, if we could sort of parameterize our model differently so that you know the learning rate that's optimal just stayed the same forever across all the scales, you know, that's great. That's really simplified our search process, right? We would like all of our hyperparameters and really choices in general to remain stable across scales, right? That's the ideal. And MUP is a is a very interesting class of approaches. Um, and you know, it teaches us some pretty interesting sort of ways of thinking about the problem. So, I'm going to actually go through some of the the details in the in the math. Um, in the years since last time I taught this, there were a couple of very nice tutorials on MUP that came out. So, I'm going to follow those um because they have math that's pretty easy to follow. And then I'll talk about some um uh work that has come out doing sort of thirdparty validation and uh evaluation of MUP style methods. So, okay. Um the focus of the first part of this lecture which is the case study is going to be on three models. Um I talked about three additional more modern models but actually the the details in those are much more sparse and I think the lessons you learn are primarily from these three papers here. So that's going to be my focus u for the first part of this lecture. So, I'm going to talk about Cerebrus, GPT, MiniCPM, um, and DeepSeek. And each one of these will have actually a pretty different mix of of scaling strategies. And it'll also have different things to teach us about how to get scaling, um, right. So, we'll get started. Um, Cerebrris GPT is the first of the uh, models and sort of scaling things that I want to talk about. Um, it's a large family of models. It's trained um, 0.1 to 13 billion parameter models. um trained with the chinchilla recipe. Um so roughly the the same number of like token to parameter counts as is optimal. Um and you know they have a interesting core finding. Um the cerebrus folks actually are are pretty interested in a lot of these like scaling and parameterization studies and they have a really interesting core finding which is they scale up this MUP thing that I mentioned before and they find that it makes scaling a lot more stable and a lot more pleasant to deal with. And and just to kind of show you the punch line, right? You've got um test loss on the pile. Um and you've got sort of the scaling curves here of like the cerebrus GPT in blue. This is with standard parameterization. You've got MUP in in orange. This is the model that they you know also train with using the maximum update parameterization. And they show that it scales uh more nicely if not better than things like Pythia or uh GPTJ. So that's nice. Um and the thing that I want to emphasize here is that this is kind of one of the few if not first public validations uh of MUP. Um we know that all of or most of the labs that are doing uh LM scaling pay close attention to how they parameterize their networks. Um their initializations as a function of the scale of the model as well as uh things like per layer learning rates are things that people pay close attention to to make scaling much more uh stable. And so things like new are pretty important in this space. Um llama 4 for example um the paper for that isn't out and I don't know if they it will be out but they talk about um a technique they call metap uh which is a variant of this um as well. So um what they show is that when they train models using sort of standard parameterization they find that you know they have sort of big oscillations kind of around the predicted scaling point. So that's the this dash line. You know, they have kind of oscillations due to the fact that, for example, they have to adjust the learning rate as a function of scale. Um and so it's hard for them to kind of really get the predicted performance sort of exactly right, which is this dash line, um using their scaling recipe. On the other hand, um what they find is you know if you have um uh sort of the three risk GPT uh sorry MUP scaling then you get this orange line which is much much closer to the scaling law fit for um this MUP version. And so their claim here at least is that using this alternative parameterization allows them to get much more predictable scaling much more nice hyperparameter tuning. Um we're going to see this in more detail. I'll return to this slide again once I've sort of gone through the mathematical derivation of MUP. Um, but in case you're you're ever interested in implementing this thing. Um, some of the cerebrous GPT folks and in general, um, the kind of artifacts that the Cerebrous research folks are putting out is very very helpful for MUP because they have this big table in the appendix that really just tells you exactly the difference between the standard initialization and parameterization or SP and the maximum update version or MUP. Um, and you'll see that, you know, I I'll just give you kind of the the oneliner version. Basically, every non-mbbedding parameter is initialized with one over the width. And then the learning rates per layer are scaled down by one over kind of the width, right? So, so the the interesting difference from standard parameterization, even if you're already doing sort of one over width scaling on the initialization, is actually there's per layer learning rates that are different. And I'm going to get to that um later. I'm going to do a full derivation of this result. Um but you can kind of see uh here there this kind of like nice quick reference and also if you want to implement this thing this gives you very easy ways of implementing uh MUP. Um another interesting thing that we also see in in some of the other scaling strategies is that you combine these strategies like MUP which makes hyperparameter selection stable with very very aggressive scaling. Um so what they do here is uh they scale down um their experiments all the way down to 40 million parameters. Um they do extensive extensive hyperparameter search on this proxy model. Um and then they scale things back up using sort of MUP to try to keep hyperparameters as stable as possible. Um and so this is sort of what they see um in their small scale hyperparameter search. Each one of these dots is a model run and there's sort of a hyperparameter associated with each one of these and then they pick the minimum across these runs giving them essentially their sort of hyperparameter grid. Um this is a very like clean approach to hyperparameter selection. It's unclear whether you know this kind of this level of aggressive scaling down is really what you want to do if you want to train these like really really large models. But this is kind of one strategy that we see also in in mini CPM and deepseek like training much smaller surrogate models and then trying to figure out how to stably scale them back up. And that's going to be kind of a a theme that we see uh throughout. And yeah, if if folks have questions uh please stop me actually. Maybe I'll stop here for a moment in case anyone has questions for the cerebrus GPT uh piece. Although maybe it'll be clearer once I talk about uh the MUP derivation later in this lecture. Okay. Um there is another paper I want to talk about mini CPM um or another you know uh artifact I guess. Um, for whatever reason, I think mini CPM hasn't been talked about quite as much. Um, especially in sort of like western academic circles. Um, but at least for me, this was one of the first sort of releases or papers I saw coming out of like a Chinese research group where they had done some like really cool in-depth, you know, scaling and other kinds of research. It really felt like, you know, stuff coming out of the frontier, right? Um, and to give you a overview of what they do, their goals here is they want to train, you know, relatively small language models, but use a lot of compute to train really good small language models. That's their ostensible goal. Um, and in doing so, they do a lot of careful scaling computations. They also once again use MUP to stabilize and simplify scaling. Um, when they sort of end up scaling these models, not in size, but in in terms of the amount of data. um and to try to convince you that you know this is a paper worth following right um you know at the time that they were trained uh this was a remarkably good 1.2 to 2.4B 4B uh models. Um it beats out, you know, most of the 2B models that were out there. Um and it matched many of the modern 7B models. Um at least modern as of 2024, uh standards. I mean, now of course you've got even better 7B models. The arms race is fierce. Um but this should give you a sense that at least, you know, given the the amount of compute and technology available, you know, back in mid 2024, this was actually really uh at the frontier and they did something right to to get models uh of this quality. And so much like cerebr um they do essentially they have to have you know some strategy to get scaling right right so so stepping back right you're going to do a really big model run what do you have to do you have to pick hyperparameters you have to make sure those hyperparameters scale nicely and then you scale up your model right so uh we can do the same thing as the cerebraas GPT folks we can try to pick hyperparameters at a small scale hope that they sort of stay stable and then scale everything up and the way to do that would to use something like mup and this has exactly the same kinds of strategy at play here. You see um so for embedding you don't really do anything you just scale it by a constant um whenever you have some sort of like residual connection like an MLP you scale it by the square root of the number of layers um you initialize it by sort of fan in one over the the base uh width and then um the learning rates are also scaled by the width of the model. we see basically the same um strategy or the same kinds of scaling factors appear as the cerebrus GPT um case, right? And they they also end up with very similar parameters um as SUS GPT, the same kinds of scale embeddings um similar learning rates um off by a factor of two or so. Um but generally you end up in similar places um as these kinds of hyperparameters. And then what you do is once you have this um you're sort of relying on your optimum learning rates to remain stable. So you're just going to keep those roughly fixed. And we know that the aspect ratio is a pretty important thing. So we just fix that after figuring out what the right one is. And then you scale up the overall model size going all the way from you know 9 or 30 30mm all the way to half or or one uh billion parameter models, right? Um, and so what they have is like a roughly 5x or maybe a little bit more compute savings going from the smallest models that they've got to the the uh largest sort of pilot run models that they have. And now you can use this and then you can sort of figure out um whether you have sort of optimal batch sizes as a function of scale. So you know you want to figure out crit which is the critical batch size. If you remember correctly this is um the critical batch size is roughly the diminishing returns point right. So as models get bigger, their losses get lower. As their loss gets lower, you you can make use of bigger and bigger batch sizes. So the critical batch size is roughly telling you for the given model size and scale that I'm operating at, what is what is a appropriate global batch size um for me to be training these models with. And so um much like the the Kaplan paper, they follow a similar kind of recipe. You know, the plots look different from the Kaplan paper, but the underlying strategy is kind of the same. What they're trying to figure out is what is the critical batch size for training or the optimal batch size in this case of for training different models and they're trying to find relatively predictable scaling relationships um between the batch size and for example the data size or the loss size um and vertical columns here sort of represent a single training curve and then sort of the the the quadratics are sort of being fitted um to try to identify the minimum right so the red line here is the is the minimum across all these points as we go um upwards and this is trying to tell us the optimum batch size for a particular choice of model size um and data set size. Um and then at this point you know you can follow the same logic as the Kaplan paper for identifying the batch sizes. Um basically you you reproduce the same kind of plot if you remember the Kaplan uh paper and the critical batch size discussion from two lectures ago. um if not you can kind of pull up the lecture slides. Um you'll remember that basically the thing that's highly predictable is the loss that you're trying to train to the terminal loss and the batch size of like the critical batch size point. And so we see that once again much like in Kaplan um you see a log log linear relationship here um between the target loss or the the terminal loss and the batch size that you want right and so from this you know you can kind of figure out what batch size you're going to get right because if you have a particular target scale you can use scaling loss to figure out what is the loss that I expect to get once you know the loss that you expect to get you can use that to to back out what batch size you can kind of operate at right so there's a fairly clean trend polinomially increase the batch size as loss decreases Um now batch sizes do sort of shift around as a function of um of uh target loss and thus compute. So we have to fit a scaling law for that guy. But we already did mu and so in theory right if the approach works at all um what we should now get is we should get that the optimum learning rate here is stable. So on this plot we're seeing um essentially different model sizes from sort of small models in the light colors to their biggest models in in the dark colors. And you see them sort of varying different learning rates. The big models they're only running for a little bit for compute reasons. But what you see is a is a fairly clear trend and you know once again very consistent with some of the earlier results that we've seen um in Kaplan at all where you have a relatively wide basin and then sort of sharp increases as your your model becomes very unstable. Right? But the important thing here is that the minimum remains fixed across relatively large orders of magnitude. Right? From your small model to the big model, the minimum or at least tied with the minimum is at the exact same point at roughly uh 10^ the -2 um learning rate. And so this is a nice sort of piece of evidence or some validation that properly scaling um your model initialization and properly scaling your per layer learning rates allow you to uh avoid tuning learning rates over and over or even fitting scaling laws on learning rates um in order to try to predict what the optimal um learning rate is. Okay. And then the final thing is, you know, you might want to figure out um essentially model size to data trade-offs. If you're training small models, um you're going to be probably overtraining your models or at least you want to justify to yourself um why you're training on so many tokens. Um and so you might want to re replicate something like the chinchilla analysis. Um so the mini CPM people um had a really cool or nice innovation. um others have done similar things but I think they were the first to really popularize this in the LM setting especially um in the context of of uh chinchilla style scaling is kind of the following so let's say I want to fit a chinchilla scaling law um when I do that what do I need to do well I need to vary um the number of uh tokens and I need to vary model sizes right and so when I do that I'm going to fix a model size and I'm going to train a model for longer and longer it would be nice if I could sort of early stop and take the checkpoints of this model and have that be sort of the difference or changes to the data set size, right? Because earlier checkpoints see less data. It would be nice if I could use a single run to collect um all this sort of data scaling things. Unfortunately, what I'm showing here, right, is that kind of the cosign learning rates for different data targets are different, right? So, if you have a very small amount of data, you have a cosine that goes up very quickly or sorry, that goes up the the the warm-up is always the same, but a very fast cool down, right? You you train for a little bit and then you come down very quickly. If you have a lot of data, then you're going to very slowly come down to the end. And so your learning rates between a small data training run and a big data training run will be different. Right? This is a very very key point. Right? Lots of people get tripped up by this. You cannot use a single run of a cosign learning rate model and try to get early checkpoints and reason about data scaling behavior based on that. Right? This is this bites people all the time. And so in order to avoid this, what you would normally need to do is you need to train a model from start to every single end point, right? So you you have to train it to every single target. And so this kind of takes you to n squared runs, right? Like some of the runs are small, but you have to basically run lots of runs, each one with a target termination point rather than using a single run and collecting checkpoints. It's it feels kind of senseless that we have to do this. So, um, the mini CPM folks, um, popularized this idea of a of a WSD or warm-up stable decay learning rate. Um, and so this plot on the left really shows you, um, you know, what's going on here. Normally, what we would train with is something that looks like this cosine learning rate shown in yellow here, right? It goes up, there's a warm-up period, um, usually very short, um, to get to your full learning rate, and then there's a cosine that goes all the way down to kind of your termination point. And maybe you stay at your minimum learning rate. Um this is all of course optional. You might terminate here as well. You might go all the way to zero, right? Um and so cosine learning rate looks like this. And the issue here of course is that if I have a different target, the cosine is going to be totally different. So everything past the warm-up can't be reused. Now if you look at the this this new WSD, which is basically a trapezoid learning rate, what it has is three phases. It's got a a warm-up phase that's the same as a cosine. It's got a stable phase that's flat. And then it's got a decay phase that rapidly cools down the model down to its minimum learning rate. And of course, you can have variations of this. You can go up, down, and then stay stable at your minimum. You know, you can do any of these variations. But I think in general, um, the simplest form to think about is warm up, stable, decay, terminate, right? Um, why is this nice? This is nice because you can reuse the stable part, right? So the thing that you do is if you want to do chinchilla in almost one run, what you do is you sort of warm up. you have a stable run all the way to the end and then you cool down and then if you want to figure out oh how would my model have been if I used less data you rewind the checkpoints and then you do another cool down right and now you've got a exact warm-up stable decay learning rate shape without having done the training from the beginning right so this is a very nice thing the fact that the stable part essentially is flat allows you to uh do chinchilla style scaling or data scaling um in a single training run or for for mostly the cost of a single training run um and a lot of people now do Okay. Um, so they work very well. Um, mini CPM I think popularized this and you know I think a lot of people have since then adopted it and and we see a lot of WSD style schedules in many many places. Um, they you see curves that look kind of like this. If you have a cosign learning rate um schedule, you'll see essentially relatively predictable smooth decay towards your terminal loss like this yellow line here. Um, if you train with WSD, uh, you'll see much much funkier learning curves that look like, you know, the the curves that I have here above them, the darker lines, right? So, you've got your warm-up phase, which doesn't really show up in this training curve. It's so short. Then you got your stable phase where it sort of goes down normally, and then as soon as you hit your decay phase, like the cool down part, your loss really rapidly drops off until you've hit your sort of either zero or or minimum learning rate point, at which point you you've gotten your terminal loss, right? So um these losses may look very disturbing to you but they are actually pretty normal when you're training with these kinds of sort of rapid cooldown learning curves. And maybe the point to make here is at every single token count you see that the warm-up stable decay curve you know the minimum point um beats or matches the cosine learning rate. That's not always the case. There can sometimes be cases where cosine works better WSD works better. But in general, I think a lot of the I think things that people say here is that the two learning rates are roughly comparable, but WSD has the additional nice advantage that you don't have to worry about your termination point. You can repeatedly cool down to get checkpoints of different data counts. Okay. Um, cool. Okay. And then of course there's um other things that have appeared for for trying to estimate chinchilla. um some folks um a collaboration of like Udub, formerly Udub and um Apple folks had this um paper on estimating sort of the chinchilla penalty that is when you keep adding more and more data you know how much worse is your loss um than if you had you know scaled according to chinchilla. So you have your sort of teal line here which is sort of m equals 20 you know 20 tokens to uh parameters and you can sort of think about okay what happens if I train with 320 tokens to parameters well then you have a separate parallel scaling line and then you have another line which is the circles which is what if I train with 640 um or the darker one is there and so the thing that they show is actually instead of doing this WSD style thing another thing you could do um is you could try to figure out okay how much does my model degree grade as a function of sort of higher tokens to parameter ratios. Well, that turns out also to have a fairly predictable shape. Um, and you can sort of extrapolate that based on sort of degradation at small training runs. Um, I don't think I've seen large scale training runs using this idea, but it's kind of an additional cool thing to know that essentially you could do chinchilla in almost one training run by sort of extrapolating the the uh excess token penalty at a small scale as well. So okay going back to mini CPM um now we have the tools that we need. We have the WSD learning rate which allows us to essentially do one training run and that one training run allows us to you know have both variation sorry not that one training run allows us to vary data as we go along and then we have multiple training runs for different model sizes that gives us all that we need to do chinchilla analysis. Um and they use method one and method three if you remember what those are. Method one is you overlay all of the learning curves and you take the lower envelope and the lower envelope of all the training curves um is supposed to be um roughly a power law. And then method three is you basically jointly fit this equation two you have here. You you hypothesize this two variable scaling law and you kind of fit it to all the data that you have uh in kind of curve fitting style fashion. Um and then that you know allows you to solve for the optimum uh token to to data ratio uh through that fit. Um so they do both of that. Um they do see for chinchilla method one fairly clear although not perfectly linear trends that allow them to essentially go from um uh compute to to token ratios. Um and their primary approach that they use to to justify a lot of their design decisions is the method three. It's the curve fitting. So the the kind of contours that you see here is the curve that they fit. The dots that they have here is the small scale runs that they did to fit the chinchilla parameters. Um and just to you know um sort of uh justify what they do they find very very high token to parameter ratios like so high that I feel like this is an outlier that doesn't really agree very closely with most of the other literature. Um they they argue that llama style architecture should all have a higher ratio because of you know improved data quality and improved model efficiency but their token to parameter ratio estimates are really really high 192 tokens per parameter which I don't think I've seen um anyone else derive I think other people have done replications of chinchillaa I don't think anyone's ever really done uh or or argued for 192 tokens to parameter regardless um you know we have seen that recent models like llama 3 have significantly higher data to model ratios. We also don't really see diminishing returns. Like these models aren't way worse than the equivalent chinchilla scaled like llama 2 models. This kind of suggests that with careful optimization and careful tuning, we should be able to go far beyond like the 20 times model size rule of thumb, right? So if there's one thing you take away from this set of sort of, you know, these last two slides, maybe not necessarily that you should trust whatever scaling law fits that mini CPM vid. um but rather that sort of the chinchilla analysis isn't really um you know a strong constraint right like 20 times model size is just a starting point you should be you know feeling free to significantly increase that token to parameter ratio um finally the the curve fits that they get are generally pretty good looking um so this is the scaling lock curves for essentially um data and model size um scaling and perplexities on code in English they do have some really weird outliers that I don't really understand um why they get these um but their sort of fitted scaling laws are generally pretty good um as they increase the amount of data on their relatively small model. So this is one example of a you know large scale training run scaling recipe. So I'll stop here. Um things like WSVR are probably new to you. So um you know if you have any questions please feel free to to ask or or any of the other bits including um like the chinchilla replication and mu and so on. Oh okay sure the main adaptation of mu is in terms of initializing the weight of parameters right so the question was the main change in mup was initialization um so there's two things that will happen when you derive and you implement mup um one of them will be the initialization will change and the other thing will be that the learning rate changes um or the learning rate changes per layer right um and that is probably a more exotic object than many of you are used to Um the initialization actually is not that different. If you're already using like um a standard like um like kynang style initialization, that's already one over the fan in uh one sorry one over the square root of the fan in um which is going to be already the right thing. Um whereas the learning rate normally like unless you're doing something really exotic, you're using like a global constant learning rate, right, everywhere. So that's going to be a big difference um from what you're you're normally training with. So you can think of that as like the practical difference for a lot of the MUP implementations. Yes, was kept constant. Uh we saw that the curve was very close to the cosine decay. Yeah. So you're talking about this this curve and you're saying like oh when when we're in the stable phase of WSD like when we're up here the curve remains pretty close and like why is that? Well, it's kind of close, but also not really, right? Like if you look at this this last curve over here, um, you know, there's a big gap before we enter the decay phase between cosine and um, WSD. And I think this is like one of the pretty interesting, you know, mysteries about deep learning optimizers. Um, clearly you need a stable phase to to kind of get you far from your initialization, but also the cool down phase is what gets you most of your gains and your losses, right? If you don't cool down, this is a gigantic loss in your losses. Um, so the cool down is actually really critical. And you know, a lot of the gains from cosine versus here, like this relative gap, this is all from cool down, right? Um, and so a lot of the optimizer learning rate uh design is about this balance between like how do I keep learning rates high to to travel far from my my initialization but still have, you know, good decay on my learning rate to be able to unneal my loss down um to a very low value. So um the other paper I want to talk about is uh DeepSeek. Um this is the original DeepSseek LLM uh paper from 2024. Um and in many ways if you read the original DeepS LLM paper, you'll kind of you can know that these are like very serious science people, you know, because they do a lot of very careful scaling ablations and they're really trying to get it right when they scale up. And that's kind of an attitude that's shared amongst um sort of the the players that that get scaling right. Um so you know they have seven and 67b parameter models. Um you know at the time very high performance relative to you know llama um which is really the primary competitor at the time. Um and you know there at the time I guess you know llama 2 and mistrol were kind of the big players. Deepse comes in and they they're able to match the performance. Not quite the the flashy impact of Deepseek V3 coming in and sort of matching um OpenAI's uh GBD40, but you know, for a first time sort of attempt, this is a pretty remarkable result. And so, let's kind of dig in and try to understand, all right, like what did DeepSeek do that allowed them to go from essentially zero to um you know, at least for open source state-of-the-art um at the time, right? So, and I think um DeepSeek more than you know most other players maybe the only comparable one being mini CPM is very very open about a lot of the experiments they did and their the approach they use to choose a lot of these hyperparameters. Um so immediately we see one difference between Deepseek v1 and uh mini CPM and also Cerebris GPT which is that they don't use any MUP. um and they're going to directly try to estimate both the optimal batch size and the optimum optimal learning rate. So it's like a really direct method you might call it um and requires kind of a strong belief in scaling laws. Um so what they do is they you know take um two relatively small models and they have uh they run a grid over different batch sizes, they run a grid over different learning rates and they get losses um across this grid. they do the same thing at a larger scale and you know you can get kind of the optimum batch size and learning rate right um and so they're saying all right well this is a pretty wide basin so we don't maybe have to be you know too scared about messing this up and so then what they do is they know that all right this the choice of learning rate and batch size are both relatively forgiving but we do want to get the order of magnitude of these things correct right so how do we get the order of magnitude of these things correct Well, you know what we're going to do is we're going to train a bunch of models um with different amounts of you know non-mbedding flops and we're going to change um essentially across a grid the the parameters that I had before both the the batch size and the learning rate and by varying these you know we're going to have the optimum batch size and the optimum uh learning rate sorry across these different scales right so you can imagine basically making these grids across many different flop scales um and basically marking down a are for each one. Um perhaps unsurprisingly because it's the scaling law lectures, these things seem to kind of follow um a scaling law line. Um at least for the batch size, things seem more clear and you can kind of fit a line to here and you can, you know, extrapolate out to the big models that you're going to train what your optimum batch sizes should kind of look like, right? Um they do the same thing with learning rate. um and they sort of fit this line and they say, "Oh, these are the two learning rates we're going to use." Um it might be because the points are being plotted on top of each other, but I find this line to be particularly not particularly like somewhat suspicious looking. I mean, I could have probably fit a horizontal line and that would have also looked okay. This one, I don't know, even as a as a scaling law enthusiast, uh I'm not quite sure I would, you know, bet my life on this one to pick the learning rate, but they did. And you know, that's how they get the learning rate. Um now they also uh kind of follow best practices at the time. Um they do a chinchilla style analysis and they use once again a WSD style learning rate. Um where you know they're trying to essentially minimize the amount of repeated work that they do. Um they do kind of something a little bit weird or a little bit uh more non-standard where what they're doing is you know they do warm up, they do stable and then they do two sets of decay steps decaying down to zero. So it's like two, you know, decay phases um consisting of um kind of like 10% plus 10%. And they sort of analyze different choices of that decay phase. Um and they kind of doesn't seem to matter very much, but generally speaking it, you know, it's about 20% of the total compute budget is going to be spent on that cool down phase. Um and so they also show once again that it matches cosine learning rates. But once again the advantage here is that we can do chinchilla style analysis um for very cheap in contrast to the learning rate sorry learning rate fits um you know chinchilla style analysis just fits really really cleanly. I think this is a a broad lesson when you look at lots of people's scaling laws. I think the stuff on hyperparameters always looks a little noisy and and tenuous. Um but the isoflops analysis from all the players look always like very very nice. And so this is you know replication of the chinchilla result. You see you know different compute scales. We see different quadratics. We draw a line through the bottom of the quadratics. We get um you know exactly the kinds of of sort of optimum uh sorry optimum flops per token and optimum token size as a function of training flops. Right? So this gives us a very straightforward way of analyzing uh the token size to model size trade-offs. Um and this allows them sort of do everything from scratch, right? Of course, I think, you know, as a side commentary, I think it's really nice that they're kind of redoing a lot of this. Like they could have certainly cargo culted um chinchilla and just picked 20 tokens per parameter but they said no like let's actually go and do the the scaling law analysis and like let's actually make sure that you know that the token uh sizes are relatively appropriate for us. Okay. Um and then they have you know a fitted scaling law at the very end. Um this is in some ways not surprising because this is after they fix their scaling strategy. They do predictable scaling. they try to predict what happens on the 7B and the 67b models. Um, you know, it's unsurprising in many ways, but very nice that they're able to extrapolate out from about uh 10 to the 20 to 10 to the 24 um and actually nail the prediction on the basis of the scaling law. Right? So, it's a very nice thing to be able to see that we can actually get um uh predictive uh measures of of model capabilities before we actually train them. So, that's um kind of the deepseek part. Anyone have questions for for kind of the Deep Seek strategy and kind of what they did and and any of the other pieces? I think most of this I think WSD was probably the newest thing that I've sort of mentioned today. Um the other thing that DeepS seek does is directly fitting a scaling law to the optimum learning rate and batch sizes rather than using something like yes. Uh do they have a global learning rate? Yeah. So so they're tuning that global learning Cool. Okay. Yeah. Once we know the problem. Yeah. So, so the question was like do people redo this kind of analysis for new frontier models? Um to be honest, I'm not actually sure and I I'm beginning to think that a lot of people maybe don't like exactly replicate some of this. Um because we see that in the newer paper is just increasingly less scaling details. Like even from deepseek um for example like deepseek v2 and then v3 we see a lot of emphasis on the new parts of each paper like so for deepseek v2 we see a lot of emphasis on like uh MLA and like the architectural improvements and then deepseeek v3 we see a lot of the systems components being emphasized like the the low bit training um but we don't see for example in either of those um any additional new scaling loss studies um and so I think my guess is that there's not much new there like maybe they're replicating it just to make sure it works but nothing new to report. Um, and I think that that will kind of um be captured in the next couple slides where I'm going to talk about uh scaling laws and papers and and models from the last year or so. Um, so I I did a little brief survey. Um, but actually there's nothing that is at the level of detail of either mini CPM or deepseek like those are really still I think the most detailed open studies into scaling that we have uh in 2025. Cool. Okay. So, you know, Llama 3 was probably one of the bigger model releases in the past year since I last taught this class. Um, and they do have some pretty interesting scaling bits. Um, for one, you know, just the question right now of like, do people actually replicate these analyses once they've run them once? Well, kind of yes. um Llama 3, you know, redo the isoflop style scaling um chinchilla scaling laws and they find you know roughly that the optimum ratio um if I if I got the calculation right is about 39 to1 right um and I do think this is interesting um because you know chinchilla got the 20 to1 parameter ratio um I think many of us have trained models at the chinchilla ratio um in our research and so on um you know it's quite clear that the 20 isn't really that stable like other people that have in fitting it have been getting generally slightly higher ratios than before. Um, and that might point to things like improved algorithmic efficiency in in sort of architectures that that learn better from data. Um, it might mean uh something else like improved data quality. Um, all of those are kind of moving parts. So, it's hard to know like what's leading to these slightly different ratios, but the results seem fairly clear. The the fits are relatively good and they do get a 40 to1 ratio. The other thing which is um close to the the data scaling stuff that I mentioned um the early parts of my first scaling lecture. Um one of the interesting things that the Llama 3 folks do is they try to essentially correlate um compute into uh NLS like log loss and then correlate those NLS back into downstream accuracies. Right? And so, um, the thinking that they're trying to do here is they would like to not really scale against log likelihoods. That's not really a thing they truly care about, right? They care about improving, I don't know, benchmark numbers on um, MMLU or Lombata or whatever other benchmarks that they've decided to hill climb on, right? Um, and so if that's the case, then what they're going to need is to have a conversion factor going from these NLS per character, these these, you know, perplexities or equivalent to perplexities, and then map them into accuracies. And so they they've done some studies in in llama 3 essentially trying to relate these two fitting sigmoids showing that you know um if you fit uh essentially these small models and you fit some llama 2 models and you fit a sigmoid on the whole thing you can accurately predict the performance of llama 3 uh 405b on the basis of those fits. It's interesting. Um I think they say that they use these kinds of ideas for for data selection. Um, but I think it's a there's not that much details there and it's unclear whether this is like a really core object um when Llama 3 was being trained or whether this was kind of a sidecaling thing that was like just of interest to the authors. Um, another recent work that has come out um sort of yet another Chinese LLM that's nicely executed is Hunuan 1. Um, hopefully I didn't really butcher the pronunciation there. um they are training and so because they're training they want to kind of redo the chinchilla style analysis they fit once again they do isoflops analysis they fit quadratics they figure out the minimums and then they're able to get a different token to parameter ratio so they get a 96 to1 data to active parameter ratio um these ratios are obviously going to be quite different because they're training there's lots of differences about the architectures we don't really expect the same thing as chinchilla right um and so we do actually see in in various papers Essentially replications of chinchilla happen again and again because a lot of these people are are very interested in understanding like how far can I push the token to um parameter size ratio. we would like to stay on the higher end of that right like have more data than parameters because then people will actually use our models or or our models will be cheap to serve right so for all those reasons um people have been replicating um chinchilla I think this is one of the the best replicated results in scaling in many ways um the the actual 20 to1 parameter ratio isn't the thing that you know consistently replicates but the fact that you can do isoflops and fit the minimum and get these like very predictable um tradeoffs in flops to uh minim optimal act optimum parameters is um quite clean um and consistent in the replications. Okay. Um the last one which is honestly a little bit more of an exotic um uh scaling law over the last year is minimax one uh which came out pretty recently. Um so minimax one is a kind of linear time or long context language model um released by another sort of Chinese startup. Um and their interest is well what we're going to do is we're going to take softmax attention which is quadratic and you know they have this thing called lightning attention which is a kind of linear um attention or linear yeah linear attention uh layer which is linear time. Um, and then you know they have a hybrid version of this model and they want to figure out like all right, how much cost am I paying in terms of the performance of the model going from softmax to to linear to hybrid um attention. And so they do things like they basically replicate method one for chinchilla where they're looking at the lower envelope of the loss curves as they train. They look at essentially the implied optimal model size and the implied optimal token count as they go. And roughly the conclusion that they draw from this is that you know the the lightning um and the hybrid models you know roughly perform the same as the softmax attention um and thus they can they're you know okay to train long context models on the basis of these architectures. Um, we've seen these kinds of plots um occur very often in research papers. Like if you look at the the Mamba paper or the Mamba 2 paper or any or the Delta paper or any of these other kinds of linear uh time complexity R&M papers, you'll see plots that look a lot like this where they say, "Oh, the full attention scaling and my linear attention scaling are basically the same as a function of compute." But this is, you know, um I would say like a kind of a rare case of this same plot being produced um almost at scale from a uh major sort of artifact release. Okay, so putting all that together, right, I know that was like a bunch of mini case studies uh that I I went through fairly quickly. Um but I want to sort of step back and recap it a little bit, right? We've seen several common ingredients being used in these scaling recipes. Um we've seen cerebrus deepseeek mini CPM um and then the few new uh papers since um so Cerebris GPT and mini CPM both use MUP as a way to make hyperparameters more stable across scale. Um and you know they essentially uh mini CPM especially has um a nice uh WSD schedule which is a a thing they popularize to be able to do chinchilla style scaling. Cerebrus doesn't bother to to replicate chinchilla. Deepse seek does a little bit different thing. They assume that most hyperparameters just don't change with scale but they do a full scaling analysis on batch size and learning rate and then they use the scaling laws as a way to figure out optimum scaling. You know I've already noted that some of the scaling looks a little bit more suspicious than others but really this is a way to at least get the order of magnitude hopefully right. They use isoflops analysis. you know, they replicate chinchilla once again to figure out the model sizing and to make sure they're kind of in the right order of magnes. You know, Llama 3 and Hunan do uh isoflops analysis only. Um Llama 3 does a little bit more, but that's basically it. And then Minimax does the more interesting thing of of basically justifying architecture choices uh through the lens of a scaling law. But we see generally speaking um that there's a few different things that get replicated like chinchilla um and learning rate and batch size are really the things that people are really deeply concerned about when they're scaling models up and they sort of do things like fixed aspect ratio and just scale the total model size up and that's generally the way that people handle a lot of the the moving pieces uh of scaling up. Okay. Um any questions about the the case studies pieces? Actually, I'm going to stay here and just make sure um I've covered any questions that people might have. Okay, cool. Okay, so the second and and kind of last part of this lecture is going to be understanding up. Um hopefully through the case studies you've seen that essentially getting the learning rate right is one of the core concerns that people have. um and also the batch size. But in general, I think we want to have scale and variant hyperparameters. And it is the case that you know our choice of initialization and our choice of per layer learning rates are essentially arbitrary, right? Like there's no reason why we have to initialize them one way and not the other. Um and so if we could manipulate those sort of three variables to get scale and variance in our learning rates, that would just be really wonderful, right? Like that that would make our lives way easier and it would make, you know, small scale experiments much more possible. So you know I'll also talk you know first through the the math of this like how it's derived what's the justification what are the core conceptual objects behind trying to make models scale predictably and then I want to talk about uh a pretty pretty nice um preprint by an independent researcher um on basically just a bunch of ablations on up like what makes it break what is it robust to um does it work on a real you know transformer language model these kinds of questions um are explored pretty well in this preprint that I'll talk about um at the very end here. So, okay, what is MUP anyway? I feel like um maybe I've jumped the gun for the last two lectures because I've mentioned what this is without really giving you the core conceptual object that it's based off of. Um on the other hand, I think I'm I'm justified in doing this because I think most of the literature doesn't explain MUP that clearly either. They're just like, yeah, just scale the the initialization by one over the width and scale the per layer learning rate by one over the width. That's MUP. Um, but I think the ideas behind MUP are pretty interesting and worth discussing because I think they speak to to some core objects that that recur in uh deep learning in general. So, I'm going to be basing my slides off this uh preprint or paper. Um, if you're interested in kind of reading about MUP, um I would point you to this one. I think this and another um uh blog post called a practitioner guides to MUP I think are the two kind of readable um descriptions of what the sort of this paradigm is. Okay. So I'm going to base myself off this. Um the math is for whatever reason not exactly the same across these different presentations. So I'll I'll you know clarify that I'm basing the math off this one. So MUP is based off of this the following relatively simple ideas, right? So there's two things that we think should happen when we're training a neural network, right? So you know when we scale a neural network, we're going to make the in this case, let's just say only the width, the width of the network bigger, right? I'm going to fix the layer size or the sorry the depth and I'm going to make the width bigger as we go. Now if I do that, um, as I make the width bigger, I want the activations at initialization to remain uh, you know, big theta of one, right? I want it to remain roughly constant, you know, bounded above and below by universal constant, you know, roughly constant as I make the width bigger. It shouldn't blow up. It shouldn't vanish, right? Seems like a pretty natural thing to want, right? You don't want your activations to get too big. This is per uh coordinate. Now, the second assertion I want is that, you know, I'm going to initialize my model and I'm going to take a single gradient step. And when I take that single gradient step, I want to make sure that the change in activation should also be um big theta of one, right? Um, so both of these seem like very natural conditions, right? Because if you violate these, it's going to mean that, you know, as I make the models bigger, either the initial activations will blow up or or vanish or after one gradient step, my activations will either blow up or vanish, right? Those are both bad bad conditions, right? Um, and as a note, right, I'm talking about individual activations like coordinates. And so if you're thinking about norms, right, of of an entire vector of activations, right, that should look like, you know, big theta of square root of NL, right? Because each one of these are going to be roughly independent. So the norm is going to look like um the square root of the width the the number of uh elements in my in my um width coordinate. So um I can derive uh you know mu from those two conditions. So the first condition which is that I want my activation to remain stable um imposes sort of constraints on the uh initialization right so so I'm going to walk you through a very very simple example right so I'm going to consider a deep linear network so this is h of l so this is the activations at layer little l and that's going to be a function of the the weight matrix at layer l and the uh activations from the previous layer no nonlinearities no fancy stuff right like you know it's all square just forget all this complexities if you want complexities you can go read the preprint they'll explain um in slightly handwavy terms why those things don't matter um now the initialization I'm going to pick a gausian initialization right so it's going to be zero centered it's going to be a rectangular size of the of the sizes um that depend on the the sizes of my activations and then I'm going to have one hyperparameter which is the um noise scale um of this matrix at this layer. Sorry, there should be a little L on this sigma. Um, so now what can we say? Well, I want to understand the size of H of L, you know, at initialization. So, how can we do that? Well, one thing we can do is we can consider sort of the limiting behavior of this system, right? I'm going to take the um basically little n of L and little N of L minus one to infinity. And if I do that, this W is going to concentrate. It's a random Gaussian matrix. If you remember your your um random matrix theory, actually that's not a prerequisite for the course, but um you know if you know some basic random matrix theory, you know that the the operator norm of a g gausian matrix is going to roughly concentrate to this object, right? It's going to be sigma which is the noise scale times, you know, the square root of both of the coordinates added. And importantly, you know, you can write down roughly that this equivalence is true, right? So the activations at layer L the norm of that is going to be approximately equal to the operator norm of WL times the activation norm of H of L minus one. Right? And this is roughly assuming that W of L is independent of H of L minus one which is true at initialization. So I think you can basically make that a right arrow if you'd like. Um now I'm going to say I'm going to pick a particular choice of sigma which is going to be square<unk> nl<unk> nl minus one times this object. You can simply think of it as this right hand side thing. Um this is the exact form. This is kind of the more asintoic form that you can think of this as, but really it's just one over the square root of the fan in of your layer times the minimum of one and sort of the the aspect ratio um of your your model in case um your fan in is much larger than your fan out then then this sort of kicks in. Okay. So let's say that I pick this sigma what happens right roughly one over the square root of my fan in. So now what happens? I can I can plug this back in to this formula, the matrix concentration limit and also this approximation here and I can sort of inductively prove that every layer is going to have the right sort of activation size. So let's just go through all the layers and assume that up until layer L minus one I have this property right so that's the inductive assumption at layer L minus one I have that my activation norm is square root of N L minus one. Okay, so that's just an assumption. Now if this is true then I just plug all of these in right so I plug in square root of nl minus one into this component into wl operator norm I plug in the limit and then for sigma I plug in this expression over here you see that this inverse cancels this and then you're going to get um exactly that h of l uh the l2 norm of h of l is equal to um square root of n of l. So this is the the thing that we wanted because before remember we said we want to make sure that the activations remains big theta of one which means that the norm should be square root of n of l. So that's exactly what we get plus some lower order terms right. Um so you know this is a fairly clear step-by-step argument that shows you what the right thing to do is for initializations. I want to pick one over the square root of the fan in plus a small correction factor um in order to make sure that my activations do not blow up at initialization. Right? Um I'll pause here for a moment in case someone has questions. I feel like this is actually maybe the first like real math that we've done in the class. So maybe it's a bit of a context switch for people. I did not warn you that I was going to talk about a bit of math. Okay. Um is this all relatively clear for people? one over square root of fann. Yes. Okay. I'm gonna assume that everyone's on board with one over square root of fann. Okay. So now um we're gonna derive uh the second part of muup. Right. So the first part of muup was about initializations. The second part of muup is going to be about learning rates. Right? Um and so how are we going to think about learning rates? Well to think about learning rates I'm going to look at the second condition. The second condition A2 which says when I take one gradient step past initialization what needs to happen is that my activation sorry my update size needs to remain constant. It can't blow up. It can't vanish. Okay. So what does that mean? So if I have an update of delta WL on the weights at at layer L um what where does that come from? Well that comes from let's say I'm doing SGD. Um that's going to come from this expression. It's going to be a learning rate times um L which is my loss, the gradient of L which is my loss and then the activations transposed. Um in the case that my batch size is one, this is a rank one object, right? This is a rank one update to delta of L, right? Um and because it's rank one, you know, there's a nice easy expression. the change of WL times the um activation of the previous layer is equal to the norm of the change in WL the operator norm of this thing um times the L2 norm of H of L minus one right and now you know combine this with the fact that the change in activation at layer L is kind of this expression you can convince yourself that this is true you can write this out by by sort of figuring out what the actual final um activation is at layer L after the update and that and canceling out um wlh of ll which is a a shared term across left and right then you'll get this expression um you'll get that you know what is the update in h of l this is the object that we want to keep roughly um square root of n of l right the norm of this object so so let's look through each of these terms and look at what the magnitude of this is the first term here wl delta h of l minus one this you know we can assume is going to be controlled from the inductive assumption because this is exactly um the uh sort of the the delta h of l um that we have plus the condition a1 argument right condition a1 basically says um sorry condition a1 is going to say that uh delta of h of l minus one is going to be square root nl and then wl is going to maintain that norm um the more complicated parts is going to be these two arguments the second and third terms that we have here delta wl h of ll minus one and delta wl delta hl minus one sorry that's that's quite the mouthful um they all have the same order of magnitude actually. And the only thing that we need to really figure out is this expression here. What is the product of the previous layers norm times the operator norm of delta WL? Right? Because we don't really know how big the update is going to be in the weight matrix W if we knew that. All very straightforward stuff. Okay. Um and so the remaining argument is actually relatively straightforward. Um even though this is actually like a you know complicated jumble of things the intuition is actually very clear. Um the intuition for this is says okay what do I really need to figure out? The one thing I really need to figure out is this expression here. How much does the weight at layer L change right? If I can figure that out then I can sort of derive all the relevant quantities and solve for the learning rate. That's at a high level that's our strategy here. Um and so how can we possibly figure out a after one gradient step how much delta WL moves right that's really the key question well um there's an additional sort of sneaky assumption that then shows up here um and the assumption is something like this if our learning is well behaved then after a single gradient step then the change in the loss delta of L this quantity has to also be big theta of one right and why is that well because we don't want the size of our losses, the update, the decrease in our losses to kind of blow up or go to zero as the width goes to infinity, right? We want essentially our our improvement in losses to remain roughly the same order of magnitude no matter how big our models get. That's a stronger assumption than what we've had before. But assuming that's true, then essentially we can say, okay, the change in the loss is kind of like multiplying the gradient with the change in the weights. This left side is O of one. We know how big um this uh delta of L should look like. So now or sorry we know how big this delta wl looks like. Now we can solve for the gradient size. And once we have that we can plug that in here. We know delta wl we know uh the gradient of l. We know the size of h of l from condition a1. And now we can solve for the learning rate. And that's exactly what you get at the bottom here. Um and the you know if you work through the the arithmetic the final result that you get here is that the learning rate for SGD is equal to the fan out over the fan in right so so lots of steps involved and lots of like substitution and slightly sketchy bigo notation being substituted into the equations here. Um but once we do that we're going to end up with a very simple formula. Um, note that this is true for SGD. And those of you that have kind of been paying attention and like kind of staring at this equation are probably, you know, um, internally complaining. You're like, you have you have misled us because, you know, in a transformer, well, what what's NL over NL minus one for like a MLP that actually is like a a four, right? Because you've got a factor of four between DFF and and D model, right? And so this thing doesn't really change. It's just a constant in most models, right? unless your aspect ratios are like dramatically changing uh through your network. Um the reason why MUP is different from standard parameterization is because this derivation is for SGD where um the uh primizations look very similar between MUP and SP. Um if you do the exact same derivation for atom, you're going to find that um actually you're going to get slightly different things, which is that it's going to be one over the fan in rather than the fan out over the fan in. Okay, so here's the recap. I have sort of dragged you through um hopefully willingly um the derivation of the basic what people call the spectral conditions that define MUP. Um but now I will give you the kind of one slide highle takeaway of that result. Right? So um you know when we want to do something like mup if we following the guidelines from before directly what we will end up with is the following blue box. at initialization, you know, you set yourself to one over the square root of fan in times a correction factor. That's, you know, one if your fan is is uh smaller than your fan out, but you know, square root of the ratio otherwise. And then this is a a simple initialization uh for the scale of your gausian. Um for your learning rate, if you're doing SGD, then you set it to fan out over fan in. Um but if you're doing atom, that's going to be slightly different. It's going to be one over the fan in. Now um in case you already sort of know the standard like kiming initialization and so on off top of your head you know you can mentally compare what this looks like um to the standard parameterization. So in a standard parameterization if you're doing it right um you should probably be already setting your um gausian to initialize to one over the square root of fann so that's good that's already perfectly set um but your learning rates are probably being set globally to a constant this is fine for sgv not so fine for atom where the really big difference between sp um and mu comes in. So okay um that brings us right back to kind of the serious GPT paper. Now we have all the context we need to understand all the operations they do. Um if you look once again at the column over here of muup um the embedding layer is special. Um it doesn't really do essentially like any scaling because embeddings are one hot so their norms don't scale linearly with the number of of uh vocab elements. But ignoring that basically you see that all the layers get scaled down by one over the width. you know that's the initialization rule and then the learning rate rules are scaled by one over the width as well right so this is once again uh the learning rate rule for atom right so if you're using atom that's exactly the right thing to do and that's also exactly what they do in cerebr cerebrus GPT so hopefully that's clear and hopefully this gives you a sense of you know both the interestingness of manipulating per layer learning rates to get more predictable scaling and also maybe an appreciation of this idea of trying to control activations and um updates as a function of model width right like you know I'll pause for a moment there and just mention right that's like a very successful idea from physics right lots of physicists think about ideas like reormalization as I take limits of certain things I want things to remain stable I want them to not blow up or go to zero this is an exact application of that idea that's that's kind of an interesting um use of that okay um any questions about I don't know m up derivation or sus GPT or any of the other things. Yes, there's no assumption about any architecture, right? So any transformer or any model? Yeah. So so that is part of the subtlety. The question was you know what's the architecture assumptions? Um well I mean technically there's a even stronger assumption here. Oh why did I go? There's an even stronger assumption here which is that I'm assuming things are a deep linear network, right? I'm just multiplying matrices repeatedly. You know this is the kind of silliest network that you can have. Um, basically there are arguments for why adding nonlinearities are fine. There are arguments for how you would take the same arguments and apply them to the uh attention layer. Um, there are arguments for why much more complex things are needed for a gated linear unit. So each one of those architecture pieces needs a careful analysis in order to have a you know corresponding object. Um, yes. How are these like ends determined? Like it looks like they're indexed by layer. Right. Right. Right. Right. Right. So, so N subl is just the output of a matrix multiply and N l minus one is the input. So for example, if you have a MLP, you're going to have a matrix multiply that takes you from D model dimension to four times D model like the DFF dimension, right? So that would give you a NL over NL minus one of four for example. So all the different matrix shapes are giving you the NL and NL minus one. Oh, the fan in and the fan out of a matrix. Yeah. Exactly. Yeah. Yeah. So, so the input and output dimensions are determining all these objects. Okay. Excellent. It's also just in and out of the Yeah, fan in and the fan out are like the input and output dimensions. Yeah, I was using those terms exchangeably, but I should have been a little bit more uh uh clear. Oh, yes. Since Deepseek only has a global learning rate, does it mean that they don't have order one updates? So, so the question was like since DeepS uses a global learning rate, does that mean they don't have an order one update? Um so, you know, all this argument is asic, right? It's basically saying as I scale my width out to infinity, things will kind of be big or small. And I mean if you if you look at um the MUP plot for example, you do you do kind of see this, right? You see that the learning rates have to shift as the model gets larger in order to compensate for the fact that the updates are getting bigger and bigger, right? Um what's empirically, you know, been seen is if you do nail the learning rate, you don't need MUP, right? Like it's not like MUP is necessary for you to train a good model. It's really just an attempt to try to keep this shift as small as possible so you can use the same learning rate, you know, throughout scaling. Um, and if you go back to to DeepSeek, you know, um, if you remember the, uh, scaling law that I was, you know, being a slight hater for, um, you'll see that, you know, they they too have learning rates that that go down as a function of scale in order to try to compensate for the fact that the bigger models are going to have bigger updates, right? And so you know to to respond more directly to the question um yes in the case of deepseek right as we scale the model up you know our our activation updates will get bigger so we have to shrink the global learning rate or we should shrink the global learning rate to compensate for that cool. Okay nice questions. Um okay so that was kind of the conceptual somewhat mathematical components of mu. Um, now I want to talk about um the uh empirical aspects of MUP. Um, and so I'm going to talk through uh a preprint or I think this one's being published at Colm um a large scale exploration of of mute transfer. Um, and I like this because it's got a bunch of ablations and I think I'm a sucker for ablations. So I'll present any paper that has large scale ablations um in the course. And so they do um essentially with MUP as we've described it. Um just look at the right hand side which is the more relevant piece. you know, they're scaling down the variances. They're scaling down the learning rates by the the width, the global width of the models M. Um, and they're primarily keeping the the depth fixed, which is a little bit of an unusual scaling regime because usually you'd see scale depth and width together, but they really want to do a controlled experiment where they're only looking at width variations, and they want to see if MUP precisely nails um scaling in this regime. Um there's also a little bit of a of a kind of weird subtlety that all of the MUP papers seem to do which is that um if you remember your 224N lecture you know you remember that there's like a scaling on the attention um activations you scale you know you do your inner product and then you scale it down by one over the square root of D you know and I told you kind of this was a magic constant that was like the right thing to do um you know mu and other papers um use uh one overd scaling um instead of one over square root d for various arguments related to activation and update size um stability. Um and so that's another thing that I think was worth pointing out because you might sort of not initially think of that as being something that's related um to MUP. Okay. Um architecture is is mostly similar to the standard you know transformer stuff. Um, and as I already mentioned before, they only consider width scaling, right? So they take a standard transformer trained auto reggressively on, you know, pre-training text and they want to basically make the model wider and wider and wider on the MLPS um, and sort of the the model residual stream dimensions. They're going to make that bigger and bigger and bigger. And what they want is for the optimum learning rate to remain the same as they scale the width up. And if if it remains stable, then that's the big victory for MUP, right? So the game is hopefully clear to everybody. um you just want to scale with I want my learning rate that's optimal to stay the same. So um question number one is you know does it work? Well um the answer is yes. So we have different width 128 512 2048 we have different learning rates across the columns. Um and you know the sort of idealized strategy here is we run a sweep of learning rates at the small scale. I pick the smallest scale um and I scale that up and hopefully that base learning rate remains um optimal um and yeah it seems like you know learning rates transfer very reliably um across model sizes if we're doing this like you know somewhat precise uh with scaling and so then I think you start asking questions of all right um very similar to the to the previous question that was just asked of like okay when does mu break right so you can ask that question in theory but you can also ask that question in practice, right? So, I'm just going to try all sorts of modern variations to architectures that people do and then I'm going to ask, you know, does this hyperparameter transfer thing continue to hold under these variations or not? And, you know, the the paper is quite nice because they just go through a lot of different stuff. They'll vary the activations, they'll vary the batch sizes, the initializations, the RMS norm gains. Uh they'll even use like really exotic optimizers like sort of sign gradient style stuff. And then they'll also vary the regularizers. So which one of these prevents learning rate transfer? Um so the first one you know which I think is probably relevant if you were kind of looking at that deep linear network and saying oh no one just multiplies matrices together there's like nonlinearities in between right so does m work when we change nonlinearities around well um swigloo squared relu um and the baseline um uh sort of mu approach um of relu all have the same minimal learning rate. So no changes at all. Um you know we just see that for example swigloo and squared relu just do better uh than baseline muup. Unsurprising sort of agrees with a lot of what we've learned in the course right um we might vary the batch sizes because we know that batch sizes are kind of going to be um sensitive to scale like we've seen mini CPM and we've seen deepseek basically fit scaling loss to batch sizes to try to get what the optimum batch size was. Um you know once again we see that uh you know as we scale up batch sizes by four up or down you know learning uh optimum learning rates uh remain stable. What about initializations right um you know there are some initializations that you know people vary like for example um some people set the query matrix to zero so that all the different items get uniform attention maybe that's more stable. Um some people uh sort of the unmbedded layer at the very top they'll scale differently based on either you use standard parameterization or mu maybe that matters a lot um turns out neither of those do you know the center column the optimum learning rate remains optimal in all of these cases um you know what is it not robust to well you know it's not going to work for for every single case um for example uh if you um add sort of learnable gains um that turns out to break new Right? So you need to remove the biases. Um but you know if you if you remove them the mu works if you add them back in um don't necessarily work. Um similarly you can try um sort of more exotic optimizers. Lion is an optimizer um that takes like the sign of the gradient updates which to me feel a little bit crazy but I think this was searched this was found through like evolutionary search or something like this uh to find like the fastest optimizer. Um, if you use this kind of a more crazy optimizer, it really breaks down. Um, and I think this is what you expect, right? New is designed to adapt to a very particular optimizer like atom W to control the update sizes. So, you know, if you're using a totally different optimizer, I don't know why you'd expect the learning rates to transfer. So, maybe expected that this thing fails. Um, and then sort of finally, what is it, you know, also not robust? Turns out if you really have much stronger weight decay, um, MUP actually starts to to fail. Um, and so this is one of the few significant MUP failures that are in there. A lot of the other ones are kind of just like, oh, we maybe expected that or that's not standard to do. Um, you know, weight decay is something that you actually do do. Okay, so um, mu seems generally useful like if you take standard parameterization like kind of going back to the baseline, right? You might ask like all right but what if I just do you know standard baseline stuff you know you can't use the same learning rate your the same learning rate results in you know significantly worse losses at 2048 right like your model just blows up gives you basically you know degenerate losses you would have been very sad scaling up at the same learning rate you know and we see also that the learning rate needs to scale down predictably as a function um of the width on the other hand you know even if you scale up all the way to a 10b parameter model you know you see that the base loss remains the same um so they do one large scale experiment and they see that the learning rate remains sort of ideal at the 2 to the negative6 level which is a kind of cool validation right so they do the whole study at a medium to small scale they do one big sort of hero run and then uh the learning rate remains um optimal so um the empirical results on that look somewhat promising um the fact that uh meta used it for llama 4 is also quite nice um but as far as I know it's not a consensus that people use mute okay um so putting it all together um you know how do you scale in the wild? Um I have never trained a you know 70B model at you know super tinchula sizes and so we're going to have to rely a lot on case studies and we saw several examples of scaling um in the wild. We saw people setting things like uh model hyperparameters especially learning rate and batch sizes um using scaling laws. We saw you know people using things like new P or assume stability to try to avoid search over these spaces. Um, and then also the use of things like alternative learning schedules like WSD can decrease the amount of compute that you need in order to fit a lot of these scaling loss. So, um, that's all I got.