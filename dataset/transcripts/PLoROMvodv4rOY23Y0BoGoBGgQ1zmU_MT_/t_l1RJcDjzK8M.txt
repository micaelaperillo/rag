All right. So today's going to be the the second of the basic systems lectures. Um and now we're going to move on to sort of multi-achine optimization. And so the focus today is going to be all about parallelism across uh machines. And so the goal today is going to move from, you know, optimizing a single GPU's throughput to being able to understand the complexities and the details that are required to train really large models, right? And model when models get large, they no longer fit on a single GPU. So you've got to split up your your models across different machines. Um, but also you've got to be able to leverage all the different, you know, uh, servers that you have in order to train these models quickly. So we've got both compute and memory concerns that we're going to have to deal with and communication across different machines. It's going to be quite heterogeneous. We have different kinds of communication across GPUs at different levels of hierarchy. And so this is going to lead to different parallelization paradigms. Um people use many different parallelization strategies all together at once. And we're going to talk through each one of the the very popular ones. Um and then we'll talk about how you combine them together in order to efficiently train a very large model. Um and then I'm going to end the lecture with sort of looking at some examples um of how people are actually using these parallelization strategies um to run their large scale distributed training runs. Okay. And so that's going to roughly map to the different parts of this lecture. We're just going to talk about the basics of networking first and then we're going to talk about you know how do each of these sort of networking hardware concepts map to different parallelization strategies and then finally some case studies to close off with to show you how it all comes together. Right? So I told you about GPU scaling um last week and you know it's quite impressive seeing this you know super exponential curve of flops per GPU going way way up. But if we want to, you know, rapidly scale out um, you know, both our compute and memory, a single GPU isn't enough, right? We're going to have to wait for, you know, another couple years for for this curve to continue going upwards and upwards and upwards. So, if we want to train a really powerful language model here and now today, well, we have to rely on multi-achine parallelism. So, if we look at, you know, the world's fastest supercomputers, that's what's being shown on the right here. you know, the the fastest supercomputers have, you know, exoflops and exoflops of compute. Um those are kind of the green um lines that you see over there. That's what you're really going to have to rely on if you're going to try to train, you know, the biggest baddest language models today. Um and so that's the compute side of why you want to think about multi-achine parallelism. But we've also got a memory angle for thinking about the same thing, right? So these two are really the core resources and the core concerns that you're going to have to think about. So in terms of memory right many of the models are getting quite big and of course you know memory on GPU is also growing but not not quite as quickly and a single GPU is not going to be able to fit these models right maybe eventually in the distant future we won't have to worry about a lot of these um but we've got you know billions and billions of parameters they're not going to fit very nicely into a single GPU so we have to be very respectful of the memory constraints that we have so those are kind of the realities that we have to deal with and what are kind of the tools that we have to have to be able to handle these well you know GPUs I'm sure you've noticed in the class uh cluster don't come in sort of singleton right a single machine will have multiple GPUs within the same sort of physical uh rack and so here's an example I I took this I think from the GPT Neo X uh paper um but this is a old example but the same lesson applies to to uh the H100 machines that you have in class so here there's eight different GPUs right? They're connected to the various CPUs through, you know, fast uh interconnects. Um within each GPUs, you see this NV switch thing at the bottom. This is very, very fast connections across these eight GPUs. But if these eight GPUs want to talk to GPUs on a different machine, they're going to have to go through a networking switch. And you see this, you know, purple line that says HDR Infiniband, you know, that's a much slower connection um uh compared to the NVLink connection, right? You can sort of see the difference in the throughput. that's like about eight times um slower per lane. Um and so this kind of hardware hierarchy that we have is going to have big implications for how we're going to end up paralyzing our models in practice, right? And so you can kind of keep this mental model with you as I talk through these things. you know, we have very very fast connections within a single machine and then when we go across machines, it's going to get slower and then depending on the the kind of hardware we're using, there might even be another level of slowness once we go beyond let's say 256 uh GPUs network together. Um, many of you may already know this having taken systems or networking classes, but here's a very very brief refresher on collective communication operations. Um, and the reason why I'm going to bring this up is there's one particular important sort of identity or equivalence that you will kind of need to know to really understand some of the the finer points of the performance characteristics uh of the parallelization algorithms. Right? So I'll I'll talk through these um and then I'll talk through uh one important sort of performance implication. So the first one which all of you probably have heard of is all reduced. Right? So you have you know four machines, four ranks in this case, each one having its own sort of piece of data and what you'd like to do is perform some sort of reduction operation. Let's say I want to sum all these these inputs and then I want the output to be sort of copied over to every single machine, right? Um and this is going to you know have roughly the cost of like two times the total number of things that you're you're all reducing. um you have a broadcast operation and here I'm taking a single sort of input from rank two and I'd like to copy it out to all the remaining ranks right and this is going to have roughly on the order of one times the total number of sort of outputs um in terms of the communication cost and then we've got reduction where we got different inputs and that's going to be summed up and then sent only to one machine and then the two that are quite important um even though these may not be quite as common is going to be the all gather and scatter right so all gather is an operation where I'm taking, you know, a single sort of subcomponent of, let's say, my parameters from rank zero and I'm copying it over to all the ranks. Um, same thing with rank 1 2 3. So each of these are handling different parts of let's say the parameters and they're copied over to the rest of the machines. So that's sort of, you know, copying what I have to everyone else. And then reduce scatter, which is, you know, I'm taking um, uh, each of the rows, let's say, I'm summing them up and then I'm sending the result only to rank zero, right? So this is a partial version of an all reduce and hopefully this diagram makes it clear um how sort of reduce scatter works and so all gather and reduce scatter are quite important because in some sense they are the primitive by which uh many of the parallelization algorithm are going to be built and so uh this is this is kind of an important sort of equivalence or an identity I will refer to it um one or two times as sort of key points in this lecture if you want to do an all reduce right let's say I've got um different GP GPUs, right? AB, B, C, D. Um, and each of the GPUs are handling a different data point, right? And so I've got different gradients for each of these data points, and I'm going to need to sum those gradients, and then I need to pass all those gradients back to the GPUs, right? This is a classic data parallel operation that I might need to do across my four GPUs. So that would be an all reduce. Um one important thing though is this could be uh replaced with two operations a reduce scatter and all gather where a reduce scatter is going to you know sum sort of each of the rows um and then leave the result of the rows in let's say GPU 0 1 2 3 respectively right and then I'm going to do a all gather to sort of copy those back out to the remaining GPUs right so each GPU now is getting uh a full sum of uh a part of the parameters and then it's going to copy it back to the remaining ing workers. Um, and in the bandwidth limited regime, this is basically the best that you can do, right? All reduce the best that you can do is roughly matching, you know, the the bandwidth that you can get out of a reduce scatter and all gather. And you can convince yourself this by writing out how many sort of communication operations happen in both all reduce um and the right hand side. Um the final thing um that I want to sort of briefly touch on before I sort of move on to talking about the parallelization algorithms and this is like the one place I'll talk about GPU versus TPU. Um most of the the discussion today can actually abstract out the underlying hardware. Um but there is actually sort of one important thing that I'll I'll mention up front so that I can refer to it later as I talk through this. Um how do we network together different machines or different sort of accelerators in sort of GPUs? Well, you know, as I showed you in uh the GPT Neo X slide here, how in the GPU world this generally works is you've got nodes, single machines that contain, let's say, eight GPUs, and then you've got these switches that connect fairly quickly to each other. And these machines are connected all to all up to about 256 GPUs. So that's a a important threshold up until which you have very fast arbitrary communication between machines. And then above that, you're actually going to need sort of much more um slow communication. These sort of leaf switches and spine switches once you go beyond sort of roughly a single rack's worth of GPU. On the other hand, you know, if you look at sort of TPU design from Google, they actually take a very different approach to networking sort of their their machines, um you've got a single sort of TPU chip and they all talk to their neighbors very very quickly. And so this is a very sort of easily expandable what they call toidal mesh but you can only talk to your neighbors. Um and the reason why I'm talking about this right after the all reduce slide is if you think about you know doing these kinds of collective communications like all reduce or reduce scatter. Um you can implement them just as efficiently on a toridal mesh than you can on a alltoall connection. Um and so if you're optimizing purely for collective communications it makes sense to think about things like TPU networking um rather than GPU networking. And I'll talk a little bit about pros and cons of this later as I go through different um parallelization um operations. So, okay. So, just to put this together right now, we're going to start talking about a new unit of sort of compute, right? Instead of the GPU, the new unit is the data center. The whole data center is going to be the thing that we're going to be doing. Um and now we're going to try to come up with algorithms and sort of sharding strategies that get us two different things. The first one is linear memory scaling. So as I scale up the number of GPUs, the sort of biggest model that I can train is going to scale linearly with that, right? So I can train bigger and bigger models if I really want to, right? Um I also want linear compute scaling, right? As I get more and more GPUs, the the useful computation that I'm doing to train the model um scales linearly, right? And then finally, a lot of this these algorithms are going to be implemented by just calling these very simple collective communications primitives in various ways. And so when we think about the performance characteristics of these uh parallel algorithms, it suffices to reason about you know basically counting the collective communications primitives. So so that's kind of an important way to think about these. We don't we don't go all the way down to the low-level implementation um of these algorithms here. Okay. Um any questions on part one? Yes. Sorry, but from the previous slide, does it mean that it's better to do reduce scatter gathering rather than all right? So so this slide, right? Yeah. So the conclusion of this slide is that they're equivalent, right? And I think if you think about something like um parallel uh doing doing um gradient descent in parallel, all reduce is a very natural operation to do because you'll scatter your sorry you'll you'll distribute your data to different machines and then you'll have to all reduce your gradients together, right? Um but what I'm saying is this very natural thing to do of all reduce can actually be written as a sum of two different operations. Um and they're equivalent. So there's no performance sort of charact by going from this left representation to this right one at least in bandwidth and that's going to have you know important implications in maybe like five slides. So you can wait a little bit to see you know why I mentioned this. Okay. Any other questions? Good. Okay. So now we're going to get started. Um in some sense this is kind of the the exciting uh algorithmic meat uh of the lecture. And there's three kinds of um parallelism you know strategies parallelism things that we should really be thinking about. So the first one is data parallelism. So data parallelism at a high level is the idea of I'm going to roughly copy the parameters across my different GPUs. I'm not going to worry about splitting my parameters up. But I will take my batch and I will split my batch up and different GPUs or different machines will get different slices of my batch. Right? So that's data parallelism. There's lots of subtleties in how we execute that. Um, model parallelism now is starting to say, okay, I don't want all my GPUs to have all the different parts of my model, right? As my models get bigger, that's going to be a very big problem. So, I need to cut up my model in very clever ways, and I need my GPU to handle different parts of my model, right? So, that's going to be model parallelism. Um, and then the final piece is kind of activation parallelism. um we don't really think too much about activations in our day-to-day lives because you know the PyTorch handles it very transparently right but as the models get bigger and the sequence lengths get longer um the activation memory starts to be a really big problem so if you want to train these really big models with big big batch sizes you have to somehow manage the memory footprint of your activations and so we have to split those up too so there's some ways to handle that right and when we put all these together we will have all the tools we need in order to scale up both compute and memory gracefully as we have lots and lots of machines. Right? So, so these are kind of the core conceptual objects. And now we're going to talk about implementing each of these ideas efficiently. So the starting point of data parallelism is just sort of SGD, right? If we're doing very naive batch stocastic gradient descent, the formula for doing this looks like this equation um that I have, you know, right here on the slide right here. um I'm taking a batch size capital B and I'm going to sum up all those gradients and I'm going to update my parameters. Right? So naive data parallelism is just saying all right take your batch size B split that up and send that to different machines. Each machine will compute some part of this sum and then I will exchange all of my gradients together to synchronize you know after each sort of before each gradient step I will synchronize my gradients and then I will take a parameter update. Right? So now I've been talking to you about compute and memory scaling and all these things. So let's just talk through, you know, what it looks like for each of these, right? So for compute scaling, uh, data parallelism is pretty great. Um, each machine, each GPU is going to get B over M examples. And if my batch size is big enough, you know, each GPU is going to get a pretty decent batch size, micro batch size, um, and it's able to hopefully saturate its compute. Okay, so that's good. What's the communication overhead? Well, I'm going to have to transmit twice the number uh of my parameters every batch. Remember, an all reduce is going to roughly be twice the amount of stuff that you're all reducing in terms of communication cost. Um, and so this is okay if the batch size is big, right? If my batch sizes are really big, I can mask the communication overhead of having to synchronize uh my gradients every now and then. Memory scaling, I'm not touching this at all. Right? Every GPU needs to replicate the number of parameters. it needs to replicate the optimizer state. It's it's pretty bad for memory scaling, right? So, if we didn't have to worry about, you know, memory at all, this is a this is okay strategy. Um, but I think in practice, memory is a problem, right? Like I think everyone of you sitting here has experienced, you know, trying to put a big model onto a GPU and PyTorch telling you, oh, you're out of memory. Um, and this is, you know, really a problem with your training as well because if you can fit, you know, more and more batch sizes, that's going to make um the the data parallel more efficient. And so ideally, you'd like to save on memory. So, let's take a closer look at the memory usage of naive data parallel, right? Um, and the memory situation is actually worse than it looks. It's actually quite terrible. Um because you know you've you've done this in assignment one but we can sort of think about how many copies of our model we we need to sort of store and it's very large right depending on the precision by which we're doing some of our training. Um you're going to need to store something like 16 bytes of data per parameter. Um and in fact you need to store something like five copies of your weights. Um, and this is really quite bad because if you just want to think about your model parameters, technically you only need two bytes, right? So where did that factor of eight come from? Well, at least you need gradients. And if you're computing your gradients in BF-16, that's another two bytes. But then your optimizer state kind of shows up and that's a that's a really big problem because you've got four bytes of sort of master weights, the things that you're kind of accumulating into SGD, like these intermediate sort of sums that you're doing. um you need you know four or two bytes for for Adam's first moment estimates because remember Adam keeps track of historical gradients and then Adam also needs second moment estimates kind of like the variance uh of the the gradients that you've gotten in the past and like that's going to need another four or two bytes and so what originally looked fine is actually now looking quite grim and so you know this 16x if I just sort of draw it as a picture you know you realize that most of your memory usage at least in terms of kind of parameter memory is really being dominated by the optimizer states of your atom optimizer. Right? So your memory consumed is going to be you know a a function of you know how many bytes are being used for your um optimizer state and that's generally going to be even more than the core uh parameter and gradient memory usage. And so for a simple example of like a 7.5b model distributed over, you know, 64 accelerators, you're using a ton of memory, right? And this memory scales linearly upwards. Total memory at least scales linearly upwards with the number of GPUs. So that's that's no good at all. Um, but if once we sort of look at this picture, we get some very simple ideas. You you might wonder clearly or or maybe not clearly you know I need the parameters and gradients to be copied across devices that seems you know necessary to do data parallel but do I really need all the optimizer states to be on every single machine right and once you ask that question you know you can maybe get to this second row here and this is going to be this going to be called um optimizer state sharding and if we could do that then at least in this case we can go from 120 GB of total memory usage down to 31.4. Um, and then maybe we can start sharding the gradients. And then now we can get to 16.6 GB of memory usage. And then if we also shard the parameters, we can go all the way down to 1.9 GB of memory usage. And that would be a pretty good place to be because now we've sort of fully sharded out, you know, all of sort of the optimizer state and parameter and gradient memory that we need. Yes. Sorry. Why could we sh optimizer state if like if we're doing um I guess the gradient computation on each of them like reducing how can we have that is a very good question and the question is how can we shard the optimizer state you know when we're doing data parallel right GPU0 has to be responsible for data point one so clearly it needs to know about all the parameters and update it so how can it possibly shard the optimizer state um And in a way I think zero which is what this is this is the the zero overhead data parallel sort of optimizer um this is a very in some ways clever idea because it shows you that even when you're doing data parallel you don't actually need to copy everything onto every machine right you can be really clever about how you do sort of communications to avoid all of this so so I will I will talk through exactly this this is a great question um so what we're going to do is we're going to split up the optimizer states as I said so the first and second moments are now split up across all the GPUs. Um, but everyone has the parameters and the gradients, right? So, so why is this important? Right? If I have the parameters and gradients, let's say I'm GPU0, I have the parameters and gradients for everything. That's enough information for me to compute the full gradient, right? Like the full gradient update for this example can be computed. The only thing I can't do is I can't take that gradient and take an atom step, right? I can't update my parameters unless I see all of the optimizer states, right? So that's kind of the key idea. And so now what going to happen is GPU0 is going to compute the gradients for everything, but GPU0 is now only responsible for updating the parameters for the shard that they own, right? And that's kind of the key idea, right? We're going to distribute the work of updating the parameters and then we're going to synchronize the parameters back. So let me show you in in sort of much more gory detail how this works and sort of the reason why it's called zero overhead. So step one right every GPU uh gets a different data point let's say right I'm just going to simplify all this batch computation I have GPU 0 through let's say four and every GPU gets a single example and they compute a full gradient on the example that they own now what I'm going to do next is I'm going to reduce scatter the gradients right so I'm going to send the gradients that you know um I'm going to collect in some sense the gradients that each GPU owns so GPU0 let's say is respon responsible for this first quarter of the parameters, right? So the parameters are are the y-axis here and the x-axis here is GPUs. And so what we're going to do is we're going to reduce scatter to make sure that GPU zero has all of the gradient information from all the other GPUs for the subset of parameters that it is responsible for. Right? So now it gets this G uh gradient information from GPU 1 and GPU 2 and GPU 3 and that's all reduced into GPU 0. Hopefully that's clear. Now, now GPU0 has all the information it needs to update its own parameters because it has the optimizer state corresponding to this first part. It has a full summed gradient for this first part. And now, so it's going to take a gradient update on their part of the parameters using gradient and state, right? And so that now I have the full updated parameters for this subset in my GPU zero. And all I need to do is all gather all of the parameter updated parameters back in to all the ranks. Okay. So there's many questions here. I'll start here. Yes. When you say the communication cost is the number of frameworks that's per machine, right? Or is that? Sorry. Say say that again. The communication cost being the number of frameworks that's per machine, right? Or is that total? Um so the question was uh whether the number of crimes communication cost was per machine or it's total. Um here it's going to be total because um so so this is going to be like 1/4 of the parameter is going to be sent three times to this machine and then you repeat that four times. Was that machine? That that was also total. Yeah. Two times number of parameters is total because each block is going to have to be sent to every other kind of machine. Okay. Yes. So this question is not unique to what you're showing here but it made me think of it. So the outlines that we showed seems to assume like large largely assume independence of parameters but we've drawn all these like diagrams that show the opposite you know like we have connected nodes and all that and it seems especially interesting when we have when we're trying to split these and update them separately. Uh is does that create any issues? Okay, so the question was Adam W seems to assume parameters operate independently. I'm assuming because you're saying like we track like gradient sums like and then we diagonally sort of update the parameters, right? Um but we know that that's not fully diagonal and so is there a problem? Um there have been, you know, better attempts um at improving sort of Atom W to not just be diagonal. There's things like KFAC and all these other like second order style optimizers that people have come up with. um they haven't dethroned Adam even though they do have their advantages and there's some really interesting things that you can do with these kinds of improved second order preconditioning uh methods. Yes. What is the reducing? What is the rows that we're reducing over? Um so you're asking like what is the rows of this picture? Yeah. Yeah. So imagine this is like parameters here uh in the rows. So like GPU0 is responsible for some number of parameters. So this is a block of parameters up top. Um, and so when we do reduce scatter, we're saying take the gradients for example zero for this block of parameters. Take the gradients for example one for this same block of parameters and then sum them all and put them in rank zero. That's kind of what we're saying here. Cool. Okay. Um, and kind of the key thing here is we're doing a reduced scatter and an all gather, right? And if you kind of remember what I was saying before, well, a reduce scatter and a all gather has the same cost as an all reduce, right? And so there is a little bit of a surprising magic thing that happened here, which is that well, you know, we were doing an all reduce before on all the gradients to make sure everyone's gradients were synchronized. And that cost us two times the number of parameters. But if we're kind of clever about how we're doing the updates, well, we can do a reduce scatter and all gather. And in between the two steps we can do some computation. Um and that gives us the same amount of of compute communication cost but now at least for the optimizer state we fully sharted the optimizer state across the model. So zero stage one is in some sense free in the bandwidth limited regime. Um and gives you memory wins. Um yes suppress the memory contribution of the higher moments. Do people modify atom to higher moments because seems like it's um when what do you mean by uh you can suppress the higher order contributions right so uh for the first and second moments the amount of memory you uh per per GPU is divided by yes so it seems like you might as well show more I see so so you're you're roughly saying like you could track way more optimizer state to rephrase what you're saying you could have even more complicated optimizer state because you can divide that by the number of GPUs. Um while this is true, um what's what we're going to do next is we're actually going to make the other components scale with NGPUs. So that's going to make things in some sense not free anymore, right? Like optimizer state will continue to be the bottleneck if we can divide everything by the number of GPUs. So hopefully that's a reasonable uh convincing answer. Okay, so we're going to build up stage by stage to zero stage three, which is which is more complicated. Um zero stage two is still relatively simple. So now hopefully that optimizer state sharding trick made sense. I I think that's very cool. Um so now we want to shard even more stuff. So I want to shard the gradients across the machines. Um so roughly we can do the same kinds of trick as stage one. But there is one additional complexity. Um and so what what's the additional complexity? Well, you know we can never instantiate a full gradient vector, right? If I ever do the full backwards pass and I try to compute a full gradient vector, um, I might go out of memory, right? So I want my maximum memory usage to basically be bounded by this, which is like full parameters, sharded gradient, sharded optimizer state. And so what we're going to have to do is when we do the backwards pass, as we're computing the gradient vector, we can't instantiate the full gradient first and then do communication. What we have to do is as we compute the gradients backwards, as soon as we compute like a layer's worth of gradient, we're going to have to send that over to the corresponding sort of GPU that that it belongs to, right? Um so this is kind of how it works. It's roughly the same idea, right? So now um everyone has their own batch component. Everyone incrementally goes backwards on the computation graph. And let's say we're going to operate layer by layer, right? So layers are sharted, you know, atomically to different GPUs. Um so what's what we're going to do then is as we go backwards on the computation graph after we compute a layer's gradients um immediately call a reduction operation to send this to the right worker right so a layer belongs to some worker maybe it's like GPU number two in this case so we're just going to immediately reduce that send that to the to the worker um at that point and gradients are now no longer needed um you know I don't need to store the gradients on ranks 0 1 and three so I can immediately free that um and then now we continue this process and So all the machines have their fully updated gradients and now they have a full gradient for their share of the parameters. They have a full optimizer state for their share of the parameters. Each machine can update their parameters and all gather the parameters back together, right? Um this looks like it's maybe more communication because you're doing this kind of like um uh reduction operation um every layer, but this is only for a small amount of parameters, right? It's sharded and so the full communication remains the same. So zero stage 2 has some more overhead because we have to synchronize layer by layer and make sure that the gradients are properly sent to the right workers. Um but the overhead is pretty minimal, right? It's still very simple, fairly straightforward. Now the last one of these zero stage 3 is more complicated for sure. Um but it allows you the greatest win of all which is now essentially everything is divided by the number of GPUs that you have and you can get the maximum savings uh possible. And if you've heard of um you know FSDP right you've probably used that in in some aspect of your life in the past. Um FSDP is exactly zero stage three. So now you'll kind of hopefully today know how FSDP um works. So the same idea applies. We're going to shard everything including the parameters. We're going to do the same thing as zero stage 2, which is we're going to incrementally communicate and compute things so that we don't keep these big vectors of gradients lying around. Um, and we're going to send and request parameters on demand while we're going stepping through the compute graph both for the forward and backward passes. You know, as we go through, we're going to send things around on demand. Um, and of course, the key is to do this with as low overhead as possible. I think the thing that's really surprising about FSDP is not that this is possible, but that this is possible with relatively low overhead. Um you'll see kind of why it's low overhead in the in the next um slide. Um I I admit that this is maybe not the the most uh friendly graphic to start with, but this is I promise the baby version um of SSDP. Uh the the next slide is a little bit more more involved. Um but conceptually this actually explains everything. So what we're doing is you know we're going to have model weights and we're going to be all gathering the model weights as we go. So for each layer you know no single uh GPU is going to have all the parameters right. So I can't do the normal thing of saying oh GPU zero go ahead and run the forward pass. That's not possible. So GPU0 let's say is let's say it only owns you know the bottommost layer. So it does that computation and then it stops and says it requests all of the the parameters from all the other workers. So it stops and it does a all gather which is right here. Um you see there's a all gather step. It gathers all the parameters. Now it has the parameters that it needs to um do a forward. So it can step forward and and sort of compute the layer that it didn't have before. And then now it can free the weights. It doesn't need the weights anymore. Get rid of it. Now I can all gather the next layer. I can do another forward. Free the weights. And I can repeat this. Right? The activations have to be stored. So the activation memory here is is growing. Right? So that's going to be a eventual problem. But if we ignore activations for the moment, this is great because I load a layer, I do a forward, I free it, you know, the memory overhead is very low here. Once I get kind of to the end, now I can do the same thing with a backward pass, right? I can call backwards and every time I move backwards through uh the neural network, you know, I all gather for the parameters that I need. You know, I can do a reduce scatter to update, you know, after the gradients that have been computed. And now I can free the weights or I can free both the gradients that I don't need and the parameters. And at the very end, you know, I've got a fully updated uh model. And so we've got three different operations that we've got to worry about here. We've got an all gather, we got another all gather, and then we got another reduce scatter basically to update um the model after we take the uh gradient update step. So conceptually this is just a a single step beyond um zero stage two. But you do kind of see that there is um sort of more overhead. So the total communication cost is now higher. Right? We were kind of before we had two times the number of parameters. Everything was kind of free in some sense. Now it's not right. There's total of three times number of parameter communication cost and there's going to be you know cost associated with waiting for these communication things to finish. Um, but I think the really cool thing about FSDP is it's actually surprisingly low overhead. Um, you might imagine that because we're doing this crazy thing of asking for and sending parameters back and forth all the time that you know things will be really slow, right? Like we have to be communicating all the time. But you can do this this core idea of overlapping communication and computation. So you want both your sort of uh you want your GPU to be working while sort of the communication is happening in the background almost like pre-fetching so that by the time you need some piece of information it's already loaded up. It's already been communicated to you and you're good to go. Um and so I'll talk through this example at the bottom here. Um but this is kind of the key to making FSTP actually uh somewhat efficient. So let's imagine we have a computation graph that looks something like this. W1 w plus w2 w0 times x some input let's say is y right so some some very simple computation graph like this and then um you might run fsdp and you will get actually uh computation and communication that looks like this block diagram at the very end here um so the CPU you know it's nice that we did the that uh insight systems example last week because hopefully this diagram will now be clear right the CPU is going to basically dispatch a bunch of commands um asking you know the the communication part of the GPU to basically go and fetch some some uh parameters. It's going to dispatch things to the GPU to say okay all right do some matrix multiplies and it's going to run you know far ahead in some sense of the GPU right we've seen this when we were looking at the profiler uh last week now let's look at the sequence of both communication and computation that happens on device now um remember that I need to sort of gather things on demand so at the very beginning I have to make sure that everyone has the weights for uh layer zero or w0 here so I do all gather zero and I'm going to wait for that to complete and once that's completed I can do a forward step on w0 I can sort of compute x * w0 let's say right um at this point you know all gather one starts at the same time that all gather 0 ends so as I'm doing this matrix multiply I'm basically already starting to load the next parameters that I need of course my communication slower and so there is some gap but I end sort of much quicker than sort of the initial load so now forward one can happen and in the background once again I've started to to load you know parameter number two and this yellow slice here I'm now freeing you know the parameters associated with forward one and then now the other thing here is I'm repeating computation w net0 is used twice and so I don't need to communicate this again this happens very quickly and I can sort of uh do this very quickly right I have forward two now already loaded before sort of I needed it and so there's no bubble here and then I can free number two that's the entirety of the forward pass and you see that the gaps are relatively small here and we able to do a lot of loads before the compute needed to happen. And so by doing this very clever thing of kind of queuing the requests for weights um before you actually need them, you can avoid a lot of the overhead associated with communication. And then now at this point, you know, um of forward two, um I'm done with the forward pass. I can free weight number two and I start on the backward pass. And you see that, you know, all gather two for the backward pass is already done. And so I can start on backward two backward zero weight zero is already stored. So that's done. And then the high overhead here happens in the backward pass because I need to do reduce scatters and then all gathers and so on and so forth. Right? Hopefully you see this picture and you say, "Wow, it's kind of surprising that even though we're doing this crazy sharding, right? Like if you go back to this picture, you know, we've fully sharted the parameters, gradients, and optimizer states. Um, but the total bandwidth that we need is only three times rather than two times. So that doesn't seem too bad. And sort of the actual, you know, bubbles that we see are not horrendous, right? The communication is is almost being fully being utilized and the computation isn't stalling for very long. So, we're actually making pretty efficient use of the resources that we do have, which is cool. Okay. Yes. Where do the get prefetched to? It's like to my understanding that like let's say the GPU memory is like full where does the weights get prefetched to? Yeah. Yeah. So, so you need a buffer in which you can store these weights. And so, you know, the this picture is is not quite right. Like you will have some overhead that you need associated with reading these weights for the current layer. And also the other big elephant in the room is I haven't talked at all about activation. That's going to be like a big chunk because you've got a big set of activations for a full model uh that is sort of living here in some sense. Yeah. Cool. Um right. Okay. So this is kind of distributed data parallel like zero is in some ways the the way that people do distributed data parallel efficiently. Um and so there's different stages and you know stage one is is basically free right it's uh doing the same communication pattern as uh naive data parallel but you get to shard your optimizer state that's great you might as well always do it right zero stage 2 is twice the number of parameters so the total bandwidth consumption is the same but there is additional overhead in having to do this like incremental freeing of the gradients as you go backwards zero stage three is more involved you do three times number of pram communication cost but it's not so bad right like we did have some overhead in the in the diagram that we saw before but if you really cleverly mask your communication patterns it's actually pretty good and so people use data parallel even for for fairly slow um sort of links in your in your networking pattern okay and this is also conceptually very simple one of the the advantages here is you know especially data parallel doesn't care too much about the architecture right I didn't talk at all about how we actually implement a transformer in any of this. It's all very abstracted. And so this is one of the reasons why, for example, FSDP is so popular. It's very easy to write a wrapper that parallelizes sort of arbitrary neural networks um without having deep knowledge or deep uh introspection of what the architecture um is actually doing. And so, you know, here's some examples. I I worked out some examples because I'm always sort of running out of memory on my GPUs. Um, and you can kind of see what's the maximum size of the model that I can fit on a eight eight times a 100 80 gig, you know, node. Um, and so for baseline, you know, you might end up with like, oh, I can fit barely six billion parameter model. Um, whereas I think if I use zero stage three, you know, I'm able to fit something like a 50 billion parameter model. There's big savings um in my ability to fit larger and larger models by doing things like FSDP to to cleverly save uh on memory. So okay. Oh sorry there's a question. Yes. I guess I'm a little unclear as to like what are the difference then once you shard the parameters what's the difference between that model? Yeah. So model parallelism is really fundamentally about making sure that the parameters just like live in separate let me see if I can find so they never be communicated across. Yeah yeah yeah like so in in some ways it's true that we have sharted the parameters. So you could call this a kind of parallelism. Um but the whole point of model parallelism is to make sure that the parameters just live entirely in one machine. We're not going to like try to ship them across in various ways. Only the activations are going to get shipped across. Um and so you'll see very different discussions in the model parallelism section like the focus there will be on communicating activations rather than uh communicating parameters and that'll be a big difference. Yes. Let me see if the parameters are only on why are why are you performing an all gather? So, so you're asking about um this step like why are we doing all gather to gather weights onto all the machines? Is that when they're only on one machine? Is that right? Yeah. So, we need to basically put we need to take the weights that live on one machine and scatter or is it gather or scatter? Sorry, I want to make sure I get this right. Um the terminology is a little bit sketchy for me. So I want to make sure I get um sorry. Yeah. So what we want to do is the same as this, right? So each machine is going to have um some parameter that I want to sc gather across all the machines um in order to make sure that each layer is sort of properly uh sort of replicated across all the GPUs. Is that the right question that you're asking? Or are you saying like is there a simpler primitive that we could have invoked? Like are you saying broadcast is the right object rather than all gather? I think maybe it's written that way because of some exceptions about layers not living on individual GPUs, but I'm not 100% sure. I agree with you that like broadcast should be able to do uh the same thing if the the parameters live on only one machine. Okay, cool. Alrighty. Okay, let me make sure where. Okay, got it. Okay, right. So, um there is a key resource in data parallel. Um and this is actually a important idea that I want you to remember. Um with data parallel, batch size is actually a really critical resource um in the sense that you can't parallelize greater than your number uh sorry than your batch size, right? Because you can have at most one example uh on each machine. you can't go to fractional examples per machine. Um, and so this means that you know there are if there's limits to your batch size, right? You stop being able to use data parallel. Um, and there's diminishing returns to batch sizes. So, you know, in your assignment one, you may have played with varying batch sizes, but you kind of know that as you crank up the batch size past a certain point, you start to see sort of fairly rapid diminishing returns to, you know, your optimization rates. Um and there's lots of papers written on this. OpenAI has a really nice one um on something called critical batch sizes um where they basically argue that you know past a certain point you have very rapid diminishing returns in how much each example is contributing to your ability to optimize. Like basically the the intuition is that below a certain point you have a lot of gradient noise and reducing that is very valuable but at a certain point you're really fundamentally limited by the number of gradient steps you're taking rather than variance reduction. Um, and so that basically means data parallel alone isn't going to get you to arbitrarily large parallelism. And this batch size thing is a really important resource, right? You want to essentially you have a fixed maximum batch size and you can spend it in different ways. Um, and I'll talk about that later because other kinds of parallelism also benefit from having uh sort of bigger batches and so you use your batch size in certain uh parts. Okay. Um, and issues are going to remain with data parallel. um you know zero stages one and two don't let you scale memory. Zero stage 3 is nice in in principle but it can be slow and maybe more importantly and this you know relates to the the earlier question it does not reduce activation memory right um I ideally want to like cut up my model entirely and make them live totally separately because then the activation memory would also sort of be reduced and so now I I want better ways to split up the model so I can fit these really big models um in these GPUs and so that's going to bring us to uh model parallelism. Um we want to scale up in memory um you know without changing the batch size and we want an alternative axis where we don't need to spend or basically have big batch sizes in order to parallelize. Um and so what what we're going to do is it's going to split up the parameters across GPUs and in some ways that's like 03. Um but we're not going to communicate parameters anymore. We're going to pass activations around and that's going to be different. Um and sometimes activations are going to be much smaller than parameters and that'll be you know very good for us. So we'll cover two different types of parallelism. Um I'm going to talk about pipeline parallel which is conceptually simpler um but much more horrible implementation wise and tensor parallel which is conceptually maybe less obvious um but honestly much nicer to implement and more commonly used. Um and they're going to correspond to two different ways of cutting up the model. So I think pipeline parallel is maybe the most obvious way to cut up a neural network, right? You know that a deep neural network comes in layers, right? So if I have layers, a very natural place to cut a network is to cut it up at the layer boundaries. And so each GPU is going to handle some subset of the layers and I'm going to pass activations around like in this case each layer belongs to a GPU and GPUs are going to pass activations from from one to the other when in the backwards case it's going to pass you know the backwards gradients backwards from GPU 3 to zero right okay so that's cool that's great um what's wrong with this picture well I think you should see that most of your GPUs are idle most of the time this This is actually quite terrible utilization. Um, and so if I do this naive kind of parallelism that I described before, right? So if I have, you know, each layer um having a forward and let's say I have a single example, that's going to result in a in a diagram that looks like this. So different rows in this picture um are different um GPU uh different layers and also different GPUs. And the x-axis here is time where I'm going from left to right. So what do you see? Well, you know, I first compute my first layer at the very left here, and then the activations get past the second layer. GPU 2 wakes up, and it's like, "All right, it's my turn. It does its job, passes it to GPU 3, and then GPU 4, and now the backwards passes can begin." Um, and so on and so forth. And you see kind of this gigantic what people call bubble. This is a big overhead where you're doing absolutely nothing. Um, and you see that the GPUs are active one overn. Um so in some sense this is the worst possible parallelism of I've added four GPUs but I get the throughput of a single GPU right um and so one thing you can do is you know you can be a little bit more clever about what you do and you can say all right I'm going to have a pipeline right I'm not just going to cut things up in layers I'm going to have a sequence of things that need to be processed by each GPU so now let's say I have a micro batch right so each machine is going to handle sort of four examples Um, and what I'm going to do is, you know, I can finish my first example, my first uh data point, and I can send off the activations for that to my second GPU as soon as I finish, and then I can then get started working on my second data point, right? And so now I've over overlapped sort of, you know, communication and computation. The second GPU can start working while the first GPU continues to work. And now the size of the bubble can potentially be reduced by having bigger batch sizes, right? Right? And you can hopefully see why I said before that batch sizes are a resource. If you have a finite batch size and you have pipeline parallel, you can use that same batch size to make your pipeline bubble size smaller, for example, or you could use it to do data parallel, right? So there's many different ways that you can take your single batch size and then split it up into different ways. Okay, so now um your micro batch size can control the bubble time and in fact you know the amount of uh the ratio of your your overhead to the useful compute that you have is the number of stages minus one over the number of micro batches. So if you have big big batch sizes pipeline parallel could potentially be efficient but as we said before you know batch sizes are finite we can't just crank that up to whatever value that we want. So, you know, in general, pipelines seem really horrible. Um, you know, why do we do it? Why do we incur this cost of a bubble um in order to, you know, parallelize? Well, there's a couple reasons. Pipelines help save memory um compared to to data parallel. I mean, 03 will will also shard the parameters, but this also shards of the activations, which is nice. Um, pipelines can also have good communication properties, right? It only depends on activations. It's also pointto-point. So it's possible that depending on your topology and depending on what you have, pipelines might actually be very favorable for the slower parts of your network. Um and so you know pipeline parallel is often going to be used on your slower network links. So inter node or um even sometimes um across different sort of uh racks or across different data centers you might do actually not data centers across different racks you might do pipeline parallel, right? Um, one of the the examples of a thing that I was recently told by by some Google folks is, you know, they were saying actually one of the big advantages of TPUs is that we don't have to do pipeline parallel very much because, you know, all of our connections are are much bigger, right? Like they have this big tooidal mesh. They don't have this limit at 256 GPUs where they're suddenly going towards a slower network link where you might want to switch to pipeline parallel, right? So that's a real world kind of example of when you would start to think about pipeline parallel. And so this is an example from an NVIDIA paper. or I'll talk about this paper in uh much greater detail later. Um they've done some some really nice work showing sort of performance characteristics of different kinds of parallelism. But you kind of see with batch size 8 as you increase the pipeline parallel size the number of devices um your you know utilization per GPU sort of starts to really drop off. Whereas if you have a big big batch size of 128 you can get away with you know pretty good utilization um for reasonably sized pipeline parallel. Right? So batch sizes are really key to hiding the size of the bubble. Um otherwise you have issues. Um of course you can do you know different kinds of pipeline sort of uh pipeline strategies. So instead of you know having um these sort of like uh standard uh patterns for scheduling the bubble you can sort of cut things up into into finer pieces um where you're sort of assigning different stages assigning different sub layers to different device and you're doing different computations at different parts. You can then sort of interle the pipeline better. And sort of an advanced version of this that I want to spend a moment talking about and this is very very clever um is zero bubble pipelining or I think in in uh DeepSeeks lingo I think they call it dualpipe but the core single trick um is the same. Um so here if you think about it let's say we're doing you know the backwards pass to compute gradients you can split this up into two different components. Um the first part is about you know back propagating the activations. Um so this is you know as I go down sort of the residual connections I need to compute essentially the the derivative with respect to the activations and then you know as I sort of you know get to a parameter I also want to compute the gradient itself like how am I going to update the parameters not just how do the activation change with respect to sort of the previous layers and so to to give you a concrete example let's look at this bottom left diagram over here right so in this diagram um you see the forward pass this is a single MLP so we've got um multiply by a I do a nonlinearity and then I'm just going to output the nonlinearity, right? So this is a kind of a naive, you know, single part of MLP. Now let's look at the backwards. You know, I have sort of the the derivative with respect to the loss. It comes in and then I can compute, you know, how that's going to change um the the x's the inputs to my MLP. So this is in some sense the the derivatives with respect to the activations here. And then as I compute these, of course, I can use them to compute the gradients that I need to update my weights, right? Um but the important thing is this part this part of computing the gradients for the weights this can be done whenever right there's no sort of dependence of this um and so I can rearrange the scheduling for this computation to any part of the computation graph and so what you can do is you can sort of do your standard pipeline parallel for the parts that are serially dependent but anytime you have to do these computations just for updating the parameters you can sort of reschedu them wherever and so the key idea is you start with sort of a nice um what it's called 1F1B pipeline. This is a nice optimized reducing the bubble size schedule. Um and then you can take this and what you can do is you can separate you know this B which is this computation of the the backwards part. um and then W which is the computation necessary to compute the gradient of the weights and now I can do the computation of the weights the W's where I would have originally had a bubble right so the parts where you know I originally had these white um white sort of idle utilization components I can now fill them in with these W's right and so by thinking carefully about what the serial dependencies actually are you know I can now have something really nice where I'm getting actually good utilization out of my GPUs um to To be clear, um this is horrendously complicated, right? Like if you actually want to implement pipeline parallel in this way, you're going to have to like intervene in how you know your your uh autodiff is actually calculating these things. You have to have a cue that can track where things go. Um I heard a a funny anecdote in a conversation recently um from someone in a in a frontier lab sort of training LMS and they said you know actually there's two people in the group that understand how the pipeline parallel in our infra works one person left and so there's a single loadbearing person in our training infra you know like there are stories like this um pipeline parallel is infrastructurally very very complicated right it looks simple here um if you if you're interested you I encourage you to try and implement it um it does get pretty hairy pretty fast and I think that's a good note on which to switch to the other kind of model parallelism um because this is much simpler and this is often you know very cleanly utilized by a lot of frameworks and a lot of sort of even people training really big models rely very very heavily or primarily on this kind of model parallelism. So what other way can we split up a model right? So if we think about it um most of what we do is matrix multiplies right in a big model most of the computation is matrix multiplies most of the parameters are matrix multiplies or matrices um and so what can we do well if we can parallelize just the the map moles that would be pretty good and so tensor parallel is this idea that we can take a big matrix multiply and split it up into a set of submatrices that can be multiplied right so if I have you know this matrix multiply at the top right we have X um and sort of uh X * A= Y. You know what I can do instead is I can cut up A into half, right? And then I can also cut up X into half and I can compute the submatrices. I can sum them up and then I will get my answer at the end, right? So conceptually pipeline parallel is cutting along the depth dimension like the layers. Um tensor parallel which is what this is is cutting up along the width dimension of your matrix multiplies. And so we're going to decompose into submatrices and then do partial sums. Um so here's an example of what it might look like in MLP, right? We we have each GPU handling a different submatrix um of uh let's say a big MLP matrix multiply and then we're going to have collective communications to synchronize um the activations as we kind of need them, right? So what are we going to do? So this is a MLP um and sort of the the top half and the bottom half there's two different paths. These are you know splitting up the matrices. So I I want to do uh this operation yals gel x * a. Um I'm going to split up my matrix A into A1 and A2. And then on the right hand side I want to compute drop out YB. Right? And then I want to return the result as Z. So I'm going to also cut up B. Right? So I've cut up both of my dig parameter matrices into two parts A and B. Um, and in the forward pass, what I'm going to do is I'm going to take my inputs X and I'm just going to copy them twice. Right? So each GPU is going to get the same inputs and they're going to operate on it with A1 and A2. Right? They have the same kind of Oh, sorry. They're the same uh row dimensions. So it's going to be fine operating on them. So X A1 and X A2 um is going to give you some activations Y1 and Y2. Those are going to go into B1 and B2. And then I'm going to do an all reduce to sum them up. That's exactly the figure I showed you before, right? So you copy and then you all reduce and you get the answer Z in the backwards pass. Now it's actually the reverse as sort of the gradients come backwards in the backwards steps. This G is going to be um the identity. So I'm going to copy sort of the derivatives on both sides and I'm going to do sort of the backwards operation all the way through. And once I get to f um this is on all reduce, right? Because I've got sort of two derivatives um sort of coming in from both paths and then I sum them back up, right? So this f and g are synchronization barriers. In the forward pass I do a single all reduce. On the backwards pass I do a single all reduce just at two different places um in the computation graph. Right? So now you can hopefully see how this is a very nice way of wherever you have a matrix multiply you can just cut up the matrix multiply and sort of parallelize them across different um devices. Okay. Um and as you might imagine this is actually somewhat expensive. we have a synchronization barrier that lives kind of per layer. It needs to communicate um an activation sort of like the the the residual activation worth of stuff twice in a forward backward pass. And so tensor parallel this very simple idea um is going to require very high-speed interconnects. And so there's a rule of thumb. It's a very simple rule of thumb to remember which is that tensor parallel is applied within device or within a single node, right? So a single box um of let's say Nvidia GPUs is going to ship with eight different GPUs that live in that same box, right? And as I showed you at the sort of beginning of lecture today, they're very very high-speed connected, right? So those eight GPUs can talk to each other very quickly. Um and so it makes sense to use something like Tensor Parallel that's very bandwidth hungry um on between those eight devices. So what we you will typically see is that tensor parallel is applied up to eight GPUs where the eight GPUs live in the same machine um because that gives you the least sort of drop in performance. And so this is an example um from hugging faces sort of parallelization tutorial um showing you sort of the throughput decreases of different levels of tensor parallelism. You see that there are hits right 10 and 12% hits to to uh throughput as you do tensor parallelism. Um, but up until eight, well, maybe this is manageable. This is kind of the price you pay for just being able to paralyze more nicely. But then you go to 16 devices and you get this like kind of astounding 42% drop in performance. You go to 32 and you see another sort of 65% drop in throughput, right? And so you see hopefully visually here that you really want to stop at 8 for tensor parallelism. That's really the sweet spot because of the the kinds of hardware interconnects uh you can get your hands on. Okay. So how do things now compare to pipeline parallel right? Um well compared to pipeline parallel we don't really have to deal with this bubble thing that we had before. We don't need to consume sort of larger batch sizes in order to reduce the bubble. Um which is nice. And there's very relatively I wouldn't say very there's relatively low complexity in applying tensor parallel right all you really need to know about are where are the big matrix multiplies? Can I split them up and make them live on different devices? Right? The forwards and backwards operations still remain the same. right? Compared to implementing something like zero overhead or dualpipe pipeline parallel, you're going to be in much much better shape um doing this. So the con is that it's much larger communication overhead. Um you've got you know in pipeline parallel batch size time sequence length time sort of residual dimension pointto-point communications per microbatch. Um in tensor parallel you've got you know eight times that per layer and you've got all reduced communication. um it's potentially a very large amount of communication that needs to be done. So you know the rule of thumb as I said before is tensor parallel is used whenever you have low latency high bandwidth interconnects. You're going to see you know two to like 16 depending on you know what kinds of machines you have um of tensor parallel out in the wild. And I'll show you examples um as I talk through at the very end here uh of examples of tensor parallel. Okay. Um any questions of on uh pipeline or tensor parallel before we move on to the kind of third kind like sequence parallel and activation uh sharding. Yes. Are these uh can they both be used simultaneously or are they? Yeah. So the question was can they be used simultaneously? Um the answer is that yeah you do use them uh both. So I think we'll we'll get to examples later but I think the typical thing that you see is you for large scale runs you very often see tensor parallel. Um pipeline parallel is often used on top of that. I think the only example I know of that does pipeline but not tensor parallel would be deepseeek v3 um as far as I know. So within a single machine I guess you have like say like you have like five different machines you have like maybe the first 20% of the parameters are across the each first machine tensor parallel one and then that pipeline parallels into the second machine would be the next step. Yeah. So the the question was there uh do you do tensor parallel within machine and like pipeline parallel across machine for example. Yeah. So so you would do something like tensor parallel within machine and a combination of data and pipeline parallel across machines for example right. Um, and I'll show you the rule of thumb later, but basically you do pipeline parallel because your models won't fit. Like if you if you could fit your entire model, you just do data parallel plus tensor parallel or, you know, just maybe even data parallel. Great. Okay. Excellent. So then, you know, we've been talking about memory and memory is you know, in some sense a very important part of parallelization because we're going to be training big models. Um and so you know when you look at your memory you realize that actually activations are a really big part of your memory usage. So if you look at you know uh standard kind of forward backward pass I think this was one from one of the pietorrch tutorials. um you see that memory usage is very dynamic, right? So I I'll just talk through this because I think it's an interesting plot in general, right? Um you always have your parameters as you're training, right? Because that's static, but you know in iteration zero, you don't still have optimizer state at all. So actually you don't have that part of your memory use. But as you do, you know, your forward and backwards, you see activation grows grows grows grows grows as you, you know, accumulate all the activations. And as you start your backwards pass, right, your activation goes down because you're freeing it as you use up your activations and then you're accumulating your gradient. So your gradient memory usage goes up. And the peak is actually somewhere, you know, partially through your backwards pass where you haven't freed all your activations yet. Um, and you're still building up your gradients. And so in iteration two, you kind of see the same thing here, right? And so, you know, the point of this diagram is to say, well, we've thought about all the other pieces. We thought about the parameters. We've thought about optimizer state. We've thought about you know the gradients. Um but we have not thought about very deeply at least the activations. And so let's do that. Right. So the final complexity that I want to talk you through is the activation memory. So tensor and pipeline parallel can linearly reduce you know basically most things but it can't actually reduce all of the activation uh memory usage. And so this is an example um from one of the NVIDIA papers that's talking about you know how do you reduce um activation memory and I think one thing that's really interesting to see is as you make your models bigger and bigger so going from left to right you see that you know a parameter and optimizer state memory can can remain the same if we if we parallelize aggressively but activation memory just kind of continues to grow because some parts of it don't parallelize very cleanly. So no matter the number of devices you have um actually you can't really get rid of the growth of activation memory per device and I'll and I'll show you why in a moment here. Whereas I think if you do some slightly more clever things like recomputation um you can keep the activation memory low and that's really key to to paralyzing some of the biggest models. Okay. Okay. So what's the activation memory per layer? You've kind of done some of this you know transformer math and and calculus before. So hopefully you're you're now familiar um with all of this, but we can compute what's the amount of activation memory we need per layer. And there's a handy formula here. And this is the amount of memory you need. It's SBH * 34 + 5 A S overH. Um and some of these numbers are mystifying, but actually they're not so mystifying. Um you know, you can very very much see that there's a left term and then there's a right term. The left term comes from, you know, the MLP and like other pointwise operations. That's where SBH * 34 comes from. These depend on the size of of your residual stream, right? The H. Um, on the right side, you have a term that's actually, if you multiply this out, A S^ squ B, right? Because the H's cancel. Um, that's the memory that you need for the softmax term and and other sort of, you know, quadratic terms in your attention, right? Um, of course, if you use flash attention, you can drastically reduce uh and and use recomputation, you we know that we can drastically reduce that second term, right? Um so then let's say we do tensor parallel right we do tensor parallel everywhere we can. So we do it in the MLPS we do it in the uh KQ computations in um the uh in the attention computation um we will end up with something that looks like this and this is looking pretty good but not quite there. So activation memory per layer divided by t which is the number of sort of devices that we're tensor paralleling over right. So if we're dividing by 8, right, ideally we would divide all the activation memory by 8. But you see there's this straggler term SBH * 10 that has not been sort of reduced down. Um and you know if you think about what these are, these are the non-mapatmo components. So the layer norm, the dropouts, the inputs to the attention and the MLP, right? All of these terms will unfortunately continue to grow with size and they will not be paralyzed very nicely, right? And so the very last thing that we need to think about is to take those simple point-wise operations which thus far we have not parallelized um and we just need to split them up right um and there's a very simple way to split them up which is to say well if we're doing like a layer norm right these layer norms across different positions in the sequence do not interact at all with each other right like they just don't care about um anything else um and so what we're going to do is let's say we have a 1024 long sequence we're going to cut that up and then each device will handle a different part of that layer norm or a different part of that dropout. Right? Those point-wise operations can now be completely split up across the sequence dimension. Um and because you know now we're cutting things up across the sequence dimension, we're going to have to do some synchronization to make sure you know the the parallel computations that we did will can get aggregated back again. Um and so in the forward pass, these G's they're going to be all gathers and G bars are going to be reduced scatters. And in the backwards pass, the two are reversed. In some sense, there's sort of a duality here uh between the two. And what we're doing here is, you know, for the layer norm, we've kind of scattered things uh around. And so we're going to have to gather them back together um so that we can do sort of our standard computation. And then now whenever we get to the dropout, we want to scatter them back out into the sort of parallel components that we have. And in the backwards pass, we're kind of doing that in the reverse, right? Okay. So hopefully that is clear. Um, this is a very simple idea, right? We're just parallelizing sort of the very last components that we failed to parallelize before. Um, and so now we can sort of put all these different pieces together and sort of get to sort of the end, which is we started up here, which is no parallelism at all. We did tensor parallel, which allows us to divide everything that's not a pointwise op by t. And then if we apply, you know, the sequence parallelism idea, we can divide this component by t once more. Um and then you know we can do things like activation recomputation which is you know the flash attention trick to remove the second term. And the minimal memory that you can kind of easily get away with is going to be this thing on the bottom which is SB8 H34 overt. Um and this is often used um if you're looking at different formulas for transformer arithmetic on like how much activation memory do I use? You know you often see something like PH34 and then if you have tensor parallel divide by T. um because this is the sort of easy minimum that you can get for that kind of a memory. Okay, any questions on on uh sequence parallel and activations? Yes, I was wondering like the transformers like stack on top of each other. I suppose a combinational graph will grow more and more like imaginative pip combinational graph as like a dag would that ever become the communication between the engineers? You're saying if we have something that's a more complicated computation graph than like a single linear chain, will that become a problem? It's a good question. I haven't thought about that. I I would guess not. Like at least for tensor parallel, this operates purely layer wise. It doesn't really care about the dependencies. Maybe for pipeline parallel, there's opportunities for increased parallelization if there's more than one branch, but I'm not too sure. this general, right? Yes. Right. Okay. Cool. All right. So, um, there's a few other parallelism strategies that I'm not going to talk about, um, just because in the interest of sort of time and, uh, sort of fatiguing you because I think I've already dragged you through a whole bunch of low-level details about how to do parallelization. Um, so the first one I want to talk about is uh, context parallel or ring attention. You may have heard the term ring attention before. Um this is a way of essentially splitting up both the computation and the activation cost of computing really large attention. Um where essentially you're just going to pass keys and values around different machines. So each machine is responsible for a different query and then keys and values are going to sort of travel from machine to machine in a sort of ring-like fashion um in order to compute your KQV uh inner products. And the cool thing here is you already kind of know how to do this because you've done the tiling for flash attention. So you know that the um so you know that attention can be computed in this kind of online tile by tile way. And that's kind of what's happening um in ring attention. The other thing um which now that you know tensor parallel is pretty straightforward is expert parallelism. Right? Expert parallelism you can kind of think of as almost like tensor parallel in the sense that you're splitting up you know one big MLP into smaller uh expert MLPS. let's say um and then scattering them across different machines. The key difference with expert parallelism is that the experts are sparssely activated. And so you have to think a little bit about routing and the routing is not going to be um sort of as predictable let's say as the alltoall communication that we had um before in tensor parallel because now you know maybe one expert is overloaded your networking is going to be a little bit more complicated but otherwise conceptually you're living in kind of the same world as tensor parallel um for expert parallelism. Okay, so just to recap all the things we talked about, um I've made a little small table of the different kinds of strategies that we have. You know, we have DDP and 01. Um this is kind of the naive data parallelism thing that you do. Um here you have some overhead per batch. You have no memory scaling, reasonable bandwidth properties. Um but you consume batch size in order to be able to do this, right? You need big batch sizes to have big data parallelism. you have FSTP which is kind of like a nicer version of 01 in the sense that you can get uh memory scaling but you're going to pay uh overhead across sort of different layers right and so now you've got higher uh communication cost and you've got potentially synchronization barriers that lead to poor utilization pipeline parallel you know is nice in that you know we no longer uh have this dependence on um on this per batch aspects but and we can get linear memory scaling But we have sort of another issue which is this also consumes batch size and it's horrendous to sort of set up and use and so a lot of people like to avoid pipeline parallelism if it's possible. Then finally tensor parallelism is very high cost in terms of bandwidth and the amount of synchronization you need to do. Um but um this has this really nice property that has no impact on batch sizes. So it's like kind of the one parallelism strategy you can use that has no cost in terms of your global batch size which is nice right. So we have to balance a number of limited resources, right? We have memory which is one resource. We have uh bandwidth and compute which is another resource and then we have batch size which is kind of an unconventional resource but one that you should really think of as a limited thing that you can spend on different aspects of these to improve your efficiency. Um and there's a very nice uh TPU parallelism or TPU book let's call it um from Google that I referred to um last week but also actually they have a really nice parallelism section and they have this great figure that I wanted to show you uh before I moved on to some of the examples. Um so the key quantity as I was saying before is the batch size and depending on you know the ratio of batch size to the number of GPUs you have different kinds of parallelism become optimal and so they use sort of certain formula on how much communication and computation you end up doing um sort of uh for each of these models. also this a simplified formula um to sort of generate this plot and you can kind of see if your batch size is too small you have lots of GPUs and really you know tiny batch sizes then there is no way for you to be efficient right you're always communication bound which is this bottom half here and in fact you're you're spending most of your time on communication um as you sort of get more and more batch size eventually you can get to a point where uh if you mix both FSDP so zero stage three and MP which in this case is tensor parallel. You can actually get basically to a place where you're compute bound. So now you're not, you know, spending um sort of wasting your flops waiting for communication. And then finally, you know, if you get to a point where your batch sizes are big, then you can just get away with pure data parallel like pure FSDP is going to get you into a regime where you know your the time you spend doing computation is higher than the time you spend doing communication, right? Right? So if your batch size is big enough, you can just get away with FSTP. Right? So this is kind of a cool illustration of this idea of, you know, why would you mix these? When would you mix these? Why is batch size a resource? Hopefully this kind of shows you in a very visual way uh what this is. Okay. Um and so when you put these all together, you end up with what people call 3D or 4D parallelism. Um I think I've heard the term 5D parallelism recently. Um I wasn't quite sure what the what the fifth dimension was yet. I'll have to to read up on that. Um, but now you can put it all together, right? The different dimensions of parallelism. And this is a really simple rule of thumb. Um, I originally sort of looked it up and put this together last year, but turns out it's still the same this year. So you can sort of follow this uh now. So the first thing you have to do is you have to fit your model and your activations in memory, right? If you don't do that, you just cannot train. So this is a requirement, right? So until your model fits in memory, we have to split up our model. So we're going to do tensor parallelism and we know that up to the number of GPUs per machine that's very efficient that's very fast. So we're going to do tensor parallel up to that point. Now after that depending on things like your desire to deal with pipeline parallel and or your bandwidth uh constraints you're either going to use 03 or pipeline parallel across the machines right until you can fit your model in memory. Now after that point well until you sort of run out of GPUs you can now run the whole thing and your only goal is to increase the amount of total flops that you have on hand. So you're going to scale the rest of the way with data parallel because data parallel is it works well on um low bandwidth communication channels and it is very simple right and so that's going to give you a way of sort of using all of your GPUs. Now if your batch size is really small then there is a way of uh trading um batch sizes for better communication efficiency like if you haven't consumed all of your batch sizes of resource what you can do is you can use gradient accumulation um on your devices right and that'll let you basically have effectively larger um uh batch sizes even if you're memory constraint and that will let you trade your batch size for better communication efficiency since you're synchronizing less often um across machines. Okay, simple rule of thumb. This will let you train models with reasonable efficiency um no matter what you're doing. Um and so to sort of make this concrete, I'll talk through a few examples at the very end here. Um I'll flash through both this really lovely paper um back in 2021 um from Megatron LM. Basically showing you exactly these things um in pictures and also a lot of ablations as well as um uh some of the models from last year. So this is a big table of how they trained models uh going from 1.7 billion parameters to 1 trillion parameters. Um and they get great utilization on all of these, right? You see percentage of theoretical peak flops that they get and it ranges from 40 to 52%. It's pretty good, right? And so you can see tensor parallel starts at one and then they eventually go up to 8 and then it caps out at 8, right? And they so they're using tensor parallelism first and then pipeline parallel stays at one but once the models get big enough you know they can't fit these big models. So t pipeline parallel has to increase in order to kind of comp in order to compensate and then the data parallel size basically starts out as big as possible and then slowly kind of goes down right because you know as we increase the amount of pipeline parallel this is now consuming in some sense the batch sizes um and so you can't have effectively as big of a batch size um if they're being used in some sense for pipeline parallel okay so um careful 3D paralle ISM is going to give you sort of linear gains um in uh aggregate flops. So you see um if you do uh careful 3D parallelism, you see sort of very flat uh overall achieved flops per GPU, which is giving you, you know, if you add more GPUs, linear scaling in the total aggregate throughput. That's great. Um tensor parallel 8 is often optimal. Um you see this is the pipeline parallel size and the tensor parallel size you see going to 88 with a batch size of 30 or sorry batch size of 128 is optimal even if you have a smaller batch size you know tensor parallel size of eight remains uh optimal and activation recomputation um enables larger batch sizes and remember that you know larger batches can in turn help you sort of mask overhead for pipeline parallel. So activation recomputation even though it's more flops can pay for itself, right? We've seen that story play out already um in uh flash attention. All right. So the last part of this is recent language models like what do they do? So you know I've gone through a few papers to look at examples of what people's parallelization strategy is. Um uh OMO uh in the DOMA paper they do FSDP for 7 billion parameter model. um deepseeek uh the the first paper does zero stage one with tensor sequence and pipeline parallel. This is you know the the vanilla thing that I that I told you. Um V3 actually does something slightly different. They do 16 way pipeline parallel um 64-way expert parallel which is kind of like tensor parallel. Um and then zero stage one for their data parallelism strategy. E which is another Chinese model does once again zero stage one tensor and pipeline parallel. Um, and e- lightning because they're doing replaces tensor parallelism with expert uh parallelism. The final thing, if you're interested in kind of state-of-the-art, you know, distributed training with lots of details, um, Llama 3's report is actually really interesting to read. They have a lot of detail about how they do their networking, what sort of things happen. Um, and you see sort of once again the kinds of things I said before. You see a tensor parallel of eight. Um you see um CP or this is context parallel. This is only relevant for long context training which is this very last step. So you can ignore that. Um you got pipeline parallel and data parallel happening um in these sort of first two uh phases. You can also even ignore the first stage here because that's kind of the small batch size training that they did in order to be stable. And if you look at kind of their their rationale for how they do their parallelism strategy, you see exactly what I had said before of basically all right you want to do TP CP pipeline parallel and DP in that order in terms of the amount of bandwidth that you need where data parallel can tolerate these like long network latencies because you can do the sort of asynchronous fetching of sharded model weights, right? And so they're using kind of the strategy that I told you in order to train some of the biggest models. Um the funny side note about llama 3 um and you may have heard this sort of in sort of not rumors but sort of casual conversation with your friends um is you know there's lots of GPU failures when you train models at a huge scale right um they had 148 interruptions from faulty GPUs um totaling about 30% of the total interruptions that they had they had things like you know unplanned maintenance of machines and that was 32 uh different things you know 32 instances of interruptions for their training and so when you're training a model this big, you know, I've talked about the algorithms, but you also need kind of fault tolerant architectures to be able to deal with these kinds of things. Um, and I've also heard, you know, various stories of people saying the even scarier thing is not actually explicit model failures, but actually data corruption. Like GPUs can silently fail on you and give you garbage data, completely ruining your run. Okay. Um, and then the last one example is for GMA 2. And I wanted to end on this because this is a TPU example. um you know they do 03 which is roughly FSTP and then they do model parallelism and data parallelism right and so here you know as I said before the sort of TPUs allows them to sort of stretch model parallelism a little bit further okay so putting it all together um scaling beyond a certain point is going to require sort of multiGPU multi-node parallelism there's no single solution right so you want to combine all three approaches to sort of leverage strength and then there's simple and interpretable rules of thumb for how you might execute this parallelism in practice Right. Thank you.