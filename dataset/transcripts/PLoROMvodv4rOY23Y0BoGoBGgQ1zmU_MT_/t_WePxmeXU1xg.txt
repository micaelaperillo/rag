So today's lecture is going to be on data. In the previous lectures up until now, we've discussed how you train a model given data. So we've talked about the architecture, we've talked about the optimizer, tokenization, um scaling laws, parallelism, that's all given a fixed data set. And now we're going to talk about what data do we train on. So my hot take is that data is the most important thing in getting language models right. So Tatsu might disagree with this. He thinks scaling laws is the most important thing. But but here's my justification. Let's see what companies actually disclose in their papers. So if you think about all the openweight models llama 3 and even deepseek they obviously fully disclose their architecture and in the papers they actually talk a lot about um the training and how um the training works but basically don't talk about the data. So if you look at the llama 3 paper which is has a lot of details about a lot of things um this is basically what they say about their data. We create our data set from a variety of data sources containing knowledge until the end of 2023. Now to be fair, they talk a bunch about how they filter the data at at least at a high level, but obviously this is not really u much information about um um the data set. And there's some reasons for this um secrecy. One is competitive dynamics um and the other is uh they don't want to get sued. uh more than they already are I guess. Um so you know data is uh before foundation models I think data was clearly recognized to be important um because you need to annotate data to drive supervised learning. Now even though there's less annotation involved there's still the data work um and a lot involves a lot of curation and cleaning. So somehow we haven't moved much. Um data is fundamentally this kind of long tale of problem and I think the reason that it um people in think about it so much is that it actually is very scalable. If you think about building a model that does all different types of things, you can easily hire a team of you know several hundred people who work on different aspects of data like multilinguality code. um if you're multimodal you can do um different types of uh you know images and so on. Whereas architecture there's one architecture you have a small team that defines it and that's it. Data is very paralyzable if you think about how are you going to allocate uh resources in your um language modeling development team. So um there's multiple stages of training. So there's pre-training which is the focus of this uh majority of this class. um and you train on raw data usually from the web. There's um mid-training um which is where you curate a smaller set of high quality data documents um aimed at targeting particular capabilities such as math or code or long context. And then there's post-raining where you fine-tune on instruction following data or chat data or you do reinforcement learning to get the model to be actually something that you can talk to. This is where typically things like safety also fit in. So, but in practice the lines are blurry and often in you know the more recent models there's more stages but you know one does not know exactly what is there. But the basic idea I think is clear you start with large amounts of low quality data and then you um sort of uh train on smaller amounts of high quality data towards the end. Okay, just a bit of terminology that you've seen. So base model typically refers to the checkpoint that you get after pre-training and mid-training and then struck models are after post-training. So let's take an example of what this looks like. So this is from um AI2 which has been releasing a bunch of open source models. So we know exactly what's in the data set. Um so pre-training this is a typical pre-training data mix at least for open source uh models. Um those so there's some web pages from this thing called DC on baseline which I'll talk about later. There's code academic papers there's math um and Wikipedia and there's about uh 3.9 trillion uh tokens here. So now if you look at mid training um you see actually uh a bunch of the same sources but you know they're filtered down. So still DCL on baseline uh but it's filtered down from 3.7 trillion which was the majority of that data set to 700 billion. Um there's some flan data sets which I'll mention later still Wikipedia. Um we like Wikipedia I guess. Um and then there's some new data sets that are synthetically generated. Um and might as well toss in the GSM8K training set. Why not? Okay. So that's about 10 billion uh training tok to tokens. And then there's a separate paper called Tulu uh which does actual you know post training and here's the various data mix. So there's like uh basically chat um data from various sources and a bunch of synthetically generated data that captures different aspects. Okay. So so what are all these data sets? Um how are they chosen and processed? Um so to set expectations that not disappoint you um uh later um there's not really a good I think as you can might imagine uh you know formalism or principle for deciding these things um I think this is maybe not that surprising given um the nature of this class even for architectures we didn't really have a good principle but for data in particular I think data is something that's I think hard to teach because what do you mean by teaching data So basically I'm going to talk through the different data sets that people have used over time, talk about where they come from, some of their properties in hopes that you can use your inductive powers to figure out some sort of a um intuition for what makes good data, what doesn't. Okay. So um I'm going to start with pre-training um and then then I'm going to talk about mid-training and post- trainining but most of it's going to be on pre-training. Um I'm going to start way back in 2018. So this is the BERT model uh which some of you might still um remember. Um this is a big deal. So Bert was trained on um you know books and Wikipedia. So let's dive into what that um exactly you know means. Um I think the data sets often not really I think discussed uh very much and um because people look at the model and the eval capabilities. So there's this website called Smashwords which came about in 2008 allows anyone to go publish an ebook. So last year there were about uh you know 500,000 books and so in 2015 there's this actually vision language uh paper that essentially scraped a smashwords and created this book p corpus consisting of self-published books that were uh priced at zero. So they got uh 7,000 books. Um and this has since been taken down because it just violated the terms of service. So back in 2015, you know, it was sort of the wild west. People didn't uh think that, you know, AI and AI copyright wasn't really a much of a thing as it is now. Um so that's the books corpus if you ever, you know, see that. Um it's old data set, but it's it sort of um I think represents the importance of books that has sort of continued. Then there's every Wikipedia. Everyone knows Wikipedia. Um, just for fun, we can just point at a random article. Um, okay, sure. This is a this is a random article from Wikipedia. Um, if you click again, you'll get a different random article. Okay, so here's a build a random building in Indonesia, I think. Okay, so this was um it's been around for over 20 years and there's a lot of different articles in different languages. Um I think it's important to you know explicitly say kind of what's Wikipedia is. So it doesn't contain any original thought. So everything is coming from cit that's why there's citations of actual um um you know original primary sources. So there's supposed to be no opinions or personal web pages or anything. um and it's based on notability which is um you know means that multiple sources must have covered it. So I think this already gives you a kind of a sense of what's in Wikipedia, what's not. Clearly there's a lot of valuable content maybe in the tales that wouldn't be in Wikipedia and there's a lot of opinion that might be useful that's also not in Wikipedia like recipes are not in Wikipedia and and and so on. Um so anyone can edit and the content um but in practice a small uh major small number of people contribute the majority. So this guy had five million edits. I think he probably used some um some tool. So Wikipedia is this website. Now every once in a while there's a dump um that gets that's gets produced and you can go download your um you know some zip file with all the Wikipedia content. Okay. So just one aside is that you know Wikipedia we think of as very high quality sources reli re reli well maybe reliable more reliable than average internet article. Um and uh but there's this thing that everyone should know about which is relevant to data is um data poisoning. So the idea is that um this is Carlin has a series of you know wonderful results um how showing that everything is broken. um you can they show that you can inject malicious edits right before these periodic dump so you know when the dump is is coming and so you inject this edit so that it goes into the dump but before the edit is rolled back so it's you know I thought it was pretty clever and we know that if you can control the training data then you can um basically get the model that's trained on such training data to do various things for example ascribing negative sentiment to trigger phases like a iPhone. Okay. So a adversary might be able to leverage this process um and inject whatever they want into something like Wikipedia even if you have this roll back policy. So I think since then u this has been some patch. So I don't think you can literally exploit this. But in general, I think it's important to realize that the data that models are trained on comes from the broad internet where attackers and anyone with various incentives have actually quite a bit of control over the behavior of the language model and it's very hard to have oversight into this process. Okay, so Bur was trained, that was a bit of a digression, but Burr was trained on books and Wikipedia. Obviously back then people didn't really care about uh data uh poisoning for language models um as much. Um and I think Burr, you know, seems very old, but this was like kind of a big transition between uh training on documents rather than sentences in contrast to the billion world word benchmark that we talked about last uh last week. Okay, so that was 2019 or 2018. Um so GBD2 um collected a data set called web text and the idea here was that well you have the web and it's kind of large and probably low quality. How can we quickly get a diverse high quality subset? So the insight was that well if you look at Reddit posts there's a bunch of links that go out and these posts can get you know karma points. So why not take the links that um uh have are on posts with more than three comma points. This resulted in 8 million pages, 40 gigabytes of text and um that's what they used to train GPT uh you know 2. Okay. Um now they didn't release um besides the paper they didn't release the data set. So there's been since an um you know open replication of web text that's um often used um in language model research. Okay. So now let's talk about common crawl. Um I think hopefully by the end of this you you'll whenever someone talks to you and say well I language models are trained on the internet you can call them out and say you know that's just uh false and what does that even mean? Um so let's talk about common crawl which is maybe an academic approximation of the internet. Um so common crawl was established in 2007. Every month they re write a web crawl. Um so there's been about a hundred different web crawls over the last um you know however many 17 years. Um the crawl itself isn't actually that expensive in compared to language model training. You know you can rent some you know a AWS machines um and just get it done in like less than two weeks. Um, so the last crawl was uh last month and just to get a sense of what this uh this crawl looks like. So here's some statistics. Um, so there's about um, you know, 2.7 billion pages that were were were added. Um, and each crawl there's there's 100 crawls. Each crawl uh might have slightly different web pages, but there's some overlap because there's for it's not clear what the heristics are, but you might imagine that sites that are rapidly changing, they crawl multiple times and sites that don't change very much, they don't crawl as much. And there's a explicit attempt to diversify. Um so crawling, just to very briefly talk about this, um they use an open source library. you start with a set of seed URLs which is actually quite a large number. So it's not like one website that somehow you you crawl the web. It's actually quite hundreds of millions um and you basically maintain a queue where you have the crawl frontier and then you have a bunch of machines that um look at that frontier and go crawl um from there. Um so basically you're doing like a BFS of the web but there's a lot of uh systems and you know dealing with the fact that some sites might you have to be um little bit careful about crawling. Um so there's questions of you know which pages do you do you download? You have to respect robots.txt. You don't have to shouldn't overload the server. Um and if you've already crawled a site when do you go back and crawl it again? Um and then there's a problem that URLs are dynamic. So some URLs are very long, multiple URLs might lead to the same content which leads to a lot of uh duplication. Um so when common crawl uh crawls the the produces data in two formats. One is a work file and this is the raw HTTP response that you get um which is often HTML uh for well HTML pages. This does get converted into text into a format called wet and this is obviously a lossy process HTML to text it to loses information. Um one note is that um this is not the only way you can use their web file which is text or you can start with a raw HTML the work file and do it yourself. And there's a a few tools out there um in your you know assignment you'll be um uh trying out different uh tools or or at least using doing the HTML to a text conversion yourself. Um and this does make a difference. So um this uh the paper from data compat line which I'll talk about a little bit later uh did this ablation where you look at different um HTML to text converters and using the raw web files is actually four points a whole four points lower than using traffic flow tour for example. Okay so there's some low-level details here. Um one other thing about I'll say about common crawl is that um this is deliberately not meant to be a uh you know comprehensive in terms of crawling the entire internet. I think part of their policies is to be kind of gentle and you know polite. Um so um for example not all Wikipedia articles are even in in common crawl. Okay, maybe I'll pause here just to in case people have any questions about um data so far. Yeah. Common crawl do any like sensitive? So the question is does common crawl do any filtering of its own on sensitive content? Offensive content. Yeah, I think um by default they're very permissive because the idea of what is offensive or not is like a fairly high level semantic decision. So there's definitely a lot of uh offensive content in in common crawl and you know harmful content. Um there might be kind of life filtering. I mean there's some sites which might be like plain illegal or or something. They there might be some you know uh block list. I'm not sure about the exact details. Yeah. Yeah. So the question is can a website uh be flagged when they don't want to be you know included. Um and the answer is yes. Uh well yes there is a way. So if a website can include a robots.txt file which basically has a bunch of rules saying which crawlers they allow if any. So um if you look at the robots txt let's um so this is uh robots robots.txt. So for example, New York Times disallows um uh okay for Google bot it disallows a bunch of stuff and then um there's different rules and you can see all your favorite uh you know um you know LM provider. So it turns out that um many of the um not all these are LM um you know pro developers but it it turns out that most of the frontier model providers or developers have their own crawlers just because common crawl is actually turns out to be quite sparse in terms of coverage even though it's quite big but the internet is a is a very big place but but there's no formal way of ensuring that robots txt is is a kind of a guidance. So there might be folks that are not respecting robots txt. Um yeah over here and then how does the brower handle like embedding media or like images? I guess those are just sort of ignored like uh so question is how are images handled? So technically common crawl does it's just like has a URL and gets the raw response. So sometimes the response will be text and sometimes it will be images. I think most of common crawl it's sort of biased towards text because that's but occasionally you'll get like you know other stuff. Um of course you know there you could develop crawlers that explicitly go after media over there. Do we have any idea what fraction of copy crawl any of the other sources are copyright material that's not quality? So question is uh what fraction of common crow is copyright material um I'm going to talk about copyright later um but I would say that most of it's copyright um um and that's a complex topic so I'll touch on it briefly later okay let's move on so common crawl is is big and I think even on the first day of lecture I showed you that if you look just look at random samples from common crawl it's really no Um so there's uh been a lot of attempts to filter common crawl. Um one of the earliest attempt is called CCNet. This is from uh Meta. Um and uh the idea is that they wanted a generic procedure that could take um common crawl and return high quality you know data sets and in particular they were interested in the multilingual coverage. Um so they had a bunch of horistics. Um so they removed uh duplication. They ran language identification which is basically a linear classifier uh to to keep only examples of a target language whether it be English or or German. Um and then this is sort of the key part is that to filter on quality they look at documents that look like Wikipedia under um a five gram model. Um so they take Wikipedia text they train an engram model and then they use that to score uh documents and the idea is that Wikipedia um as you'll see has been used sort of as a surrogate for high quality data and using that you can get more things that look high like high quality where Wikipedia serves as a surrogate for um high quality and as we discussed Wikipedia obviously doesn't cover everything. Um uh so this is also not going to cover everything. So they train um a bunch of bird models at the time. Um um and they show that they outperform only training on Wikipedia. Um so um and CCNET uh is a bit confusing sometimes because it refers to both the tool which is a function of a filtering function but also the data set that they release from the paper. Okay. So, uh meanwhile, um Google was uh doing some stuff as well. So, they released this uh C4, which stands for colossal clean crawl corpus. Um and it's sort of a um I guess the same insight that you want to take common crawl, you might you want to leverage this large text somehow. Um this paper actually by column raffle uh is more famous for introducing the T5 model. Um but it actually introduced the C4 data set which is a main contribution. It's a long paper. Um and the observation is that you know common crawl as we mentioned earlier doesn't have um is most of it is not useful in natural language. Um so if you let's say you start with one snapshot so that's 1.4 trillion tokens already. They decided to use just heruristics. Um so they keep uh lines that end in punctuation. Remove pages with fewer than three sentences. Remove bad words. You can look click on this to see the bad words. I'm not going to show that here. Um they remove brace which is interesting which clearly re removes a lot of code. I guess Python may might be kept um and some like boilerplate text um and they only kept English. Um they got a lot of tokens out of that. So it's kind of interesting. You see the sort of trade-off here is that whereas CCNET used a model based approach to filtering to make it look like Wikipedia, this is entirely rulebased. So the advantage here is that there are sentences that don't look like wiku but nonetheless are wellformed sentences that would end up in C4. On the other hand, there are sentences that might be just very spammy and also wellformed sentences that might look fall into C4. So it's it's kind of interesting that there's this sort of complimentary nature. If you use modelbased, it's only as good as the p your ability to curate positive examples that are representative of what you want. And when you want a very broad set of data, it's often can be hard to um get that coverage because well, that's the whole point. You're trying to um get a lot of uh you're trying to curate a diverse data set in the first place. Um they also created a web text like data set where um they um took uh pages from uh open web text links. So this is remember open web text you was open reproduction of web text which was used to train GPT2. Um they looked at links from Reddit posts with um greater than three karma. um even if you they use 12 dumps they only get 17 gigabytes of text. Web text was 40. So this suggests gives you a sense that common crawl is quite incomplete, right? Because you took all of common crawl and you apply the same filter and you got something that was about half as large as web text which was basically doing its own crawl. Um but nonetheless this was useful for improving a bunch of NLP benchmarks at the time. And if you look at uh you know now going back to C4 if you look at what um its composition is um you see that there's well there's Wikipedia in there um there's a lot of uh patents and um news and and so on. Okay. All right. So we've talked about um common crawl and different ways to filter. Now let's talk about um more now we're sort of entering the GBT3 era. There's a bunch of models um and data sets which will allow us to get into um you know some other ideas here. So GPD3 data set there's common crawl which was processed web text uh 2 which is essentially the same idea as what they used for GPD2 this mysterious uh set of uh books corpra books one and books two um and Wikipedia um so the result was I have about 400 billion tokens um which by modern standards is actually quite uh small but at that time was quite impressive. Um so the comic uh processing was they train a quality classifier to distinguish u web text high quality web data Wikipedia and books uh from the rest. So basically the idea of quality classification is that you identify a bunch of positive examples and then you try to look for more stuff like that in a larger pool. So this is what they determined to be high quality and then they wanted to get more of this. Um and um so yeah that's it. Um so the pile uh came shortly after. So in particular, Eluther um AI was this organization that um kind of re bounced up in reaction to GPD3 and the how closed everything was and they were trying to reproduce open source uh language models. Um and this was a largely a kind of decentralized discord driven volunteer effort um where everyone was just like tossing in um data that they felt were high quality. So they curated 22 high quality um domains. So you have um some common crawl, open web text, sackchain, Wikipedia, archive and and so on. Um here's some more statistics about the the general you know weight. So this is still if you look at it quite you know diverse. Um, and it's it's interesting to think about, you know, technically the web common crawler could have most of this uh stuff assuming that you can crawl. But often um you'll see that people will go and sort of special case different types. for example they want to get more Wikipedia is handled differently or um you know like you know mathematics is handled differently um so if you have prior knowledge about what's good data then you can just go out and use it directly okay so this was um actually more data than GPT3 was trained on um so um so they used uh Uh they also noticed that work was better than wet. So they used this different tool just text to convert it. Um so there there's a PubMed central um which a lot of papers which is nice. So you know man there's a mandate that says um NIH funded work has to be the papers have to be open access. I think uh in AI we're sort of uh sort of take it for granted that things show up in archive but that's not true for many other fields. Um there's archive of course and then there's um you know Enron emails actually uh which is a old data set um uh which came out of a subpoena after the whole Enron ship sank. Um and why is this in there? Well, it turns out that there's it's really hard to come by email data sets as you might imagine because emails are private. So, this is really the best thing we have. So, um you could might imagine there might be some bias in terms of uh the the email knowledge of these this language model that's trained on that. But that's um something to think about. Okay. So, um just diving into some of the the different sources here. So project Gutenberg uh was started um a long time ago it's mostly English books now there's about 75,000 books and the main biggest uh you know draw of this is that these are books that have copyright clearance which mostly means that they're in the public domain um 75 I think 75 years have passed since um it was published so now anyone can use it you know freely um but there's I think some books in there that are technically not in public domain but it's okay um to use. There's a data set PG19 which is books from project Genberg um and this was a paper that was trying to benchmark um you know language models on long context. So the you know appealing thing of books is that you have really long uh context compared to news articles or even papers. Yeah. So based on what you said data set is it possible that the meteoric rise of X AI is because they have access to like mining the tweets data for training their models with let's say other AI models scanned which means that like they have access to better understanding influence of spoken human language more and therefore in the long run it's really Google and X are the only big companies with access to like tons of tons of human generated data with like natural language yeah so so I think the general question is well the the narrow question is you know does X have an advantage because they have access to tweets and for example any of these big platforms Google has YouTube, Meta has you know Facebook. Um so there are you know uh restrictions on what data companies can use even if they have the data. Um uh so it's not that literally the you know Google can train on all of its you know like your Gmail or something. I don't think that's the the case. Um that said, um it is true that companies will have distinct advantages and access to certain types of of data that might be you know uh public. Um so I think public um doesn't mean that anyone can train on it. For example, YouTube is you know public but Google has special access to it. Um so I think the broader question is um will companies that have access to special data you know win essentially um and by default the answer is yes because I think that you know data is is sort of the name of the game now interestingly you know obviously anthropic has a really good model and they don't have like a particular secret source at least I don't know of so it is not everything but uh over time I think um this is my opinion is that I think you know you'll see perhaps more differentiation and more uh you know specialization and as companies leverage the resources that they have. Um okay so that's project Gutenberg books three was uh this this project that um produced a lot of books from this shadow library. Um it contains books notably from famous authors and since then has been taken down due to copyright infringement. So this was part of the pile. Um and so just a note about shadow libraries. Um these are uh there's a bunch of different you know libraries that basically disregard copyright and bypasses pay walls. Um this is basically you know illegal. Um, and they've there's been lots of takeout orders and lawsuits and so on, but usually these controls are circumvented because they just put them on, you know, servers in different countries. Um, and you know, the proponents say that it makes free what's really should be free, but you know, obviously the law thinks uh quite differently. In particular, LibGen has um four million um you know books which is a lot of books uh compared to project which has only 75,000 books um and it has been revealed that meta train models on libgen uh for example and there's a big you know lawsuit about it um so uh moving on stack exchange is uh is a collection of sites most prominently it's uh It's Stack Overflow which it started with but has grown to other areas like math and literature. Um there's some reputation and badges to incentivize uh participation. Um all of you probably have used uh you know Stack Overflow so I don't need to um well just for fun. I like always looking at random examples. Um, okay. So, here's some this is a page that basically gives you random stack exchange. So, um I don't know if these are any uh good but here's a question and here's some some answers. Okay. So, I guess this is pretty familiar stuff. Um so one thing to note is that if you look at um these type of this data it's really kind of like looks like a QA data set right um so which is kind of what you expect in um for instruction following capabilities and real applications so I guess the sort of the lesson here is that you know by training on you know web data or a large in pre-training a lot of it is just documents which don't look anything like what you would a user would type into a chatbot. But there are subsets of the pre-training data that look remarkably similar to what a user would type coupled with the response. Um and this is why you know the lines are a little bit blurry between what is uh pre-training and what's uh you know post- training. Um the cool thing about stack exchange is that there's also metadata like comments and votes which can be used to filter. Um and the data dumps are you know are you know provided although I think I think right now this is if you use it non-commercially it's fine but if you um are a commercial entity then you have to pay uh for a license. Okay. So in GitHub which everyone here knows about um and this is a I think the primary means in which you get code for language model training. Um so um this is help and code is generally helpful for you know programming of course but it has also been um I think thought to be helpful for reasoning and other uh capabilities although I don't know what's if there's a paper that makes that more rigorous. So here's a random GitHub. Okay, maybe this doesn't work. Never mind. It used to work if you a random GitHub repository. And the reason I'm doing this is that I think the GitHub repos that you visit and the Wikipedia pages you visit are distinctly not a representative sample and by sampling randomly it gives you a sense of what's actually in this data set. So when you look at numbers like oh god GitHub has you know however many millions of uh you know repositories not all repositories created equal. a random repo might uh disappoint. Um, okay. So, there's 28 million public re repositories. And one thing that's interesting is that, you know, GitHub, what is a repository? It's a directory. And some of it's code, not some of it's not code. There's also notions of issues and commit history and all this other stuff. Um, and there's also a lot of duplicates. So in order to take GitHub which is all this raw um you know data and make it like trainable tokens there's actually a lot of work that has to go into think about how you want to best um you know do that. Um so GitHub archive is uh this snapshot of all the GitHub events that have happened and you can access it using Google uh you know big query. Um so the stack is uh you know a project that fortunately produced this opensource version of code based on GitHub. So they took all the repository names from GitHub archive and git cloned 137 repositories um commit kept only permissively licensed ones um and remove duplicates and the result was 3.1 terabytes of code. So the nice thing about code is that often but not always the license is made more clear compared to web pages where the license is almost never made uh clear. So if you think about this this is sort of you you see that there is the live service there's a website GitHub that you go to you know every day and then there's the snapshot which gives you sort of raw dump and then there's the processing that happens that turns it into an actual trainable data set. So when someone comes to you and say I train on GitHub then you'll have to ask them you know what exactly does that mean? Um what was the pre-processing steps that were uh taken okay so um that was the pile although I guess I took a little bit of liberty and digressed into the different components of the pile. Um so now moving on. So now we're in 2021. So uh Deep Mind um also came on to the scene. They the first large language model they trained was Gopher on this massive text data set um which was um the Gopher model actually was not very very good. Um but the data set um this the paper does a great job at describing the data set. So massive text contains massive web which I'll talk about a little bit later. There's uh C4 um and then books news, GitHub, Wikipedia. There's no details about how those were processed. Um and as I said mentioned before um this is obviously doesn't is not reproducible. It's fine. You train on GitHub but you know how exactly was the data processed. Um so massive web um you kept English and here again like CCNET they used quality filters that were based on manual rules. Um and they had rules which you'll implement in your assignment that look at things like you know 80% of the words has to contain at least one alphabetic uh character. Um they also used uh Google safe search for uh toxicity uh filtering. Um, and I think in those days, one of the main arguments for using manual rules is that you didn't want to be biased against the model because models, um, the only models that they can run is very weak models and weak models don't really understand the page and they're just going to have, you know, probably, um, pretty awful bias. Um and also there was a consideration that um this type of filtering can you know um filter out kind of marginalized data from marginalized groups that didn't look exactly like you know Wikipedia. Um but as you see later this sort of has flipped and now everyone's doing model based uh you know filtering. Okay. So their data set was 10 uh terabytes of of text um which is I guess you know maybe like let's say five four or five trillion uh tokens just an estimate although Gopher was only trained on 300 billion which is not very many tokens I think that's the same maybe it's around the same number of GP3 um so in 2022 we have llama um so the data set for llama was um common crawl process with CCNET. Um and here the classifier here there's a subtlety here. Remember GPD3 was classify whether it looked like Wikipedia page or not. Lammo was class trained on classifier which predicted do you look like a page that was referenced out of Wikipedia or not. And I guess the idea is that Wikipedia will site uh high quality pages and most of those pages might not look like a Wikipedia article but nonetheless are high quality. So again the kind of this link structure which we saw in the GBT2 uh web text is kind of showing up here. Um they also included C4. Um why not? They use GitHub. Um they kept permissive licenses filtered based on some manual rules. um Wikipedia, project Gutenberg and books three which got them in a lot of uh trouble um and uh you know archive um sack exchange so they got 1.2 trillion tokens um so they didn't release the data set but you know uh together reproduced this um this uh data set in something called red pajama which now you can go and um you have the data processing code and the the data which you can um take out. So this was a reproduction. So this was clearly not optimal and cerebrus did further dduplication and ended up with a billion primer subset. There's also repa v2 which uh is a little bit confusing because this is something else. This is essentially um uh taking common crawl snapshots and producing 30 trillion tokens with all sorts of different quality signals. Um so this is a resource for doing uh research on how to filter based on um the quality signals that are computed. Okay. So that was llama. Um and then the refined web um was another paper and here the thesis is well remember how we saw the pile there was web data and there was all this other stuff. And their point was like well maybe if we do a good enough job filtering the web data that's all you need because technically you know the internet has everything in some sense. If you think about it, if you can access it via a computer and it's connected on the internet, then maybe that's uh good enough. Um and uh so the refined web um let's see if we can want to look at some new examples here. Um the data uh is an hugging face and it kind of looks like you know this uh okay this is okay the resolution is probably not large enough to okay anyway uh scrap that um they use for extracting content because as we noted is is better than just using the wet files that uh common crawl provides. um they use gopher rules um and they made a point of we're going to avoid MLbased filtering to avoid biases um and then they did some fuzzy duplication okay so they had a five trillion token data set which is you know quite large but um they only released 600 billion of it fine web from hugging face was started as a replication of refined web but um they tried to improve it. So they used all the common crawl dumps I think at the time. Um did some filtering. Again this is still using manual rules. No model based filtering. Um and they did some dduplication and did some you know basic anonymization. So they got 15 trillion tokens out of this. So this is still a um I think a really nice data set because it you know dealing with common crawl is is is going to be a pain. But this is sort of a I would consider fine web as a lightly uh filtered um data set that you can further do model based filtering on. Okay. So um jumping ahead. So AI2 um put out has a series of models called OMO. um their initial model was trained on um the DOMA you know data set and um this is a composition. So we have common crawl, we have the stack which we talked about which has code C4 which you know um you know Reddit um A2 has a um semantic scholar. So I think this is derived from from that project Gutenberg and Wikipedia and you know about um so the Reddit uh comes from this this project which um uh but they include sort of the submissions and the comments um separately so you don't have this thread structure. I think this project I don't know if it's still uh I guess it's no longer exists anymore. So I think around 2023 all these sites like Stack Exchange and Reddit realized wait people are just taking our data and um training models and making money off of it. So I I think that sort of uh uh came to a stop. Um so we have 40 million academic papers um from Semantic Scholar which craw crawls a bunch of different you know sites and then we have our usual suspects. So the common crawl processing which is um uh fairly I think I would say standard um so they use language identification to keep only the English part um quality filtering again they in DOMA they uh for training initial model they avoided model based filtering um and then toxicity filtering they here you they use a classifier um and then they who did dduplication. So three trillion tokens came out of that. Um and then in the same year, so this was last year. So there's a paper uh data comp um which was a collaboration from multiple different organizations. Um here what they wanted to do is foremost define essentially a competition for creating data sets. Um and so they wanted to set up basic infrastructure. So they define a standard data sets which you can essentially try out different data processing algorithms. Um so they process common crawl um all the dumps to produce DCM pool which has 240 trillion tokens. So that's a lot of tokens but as you know common crawl is is not the highest quality by on average. So that's going to get filtered down quite a bit. um they had a particular recipe for filtering down that data set into which is called DCM pool into DCM baseline and here they were very aggressive in using a quality filter. So this is what it looks like. They do some rule-based filtering basic stuff and then they the main thing that's interesting is that they took this uh fast filter that uh filtered DC alum pool into don baseline. So they they only kept I guess um you know 1.4% of the total data set. Okay. So what do they do for this model based filtering? So again in quality filtering you define your positive examples negative examples in trainer classifier. So the positive examples um come from two sources. There's this uh open Hermes data set. So this is mostly GPT4 generate instruction data. So this is kind of interesting. They're using actually instruction data to curate pre-train data. So not they're not explicitly in training on instruction data but they're looking for data that looks like instruction data. Okay. And then Eli 5, which is um basically this subreddit um called uh Eli 5 ask me uh like I'm five. Um and uh this is what I guess the data set look like. Um okay, what's the point of wasting the first two plays with a rush? Okay. Um so um these are sort of like you know questions you might ask to a a chatbot for example. Um negative examples are just uh sampled from refined web which is it's not lowquality data but it's um it's you know not as curated as these other two sources. Okay. So um the result is that they took the CLM baseline which is 240 trillion tokens and reduced it to um 3.8 trillion tokens which is you know it's still a good sizable chunk. So then they train a fastex classifier um on these u and run it on all of DCM pool. Um and here's a one of the results tables. Um so the benchmarks here core includes a bunch of standard um language modeling you know uh benchmarks like hello swag and so on. Um and uh they show that using this classifier they actually outperformed um the refined web by you know 3% and a bunch of other things by you know one or two%. So this is the model. This is the procedure they use to create the the the D cell baseline which they then you train a a pretty reasonable model from there. Um it is worth noting that after this happened the second OMO model um if you remember they started training on um you know DCLM you know baseline as well. So, so I think this era of we're going to be unbiased and try to not use model to bias, I think has kind of largely gone away because I think people realize that well, if you use models in the loop, you can just like do a much better job of um getting high quality data and and at least for you know uh increasing benchmark scores. Okay, so um the final um pre-training data set I'll talk about is Neatron CC. So this came out of Nvidia which has been doing some work on training the Neimotron models although more recently they've been doing like post-training um and data curation. So um their main thesis is that DCLM great data the baseline is great data set um but it filters very aggressively remember it filtered down all the way from you know 240 trillion tokens to 3.8 8 trillion tokens. Um, and if you want to train on more, you know, uh, um, you know, train larger models for longer, you need more tokens because, um, this aggressive filter training 3.8 trillion isn't really enough to sustain let's say, you know, like a 400 billion model training run. Um so the first thing they interesting they realized is that um they did ablations for HTML to text but not just based on the the quality but on how many tokens were left. So they're really trying to get not throwaway tokens and it turned out that just text rather than Traffa would actually keep more tokens. So they went with just text and then they used a bunch of different techniques to do quality filtering. um they prompted their gigantic neatron model to essentially score documents based on educational value to soda into a faster model and use that to train. So this is a filter based on you know uh what a language model thinks is educational value and they also use the DCM classifier. There's a sort of a interesting way that they uh ensemble they basically ran all these classifiers and look at bucketed their scores and then from each bucket they sampled a bunch of um you know data set. So it's not just taking the top because I think they were trying to make sure they have good coverage over um um different um experts kind of opinion on the notion of quality. Um they also did this thing which is interesting. They use the language model to not just filter but also rephrase data sets. So for highquality data sets uh sorry this is backwards. For lowquality data sets they basically use a language model to rewrite it into something that looks higher quality. Um so obviously there could be mistakes there but you know in the grand scheme of things maybe you know it's no worse than training on lowquality internet data. And for high quality data sets, uh they use the language model to generate task things that look like tasks. So you take a Wikipedia article, you ask a language model to create essentially input output pairs where the input is might be a question, outputs an answer or the input might be you know summarize this document and outputs a summary or um the input is extract the key information and the output's key information. So this is again trying to get at the idea of well eventually at instruction uh at at instruction tuning time we want to be able to um you know follow instructions so we might as well start get a head start on this. So they got 6.3 trillion tokens out of that which is more than uh three trillion tokens almost double um which is I guess pretty good because I mean all of this is coming from common crawl but they were able to essentially double the um maybe not quite double but almost double the size um you know for reference you know llama 3 is trained on 15 trillion quan 3 is trained on 36 trillion which includes I think multimodal data so you know 6.3 trillion at this point is not like enormous but for open you know source models with data sets it's you know pretty decent. Um like most of us 6.3 trillion is more than enough for training um even to do one epoch. Um this table shows that um basically on average they show that their neotron CC data is better than the DCLM data which is has been shown to be better than fine web at least on benchmarks. Um and then they actually have this one trillion high quality subset that's even even better. Okay. So any questions before I move on? I know this was a lot of like random specific details about different um models of data sets, but hopefully this gives you a sense of the type of things and hopefully you can kind of see different patterns um whether to use models to filter or not, whether you're um using um links out of um high quality pages or using the pages themselves um and so on. So yeah, talk about like English data sets. I was wondering if like multilingual data sets. Yeah, good point. So the question is what about multilingual data sets? Um I've sort of focused on English because that's where primarily all the a lot of the research done is done. Um but obviously common crawl does have multilingual data and there's multilingual uh data sets that are you know produced as well. Okay, maybe interest of time I'll move on to to copyright. So this was an early question that was asked um how much of the web is copyrighted? So let's just understand what copyright is really about. So nowadays there's a lot of lawsuits um around generative AI mostly around copyright. Um so in general the copyright law falls under kind of intellectual property law and the goal here is to incentivize the creation of intellectual goods. That's why copyright law even you know exists. Um and there's many types copyrights, patents, trade marks, trade secrets. Um so copyright law is the one that has been is most relevant for training data and this goes back to the 1700s in England. Um but in the US um since 1976 there's been um the copyright act which has essentially has established what copyright you know means and this is formally it's a applies to original works of authorship fixed in a tangible medium of expression. Um and so you know it's original work. So if you are just a collection it's not copyrightable like telephone directories are not copyrightable unless there's some creativity in their selection or arrangement. Um so copyright also applies to just expression not idea. So you can't copyright a you know an an algorithm you can copyright the um you know the code. Um and one thing that has been uh what this did is that um before copyright only applied to things that had be you know published and now it's just this looser notion of fix in general um copyright has sort of increased in scope um and you know registration are is not required for copyright. So this is different from patents. If you invent something, you don't register, then you have no claim to it. Whereas copyright is you put something and you throw it up on your website, it's it's copyrighted even if you don't explicitly um you know write the um you know copyrighted. Um but there is a you know thing that registration is required um before a creator can sue someone for copyright infringement. But you know the bar for registration is also only $65 opposed to patents which are you know can be thousands. Um it now lasts 75 years and then the copyright expires and becomes part of the public domain. So the all the classics and most of ku project Gutenberg is um has been gone out of copyright. Um so you know the thing that maybe people might not realize is most things on the internet are actually copyrighted. So whether something copyright isn't really the issue. So the issue is whether you can use it and there's two ways you can use it. You can either get a license for it or you appeal to the fair use clause. So, if you're going the license route, um you can either go um sign a contract with the the creator uh to get a license to use the data in some terms and this is essentially what does with you know for example Google and Reddit have um this you know relationship and effective license says don't sue me um um there's a special type of license called a creative comments license which allows the free distribution of copyright work. So creative comments all that itself is still copyrighted just that you have a license that enables it to act like if it was in the public domain. So a lot of things are like Wikipedia for example is all um you know creative common license and a lot of YouTube videos are also creative comments and this was created um almost I guess 20 years ago to essentially bridge the gap between public domain and copyright. Um, and the idea is that you want people to be able to use the stuff without waiting 75 years. Um, and there's cases where the creator is actually happy for people to use the their, you know, content, but it's just most of the time it's just not clear because they haven't said yes or no. Um, so now many model developers license data for training foundation models. for example, Google and Reddit, Open Shuttertock, Open Sack Exchange, and so on. Okay. So, um if you have money, you go get a a license. Um if you don't, then I guess you um have to say you're a poor academic and then maybe they'll let you use it. Um okay. So, if you But the problem is that you can't get a license to like the internet, right? Like who do you go to a random website? like who do you even go talk to? So the only way you can use it legally is to you appeal to fair use. So fair use um uh says basically even if something is copyright and you don't have a license, you can still use it under some conditions and the conditions determine are determined by um the purpose and character of the use. So, for example, if you're using it for educational uh rather than commercial or you're transforming the work in some way rather than just like copying it and hosting it on your website and pretending it's your own, that's going to help you. um the nature what the work is. What if it's um fictional? That's um um it's more likely to be um you know uh actually if it's sorry factual it's more likely to be uh fair use like for example the telephone book you know you can't really copyright um you know things that are closer to facts. The um whether if you use this just a snippet that it's more likely to be copyright uh fair use. Although for language models that doesn't really apply because you probably want to train on all of it, not just a snippet. Um and then effect on the market. So for example, if you're um using the work to essentially displace the creator, then that's going to be seen less favorably than if you're using the work to do something completely different. So if you obviously watch a movie and write a summary of it, it's fair use. If you reimplement the idea, that's fine. Um there's a big long decade fight over whether Google books when they show snippets whether that's fair use or not and eventually it ruled in favor of uh of Google. Um it is also worth noting that copyright isn't just about verbatim memorization. So it turns out that plot and characters can be copyrightable. So even if you have um essentially very little engram overlap but you take Harry Potter the character and you essentially develop it then that's uh that is a violation could be a violation of copyright. But on the other hand if you parody it's might be fair use. So these things are quite you know subtle and it's copyright is all about the semantics and the economics and the kind of content. So it's a very complicated uh topic. Now so what about for training? So one thing is like copyright is you know the copy is in the name copyright. So even the first step of training you copying the data is technically already a violation even if you don't do anything you know with it. Um you could argue as many have that uh training ML model is transformative um because you know it's definitely far from just like you know copy and pasting. Um this does make I think open-source um models with open data bit challenging because if you want to train a model and you also want to show people your data and you host that data that could be in violation of copyright. Um people have also made arguments that um you know the machine learning system is interested in the idea not the expression right you're training all this data because you're trying to extract the you know how language works and general knowledge rather than uh interest in a particular work but of course the learning algorithm has been uh sorry the models can often memorize and you can extract um data training data from the models quite easily. Okay. Um, and there's also this, you know, problem that, uh, language models can definitely affect the market regardless of copyright. Okay. One, I guess other thing to note is that even if you have a license and if you can appeal to fair use for a particular work, you still might not be able to legally get the data because of terms of use. For example, YouTube um has a lot of creative comments videos, but if you write a script that goes and downloads uh videos from YouTube, that is against the you YouTube's term of service. So, there's another gating for these uh platforms. And there's a bunch of um you know uh works that you can kind of read about later. Okay. So, let me quickly go on in the interest of time. So, um this section is going to be a bit shorter. Um and I've sort of collapsed mid-training and post-training together because the boundary is not often quite clear. Um often now we're thinking about less high quality in general but focused on how do you instill particular capabilities although you know even in pre-training we were already thinking about you know quality classifiers and high quality. So again the the line is not quite clear. So one thing that I think is um which we haven't really talked about in this class is uh long context. So uh models if you look at the top models have quite a bit of context. Gemini I think still has believe I think llama 4 might advertise a 10 million uh million context but you know the context lengths are quite large and uh transformers um scale quadratically with the sequence length. I mean we saw in inference lecture you can get around that but still you need some I think full attention to get the uh the best results and clearly you don't want to start at the beginning training on long context um so what people do is they add it later so that's why long context extension often shows up at mid training um because you don't want to uh waste cycles training on long context uh if your model is not very good. Um so there's multiple ways of uh of doing that. Um but since this is a data lecture I'll talk about um you know books and um and and math are two sources that have been used to do context extension. Basically for context extension you just need to create data that has long uh range dependencies and some of this data can also be you know synthesized. Um so uh people also look at um tasks. So there's a bunch of works that essentially convert um data traditional NLP benchmarks into um a standard format that they can be fine-tuned on. So supernatural instructions is one such uh data sets that was um leveraged the community to come together and create uh 1.6 six uh thousand um tasks um and they sort of standardized into a you know prompt u FLAN was around the same year um it was came out in 2022 but the paper was in 2023 so 2022 was a year of let's take all the NLP tasks and shove them into um instruction following you know format um and so so this you One I think advantage of this is now you have a language model that can solve all your favorite NLP tasks and you benefit from transfer learning. This is kind of a lot of the thinking about like you know going back to T5. Um but uh one problem with this is that often the the prompts that you have are very templatized. If you look at the supernatural instructions, um some of it is not supernatural um because they're they're sort of like all look kind of the same. So that sort of motivates um you know these instruction following data sets. And since 2022, there's um there's been sort of this expectation that language model should just be able to answer any sort of one-off task that you give it. So the notion of even task sort of disappears. Um so a lot of the work in the open community has been based on synthetic data starting with uh you know alpaka uh which uh use this idea of self-instruct to prompt a language model to generate um examples which you can then use for you know fine-tuning. Um there's vicunia which used these uh conversations that had been shared by users on shared GBT which is deprecated. Now you can uh get language models to chat with themselves seated with some questions and that creates some um you know synthetic data. Um and you can also have these uh you know evol instruct um methods that essentially take questions and make them more complicated. Um there's uh other ways of um you know taking this one takes common crawl um and then uh uses it to essentially look at identify quiz sites and then extracts QA pairs using a language model. Um and then this is open hermes which we saw earlier from the DCLM work is just a glomeorration of different um data sets. uh llama 2 chat we don't know what the exact data set is but they used uh annotators to essentially write highquality instruction data um and they claim in this paper um that this was better than using the millions of examples from open data sets um which um and uh but they could have even saved more you know money by um using um you know by less annotation and more just like RLHF which we'll talk about um later. And finally um the last data set that I'll mention just came out um pretty recently. Um so the llama neatron postraining data uh this consists of a bunch of there aren't that many details about this data set but the data set really so you can go and look at it. um they have a public data sets like wild chat and then they synthetically generated some data from all the models that are you're able to generate uh data from they also include reasoning traces thanks to R1 um and um so if you look at this um you know this this style this data you you can kind of divide it into a few buckets right one is A lot of the early work was just okay there's GPD4 um this is the sort of the easiest way um to generate synthetic data. The problem with that is that for academic research is fine but you know it is against the terms of open AI to use GP4 to create a data set then you train a competing model. Um whereas uh these um openw weight models have more permissive licenses which mean that you can essentially distill from them and do whatever you uh want. There might there might be some restriction on llama but um but I think broadly speaking I think they're more permissive than open AI. And then finally, if you are really, you know, paranoid, then you can actually just hire annotators to create um high quality instructions, which is obviously more expensive and slower. And there's also this uh you know worry that annotators might actually use GPD4 to uh create your data. So you have to be careful there. Okay. So to summarize, um so the key lesson is, you know, data just doesn't fall from the the sky. you have to really work hard to get it. Um, and it's important to think that out there in the world there's these live services like GitHub, right? And first you have to to use it, you have to first get a dump of the raw data, but you can't train on the raw data. It's too big or too noisy or it has it's not even tokens. Um, and you often have to process it. And that's where a lot of the you know horistics that we saw for quality filtering duplication kind of fit in. Um this was sort of touched on earlier. Data is really the key ingredient that essentially differentiates you know language models. I think all of the language model you know architectures there's some you know transformer style. I think uh largely the transfer these architecture is so kind of general purpose that the behaviors aren't really going to be that different. It's really the data that drives um the quality there assuming you can like train uh and fit the data. Um there are some you know legal and ethical issues here. We talked about copyright but there's more much more to be said here. And then finally if you think that this whole field is a mess, you're right. It's very heristic. um which means that there's many opportunities to hopefully improve that. Okay. Uh that is it and I'll see you on Thursday.