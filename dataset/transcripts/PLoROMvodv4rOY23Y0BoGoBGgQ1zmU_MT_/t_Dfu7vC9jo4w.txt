Okay. Um so we'll get started. Uh welcome to lecture 15. Um we've got two pieces left to the class and that's going to be you know various aspects of post-training. Um, up until now we focused very much on the big pre-training systems data components and then now we're going to take the big pre-trained model and we're going to make it useful and safe in various ways. So that's going to be the next two lectures from me. Today is going to be RLHF and sort of um safety alignment stuff and then uh Thursday is going to be uh RL from um verifiable rewards. So things like reasoning training and math and so on uh will be on Thursday. As I said before, today we're going to shift from pre-training to post-training. Um Percy in the very last lecture did cover some stuff about post-training data. Um but really I think you know the focus today is going to be in going from you know essentially this big transition that we saw in the field right so we have GPT3 uh really remarkable system really impressive um lots of pre-training lots of compute but this is not really a useful system right I guess there was a couple startups around you know building um ad copy and things like that but it was not very useful it didn't follow instructions it didn't you know do anything particularly too interesting from a from a product point of view And then all of a sudden, you know, we got chat GPT. Um, and chat GPT can do all sorts of amazing things and follow instructions. And we've kind of seen what that that has done to society since then, right? So today's focus is going to be on this arrow right here, like how do we take a pre-trained system like GPT3 and how do we make something like chat GPT? And then we're going to try to get to the to the nuts and bolts of that process. Um, and I think many most of you, you know, have never, you know, worked on things like controllable generation or like the previous generation of of text generation systems, but really like modern instruction following models are just amazing, right? Like this is one of my favorite examples from um Sebastian Bubck's uh sparks of AGI paper in in 2023 um around when GBD4 came out. Um but you know it can follow this like very long block of like nested compound instructions um and then combine that with its coding capability to output you know zero shot um mapplot lib code um and I think all of you just like take this for granted now it's like yes of course chat GPD can follow 10 instructions at once um but it's just kind of amazing that it can do this right and I think part of my my excitement about this lecture is the fact that it can do all of this um and the other thing that I think is very important right is that now that these systems are out in the wild. Um, safety and content moderation just becomes really important, right? Safety from the perspective of, you know, these models might get misused. Someone might try to use them for scams. Um, and also content moderation, if you're thinking about these this being like useful products that you can like ship and like people would pay for, right? Like people don't really want to to pay for or put ads on um systems that are like horrifically toxic, right? Right? Like I think one of the big big reasons why chat GBT has been so successful is that you know it has really significant guard rails around it. So okay given that the goal today is to try to enable much tighter better controls on language models right um pre-training you can think of mental the mental model can be um that it packs the model with all sorts of capabilities right like after pre-training the model is able to somewhere within the parameters do lots of things like reason and answer questions um but it's not going to do them out of the box and so today what we're going to try to do is get models to do that out of the Um, and so what we're going to do is to collect data of various kinds of behaviors that we do want from the language model. Um, and train it to do those things, right? And so the questions that you should be asking now is, you know, what does that data look like? How hard is it to collect that data? Um, Percy has touched on it a little bit, but given the importance of data, I'm going to re-emphasize it a little bit. I'm going to have some interactive um some exercises uh to go over that. Um, and then there's algorithmic questions like how do we me make use of that data, right? Um certain kinds of data are easy to use. Um like you know if you have expert demonstrations you just train to imitate that. But if you have things like pair wise feedback like model output A is better than model output B. How do we how do we make use of that? And then finally like you know how do we scale this up? How do we do the usual things that we've been doing in this class? So um the structure of this lecture is roughly going to mirror the instruct GPT paper because a lot of the post-training pipeline that we have today is still off the instruct GPT paper. Um and so the first part of this is uh lecture is going to be um on supervised fine-tuning. So if you look at um the instru GPD paper, you'll see this this diagram that roughly describes a three-step process uh for building an instruction following model. Um so part one of this lecture is going to be the leftmost part, the part where what we're going to do is we're going to do supervised fine-tuning on expert demonstration and then part two we're going to follow the next two parts uh of this structure. We're going to talk about reinforcement learning and pair wise feedback loop. Um, broadly, um, I'm going to say that the ingredients, um, in order to get, you know, this first part working, I mean, there's two things that we have to kind of think about. Um, the first part is the training data, right? If you're going to imitate expert demonstrations, you better have expert demonstrations. Like, what does that look like? Um, and then the second thing I want to talk about is kind of the method. Like, you you have data now. Like, how are you going to adapt to it? Um, and there's an obvious answer. I'll, you know, talk about this again, but like just do gradient descent. Um, but there's also a kind of nonobvious part to this answer. And, you know, in case you haven't been following how people build these models today, this might, you know, be still surprising. So, I'll I'll I'll leave that as a teaser for later. So, okay. So, in Percy's lecture, you know, he's mentioned already several different kinds of instruction data, but today we're going to like walk through a couple of them, and we're going to do a little bit of an interactive exercise. So you know those of you who have your laptops open can use those for good. Um so I want to talk about two different details like one of them is what's inside these data sets. Um people often say data matters a lot. I think post- trainining is is one place where this is even more true than before. Um because you're using very small amounts of data to get exactly the behaviors you want. So if you have noisy instruction tuning data, you're going to get some pretty crazy behavior out of your models. Um and then what kinds of things should we be paying attention to? if you're in charge of of post- training data collection, what kinds of things might matter? So, um I have taken three different data sets um from basically constructed in three very different ways like you might also you might even call them like kind of three different paradigms um to building instruction following or post- training data. Um and we're going to go through each one and then we'll look at them closely and then we'll think a little bit about what's going on with these data sets. Okay. So, I'm going to talk about Flan. Um this is uh by a bunch of Google folks and Fla is going to be um essentially constructed by aggregating a bunch of uh training data sets from NLP tasks right so if you look at it you know you see all sorts of different tasks like you know natural instructions v2 which has a bunch of you know question answering and things it's got T0 SF you know adversarial QA and like topic classification so basically this this was constructed by taking existing NLP data sets um that do all sorts of individual tasks and then aggregating them into one big metadata set. Right? So this is one approach to building such data sets. Um we've got open assistant on the right. Um and this was I think a pretty unique uh sort of endeavor in which a bunch of online enthusiasts got together and decided to write instruction tuning data um for language models. Like right after the release of chat GPT I think the excitement for this kind of thing was really high. And so there's actually a lot of good high quality human written data from that effort. And lastly, and you know, of course, this is this is a bit of self- advertisement here, but um as a representative of the kind of like language model generated post-training data or like AI feedback style data, I'm going to talk a little bit about um some of the data from Stanford alpaca. Um so let's just look at examples, right? Like I think looking at examples and talking about them are very useful. Um now this is from random examples taken from the FL data set. Um, and you can kind of see the types of stuff uh that are in here. Um, so you know, you've got things that look like pretty normal instruction tuning data like write highlights for this article. Sauntering down leafy avenues past Dutch stepgable buildings dot dot dot dot dot. And then you know in in the end there's even like more information on travel in the Netherlands at www.holland.com. And then it answers you know the least known of the Dutch cities, the hug was a village dot dot dot dot. Um, and then it it sort of summarizes, you know, this as a highlight. Um, you know, you've got something like this, uh, where, you know, this is like what is this text about? Here are your four options. It's business, right? Um, so this is kind of a multiplechoice training, uh, thing that's happening. um this you know uh Pissy talked about the Enron data set so you all can like you know smile a little bit but things like this of of taking let's say maybe the Enron email data set and you you paste back write a subject line for this email and now you've got kind of supervision for that task right um this one um I guess no one here has probably worked on uh text generation but this is from a data set called E2E where you have like a database entry and then you're supposed to write a sentence um that describes uh that restaurant so immediately you kind of see you know you can probably get a lot of data for free this way, right? There's a lot of NLP uh training data sets and you can like put them all together um and you will get a really big aggregated data set. And so in that sense, FOMO was a you know ahead of its time and produced a ton of data for this kind of thing. But also we see in many ways that this can be somewhat unnatural, right? Like we can we've already seen that like this Enron data set is a little bit weird. we can really definitely see things like, oh, here's a text and then now you sort of append sort of the options to turn it into a task. And so you can kind of see the surgery that you have to do uh very visibly in order to um make this kind of data set. And I think if you look at this, you'll you'll agree with me that this isn't your usual chat interaction, right, for for something like chat GP. Um another example for this um is uh Alpaca. This was like a really really early um uh attempt at using language models to generate instruction tuning data. Um and so you know just to describe the procedure here you know a language model was used um there was a seat set of human written instructions and then the language model was used to essentially generate more instructions. So that's the left column. And then you use something like instruct GPT to essentially fill in the response, right? And so here, you know, now we have something that looks a little bit more like, you know, on the left standard um sort of chat GBD inputs. If you compare this to um something on the left, this is a very benchmark ccentric set of tasks. This feels a lot more like a set of interactions that someone might just throw into a chatbot. Um and the response is almost always in long form natural language versus with fla often it can be quite short like one word or a phrase or something like that right so we kind of see that of course you know we also see that on the left these are in some ways not very diverse inputs they're very short instructions um and then open assistant is kind of the third leg of this instruction tuning saga um you kind of see more complex queries on the left and then because Back then, I guess people were just really um into writing long detailed uh supervision for models. You see like actually really detailed responses and this one even has a citation on uh you know how like what what makes this answer correct, right? Um and so, you know, very high quality but also kind of very difficult. And so, um now this is this is the first interactive task in this class. Um but especially those of you that have your laptops open, um you know, please go to this URL. This should be a Google form. Um, and there will be a sort of one sort of uh sort of prompt. Um, and now we're collectively going to crowdsource um a instruction tuning response. And I'll give you all let's say five minutes um to do so. Let me know if the if the link is wrong, but I I did test this last night. So hopefully this is uh uh working and then we'll look at the the responses briefly. Um and then uh I want to talk about why I did this exercise. There there is a a teachable moment here rather than sort of getting you just off your laptops for a moment. Okay, excellent. Um I think I think there's a decent number of responses, so I'm going to um maybe put them up. Uh let's see if I can put them up then. Yeah, there we go. Okay. Um so in in many ways, I think this reflects the kinds of data you get. I mean, if anything, you guys are all motivated to do this task than I think the the standard crowd worker, you know, but you've got the person. I mean, I I'm not sure what's going on here, but this is probably chat GPT. There's a lot of emojis. Um, I'm I'm getting trolled, but this is I mean, I am preparing for lift thoughts. Uh, that's good uh response. Um, you've got, you know, the nafam, which, you know, of course is the kinds of things that you'll get out of crowd sourcing. Um, and I think hopefully one thing that that you've seen or like felt as you were doing this is that it's actually really difficult to write long form responses, right? Especially for something that you aren't prepared for, right? Um, and so, you know, you get a lot of short responses like this. It's very difficult, I think, to get people to write sort of long detailed, you know, responses like this one at the very top. Um, often those uh kinds of things are um, you know, uh, from Chachibbe. And of course, you'll get things like, you know, I saw this one. Ice cream is a frozen dessert typically made from milk or cream. And you're going to have to, you know, filter out those kinds of things that you're going to get um through, uh, crowd sourcing. So, um, why did I, um, talk about this? Well, of course, you know, now you have a sense of what this task is like. Annotators in the wild, even if they're experts, will be under time constraints. Um, and I think one of the reasons why, you know, things like like AI feedback or using LMS to try to refine or generate these kinds of data has really gotten popular is, you know, if you look at the GPT40 response um to this, you know, it's it's pretty good. Um, it's a pretty good response to this question. It's very long. It's very detailed. Um, and generating this this kind of a human uh response is going to take a lot of effort and a lot of cost, right? And so you have to, you know, if you're in charge of human data collection at one of these labs, you have to think about, okay, like how do we take, you know, what I showed you in the spreadsheet and incentivize people to generate something that looks like this instead. That is no easy task um at all. That is a very difficult um crowding task. Okay? And so um these things that we've we've just seen um they vary quite a bit um in things like length. Um we saw you know the chat GPM which is bullet points like lots of style variations. Um we saw in the uh open assistant example that you know sometimes people put in references um sometimes they put in sort of very complex deep knowledge like is that good or bad? I'll talk about that in a moment. Um and there's also other important aspects to this process right like maybe you want to collect a ton of data or very little data that's high quality. So you got this trade-off. Um, you also have to think a lot about safety, right? Like the the data that we collected just now, that's just capabilities data, right? It just makes models answer things like what is CS36? It does not help us make our models, you know, um, you know, uh, refuse uh, malicious instructions and things like that. So, we have to think a little bit about what that kind of data looks like, too. Okay. Um, I think length has always been a big kind of gorilla in the room issue for all of these data sets. um you know when uh back in 2023 when it was very very popular to generate these kinds of instruction tuning data sets um there's a survey like Ejong Wong and others at UDub came up with this very nice survey coming looking over the many different kinds of data sets that were created um early in that year um and you kind of see if you look at the length um of both the prompts so that's the inputs and the responses the completions you see like really different lengths of both inputs and outputs and inputs are probably a measure of sort of complexity of the task um in many ways and then the outputs are in some sense a measure of how you know much you push the annotators or if you used AI generated responses um and one of the things that that you should be you know all aware of you know let's say you got put in charge of making a new language model you're in charge of post- training well you know if you're using human evals uh people have a strong preference for lists I mean so do actually if you use AI as a judge they also have a strong preference for lists um And people have a very long preference for outputs, like 60 70%ish preference for longer outputs. Um, and so do sort of AI judges, right? And so this is this is a little concerning um because you want to be optimizing for not just kind of the stylistic content of your uh responses, you ideally want to be using post- training to do things like reduce hallucinations um and actually make the model hopefully more capable. Um one thing that we do see is that you know these factors are not super relevant for benchmark performance. So if you look at for example MMLU performance um kind of despite the the really big variation um in length for a lot of these models um most of the the instruction tuning data sets like the simple ones you know give you boosts over kind of the base model which is the the very top row um above. So I think one of the things that I'll say here is you know chat style evaluations have their place you know chatbot arena alpaca eval these kinds of automated you know eval like user engagement and things like this but benchmarks also have a very important place um because when you post train you don't necessarily want to be too affected by for example length biases and open-ended domains and so on um and so you want to be really careful of these effects. You want to have a different diverse array of evaluation strategies to try to avoid those pitfalls. Um, one other thing that I think really trips people up when they initially start thinking about these things is to say, "Oh, what I'm doing is I want to collect highquality data and highquality data has lots of deep knowledge and has lots of citations, right?" Like that's a reasonable thing to say. Um, and I think, you know, open assistant, I think, had a great example of this, right? Um, and so here's an example input output pair. Um, you know, you've got right introduction about monopsin economics. um and then there's these references um on the response on the right. So now let's say we have a model and we fine-tune the model to take the left side as input and reproduce the right side as output. Right? So um you can kind of think about two different things that this process is going to do at the same time. Right? So one of the things that this is going to do is it's going to associate you know uh monopsin with that citation. Right? So it's learning new knowledge. So that is good. Right? that is a positive thing to do. Um, but it's also going to do a second thing at the same time, which is this kind of generalized thing of saying if you ask me a complicated concept, I had better finish the output with a reference, right? And so it's it's basically the sec the first thing is teaching new knowledge, which is good, but the second thing here is kind of teaching the model to hallucinate, right? like if the model doesn't already have somewhere within its parameters an association between monopsin and uh this citation this uh bivven's initial book um you know what might happen instead is it just learns that oh what I should do is whenever I have a complicated input I should give a response and then make up a reference at the very end right those are two competing explanations for what is happening here um and this is going to motivate in some ways the second part of this lecture right um John Schulman has this kind of great talk I think he gave it at Berkeley um where you know his argument is um basically if you do this kind of thing you're going to encourage the model to hallucinate right like the model doesn't have the knowledge of answering a question you force it to answer that question what it's going to learn is of course it'll learn the knowledge in some abstract sense but it will also learn the other aspect of I just need to make something up in order to sort of type check what the response should look like okay yes there's a question question so human right one writing has a sense of like okay I think I need to add a citation here let me search for another citation either from memory or let me actually use a database it is the fact that like the LLM is learning that like okay here I should insert a citation is actually a correct and desirable thing and it's and the fact that it's a madeup citation is a either a memory issue or b something you can maybe augment a tool usage in the site but like I don't see why the behavior of needing to add a citation itself self is problematic whe like if you can either fix the memory issue or the tool user issue. Sure. I mean I think the Okay, so to to repeat the question, the question was like I guess that was a more comment than a question. Um was that the the uh learning to put in a citation isn't a bad thing, right? Like I mean maybe you augment it with tools and it'll actually give the right citation. I mean, that's a fair point, but I think the thing to to maybe point out here is like the the deeper conceptual or not conceptual, the deeper issue with token prediction, right, is that you're teaching the model to kind of predict the right kinds of tokens. And here, you know, essentially the the lesser of the two errors is to say hallucinating is less bad for my loss than not making up the reference at all, right? kind of the the structure of the the response always has to be fulfilled because you have to fill the tokens in at the right places, right? Of course, at scale, if you know the facts, if you have the right tools, right, those are those are good. You want to kind of make the predictions on the right spaces. Um, but I do kind of think this is very indicative of this like this uh failure mode that models can get into where you're trying to get it to do things that it can't, right? like if your SFT data is just much more advanced than what your pre-trained model naturally can do, um you run this risk of teaching the model kind of this alternative shortcut behavior instead of teaching models the right behavior. Um yeah, so so that's John Schulman. Um I think he makes a a fairly reasonable case that you know this is one of the reasons why like on policy RL like reinforcement learning style things is an important thing to do because you want to know what the model already knows and only teach it those things to avoid you know hallucinating and whenever it it's encountering some fact that it doesn't know then maybe you should change your fine-tuning data to say oh I don't know that fact instead of forcing the model to try to answer right um and we kind of see this um on the on the kind of other sort of uh sort of knowledge storage studies as well. Um where people have kind of talked about, you know, it's much easier for models to sort of reproduce known facts um than sort of uh to to learn sort of unknown facts where it just takes a lot longer um for models to kind of learn facts that aren't shown in pre-training. And this sort of is is sort of matching uh what you might expect from these phenomena. So, okay. Um I think one of the things that I'll sort of you know summarize that with is that there's a very counterintuitive phenomenon for instruction tuning which is that you know you can have a instruction tuning data set that is fully correct and like actually very rich but actually that might not be good for your LM because it's going to teach your your language model um to sort of try to make up facts to match that depth of knowledge. That's always been, I think, one of the arguments for why you want to be really careful with both distillation data where the the dis uh teacher model is stronger than your student model. And also really human uh annotation where the human might be much more knowledgeable um than the model, right? You want to be really careful to make the model abstain nicely when it doesn't know things. Um and in principle, you know, reinforcement learning style correctness could help and we'll talk about that in a moment. um and sort of optimizing this at the at the instruction tuning level is just really messy and very difficult. I don't think people have really nailed it down um at least in the open research literature. Um the other thing I want to talk about um briefly because I think this isn't necessarily um something that can be solved with um instruction tuning alone um is you know to to touch on safety um and to think a little bit about what the trade-offs are here. So, we know, you know, language models um need some guardrails. They're deployed straight to end users. They're very capable. So, they might be used for misinformation or for generating things like scams or spam. Um and so there's a need for in uh safety tuning um these models. And I think in parallel with a lot of the research on instruction tuning, there's been um actually quite a bit of of work studying safety tuning as well. Um and I think some of the early work um in this in this area um you know kind of has shown that even a small amount of sort of safety tuning data that's mixed in uh to to instruction tuning process can make models uh much safer. Sort of paralleling a lot of the findings that people had that actually for instruction tuning as well if you have a strong enough pre-trained model even a small amount of instruction tuning data can can get you a lot of the way. Um not to say that's sufficient um but actually that's you know um sort of gets you to a reasonable point. Um, and I think the core trade-off with safety tuning that I'll sort of touch on um, in this brief section is this trade-off between refusing things and not refusing kind of too much, right? Um, so there's always this thing of, you know, if you have unsafe responses, you want your safety tuned model to just refuse to answer. And then maybe you have these other, you know, actually safe responses, but things that look like unsafe responses, like how can I kill a Python process, right? Um, we all know that is a reasonable question to ask. But I guess if you're not, you know, if you don't understand English very deeply, you're like, oh, killing, killing sounds very dangerous, so maybe I should refuse to answer that question, right? Um, so how can you make models sort of understand this nuance? It's a very tricky thing to do um you know purely in the instruction tuning setting. Um and so a lot of what people have done is come up with carefully curated small instruction tuning data sets to try to balance this trade-off. Um so even um some research has shown that even like 500 examples can make models follow uh some of the safety guidelines um well so okay um to put this together um instruction tuning is surprisingly powerful. I think you know you would think that given how powerful things like chat GPT are that there's actually a ton of complexity into getting anything that works. I think you'll find that even if you take you know a fairly standard instruction tuning data set like open Hermes or um open assistant or any of these data sets and you take a base model and you fine-tune on it with reasonable hyperparameters, you'll get a model that behaves a lot like llama or chat GPT. It won't be quite as good. there's a lot of of uh extra work you need to do to optimize it, but you can get pretty far. Um the second thing that's you know good to remember is basically it the notion of high quality data is just very complex and you have to reason about it really carefully. Um it's not obvious how to do and then the last thing is you know actually even a small amount of data can have great leverage at this stage in changing how models um behave. The last thing I want to end this section on is um how to do this instruction tuning. Um there's kind of a you know a flippant answer to this which is well you've got demonstrations um just put in the instruction and the response and just do some gradient descent, right? We all know how to do gradient descent at this point. Um and I think in most academic settings that's basically it, right? Like you're done. You do your small scale um gradient descent and you're done. But I think if you're at like a frontier lab and you've got more compute and you've got more money than you know what to to do with, then you've got a lot of compute and you've got a lot of data. Um, and so you can scale this whole process up quite a bit. Like you can scale it up a lot. And modern instruction tuning pipelines um are starting to look a lot like pre-training pipelines. Um, and so increasingly the boundaries between uh pre-training and instruction tuning are just getting blurred. Um, because if you think about it, instruction tuning data is still a sequence, right? It's just a sequence of tokens. And so I can throw that in into my pre-training process and that's a totally valid thing to do. Um, and so this is an increasingly popular idea. Um I think you know the close labs don't tell us anything but you know I think uh the things that people have tell me um in bits and pieces suggest that this is you know what they're doing. Um a lot of the open groups from China do basically this now um and so what you do is you have your you know usual pre-training setting right you do pure pre-training and then what you're going to do is you're going to start um mixing in instruction tuning data into pre-training. So the kind of the tail end of your pre-training, especially as you're kind of annealing the learning rate, um you're going to start putting in a lot of this higher quality data or instruction tuning data. Um and then in the end, you might actually do a second short instruction tuning round, but maybe this is smaller because most of your data has already gone into to the second stage, what people call um mid-training. Um and this is cool because it lets you scale up without uh catastrophic forgetting issues. You might get more leverage out of your data because it's integrated more deeply into pre-training. Um, and to give you an example or a sense of what this looks like, um, and it's a bit of a shame that, uh, data mixes are often pretty, um, closely guarded secrets by a lot of the groups. Um, so I've taken this figure from, um, mini CPM, which we've talked about before, um, great paper from one of the Chinese groups, where basically they have, you know, a two-stage training pipeline where they have a first stage where they do pure pre-training. And if you look at this pie chart, this is all pre-training data sets. Common crawl uh code pre-training pile dolma. They they've thrown it all in into one big uh pie. Um and then they have a second stage uh which they call the decay stage. And so if you remember my lecture on scaling laws, you know, I talked about WSD, warm-up, stable, decay. Uh so that's the stable stage. That's the decay stage. And in the decay stage, what have we got? We've got um you know, Wikipedia, what people might call high quality data. We've got still the pre-training stuff mixed in there. So, it's not pure post-training data. But then if you look at the right, we've got code SFT, we've got um Chinese books, we've got ultra chat, we've got stack exchange question answering and and uh evil instruct and OSS instruct and all sorts of other things. Um, so those are all kind of instruction tuning or instruction tuning adjacent data sets that you've thrown in onto the second half uh of pre-training. Um, and it's I think used by most models today. Um, and many CPM and other sort of you know derived LMS that are derived from that lineage of models um are have definitely sort of publicized this um I think it's it's extremely effective uh to do this and so I think everyone has been following this. Um one last commentary I'll make before we move on to RHF here um is that uh this whole process makes it very very difficult to reason about pre-trained models versus post-trained models. Right? If you look at recent releases from like Quen or what whatever other companies that you're looking at and they say base model um you know that base model is probably at the end of you know this process and so it has basically gone through an instruction tuning phase uh implicitly through its mid-training process. Um we don't exactly know what the mixes are for for a lot of these closed models but it does actually mean that I think the term base models is increasingly uh questionable what that really means. Um so that's my sort of side comment that is useful for you if you're if you're thinking about base models so yes the data mixture you change it during the decaying stage like when you have linear activity it goes to zero that's when you get like these big loss is that like directly totally it's seen in that lasting stage. Yeah. So that's right. Um that was the motivation for a lot of the the two-phase training for for these um groups. Like they basically use uh essentially the large drop in loss as a way to try to anneal the model into the right you know mode. Um I think there's you know increasing studies into like what's the optimal point at which to switch um and I think it's a little bit more nuanced than that but I think a first order this has been a very effective recipe. Yes. between lending one has trouble computing or I'm trying to think about the incentive citization thing from earlier does this also have that yeah so the I guess there were two questions packed in one but the the question was like is uh this primarily for catastrophic forgetting and also um does this help with the citations issue um so to to answer the second part first I think it doesn't help with the citation issue because you know just as sort of like a type signature The only things that can help with the the citation issue is if you know what facts the model knows, right? So you have to either ensure that the model like always knows the citation facts before you show it the SFD data or um you have to like check to see if the model knows it and then show it that that uh data if it if it does know it, right? Which this doesn't do. This will always unconditionally put in those data points whether or not that citation is learned. So it has no way of of fixing that adaptively. um catastrophic forgetting wise, I think that is one of the the motivations that um if you have so much SFP data, um your trade-offs are pretty tricky, right? Because unless you're going to do this kind of almost pre-training mixed in with post-training, you have to think about regularization. You have to think about tiny step sizes to avoid, you know, messing up your pre-training. And so I think this is um partially motivated by uh sort of catastrophic forgetting adjacent issues. It keeps the model sort of more more general. Yes, I understand um with the with the John Dolman example um foration is that if the model doesn't know the reference that's included in the post training data that it know it does know that fact then it would not have the same behavior. That's right. Or that's the that's the claim, right? um that if the model did know um you know this citation that's right here um then the model wouldn't necessarily out of these two sort of competing mechanisms um what it would learn is oh whenever I see this example I should retrieve you know my knowledge about you know bivvens and Michelle L um and then use that as a citation um I think the the reality of this is that it's always very complicated right like what does it mean for a model to know something how reliably does it know something um and So these two kind of mechanisms are always maybe in superp position for a model. It's really just a question of which one is more dominant. Like if a model just has no idea about this, it's probably two that is, you know, more dominant. Whereas I think if the model knows it reliably, it's more likely that it's just going to learn the correct citation rather than encouraging broad general hallucinations. Yes. Have people tried putting into all of the pre- data some kind of thought tokens that tell the model? Oh, I should this looks like a fact. Let me check if I actually know it. And so I'm going to query myself, you know, and check if I'm if I'm getting consistent answers. And if so, I'll I'll print out right. So in the entire pre-training process process, I need to check myself. Okay. Um that is a very interesting idea. So just to repeat it, it's um has anyone done something where you put in like kind of thought tokens or the model's checking itself for its knowledge of facts as it trains or something like that? Right. Is that roughly right? Yeah. Um, so depending on how you interpret or like implement that exact idea, it starts to look a lot like reinforcement learning. Um because for example there's a a method called a quiet star um from some folks here Noah Goodman and and Eric Zelkman and others have done this where they do um essentially they try to learn the thinking process of a model um by sort of you know predicting what happens um on the answer token and then based on whether or not it's correct it tries to like you know uh reinforce the model to have good um thought process. Actually the the even closer analogy to this is STAR which is the original paper which is if the model gets something correct then that thinking process gets fed back into the model training and if it's wrong then it doesn't right and it's kind of very similar to to what you're proposing which is to adaptively train the model based on kind of correctness of its its knowledge or whatever else. I propose you just give a thought process that say let me make use of this tool that checks myself and I can later at some point change what that tool is but the point is that tool will come with a response that says yes I do have the knowledge no I don't and then you' actually see that knowledge gets printed out or doesn't depending on I see uh so so in this like tool use example are you imagining that um kind of the fact would get replaced by a tool call or will the fact still be there like I think the key question is do you force the model to predict the fact tokens or do you just force it to predict uh like use the tool to look it up on Google token? No. You don't know what the tool how you implement it. But you see if the tool says I yes I do know it then the then you there will actually be some response printed out in the pre-training data and if the tool says no I don't know it then the pre- training data will just say actually I don't know this I'm going to skip and so right so so I think um that's hard because when you do pre-training like you have to know whether you know the fact to to know whether to take losses on that knowledge token right like you can't defer it to inference time because you have to decide whether or not you're going to take gradient steps, right? And the other sort of logistical difficulty here is during pre-training time um you have a static data set where you know you for computational reasons you would want a static data set and if you have a static data set you can't adaptively do updates right like anything that solves the hallucination problem has to be kind of reactive of the form what does the model know and then do I take updates on this or not at the pre-training stage that's very difficult um unless you're doing RL style stuff at pre-training scale which would get you very close to that but but still very difficult um I'm happy to follow But I think hopefully that answers the the question. Oh, there's more stuff. Yes. Okay. So if I uh for example um um in case of llama uh in the green data we haven't deal with the emoji but if you have a lot of the emojis will cause the effect that when we start to the model and the result model we don't have a lot of repeating sentence similar to emoji partners uh at the end. Yeah. Okay. So the question was if at pre-training we don't see emojis but at post- training we put in a bunch of emojis at the end what will happen? Um it depends on the the structure of the emojis I guess. Um if the emojis are dependent on the inputs in a very complex way and that's very difficult to learn. Maybe what the model will will learn is well in post- training what I saw was a bunch of emojis. I don't have enough data or training to know what the complex pattern is. So the model will just learn to put a bunch of random emojis at the end. um if there's no pattern, if there's no comp complex dependence, then you know maybe the model will learn to do the right thing, which is just to put a bunch of random emojis at the end. Um really the the key way to think about kind of the the SFT issues is instruction tuning will will reliably teach the style of the output like the type signature of the output, right? And the model will most likely follow that type signature at the very least. And the real question is, do you have enough instruction tuning data that you could do something more than that? And that's kind of the more complex open question. So in your emoji case, at the very least you'll get a bunch of emojis. Whether those emojis are the right emojis, open question, right? Depends on how much instruction tuning data, depends on pre-training, so on and so forth. Yes, I was wondering like uh earlier in the lecture, I think it was said that um the post- training part doesn't really teach like model new knowledge, right? It's mostly about styles and then like the line kind of gets more blurry like when M is like me training stage. Um so and then he showed like the mini CPM paper. So like in that in this sort of new like scenario like the training part could also instill some of new kind of like world knowledge into your model. Yeah that's right. So so I guess the the question was like you know if I rephrase it you know can instruction tuning essentially teach new world knowledge because mid-training blurs the line between pre-training and instruction tuning and we know pre-training teaches knowledge. So why not instruction tuning? And I think that's right that like in some ways instruction tuning if it's scaled up enough and it's diverse enough will teach knowledge right but I think instruction tuning in its like smaller like non-midraining form it is very difficult to have the scale and diversity of data needed to reliably teach you know various facts. Um I think modern mid-raining is starting to become a different game. Um but it's still sort of an emerging object I think. Cool. Okay. So now um we get to the part two, right? So part one, this is the the quick intro to uh instruction tuning and SFT. Now we get to um reinforcement learning from human feedback, right? The RL part of this lecture. Um and conceptually, right, and I'm going to take it slow here because I think this this is an important conceptual transition, right? We're going to move from the world of of I think generative modeling um at the very top here which is you know there's a very simple goal in this world which is there's a p star is a reference distribution from which completions are drawn that reference distribution probably looks like some mixture of um you know internet data as well as annotator written data but there exists some abstract p star that we're trying to imitate right that's all there is to it um so this is pure generative modeling now we're going to move to a second perspective now which is um RLHF and in this world um I no longer care about matching any distribution right so probabilistic perspectives kind of don't really entirely go out the window but you want to be careful about adopting those because really what we're looking for is we're really just looking for some policy P of Y given X such that we maximize our rewards right there's some reward function R of Y and X that you know take in both my completion and my prompt and then gives me a reward and all I'm looking for is any policy that gives me good rewards, right? Um, and so now LM are not necessarily a model for some underlying distribution. They are policies that give us good rewards. Um, and so why would we go and do RHF? Um, there are kind of two reasons that we might do this. One of them is on the top one in SFT in order to do this process of imitation we have to get samples from P star and that can be quite expensive, right? Um and the second one all we need to do is get measurements of rewards R. Um and SFT data can just be really really expensive. Um you know this is kind of like a caricature of you know various costs that you might have in different stages right you have some compute costs when you uh train your base model and then you've got you're doing like supervised learning like SFT and then you're going to go collect a bunch of you know pair-wise feedback and do RL and do evaluation and so on. Um, so when you do this, right, um, you know, the the SFT is just really really expensive. You're you're getting like really expert people to write very long form responses. And you kind of saw how annoying and difficult that was. Um, and Frontier Labs are going to be spending millions on this post-training data. Well, maybe there's a nicer way of collecting data that makes models better. So that's one argument for why we're going to do all the things that we're going to do in the second part of this lecture. There's also a second reason um that is equally or maybe even more important um that I think um you know people uh do this kind of RL training and one of the things that's very interesting is you know people don't always agree with themselves about what is good. So if you ask somebody to write a summary um you know they can write one summary and then you ask them um to compare their own summaries to LM written summaries um there's a good amount of people that will actually prefer LM written summaries and this was a really surprising result from one of my students uh papers uh I guess two years back now um where we were benchmarking you know summarization systems um and there was one person who is you know of course anonymized annotator one who you know um wrote a bunch of summaries and they actually preferred you know the the AI summaries uh actually significantly more than their own and they're like a freelance expert writer or something um and we went and like interviewed them and they were like yeah when you asked me to write stuff I just felt like I had to write more with like flowery language but then I like read the the AI ones and they just read better you know um and I'm sure you've had similar experiences you know where you look at the output and it it is actually you know uh different from your own assessment of how to generate And so there's a, you know, not just cheaper to to verify than generate, but actually um maybe higher quality to verify than to generate. And so there's this this generator validator gap. Okay. So we're going to cover different aspects of uh this RHF process. Um we're going to, you know, talk about how we collect data and like what are things you should worry about if you're in charge of RHF data collection. Um and we're going to talk about how we do RHF. Um I'm going to talk about two representative algorithms PO and DPO. Um I'll defer some of the more like detailed explanations of PO till next lecture just for kind of space reasons. Um and then finally we'll end with some you know things to worry about almost like pitfalls of RHF at the very end here. Okay. Um so how do we get pair wise feedback? Right. So pair wise feedback um is oh sorry I'll I'll go back here and just like take it a little slower. Um, so when we do this second part of this instruct GPT process, right, like how does this work? Well, we have the model sort of generate its own outputs, right? These are rollouts in RL terms. And then, um, we're going to compare, you know, these different outputs. And, and although four outputs are shown here, um, you know, in kind of standard settings, you often just have a pair of outputs. And let's see, we have A and B. All I want to know is A, is A better than B or not, right? And then given these pair-wise feedbacks, um, I'm going to train a reward model that can essentially internally give every single output a scalar value. Um, and then just use that to do reinforcement learning. Like this reward model is now my rewards and I want my model to maximize those rewards, right? Fair fairly hopefully simple um, pipeline. So how do we collect pair-wise feedback data? Um well, you know, the the the obvious simple thing to do um is just say, okay, like, you know, I'm just going to make some web app. You know, I've got two different AI responses. Um and you're going to have a little, you know, four-way check box that like, you know, checks which response is better. I took this from from one of the studies that we did, right? All a lot of pair wise feedback uh responses look, you know, similar to this. But I thought, you know, one thing that would be helpful and useful into like actually getting a sense of like what does this look like for real? Um I have gone and sort of uh dug up examples of annotation guidelines um from different papers and different places um sort of you know talking about this process right so um if we look at the instruct GPT guideline this is one of the very few I would say um sort of released materials from one of these companies describing their annotation guidelines um you know they say okay your job is to evaluate these outputs to ensure that they're helpful truthful and harmless right So those are their three pillars. Um you've got helpfulness which is like you know writing in clear language you know answer the question they mean to ask like being sensitive to to internationality like if someone says football you know they shouldn't assume American football. Um if it's too confusing ask for clarifications. Um I want you to be truthful and not hallucinate outputs. Um and by harmless you know um you should sort of be not toxic and be very nice and not say NSW things. um all fairly reasonable uh things but you can kind of see how um sort of there's a the interplay between um things like the model spec which openai publishes publicly then there's a very detailed annotation guideline which you know this is not that detailed it's probably much bigger in practice um where you would write down these kinds of bullet points you would hand this to annotators um and then uh they would sort of go and and make annotations this was you know instruct GPT is is not even like kind of production grade this is early days but you see how this process kind of um works. Um the kind of other interesting example um you can kind of go look this one up uh later if that the actual text is too small because I'm not going to read through all of it. I'll just sort of uh touch on it a bit. Um there's actually a leaked version of the the actual the true annotation guideline apparently um for Google Bard um that I think was part of some like news story. Um and you can kind of see very very similar things happening here, right? We've got on the top left box like helpfulness like you should address the intent of the user's prompt, you should adhere to any requirements, don't have misleading information, like very similar to the instruct GPD setup. Um, you know, we've got actually here a style box which is, you know, what kinds of style are good or bad. Um, and then we've got, you know, different rating scales uh for the different responses. Um and if I remember right, I think uh the Google bard folks I think gets like a minute per question to be doing this task which is which is quite difficult. Um okay and then uh for instruct GPT you know they go through like scale and upwork and they collect about uh data from about 40 people like this is really really tiny um sort of by today's standards but hopefully you kind of get a sense of you know what types of groups are being involved here. Okay, so this is the second part of our uh interactive exercise. Okay, cool. Um, so that's five minutes. Um, I guess uh to take a straw poll, um, I think about 27 of you managed to complete this, which is great. Thank you for your participation. Um, you know, how many of you managed to fact check all the facts? So one of the questions yeah the fifth one the the long word expression completely factually wrong right yes yes that's right um so so how many of you managed to fact check you know any or all of the facts you managed to fact check things just one just one okay um how many of you managed to check the math in the in the five minutes okay so so there a couple people okay good good excellent okay that that makes me that makes me happy um so I you know the The point of this exercise partially, I mean, I think maybe you could have guessed what was going to happen here. Um, the shorter ones, um, most of them are are essentially taking the longer ones and actually just removing, uh, the hallucinations to the best of my ability. And so, um, for the most part, basically, for those of you that pick the longer one, um, are sort of picking the slightly longer but hallucinated ones, like I don't quite remember which of the paragames ones are hallucinated, but but many of them, uh, are. Um and so you know we see strong disagreement and actually the longer one gets more votes um despite having strong hallucinations. This one I think both is correct. So um B is probably the the better choice. Um the two math ones um I think you can also back out by the unnaturalness of this construction. Um the the one that's like sort of more conclusive actually is is you know going to the wrong conclusion. Um this is not mathematically um fully correct here. Um and so you know you probably should find it difficult to try to do these judgments this quickly. um there are very very strong challenges in collecting this kind of pair-wise feedback um at very very large scales just because even though you only have to verify if I'm showing you a math problem or I showing you you know something like a very you know strong factually laden you know uh text you know you're going to have to basically break this down into claims and check each one to know whether one is you know wrong and one is right um so this is just like a very laborintensive and difficult task And you know I gave you five minutes because uh you know essentially I think the the news story that the Google bard article was associated with was um basically that you know the the annotators were given one minute per example and you know there was you know big complaints that like this is not enough for us to judge the safety and factual accuracy of the model and you you hopefully you know somewhat agree that it is a very difficult task right. Um so okay um the the lessons here um going back to the you know the flow of the lecture right it's very hard to get highquality verifiable annotators like I think you are in many ways like pretty high on the bar of like people that are actually motivated to do this because you're just doing it because I asked you to not because I'm paying you to or to to get grades. Um it's very difficult to get people to check correctness especially under time constraints. Um, and the last one, um, I don't know if if you know those of you who are doing this, right, but if you you put in an online survey like this, you know, someone's just going to like take the whole thing, dump it into GPT4, and then just like kind of copy the answer straight back onto your pair wise responses. Um, you know, we we've had several studies in the past where an annotator had like 95 plus% agreement with GPT4, and you kind of have to wonder what's going on um there. And so it's, you know, despite the fact that pair wise feedback is easier to collect than supervised uh imitation data, um there are still significant issues. Um and I'd be remiss to not point out, you know, the the many things that have been written about sort of the the kinds of problems that this creates if you try to outsource this to to third countries. Um there's sort of pricing concerns and sort of lots of ethical issues that you all should be aware of. you know, if you're sort of going to be in the future, be a part of these kinds of sort of data collection pipelines, right? You want to make sure to get high quality data and to make also make sure that, you know, people are being paid kind of living wage. Um the other thing to be aware of you know this is in the sort of bias and safety angle because for alignment I think this is a really important thing to to also touch on is that you know in some sense RLHF and alignment come at the end of the pipeline and because they come at the end of the pipeline they have very strong influence on model behaviors. Um, one of the papers that actually Percy and my postoc Shivani um and and Essen uh a postoc on mine worked on um was this paper on trying to figure out like how do subjective opinions of LMS align with different you know groups of people and one of the really interesting patterns that we found was actually for instruct GPT like these are old models but you know the now still useful models. um these models somehow became more aligned with sort of Southeast Asian religions than before. And then we looked in the appendix of instruct GPT and actually like what's the nationality of the people doing the annotation? It's it's Filipino and Bangladeshi and 17% American. I was like oh it's kind of surprising but it it does circumstantial evidence lines up you know with this kind of thing. And so you do have to be very careful because this is the thing that in some sense goes out in ships. Um you know uh others have also noted that you you know depending on the annotator what they pay attention to is very different. Um I really like this paper from um Hosking Blunam and Barlo where they basically study you know two different kinds of annotators. One is like the authors and they're like very motivated to like you know judge things correctly with quotes. And then crowd workers and what they really find is you know crowd workers don't pay much attention to factuality. Um that that's kind of this row here. um they pay more attention to formatting, right? So depending on the annotator, you're kind of getting different kinds of feedback even though you're asking them the same things, right? Um and so kind of increasingly um people have turned to as with the instruction tuning phase um AI feedback where LM generated uh feedback. And so there's been uh many works um including some of our own um that have shown things like oh if you try to get pair wise feedback from GPT4 it has very strong agreement from like GPD4's you know estimated win rates of models or or responses and sort of the human estimated ones on the y-axis. Same here. um the agreement between human to human which is the the blue box here is roughly the same as the agreement between GPD4 and human um and it but is much cheaper right so so you know there's lots of reasons I think why sort of AI feedback has become popular um and it has been used very extensively in RHF so if you look at ultra feedback which I think is one of the the very popular open source data sets for sort of off policy RLHF um you see this if uh Zephier 7B I think was a hugging face effort uh I want to say last year uh to build a big strong open model and the reason why I bring this up I think Zephr is probably not the most well-known uh model but one of the things I kind of remember about sort of the the hugging face um model process was you know initially they were really interested in human data collection like they were convinced that like human crowdworker like if you paid them enough and got the right vendors you know would outperform um sort of AI generated feedback um but kind of late in the process they kind of realized, you know, GPT4 generated feedback for this kind of thing just worked much better. Um, and so a more modern example of this um is Tulu 3, which is a um sort of post-training paper/ project out of AI2. Um, and they've done kind of roughly this like they take different prompts. They have lots of different models to generate responses and they have, you know, LM sort of rate these to get chosen versus non-chosen. Um and this really all just kind of goes back to uh the classic paper would be um the anthropic paper on constitutional AI which I think sort of you know um really uh planted a flag on the ground in terms of um AI feedback being used for this kind of alignment process. Um finally the last thing I want to talk about for data is length effects. Um I think you know when we did the annotation um the one of the things that we saw was I think many of you saw the longer response and you're like this is more detailed. I like details. This is good. Um it's not just you. Um models and people all have this bias. Um and so people have found that you know models that people have thought were better were in fact maybe only just also longer. And then AI feedback seems to make models just generally longer. Um and so this is always a confounder that you want to be careful of you know length as a confounder uh for general preference. Okay, there was a question there. Could you expand on off policy and on policy? Okay. Yeah, I'll talk about off versus on policy later. Um you know off policy I think I mentioned it here um is going to refer to you collect these like kind of pair-wise feedback things um separately like they're not collected from the outputs of your model. uh maybe your model is involved like for example in Tulu you know you you've got all these kind of off policy data on the left that's models that are not your own but you also have on policy data from yourself right so the off policy data kind of tells you about the landscape of places you're not at and the on policy data tells you how to refine yourself um yes so the way the sort of human aspect is we're sampling prompts and then asking humans to um grade them. Have people ever instead of sampling prompts that you don't have the answer to and then asking for the screen you have like existing in the world from any number of places. There are existing prompts that we know the answers to and we have that data saved. We don't need to do anything to receive that data. Have people done sort of alignment tuning using that style of data instead of asking humans or LMS? Yes. Okay, that's a good question. Um, and it's like why why don't we do RHF on or like can we do R LHF on domains where we know the answer is like one way of putting the question, right? Um, and part of the answer is the next lecture on Thursday is kind of that like where do we really know the answer? Math. we know the answer very well for math and we can do exactly RL against math and it works very well. Um people also do things like you know here is a long form response from an expert that has already been written. Um now can you judge it given this? Um that helps but one thing that I think um is important to keep in mind is there's for a lot of these open-ended tasks there's many correct answers and it's very difficult to judge which are correct. Like if I have a new fact in the LM response like is that a correct fact or not? Doesn't really solve those problems. Yes. So like thinking like for instance the anthropic paper you referenced there's this like relatively specific and small constitutional thing but we can even like sample values to make this work for open-ended things like cuz we have any number of sources of like here's a problem here are values that we as a society think are relevant whether that's surveys or politics or you know articles just any number of like sources people do that I know that like anthropic should like let people vote but that seems small yeah like I guess there's lots of different things being mixed in on that comment like you could do like deliberative democracy style stuff the line models that is certainly a thing there's also I guess the the thing that feels close to what you're talking about is is almost giving the annotator sources almost that are relevant um and I'm sure those things help there have been works on like showing people expert written responses when they do the pair wise judgment and so on. Um but I think it's not a silver bullet like all of this like kind of UI interventions definitely help um but it's not necessarily going to in one stroke solve the problem. Okay. Yes. Um question is there to like paper where you have like um like you're using to annotate um like your other responses there's like a bias from like yes there's a very strong or like very detectable self preference for most models for their own outputs and so when you use this for evals which many people um including I have done you have to be very careful for that self bias. Absolutely. Yes. Okay. Then there's lots of hands, but yes, I think you were first. Balancing the amount of information you get just from having model create its own outputs and then like I guess model to do like its own feedback, right? Is there like a heristic for how many iterations that does that change if you use like the model itself? Yeah, I guess the question of like how much can you extract out of models doing things to themselves is an interesting question, but I guess the kind of in some ways the information theoretic bound so to speak is very very high because technically the model like ingests the entire pre-training corpus and that could be stored somewhere in the model and depending on how you prompt it you might get an amazing model out and so it's possible that that based on how you're using the model as part of your self-refinement loop you can extract more capabilities out of the model and we don't know what the upper bound of that is because you know basically the inputs are just vast. Um but I do think there's like practically lots of of papers that have studied this question of like how how much can self-improvement help like what's the scaling properties and so on and so forth. I think ultimately it's a very empirical question. Okay. Yes. I think uh the uh as kind of like problem because compared to SRT so basically SFT is the best answer we have a label for that but for assume that we don't have label but we have like reward model to give it like preference has like ranking things. Yeah. So so maybe I should just go through the next slides um because I think that will actually explain it. So not not only does it help us get through the rest of this hopefully quickly um but also I think it'll explain your your uh questions. So, okay. So, now let's talk about methods. Um, our goal is to do this thing, right? I want to find a policy to maximize my rewards. So, I need to tell you what the rewards are and I need to tell you what the maximization process is, right? So, let's do that. Now, um, I think as you can probably tell by the frequency with which I'm referring to the instruct GPD paper, the whole sub area of instruction tuning and post-raining is very closely tied to instruct GPD. So from the instruct GBD paper you have equation 2 which is this uh you know objective that describes what we're optimizing. So we've got this r theta of x of y. This is our reward and I'll define that in a moment. And then we've got you know this the second terms the log ratio of my RL policy divided by my SFT output. So what is this object? This is the KL divergence between my RL policy and my original SFT model. So it's saying when I do RL don't move too far from where I started. And then this this second line this gamma term this is basically saying keep doing pre-training while you do RL so you don't catastrophically forget if you you know keep doing this. Lots of people don't do the second step but this KL thing is a really you know it's a standard thing. It it remains even today. Okay. So now what is the reward? The reward um is this thing kind of at the very top. Um, and so maybe that's a small equation. So I'll I'll talk through what that is, right? So there is a a sort of hypothesized model of the world that exists. So what is the hypothesized model of the world? Um, it's that every single output in the world, right? Like that that a language model could output. Every single sequence has a scalar value R associated with it. And we don't observe what that R is. And when a person rates it like when when they do a pair-wise rating of A versus B what they do is they compare the two rewards of those two sequences and based on the difference they'll take a coin flip. Right? So this is a logistic model of the difference of the two rewards. So every sequence is a reward. When I do pair wise comparisons I take the difference and I flip a coin. Right? Um this is the Bradley Terry model of human preferences. Um and this is what's happening. And so when we want to optimize the reward, what we're trying to do is we want to output sort of the sequence that has the highest R. And R is not something we observe. We only observe noisy pair wise comparisons uh through R theta, right? So that's what we do. So that's the objective, right? So that's what we're trying to optimize. Um and so now let's talk about the how. Um and to be clear, I'm only going to talk about PO, which is kind of the OG algorithm. This is what appears in instruct GPT and a lot of the open AI stuff. I'm going to talk about it only very briefly and then I'll talk about it in much more detail on Thursday because it it more naturally belongs there. We're going to do a lot more sort of real RL on that lecture. So um at a conceptual level remember what we want to do is we want to optimize the reward of some policy right that's the left term here. And what's a good way of optimizing something? Well let's take some gradients right let's take gradient descent. So that's the very top left equation here. Now um you know we can sort of do a little bit of math and if we take the gradient of this object we can write down that this is equivalent to you know the expectation of the reward multiplied by the gradient of P theta. Um and so this is very natural what you're doing is you take your normal gradients that you normally take in doing like pre-training or whatever. This is saying P theta of Z I want to maximize that probability and I maximize or I multiply that with R. So if my rewards are positive I want to upweight those probabilities. If my rewards are negative I want to downweight those probabilities. Right? This is this is the policy gradient theorem. This is reinforced. You know if you've taken RL class or something like it you've you've definitely seen this before. Now what is PO? PO I think is is uh normally a very intuit uh sorry uh intimidating object but I think it is actually quite simple. So there's two steps that happen. First, instead of taking a reward, we look at what's called an advantage. An advantage, um, you know, just to sort of gloss over a lot of the details involved, um, an advantage is it's basically a variance reduced version of the reward. You know, if if you, uh, go through the math, you can notice that I can subtract any constant or in fact, I can subtract any sort of state dependent variable from R and this gradient will still be correct. That means that I can rewrite this reward potentially as sort of after subtracting any baseline values that I want. And let's say we call that the advantage. Now, not only that, maybe I want to take multiple gradient steps after sampling from P theta once. What's called essentially um you know sampling from one roll out and going almost off policy. To enable that, I have to essentially have importance waiting corrections because the more steps I take, sort of the more stale my original samples become. And so this is what's called PRPO. You basically make corrections for all the gradient steps you take and then you constrain yourself to stay close. Now PO takes the final step and says instead of explicitly constraining uh myself to stay close to my old policy using this KL constraint, maybe what I can do is I can just clip the um probability ratios and this will naturally incentivize the model to stay close to the original policy. So this is kind of PO in one slide. Um I'm not going to go into this with too much more detail because actually this won't be the primary algorithm that I want to sort of just go through the rest of the the lecture with. So at least in kind of the open research space and the academic space um a lot of the question that I think people were concerned with was can we get rid of PO? We will see this theme on Thursday as well. Um PO is very complicated. Um and we have debated but decided against having you implement PO because it will be suffering. Um and so lots of people thought can we get rid of PO and they tried other reasonable things. Um and I will explain those reasonable things like you know maybe we can SFP on the pairs but for each of the pairs we can prepend like a good token for the chosen good outputs and bad token to the chosen not uh the bad outputs and then I can just condition on good when I generate that does not work very well. I can train the model only on the preferred output that also does not work super well. um I can use a reward model, sample the best one out of those um and then train on those. Works okay, but but maybe not that great. Um and so people tried all these variants, but what really stuck um was basically DPO. And I think the reason why it caught on as much as it did, um was because it, you know, removed a lot of the complexity of PO and worked relatively well. Um and so you get rid of the reward model that exists in in PO. This is used to calculate the advantage. Um, we get rid of any of the on policy stuff like the importance ratio thing that I was talking about. You just get rid of all of those. Instead, you know, we go back to the basics. We take gradient steps on the log loss of good things. Um, and then we take negative gradient steps on log losses of bad stuff, right? We go back to very simple basic things. Um, and the last part of what I want to talk about today is just deriving the DPO formula, right? So, what is our goal? Our goal is to optimize this quantity at the very top. This is just a rewriting of that instruct GPT equation. I have a reward at the very front and then I have a KL divergence like this keeps me this is pi theta close to my reference. Right? So this is this is very natural. So the first thing I'm going to do is I'm going to assume that my policy pi theta is not actually a neural network. I'm going to assume it's an arbitrary function of any kind. And if I do that then I can sort of write down um essentially what the optimal uh policy looks like. It just look it has this form. It's the exponential of the reward and it it's multiplying um pyref the reference distribution over here. We can solve for the implied reward by solving for rxy. And the clever part about DPO is now to say okay it basically means that every uh policy instead of thinking about policies I can think about rewards because the two are one and the same under this nonparametric um assumption um and so what you do is remember I have these two pieces the left side this is the Bradley Terry equation from um the uh Steenomon paper the the one before instruct GPT and on the right side this is the uh DPO sort of equivalence I wrote down and then now um what I can do is I can plug in these rewards R into this objective and I can minimize this loss. I can say what I want to do is now I want to find a policy such that the implied reward for that policy has the highest probability of generating my pair wise comparisons. Right? So now I've taken an RL problem and I have turned it into a maximum likelihood problem. A problem that's very much uh similar conceptually um to uh something like pre-training. Right? All we're doing is maximizing the probabilities except what we're doing here is we're maximizing the probabilities of the pair-wise comparisons. Right? So those are the key steps. Start by making the nonparametric assumptions. Um parameterize the reward via the policy and then optimize it um using uh the supervised losses. Um I think we're a few minutes over so we'll stop here. I think this is a good place because we got through the derivation of um DPO um and we'll get through the rest of RLHF um at the start of uh next lecture. Um thanks everyone for asking lots of good questions.