So this is lecture 10. We're going to take a brief respit from scaling laws. Um and we're going to talk about inference. So the question of inference is a very simple one. Given a fixed model that we've trained, generate responses given prompts. Okay. Um first we're going to start by understanding what the implications of inference are um and the workload um that it entails. Um and then we're going to talk about ways of making inference uh faster. And throughout this lecture you're going to see that there's a lot of inference is a very deep topic. It's actually the uh we didn't do inference uh last year in lectures. So this is the first year we're doing it. But there's actually many many topics that could span multiple lectures which I'll try to condense into one. So inference shows up in multiple different places. The most obvious place is if you actually want to use a model, you want to use it to chat. Um you're using cursor or something to do code completion. If you're running batch a data processing job using your language model, all of these cases demand inference because you need to generate tokens from your actual model. But it also shows up in other contexts. If you want to even evaluate your model um let's say on instruction following you need to do inference. Um there's a lot of interest in test time compute which means uh thinking more before um you actually output some um uh the final answer and that's also more inference because thinking is basically generate tokens. And then finally, even training itself, if you're using reinforcement learning, you need to sample responses and then evaluate them based on some reward. And that also requires inference. So inference isn't just I want to put up a chatbot demo. Inference actually is going to underly many of the basic functions of a language model. And even though it's one lecture, I want to stress how actually important it is for uh many things. And we'll probably come back to this when we talk about um in alignment um later in the class. So now inference is important. So the theme of this class is efficiency and efficiency clearly matters. Training is a one-time cost but inference you repeat multiple times. So here's some you know anecdotal uh um stats on why inference is a big deal. So Sam says OpenAI generates a 100red billion words a day which is quite a quite a lot. Um and even cursor which is not that new of a a product is um allegedly generating a billion lines of accepted code um each day. So um it's just giving you an idea of how much inference is accounting for and you know costs of inference compared to training um are definitely you know increasing. So how do you measure what inference good inference looks like? So there's time to first token TTFT. So this is how long a user an individual user needs to wait before any generation happens at all. And this matters clearly for interactive applications. If you have a big prompt and then you have to wait there for 10 seconds, that may not be a good user experience. Latency is how fast tokens are arriving after maybe the first first token. Um this also matters for interactive um applications. Throughput is is something a bit different. Throughput is um how many tokens in general are generated per not for overall users. So this is it's particularly useful in batch processing applications. So you can think about the throughput is um high throughput doesn't mean low latency because uh some you know requests might just take a very long time and you still have high throughput. Latency is is kind of like the you know worst case over any individual user. So what do you need to uh think about when you think about the efficiency of inference? Um so in training um the the key idea is that you get to see all the tokens at least a supervised training which means that you can parallelize over the the sequence. This is exploited heavily in the transformer, right? So you've done the transformer training. You know that you basically construct these tensors over the entire sequence and it's just like tensor tensor tensor uh you know mats and then you get your output. But the the key defining feature of inference at least for transformers is that you have to generate sequentially. You can't parallelize because um the generation of a a token depends on all the past. So this is going to be the key thing that's going to make inference a lot harder and in particular it's going to be harder to utilize all the compute that's uh available and it's going to be memory limited as we'll see uh in detail later. So a lot of people are doing inference. Um anyone who's actually has a product and platform quickly realizes that these uh cost in doing large models is going to go up. So they spent a lot of uh time and you know engineering effort trying to reduce that time. So both providers serving closed models and providers serve open m open weight models uh pay a lot of attention to um to imprints um more so than I think the average academic because we're not actually serving any models. We're just training and getting a score and putting in the paper. But people who are actually serving models pay a lot of attention to inference. Um so there's also a bunch of open source packages which are um interesting to to look at uh as well. Okay. So um I want to understand the inference workload uh kind of in detail. So I'm going to review briefly um the sort of this transformer math that we uh um you did in assignment one and we talked a little bit about it during the the first week of of class. So this is from the scaling um JXML book which um is is something you guys should really take a look at. I think it does an excellent job of outlining many of the of the key concepts um here. Um and they have this really nice diagram that shows essentially the the computation graph taking an input and having it go through attention and the MLP layers. Um in particular um we're going to use this notation. So just to kind of review this this quickly. So B is the number of sequences in your batch. L is the number of layers. T is the sequence length. Uh you can think about as the number of tokens you're going to generate u or or query using. S is also the sequence length but how many you're um kind of conditioning on in your prompt. V is the vocabulary. D is the dimensionality of model. F is the MLP hidden dimension which is usually four times D. um h is the attention head dimension. N is the number of query heads. So generally N times H equals D. Um and then in GQA group query attention you have a different number of key value heads as query heads. Usually K is smaller than N and um G is the number of um of groups. Um so K * G equals N. Okay. Okay. And this diagram shows that you take your X, you feed through the the Q KV matrices, um, and you do a bunch of bunch of things. Um, okay. So, so remember that the flops required for a fever pass is six times the number of of tokens, which is B * T times the number of parameters uh, plus for the tension. there's another order t. So t * t is t ^2 um dependence. Okay, so let's also review arithmetic and intensity which is going to help us characterize when something is compute limited versus memory limited. Um so just to start with the basic you know u map mall. So let's take a matrix X which is B by B by D and a matrix W D by F and you know just to give some color to this uh computation B is the batch size D is the hidden dimension and F is the up projection matrix in the gated MLP. Um so next let's do count the number of flops and memory um read and writes u for just doing X times W. Okay, so we're going to start with um initialize to zero. And what one has to do for this is we're going to um read X from HBM. So you that means it incurs a memory cost of 2 * B * D assuming everything is in BF 16. Um you also read W. So that's 2 * D * F. Um then you do the the mat mole and that encurs 2 * um b * d * f flops. So remember this is from the first lecture so hopefully this is review. Um and then you have to write it back out which is you uh have to pay another transfer. Okay. So the total number of flops is uh just the the mammal and the number of bytes transferred is essentially the size of all the the matrices that are read and written and arithmetic intensity is basically the ratio. Um so the ratio is this expression. Um and in general just to simplify things a bit um generally the batch size is much less than D and F. um B maybe you know hundreds and a DNF might be you know thousands or tens of thousands. Um so I'm using simp here just to keep myself from making silly mistakes. Um so basically I'm letting C go to infinity um and D scales as C * B and F scales as C * B and that gets you a simplified equation of B. Okay. So the arithmetic intensity is B for this particular matrix multiplication. Um and um and the okay so the way to interpret this is how many flops are done per bite that was transferred. So now um the second part is you look at the accelerator which for H100 flops per second is um 98989 teraflops memory bandwidth 3.3 ter bytes per second and you divide and that gives you what is called the accelerator intensity and um if you look at the computational intensity um which is B if it's greater accelerated intensity that means you're compute limited that means you're able to use all the the GPUs or TPUs and if you're less than that then you're memory limited which is uh you know bad and so you're computer limited in this matrix multiplication case if B is greater than uh 295 uh for a H100 and all of this is a bit idealized the actual details there's it's this is giving you a kind of a first order approximation um so in extreme extreme case. So generally if you that means if you use batches of size let's say 300 then you'll be be able to saturate the GPU. But what happens if your batch is really small? So in particular B equals 1 which essentially corresponds to a matrix vector product then the arithmetic intensity is basically one and that is really really bad. that means you're going to be um memory limited and which which kind of makes sense because basically you're reading and writing this D times or actually you're just reading this D* F matrix and you're performing essentially in the same number of flops right so the ratio between the flops and the reads is the same which is gives you one and one is bad you want a lot of flops to be done for any memory read because memory reads are uh slow But this is in in essence what happens with generation right because you're proceeding token by token. We'll see that basically um your arithmetic intensity is going to be like one and that's why uh generation is going to be memory limited um and not compute limited. Okay. So this is a very simple example that I think gets at the core of why uh generation is going to be slow. So maybe I'll pause and take any questions on this um just to make sure everyone's clear. Yeah, I mean when doing why don't we have a batch size of say larger than one. So I think I heard the question why don't we have a batch size more than one. So I'll get to why you can uh but there's um batch size is going to mean batch size time sequence length um later. Okay. So in summary um matrix multiplications are the kind of the core computation. So we just studied a matrix multiplication and counted the number of flops it requires over the number of read and writes and we show that that ratio which is arithmetic intensity depends on one of the dimensions in case in this case the batch dimension and that's why big matrices are are good because that can saturate your um your compute whereas if you have even a thin matrix um B equals one that's really bad because you're spending a lot of time reading from memory and not doing that much uh compute. Okay. So um now let's talk about the arithmetic intensity of of inference. Um okay so let's let's just uh kind of get more into the weeds of what inference looks like. So, so the naive thing you can imagine doing and all these nice pictures are taken from this this book um is that you have a transformer, you give the prompt in, you gives you logits over the the vocabulary of the next token and you just sample from that and then once you get that you attach it to the prompt and then you feed it through the transformer and you look at the logic sample again and you repeat, right? So that's um the sort of most naive thing to do. Um and you know the complexity here is is pretty bad because each token you generate is like n squ or t squ um you know computation through the through the transformer. Okay. So that that's no good. But if you look at this closely, you'll notice that you're doing a lot of redundant work, right? All these um the work um in basically encoding the prefix basically stays the same. So this is for a uh for a birectional uh transformer it would be different but at least for a auto reggressive causal transformer it is the case that you should be able to share a lot between prefixes and so the solution is you cache and you cache in the IBM HPM because that's uh where you have enough space to store stuff. So this is what it looks like if you have a KV cache in schematically. Um so you take your prompt the prefill step is you feed it through the transformer and you compute this KV cache um and then you generate the logits over the next token and then you um put that into um you know take that generated token and the the cache and then you can feed it through the transformer. But you've already computed these so you don't have to do that again. you just need to compute this new um KV uh vector for this token and now that allows you to more quickly generate the next um token and and so on. So basically you're filling up this KV cache which corresponds to the tokens that you've either prefilled with or that you've generated so far. Okay. So instead of T squ per token, it's going to be more like T. Okay. So concretely the KV cache is for every sequence in your batch for every token in your sequence for every layer of the transformer for every head you're going to store a hdimensional uh vector. So you might think that this is going to take a lot of memory and you wouldn't be wrong. Okay. So there's two stages of inference. So prefill is you're given your prompt encoded a vector. So this is just like what you do in training. It's paralyzable. It's fast. Your compute limited life is good. And then you're doing generation which is you're generating response tokens one by one sequentially. And this is a part that's going to give us a lot of trouble in terms of uh efficiency. So now let's compute the flops and memory IO for both the for the transformer. So we're going to break it down into MLP layers and the attention layers. Um and um just notationwise, we're going to do this computation with s being the number of tokens we're conditioning on. Think about the length of the prompt. And T is the number of tokens we're generating or or quering using. Um and in prefill t is going to be s because we're sort of I mean we're not generating uh t tokens, but we're sort of like querying um using each of these tokens. and a generation where t is just one. Okay, so hopefully the matrix multiplication is still fresh in your head because this is going to be essentially that but a little bit more complicated because it's a transformer. So we're going to count the flops and bytes generated. Um so first we're going to um take x which is a um b by t by d u matrix. I think maybe these T's should be S's. Um but um anyway, so um so that involves doing a bunch of uh transfers. Um uh basically the size of that matrix times two because BF 16. Um then there's the the three-way matrices, the up projection, the gate, and the down projection. They're all the same um you know size up to transposition. Um so you need to transfer those. Um then you do the um you know up projection um that's some number of flops. So B * D * C* F. So whenever you multiply you know two tensors basically the contracting dimension only gets counted once whereas other dimensions you just you kind of gather together. Um you need to write it out. You also have the gate which is the uh same same thing. You write it out. You compute uh your um nonlinearity. You multiply some stuff and you down project um and that's b * t * d * f which is basically the same number of flops and you write um out uh the the result. Okay. Okay. So if you look at the the counting um I guess maybe I'll just uh you know you can check the um you know results. Actually you don't need to check it because this is simp and it's guaranteed to be you know correct. Um so but again we're going to assume that B * D is much smaller than DN and F and we get that the intensity is B * T. Okay. So this is analogous to the matrix uh multiplication case where the in arithmetic intensity which we want to be high depends on how large your batches and how many tokens you're you're essentially generating. Okay. Okay. So now if you look at the two stages prefill life is good remember because we can just make BT large enough you use a batch size even a batch size of one actually is is maybe okay if you have long enough uh sequence um so that's not a problem. Now generation this is where it becomes a little bit harder because you're generating one token at a time. So T is one right? So if t is one that means for bt to be large you need b to be large and b is essentially the number of concurrent requests. Okay. So this is kind of interesting because your your sort of efficiency depends on um having large batch sizes because I mean intuitively it makes sense if you can take a lot of requests batch them together then uh you can get um you know better uh efficiency at least throughput. Um, but this also depends on what um B is because if it's um you're only getting a few requests at a time, then you're not going to be able to use your hardware very efficiently. Um, and this is talks speaks to the sort of the very dynamic aspect of inference which we'll come back to later in the lecture. Okay, so now what about attention? Turns out attention is even worse um for reasons I'll I'll try to get into. So let's do the counting flops um bytes transferred. Okay. So I'm going to read the QKV um matrices from HPM. I'm going to compute the the tension which is a matrix which is Q * K. And the number of flops is B * S * T * D. So remember S and T are the same during a prefill. So that's your um sequence length squared times b * d and then um I'm I'm sort of only looking at the matrix multiplications because the flops from the other steps don't really matter. Um and then you project out to your uh oh sorry you you take a combination of this and v. So actually this is mathematically incorrect because there's some soft maxes there but the essence of the map malls are are the are the same. So that's the same number of flops and then you write to HPM. Okay. So and here I'm assuming there'll be more bytes transferred if you didn't use fly flash attention. Flash attention means that you don't have to keep on writing back to HPM intermediate steps. So um but the order is actually not u really affected. So qualitatively doesn't really uh matter whether you use flash attention or not but the the math here depends on the con the constants matter. Um but let's look at the flops and the bytes transferred and if you uh divide and simplify you get this rather nice expression. I mean nice in that it's simple, not nice in that it's uh it's good efficiency which is S * T / S plus T. Okay, so let's try to interpret this a bit. So in prefill T equals S. So that means your prefill intensity is order S. So that's good, right? Because as long as you have long enough uh you know sequences then uh you're you're you know good to go right and generally the sequences can you assume are kind of long enough. Um during generation however um you'll see that the entity is uh essentially one s over s+ one but that's basically one um and remember one is is really bound. Okay. So, but but notice like what there's no dependence on V at all. So, unlike in MLPS, remember in MLPS um the the generation the prefill was BT which is great um and then arithmetic intensity was B which was not great because it depends on the whims of your users and workloads but still could be larger than one. Whereas um for attention it's actually just always less than one. No matter how many how long your sequences there are, how how how many users there are um it's always one. So why why is this intuitively that there's no dependence on on B the batch dimension. So the reason is that in the MLP layers intuitively every sequence hits the same MLP weights. So whereas in attention layer each sequence has its own uh KV cache right because the KV cache is sequence specific which means that you you can't really you know uh use in ML case you can kind of read all the weights and then you process a batch intuitively whereas in attention case um every sequence kind of requires additional memory you don't get any kind of savings if you um if you kind of batch them up mathematically I guess you can look at it through here where um the number of flops there's a b here which is expected but the number of bytes transferred is um you know b times um there's a scaling in in b so when you divide that b uh cancels um whereas over here um there is a b here but um we're assuming the df f dominates. So when you divide um basically there's no b essentially left in the the denominator. So you can look at it mathematically or you can just kind of reason about it intuitively as um during for the attention the kv cache is sort of every sequence's own unique uh snowflake. Okay. So the summary is prefill is compute limited whereas generation is memory limited. Um the MLP arithmetic intensity is B. Um which to make good enough you need bunch of concurrent requests but attention intensity is one which and it's also impossible to improve that. Okay. I'll pause bit for any any questions. Okay. So let's let's move on. Um so now we know that inference it due thanks to generation is memory limited. Let's uh try to study the throughput and latency um um at least in in theory um so let's focus on um let's see actually okay so um we're going to make some assumptions. So all of this is sort of napkin math is a little bit stylized um but it gives you roughly the right kind of scaling and the right way to think about things. So we're going to assume that commu communication and compute can be perfectly um overlapped which is obviously false but it's it's good enough for for making these qualitative uh um estimates. Um so what we're going to do is we're going to instantiate the latency and throughput for a llama 2 13B on H100. Okay. So for a 13B um here are the values. So the se let's just put the sequence length to be a th00and hidden dimension to be model dimension to be 5,000 four times uh I don't know if that's that's not four times but anyway f is um some multiple of that number of heads number of uh key value um I guess uh query heads number of key value heads which for llama 2 is the same we'll get to that point later um and and so on and for the memory bandwidth of H100 that's the Okay, so that's uh the config and we're going to compute the the memory latency and throughput. Okay, so Oh, okay. Um so first uh let's just quickly get the number of parameters. You guys did this in assignment one. So I won't bel labor this but it's some expression. Um that depends on all the different knowh um and uh the to store the parameters we're going to use f B bf16 um because inference is generally going to be 16 bit not 32-bit um so we're going to multiply between two. So that's the memory that the parameters take. Okay, we don't need gradients. We don't need optimizer states because we're not training. But we do have to store the KV cache which are the act some of the activations um not all the activations but some of them for every sequence of length s and how much we have to per store per sequence. It's basically the sequence lengths times the number of key value uh um heads times the dimension of that head times the number of layers times um basically two for basically both a key and a value and two for BF 16. Okay, so that's how much the cache size uh takes. Um and so the total memory is batch size times the cache per sequence plus the memories uh plus the parameter size. So now latency is going to be determined by memory. Remember it's memory limited. So we're just going to compute how much memory needs to be transferred uh into the GPU to do this computation. is simply memory over the memory bandwidth. Um, and throughput is essentially the inverse of latency but scaled up by B because we're looking at generating B tokens in in parallel. Okay. So now if we substitute our um llama 2 config um we'll see that the number of parameters checks out it's it's um you know 13 billion roughly the memory latency and throughput have these expressions um so memory you know grows obviously this is the parameter size this is the key value uh cache size times times B latency also goes up as a function of B throughput um um increases but you'll see that it increases up to a point. The B shows up in both the numerator and the denominator. Um so there's limits to how much you can stretch throughput even if you could fit everything in memory. Okay. So those are um the expressions uh for latency throughput and memory for this particular model. So now let's instantiate with different batch sizes. So if B equals 1 um then the latency is about 8 milliseconds. So 8 mill every 8 milliseconds you generate a token and the throughput is 124 uh tokens uh per second. Okay. So that's 13B on a H100 um if you're using batch size of one. So now what happens if you use um batch size of 16? So you'll see that the um the memory usage increases because you need to store the KV cache for all 64 sequences. Now the latency goes up. Um because you kind of have to instead of just processing one, you have to kind of wait for everything to finish. Um but the throughput also goes up actually quite a lot. Okay. So you're seeing kind of this uh immediate trade-off between latency and throughput. If you want low latency, you just use one um one uh B equals 1. But if you want high throughput, you want larger B in general. Um what happens if you use a batch size of even larger? So 256. Um you'll see that the latency goes up, throughput goes up, but you see that throughput isn't going up that much because you get diminishing returns after a while. But the most kind of uh you can't actually do this on a 100 because if you look at the uh the memory it's 240 uh gigs. So it doesn't even fit. Okay. So batch size you can only increase to a certain point um because of memory. Okay. So just to recap, there's a trade-off between latency and throughput. Smaller batch sizes yield battery latency. larger batch sizes yield better uh throughput. And finally, there's uh we talked last week talked about parallelism for training and it was, you know, kind of complicated, annoying. Um at least one type of parallelism for inferences is really really nice and simple. Um you just launch M copies of the model. Okay, no communication because the model you don't need to update the models. the latency is the same and the throughput increases by m. So that's that's pretty good. So always remember that uh you know don't forget easy things. Um now there are cases where you if you have a large enough model then uh maybe it doesn't even fit on a single uh GPU and you need to shard the model and in this case you also want to uh start sharding the KV cache in some cases um to get you know better you know efficiency. So um there's for more details uh check out this uh book chapter. Okay. So uh the time to first token um which is a metric I mentioned earlier is essentially a function of of the prefill. It's basically how long does it take to encode the prompt and usually it's uh you know this is compute limited so you're basically going as fast as you can. Um and there's not much you can do about it given a fixed architecture. Um and uh well okay so sorry you you can improve it if you um if you reduce the the batch size uh still um but if you want to improve the throughput you have to um you know increase the batch size. Okay so any uh questions about that? So this was on computing the throughput and latency and because of the memory limited uh argument that I gave in the the previous um part I just focus on memory and compute how many bytes need to be sent and that gives me uh a rough bound on um the latency. In practice the compute there are some regimes where compute does matter but I'm sort of ignoring that just u to keep things simple. Okay, a question. Is this assuming a single GPU? Uh the qu Yes, this is assuming a single GPU. Uh while creating a batch from multiple users, uh each of these will be completed different tokens. Yeah. So the question is if you have multiple users and you're batching them together, they might arrive at different times. They're going to finish at different times. So, we're going to get to that. Um, that's going to be a special issue that we're going to have to deal with. Any other questions? Okay, so now we have a good handle on what the inference workload looks like. U, we looked at the arithmetic intensity. We looked at the the transformer inference with respect to in arithmetic intensity. We saw that it was memory limited thanks to the tension where the KV cache has to be special for every sequence and then using that we can compute throughput and and latency which are the main inference metrics that we we care about. Now how do we make things better? Okay, so there are some things that you can do on that are lossless. You can write better kernels. You can improve your your systems. But I would say that the the high kind of there's a lot you can do if you're willing to take uh you know shortcuts. Um and these are kind of really interesting because technically this lecture is on inference but secretly it's on model architectures because what you'll see is that a lot of the changes in model architecture are going to have direct impact on inference and we're actually inspired by needing to do inference quickly. Okay, so the big bottleneck here is the KV cache, right? Because remember memory limited, which means that the less memory stuff takes um then the faster you go. Not just because of flops, even though that's the part of it, but mostly due to memory because um it's mostly about the memory transfers. If that's one thing you take away from this lecture, it's like all about the the kind of the memory uh for speed. Okay. So the the problem is that if you just start whacking away at the KV cache, you're you might lose accuracy. So how can you make sure you don't lose too much accuracy but still uh maintain um your your KV cache uh small? So there's a bunch of ideas I'm going to go through um that all essentially try to change the architecture to reduce your KV cache. Some of these ideas I think you you've seen um but I'll go through them kind of in this sort of more systematic way. So there's this idea called group query attention. So multi-headed attention which is the vanilla transformer keeps around uh basically number of heads and for each of the that number you have same number of keys values and queries. Um there was one time a multi-query attention which you only have one uh key and one value basically one key value head. Uh turned out that that was not very expressive. So there is a sort of intermediate point where um you have a reduced number of keys and values and then you have more queries. So why are we doing this? Well remember we want to reduce the KV cache size. So the fewer keys and values there are the better. So the batch size and the sequence length doesn't uh get changed but it's and the dimensionality of the the these vectors don't change but it's the number of key value heads that we're reducing. Okay. So, so that's basically the idea and this paper shows that you do get latency and throughput um improvements. So, times per sle sample and as you increase the number of groups um then uh up to eight or so basically there's a negligible um uh is it's really fast compared to um the full attention. Um and as you increase the number of groups obviously you kind of end up at the the original. So that's uh latency and throughput improvements. Um and just to actually do this kind of more rigorously. So we have our llama 213 uh B model. And if we compute the the statistics um this is using a batch size of 64. Remember this is what we got. Um uh I guess I should have printed out latency here. Oh well. Um and then um if you run it with GQA you see that the memory is reduced and the throughput goes way up. So this is actually great. Um so this is what happens if I take the llama to 13b architecture and I just reduce for every query head I'm going so for every key value head I have five query heads that's what 1:5 ratio means um so which this also means we can use a larger batch size because remember last time we tried to do 256 it didn't even fit in um an H100's memory so now um we can actually comfortably fit into the H100 memory and then we can further improve the throughput by using a larger batch size. So you can see kind of a lot of different effects here. By reducing the number of uh key value pairs, the memory of the KV cache reduces. Um that means the the throughput and latency go up automatically because fewer memory transfers and furthermore as a secondary effect I can increase the batch size within the GPU and that further improves the throughput. Okay. So that's that's wonderful. Um we have to also make sure the accuracies doesn't drop. So this is um this original paper that shows that uh this is full attention. This is GQA. Um the time is much less but the accuracy is basically the same. Okay. Now um you know what actually happened? So I don't uh so llama 2 did not use this ratio but llama 3 actually picked up you know GQA and um you know probably motivated by the kind of inference costs actually llama 2 I think the 70 the large model did have GQA but not the smaller ones okay so that's GQA there's another way to reduce the key value cache and this comes from deepseek So this is actually from the deepseek v2 paper and it's called multi head latent which tatsu lectured about uh previously but I'll try to um talk about it in the context of inference and its implications. So the basic idea is uh here's full attention and GQA says I'm going to use fewer keys and values. MLA says I'm not going to change the number of key and values. I'm going to project these into a lower dimensional space. So, it's another way of shrinking the KV size, but just in a I guess in a different um dimension. So, instead of using N* H dimensions for each uh um for the KV cache of each uh you know uh token, I'm going to project out to C dimensions. Um and this is what Deep Seek did. It's actually quite a aggressive reduction from 16,000 to 512. Only wrinkle is that this is not compatible with rope. So they need to add a few more more dimensions to put rope back in. But overall, this is actually quite uh promising um from a KV reduction perspective. I'm not going to do the math. you can but you can just trust me that you can see kind of how the KV cache would be reduced a lot and you get the same kind of latency and throughput advantages. Um, and in terms of accuracy, they actually showed um that compared to GQA um the MH uh sorry, the um actually maybe I'm showing the wrong thing here. Um maybe okay. Um I meant to show that the um MLA actually improves, but this table does not show that. So I'll have to um dig that up later. But but anyway it MLA does um preserve the the accuracy as well. Okay. So there's another idea which says well um you know GQA basically shares you can think about it as a sharing key value uh vectors right within um a token and within a um sequence. Um but we can also look at something called cross layer attention which uh there's a paper on this but I think many people have been kind of uh thinking about this and doing this uh so I don't know know if this is actually the first paper but basically if you look at the transformer I diagram um you have the key value projection of one layer and then you have the next layer and these key value um vectors are separate um usually um but IDE idea here with uh CLA is that we're just going to use the same key value projection across layers. That's why it's called cross layer attention. Um so um so just as GQA shares across heads, CLA shares across layers. So here we they show that they empirically improve the paralo frontier of accuracy um and the KV cache uh size. So KV cache size which relates to throughput and latency um you want to be small and you want perplexity also to be um you know small. So they're able to you know improve uh that um okay so so notice that I mean for example H6 64 heads you know the cache size goes uh it gets reduced but the validation perplexity does go up a little bit but overall the the there's kind of advantage um in you know making that trade-off. Okay, so there's yet another way to do things. Um, so local attention um, which has been explored actually quite a bit since even kind of the there's a long former there's an open AI paper and then Mistro and I think many others use this as well. Um, it's a very I guess a natural idea instead of if you look at a full attention diagram it's dense n squared. Um, and that's where a lot of your complexity comes from. Um, and basically the idea is, uh, you're going to just attend to only the past K tokens. Um, which means that in the KV cache, as you're generating the sequence, you don't have to remember everything as as soon as the token kind of falls outside your window that you have of attention, you can just like throw it away, right? So local contention is very uh you you could say that the KV cache size remains constant as opposed to growing with the sequence length. So this is really good right because that means for even long se sequences you can have um quite a small cache. Okay. Um but you know the problem is that this still you know hurts accuracy because if you just think about it like why are we doing attention instead of RNN's is that we needed to have long range to MAC model run long range dependencies and this is in some sense even to call it attention is a little bit kind of a um overselling this this is only looking at the local context um which is not very expressive. So what you do here is you can interle local attention with full global attention hybrid layers. Um so for example character AI used um for every six layers they had one global attent global layer and five um local layers. So it looks something in addition to cross layer attention. So it looks something like uh this um where full attention every layer um you have to store the you know KV cache um and for um what they did is that for every six layers you have the full attention but in between you have this local attention and on top of that they have KV cache sharing um locally both at the for the local attention and the global attention. So this is like all the tricks kind of you know not all the tricks but many of the tricks kind of combined you know together. Um so in summary these are a few ways to reduce the KV cache size because remember inference is memory limited. Um so you want to reduce the cache size but you don't want to hurt accuracy too much. And there's many ways to do it. You can lower the dimensionality of the KV cache. You can have few KV cache vectors. You can reduce the dimensionality of a a KV vector. You can share the KV cache across layers. Um and also you can use local attention on some of the layers. Okay. Any questions about um the set of tricks for reducing the KV cache? Uh yeah. Yeah. Question about the costion. So like how many like I feel like the weights are all being shared across the like the same layers like do you just have like one set of weights which just like KV that's shared across the blue ones? Yeah. So the the question is are the weights shared? So the KV cache is shared but the weights are shared. So what happens is the weights for doing the um you know the projection need to be shared so there's some consistency. Yeah. Yeah, there was another question. The the context size is is too large every you know and then it increases the KV catch as well the contact size when you pump the context that is given to the trans. Yeah long then it increases the size. So we try reducing those context by summarizing the context context. Yeah. So the question is uh if you have really long context let's say your prompt is huge that's going to intrinsically take a lot of KV cache um so all these tricks can try to reduce uh that um you can do more aggressive things that like you know there's ideas like you know just tokens or ways to summarize the prompt um which we're not going to talk about in this class but there's ways of that address the long prompt situation as Okay. So now I'm going to talk about even more radical ways of uh making inference go faster by changing the transformer. So the KV cache these are basically variants of the transformer. Um but maybe you can actually go um outside the transformer and and do better because the transformer wasn't really designed with you know heavy inference workloads in mind. They were just trying to try and train a good model that um efficiently. It was mostly about training efficiency and the auto regression as we sort of pointed out is really causing this kind of um you know bottleneck here with the auto regression plus the kind of the full attention. So um we're going to talk about two directions uh state space models and diffusion models. Um this is going to be fairly quick. Um so the idea of state space models is actually drawing ideas from you know signal processing and control theory. Um initially the motivation was trying to model long context uh sequences without suffering the n squ blowup. So it wasn't necessary about inference speed but it turns out if you solve that problem you get faster inference too. So there's a kind of early paper on S S4 U which uses classical uh state space models which are basically these kind of linear dynamical systems u which are you been used to kind of model along context and sort of shoehorning them into the kind of a modern neural um setup. Um this work is uh allows has is you know nice in that it has sort of this RNN kind of interpretation due to the linearity structure and also has a you know convolution um interpretation as well. So they published this uh this uh this paper and um you know showed that it I think worked really well on these long context you know synthetic tasks. Um but what they found is that what I guess what was uh discovered is that they don't really work well for language modeling. uh and that's obviously kind of a disappointment because a lot of the value of transformers is being able to do language well. Um so in a series of papers there was um they sort of identified a set of kind of synthetic tasks that captured the essence of why these models weren't working well and that's basically these associative recall tasks. So here's a synthetic task where you're given a basically a sequence of key value pairs and um and the goal is to predict basically look up the key and output the value. So in some sense it's kind of a logically a trivial task but it's long sequence because I can have a lot of key value pairs and um I'm going to have to look far back. It can be arbitrary long dependence and you can see that local attention is not going to work very well because it's just going to remember the last few sequences and um what the problem with states space models is that they're sort of good for these kind of signal processing tasks but um really this is like you need to go isolate a particular key value pair and pull out the the answer and for those type of tasks it didn't really work. So there's a bunch of work um I'm not citing there's like hyena h3 and then mamba um which basically tweaked the or changed the hssm um to basically handle these associative recall tasks and eventually it worked better up to kind of 1b scale was matching you know transformers um and uh mamb idea of mamba has been popular and scaled up um even to a 52b um um by AI21 folks. Um notice that in this case they still had to use a transformer. So a transformer but only every I guess eight layers they had a transformer. The rest of them were mamba layers. Um and so that still get led to a fairly big you know uh savings and and speed up. Um and uh but more recently there's this kind of revival of this older idea called linear attention. Um where instead of um let's see if I can make this bigger. Um is actually a kind of a very simple idea. So you know what local attention or sliding window attention is? Um linear attention is this idea that uh you essentially in so in this basically in the um attention computation there's a key and a query and you uh dotproduct them and you take the x of that which is basically giving you a x kernel. So you can basically take a tailor expansion of that and write that computation as basically dotproducts of some nonlinear map. So then what you essentially have is you you can think about for every uh key value position you are basically applying some sort of nonlinearity blowing up into some space and then doing some linear computation over it. And because it's linear attention, it actually kind of behaves like an RNN and it's linear in the the sequence length rather than quadratic. I know that was a little bit uh fast, but um I just want to give you sort of the the taste of it. And so this idea has been actually uh scaled up quite successfully. So there's this um um organization called Miniax that's training um pretty legitimate models um up to 456 billion parameteres. Um and they use this basically linear attention idea. Now they have to use full attention um still once in a while. It seems it I don't think um people have been able to get around having some full attention but at least it seems like people have been able to get rid of most of full attention in like most of the layers are not full attention anymore. They're just either linear layers or local attention layers which are much much more efficient. Okay, so the linear plus local attention um you know now are actually yielding serious state-of-the-art models and it's probably you know safe to say that well I don't know what's uh exactly the the closed model providers are doing but I would suspect that there would be at least as kind of advance in terms of as efficient as um this and leveraging sparsity So it's it's kind of an interesting question when people ask like well you know is trans attention all you need is transformers it well you know yes and no I mean I guess in some sense there is still the sense squared there maybe we'll be able to get rid of it but most of the transformer has been like pretty radically changed by having other much lighter weight you know components and you're still able to get much of the same and kind of accuracy And all of this is, you know, really helpful for inference because on these non-ful attention layers, you're basically replacing the ordered t KV cache which grows as a sequence length with something that's uh constant. And there's there's papers by um uh that followup I think they uh on the based paper where did that go? there's some f uh either in this paper or in follow-up work analyzing basically the trade-off between the KV size and the ability to do uh various types of kind of recall tasks which makes sense because if you don't store very much you won't be able to solve certain tasks but there is this trade-off curve that you can try to play with okay so that's all I'll say about um the state space models So now let's talk about a completely different style of uh generation models, diffusion models. So diffusion models have been very popular image generation but they turn out to be fairly tricky to get working in text although there recently have been some advances here. So the idea of diffusion is that you instead of generating all regressively you just generate every token in parallel. So obviously if you only do that you know via some simple layer it's not going to be very good. You can't generate all the words in parallel and expect it to be coherent. But what you do is you iterate and you keep on refining um this this generation until it gets to your final uh generation that you output. And the the idea behind generating in parallel, you're no longer auto reggressively, you know, bound and that generating all tokens in parallel well can be done in parallel. So you get to saturate your your G GPUs relatively easy as long as your context length is large enough. Um so recently there's this uh and session labs has um produced some pretty interesting models. Um there's not much written about them but you can see kind of a a demo of the generation and process. it just kind of generates um code instantaneously but it's obviously kind of broken code and then it uh you know kind of refineses over time. So, and this is one of their their benchmarks that show that at least on coding, I'm not sure about other uh tasks that if you look at that at tokens per second, these models are like way out here in terms of speed compared to anything that's transformer. Even Jamba remember is like a hybrid mamba um um uh transformer architecture is is quite slow compared to these diffusion models. So now whether diffusion models are you know be will be kind of general purpose and powerful enough in all these that remains to be seen but I think it's you know you have such a lead on the the kind of the token speed here that um even if you um I think you can put more compute and kind of recover some of the accuracy uh losses if you if you need to. Okay. So the the summary here is that I think this whole kind of architecture novel architecture thing is actually really exciting for inference um because um they allow you to sidestep kind of fundamental obstacles right so if you're dealing with attention you just have this fundamental KV cache obstacle that you can quantize you can optimize but it's still there and and so by making a kind of safe space model you're shrinking that to like a constant size and as long as you can keep up the accuracy which is big if um then you win big time. Same with the diffusion models auto reggressive generation is a key bottleneck. Now if you just generalize generate things in parallel now all of a sudden you kind of like change the game you know completely. So there's much more work to be done here um in proving inference. So as you can see now inference is the inference game is is much broader than it seems at first sight. It's not really about kind of necessarily the systems optimizations to make it fast although you obviously need those but I think the real gains are coming from um like real radical changes in architecture. Um okay so I have about 10 minutes left. I'll go through these quickly. Uh quantization and model pruning. So quantization the key idea is just reduce the precision of the numbers. Okay. So easily e very easy to do and the thought is that less memory means uh less bytes transferred higher sorry there should be lower latency higher throughput. Um and you do have to worry about accuracy of course that's the the trade-off. Um if you look at the different types of uh you know formats FP32 used for training not used for inference really um BF-16 is sort of the default for inference. You can go down to FP8 or INT8 which now is um is less accurate but much you know cheaper than even FP8. Um so people do do a bunch of inference in int 8 which if you look at the range I mean it's an integer between 127 minus 128 which is not that uh it's pretty low precision and people even going down to in4 which is they're not oh yeah so int4 is pretty you know low um there's also other ways you can do okay so you can so once you kind of decide that you want to um quantize. I guess you could do several things. You can train with a quantization, but obviously that means you need to retrain models and more I guess commonly you do post-training quantization where you take an existing model and you try to quantize it and try not to screw things up too much. Um so the there's a paper called element int 8 which I'll I'll talk through you know kind of briefly. So in quantization um basically what happens is that you take your you know your your vector which is let's say FP16 um and then you need to figure out the dynamic range right if you want to pack it into int 8 you need to figure out what the largest value is and once you figure that out you can kind of um you know divide uh by that and multiply by 128 and then you get your integers and then if you need to um dequantize then you kind of go the other way. So, so basically quantization means that remember memory is a bandwidth, right? So, bottleneck. So, you're all your transfers are happening in um in in end date but when you actually do the you know I guess uh you sometimes have to upcast to um a floating point to actually do the arithmetic. Um okay so the problem with int8 is that not everything you know fits nicely and you have these outliers which appear in larger networks that screw things up. So what this paper did is that you take a this matrix you identify the really large outlier values and then you handle them separately use in full 16bit precision and then do the most the vast majority in 88. Um so this works well but is actually a bit um you know uh slower. So the motivation here wasn't inference speed but more even being able to fit your model into u memory. There's another paper called activation aware quantization and here uh the idea is that you you're kind of quantizing the bas kind of the the weights but you're going to figure out which weights to quantize based on the the activations um you know really quickly this you're going down to actually in three um and this obviously reduces memory by quite a bit and leads to a 3x uh you know speed up And so the general idea here is that you want to um you get a trained model and it just happens that some of the weights or activations are going to be abnormally large. So for those you handle separately and then everything else you can kind of work in low you know precision. Okay. Um talk about model pruning um ideas very like quantization. It's sort of the basic idea is very simple. you just rip out parts of an expensive model to make it cheaper and then you fix it up. So in this Nvidia paper what they do is they first identify important either layers or heads or hidden dimensions um using a small calibration size. say use some you know uh simple scores to compute that and then you just remove the unimportant layers or hidden units or heads and then now if you just take that model it's going to be clearly worse right but so then the last step is you distill the original model into the prune model so you kind of repair the model from the initialization which is your prune so you're not starting from scratch you're starting from something that's worse but hopefully not worse and hopefully retains a lot of the same structural properties of the original model. It's just maybe not kind of like calibrated in some sense. Um, and the results are pretty good on that. So they have these 15 billion parameter models that they're able to reduce to 8B with hardly any drop in this I guess at least according to MMLU and then down to 4B with uh some drop but you also are going down quite a bit to a 4B model. Okay. So um so maybe summarize this taking shortcuts idea. You can uh reduce inference complexity without hurting accuracy. You can do it from scratch where you just define a fresh architecture that's by construction fast and just train it. Or you can distill you define that architecture. You can take a slow model and you figure out a some sort of scheme to initialize the new model with the old model. And then you basically do distillation. Um so okay so now all of these are a little bit unsatisfying because they're lossy. So you get massive speed ups but you always wonder well maybe this model isn't as actually good as original. So speculative decoding or speculative sampling allows you to um basically have your cake and eat it too. So recall there's two stages of inference. You prefill um which you're given a sequence, you encode all the tokens in parallel. This is compute limited um which is great. Notice that this also gives you log probabilities for each of the tokens. And then there's generation which is one token at a time. It's memory limit. It's slow. So in other words, checking is faster than generation. So intuitively this makes sense. But hopefully now you also appreciate the math behind why this is true. Um and the speculative sampling idea is actually really really again it was proposed in parallel by these two independent teams from Google. Um and uh the idea is to use a cheap draft model P to just run ahead and generate some tokens and then you're going to evaluate those tokens with a target model and because evaluation of given tokens is just prefilled so you can do that in parallel which is fast and then you accept it if it looks good. So uh this is what it looks like in uh real life. So if you're using a big model generating one token at a time that's slow but in speculative decoding you have the draft model that's racing ahead generating um a lot of tokens and using the big model to essentially verify and sometimes it will reject and sometimes it'll accept and the acceptance rate basically determines how fast of a speed up you you have. Okay. So here is the more formal um algorithm. Um so you're going to have a look ahead of K. So you're going to use your draft model and generate K tokens auto reggressively. So this is hopefully fast because your draft model is small and then you're given these K tokens that you generated and I'm going to score them based on I'm going to compute the probability under the target model Q. Now I'm going to decide whether I want to accept this not or not. So I go through each token and I'm going to essentially accept it with probability um Q over P and the one just is uh make sure this you know probabilities are between zero and one. Um if this kind of looks like if people are familiar with Metropolis Hastings this is kind of where this uh this kind of comes from. Um so intuitively you're you're sampling with P. Um so you need to divide that out because you don't want P you want Q. So this is kind of importance weight um on this. So if you accept it then great you kind of move on and you look at the next draft token and so on. Um and if you don't accept it then you're going to sample from the the the target model the slow model but you kind of do this correction where you've already tried to sample using P. So you don't need to do that anymore. You subtract it out and you sample from Q. So this is basically kind of a rejection sampling with a proposal P and a target uh sorry target Q. Um the only difference is that um you are sampling you you injection sampling you if you reject then you reject and you just try again and you try again. Um, and here we don't want to keep on kind of looping forever. Uh because if you reject, we're just going to say, "Okay, fine. We'll bite the bullet and just sample from the the more expensive model." So the cool thing here is that you're guaranteed to get an exact sample from the target model. Okay, so those of you familiar with sampling, this is it shouldn't be too surprising. You're able to use kind of prior information to speed up sampling. Um but in a language modeling context this is kind of nice. Um I'm going to skip the uh this is not really a proof. This is just kind of some derivation to show that for a case of vocab 2 it why this these formulas kind of give you the right unbiased sampling procedure. Um and it works uh pretty well. So the accuracy uh is you know it should be actually the the same since it's the same model. But maybe there's some randomness there. But the speed up is you're getting a factor of two speed up essentially. Um so and so in practice what you do is you take you have something like a 70B model and your draft model is much much smaller. Um and if your target model is 80B 8B then your draft model might be 1B. And you generally want to make the draft model as closest to your target as possible. Um and so if you're doing some distillation that could um make it even better. There's a bunch of this is a pretty uh hot area of of research and inference. There's a lot of ways to improve this process. Um you can use um Medusa which is this uh way to have the draft model instead of generating auto reggressively sample multiple tokens in parallel or eagle where you're actually taking highle features of the target model and pumping them into the draft model to generate. So the draft model doesn't actually have to stand alone. It can be kind of glom on to the target model to help it generate. Um so uh summary exact sampling from the target model thanks to math. Um and this exploits the symmetry between checking and generation right so or um prefill and generation and there's actually a lot of room for innovation on a draft model um which can you know everything that we talked about before where you can have different radical architectures different ways of quantizing all of those apply right the only thing is that um you get to basically guarantee that you're getting exact act uh sample. Okay. So now I'll go uh I'm out of time but quickly go through the um the question that came up earlier which is that in practice when you're serving there's live traffic requests come at different times. Um they finish at different times they have some of them have shared prefixes some of them don't. They have different lengths. So it's very heterogeneous um in comparison to training where you get basically a dense block of tokens and you're basically going to p push it through your GPU at at full speed. So what do you do in this case? So there's a series of papers that kind of ex explore this um and the the basic idea is is this is so the last two parties are more of a kind of systems level um contribution. So the idea is that you don't wait for batches to, you know, the the train leaves. There's no the train doesn't um wait for you. Um so when a new batch comes, you're just going to put it in, right? Which means that um the the sort of this uh the the the um the worker that's generating tokens needs to kind of hand control back to theuler every step. So you generate a token, come back to the scheduleuler and say if there's new um uh new u requests then they get stuck in and then it kind of continues. So you're kind of not wasting any time waiting around for requests. Um now there's a kind of a problem with batching I think which is behind the question. Um batching works when everything's the same dimensionality but every request might be a different length. So there's this idea of selective batching where um you basically break up your computation for attention. Everything has to be handled separately. But for your MPs remember which are the bulk of the computation you can actually take tensors of different sizes and you just flatten them. And because they don't interact um they can just like be kind of kind of along for the ride in the batch dimension. Okay, I know that was fast, but uh I'll just quickly go over page detention. Now, um this is the the paper behind VLM, which some of you probably have used. Um and this addresses the memory usage problem. So, if you have a KV cache and prompts are coming in and finishing, then your cache is going to get fragmented. So, you're going to have um you're going to allocate a bunch of uh space for a request, but you don't know how many tokens are getting generated. So um there's going to be internal fragmentation and then there's also going to be external fragmentation where there's padding between the the requests and responses. So that's no good. So the page attention basically says remember operating systems we have um and how kind of virtual memory works we divide the KV cache into a sequence of contiguous blocks and then we just put them wherever um we find white space. So if you have two requests coming in then um they might just you know the first request might be here here and here and the second request might be here and here. So the blocks are the things that you're going to keep contiguous and that's going to give you your allow you time to coales your memory. Um so you can also play these tricks where if you have uh sharing of prefixes then there's another idea from uh operating systems which is copy on write. So you basically maintain reference counters for how many basically uh sequences are using this particular block and then if you need to kind of diverge and have blocks go in different directions then you copy and you reduce the reference count. Um there's a bunch of other VLM optimizations which I won't go through but basically the summary is remember your your operating systems classes you can apply them to inference as well. Okay, so quick summary, inference is really really important. It's where um and the characteristics are distinct from training. You're memory limited and it's also dynamic. So which leads a bunch of uh uh new challenges. We saw a whole host of different techniques around new architectures, quantization, pruning, distillation, um speculative decoding. uh there's ideas from systems which can allow you to better use your memory um overlap communication and compute and things like that but but I would say that there's probably even more um opportunity in sort of the modeling and architecture because if you think about it all you you don't inference narrowly is inference in a particular model how do I run this particular model but who cares running about that particular model you care about delivering good accuracy given your resource budget. So a lot of these ideas that are trying to change the reduce the KV cache, changing the transformer are basically ways to sidestep the problem and say, well, I have something that's more efficient and if I can train in a way that gets me better accuracy, then then I win. Okay, so that's all I have and I will see you next time. Then we're back to scaling lots.