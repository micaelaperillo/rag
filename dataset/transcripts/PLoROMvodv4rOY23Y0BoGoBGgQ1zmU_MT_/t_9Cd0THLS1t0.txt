So this is the second lecture on data. In the last lecture we talked about different data sets that were used to train various language models. We sort of did a historical overview from the data sets that were used to train bird all the way up to homo and everything in between. And one of the things I wanted to emphasize is that data doesn't just fall from the sky. It exists often in live services. They have to be explicitly crawled or dumped and then there's a bunch of processing that has to happen and the processing involves converting the raw HTML to text doing all sorts of quality and language and toxicity filtering, dduplication and so on so forth. So in this lecture I would like to take a deep dive into some of the mechanics on how quality filtering and dduplication work in particular. So first we're going to look at different algorithms for filtering which uh mostly are model based. So you train a classifier or train some sort of model to filter. Then we're going to show that this primitive can be used for all sorts of different types of filtering. And then finally we're going to look at some algorithms for uh dduplication. Um so this lecture will be a little bit different in that it's going to focus a lot on sort of classical big data processing algorithms and there'll be some fun math uh in there for some of you. Okay. So let's get started with filtering algorithms. So the basic high level picture here is that you're given some target data and which you don't have very much of it. Suppose it's data that's high quality and you have a lot of raw data. So imagine this is common crawl and your goal is you want to find a subset of that raw data. It's called it t prime which is similar to t. Okay. So this is a pattern that you should kind of recognize in basically any sort of filtering uh pipeline. Um and what do you want for this filtering algorithm? You want it obviously to generalize from t, right? You don't want you already have t so there's no point in getting exactly the same data t. So there's has to be some generalization and also has to be extremely fast. So you have to run it on all of the web for example. So if this is running a huge model that's going to be um might be as expensive as training. So you don't definitely don't want that otherwise you might as well train. So we're going to look at three different ways that you can um implement this highle abstract primitive. Um so we'll start with training an engram model. So this is something that uh came up in the last lecture. So you can train an engram model with canern smoothing. Um you can use other forms of smoothing but caner has sort of been inherited from the you know you know engram kind of modeling. um um statistical language processing era um and there's a particular implementation that people tend to use in this uh space called CANLM which is open source originally developed for machine translation um which implements can conernnite smoothing. So you can go and read about um it's it's it's pretty interesting but um we're not going to talk too much about the details here. Um so this happens to be very popular in data filtering. Uh there's no particular reason has to be this one, but this is just what people use. Um and the nice thing about engram modeling is it's very simple. When I say fitting a model, you you should think of this as just counting the number of engrams and then normalizing. Okay, so just to go into a little bit of depth here. So it's your starting point is maximum likelihood estimation of language models. So you have a corpus of text and you want to estimate conditional probabilities like p of in given the last n minus one words the cat and what you do is you count the number of n grams um and then divide it by the number of you know the um n minus one grams of the conditioning context. Okay so this is you know very very easy um in principle. Now implementing this efficiently is uh takes a bit of work. Um but the main problem is that there's sparse counts. So many of the engrams are exactly show up zero times even though they're very reasonable especially when n grows. Um and this was the whole point why engram models couldn't really be scaled appropriately because you hit the curse of dimensionality. So to mitigate this um you know some people devised uh you know cannon smoothing to handle unseen grams and there's more details on this but roughly this says well maybe if I don't have enough data to support this count I can sort of use a lower order engram. So you know remove one word from the context and count the number of times in uh appears given cat um and then try to either interpolate or back off um to that. Um okay so that's all I'll say about how engram you know modeling works. Um I think even in this era of neural language model you know it's good to know that how engram models work. So um let's download um the engram. So this is a a model that's been trained on Wikipedia. I'm just going to download it. Um and then let's uh punch some sentences through. So this sentence is Sanford University was found in 1885. Um this is actually a excerpt taken from Wikipedia. So it should be you know reasonable. Um and if you compute um the the probability so you first compute the log prop and then you compute the perplexity by this uh normalization um then you get you know 87 okay so you know it's it's some number um and then here's another example um you know this is taken from the CS336 you know website um so let's see how well it does So this is a higher perplexity means that it's less likely. Okay, kind of makes sense because this stuff doesn't really probably show up in Wikipedia. It's not in, you know, it's not abnormally high because it's still fairly good well-formed English and many engrams there, um, I think show up in Wikipedia as well. Um, what about ESDF? This is just gibberish. This gets assigned higher perplexity. Um and then there's the the the so this should get assigned pretty high perplexity but um for some reason this particular engram assigns a a pretty low perplexity. So you know it's an engram model so it's not particularly clever or good. Um but you know you don't need to be the best model. You're just doing data filtering to create a model. So this can be very crude and and fast. So, CCNET, which is um a paper out of um you know, I guess it was Facebook back then. Um uh they which we talked about last time. Um they um basically did uh this and actually okay that's fine. Um so they basically looked at uh paragraphs of of text um uh and those were the items they sorted the paragraphs by increasing perplexity and just kept the top 1/3 um and this is what they used to create the first llama um data set. Okay so this is you know horistic but you know kind of you know if you had to write down something really quickly and this kind of makes sense. Okay. So, NASA language modeling is fast uh but um it's crude. All right. Um I feel like there was some content that was I meant to go through that's missing. Oh well, maybe it got deleted. Um all right. So that's you can fit an engram model um to your target data and then you can use that to score the raw data and choose the top uh scoring documents. Another thing you can do is fastex and this is in some ways more uh you know popular um this is also from you know uh Facebook they released a lot of open source uh you know tools um although klm I think was um yeah created before before that um so here they develop it for text classification um and they this paper I mean this is 2016 so a lot of people were developing all sorts of fancy neural models and they said show that well this actually almost linear classifier worked as well and was much faster. Um so that's why the software package actually became very uh popular. So um the motivation here um was that if you're doing let's say bag of words classification. So you have a a sentence of length L your vocab size is V and you want to classify into 64 classes. Then immediately you have to define this um V by K matrix um which if V and K are both large then this gets uh pretty big and you get into sparsity issues um and the way it would work is that you take your um your documents and then you just do linear classification. So that's that's fairly you know straightforward. So the problem is that the number of parameters is huge. So fastex says, well, let me just do a a linear um basically basically dimensionality reduction. So instead, I'm going to map my vocab space into a hidden dimension which is smaller than k um possibly smaller than k 16. And then I'm going to classify from h. So notice that there's when when you look at the prediction the forward pass there's no nonlinearity it's just a linear classifier you can also think about as a matrix factorization um if you want and the number of parameters is greatly you know reduced um so the implication I think is uh you know reasonably um you know optimized it's parallelized and they use asynchronous SGD um and um so here's um okay so The the question is this is bag of words. Um and you know we want to not just look at words but we want to look at engrams. So this uh paper also you know extends to engrams in a fairly simple way. So you take a sentence and you get some chop it up into new engrams. Um and here the problem is that the number of biograms can get um you know quite large and actually can be un unbounded. Right? So remember when we talked about tokenizers if you just chop up into words you're going to get you know you don't know how many words there are. So the simple thing is that you just do hashing. So you define some number of bins maybe 10 million but in this example eight and then you just um you just hash every biogram um into that. So you these the cat maps to two and and cat in maps to one and so on. Okay. So of course there might be some um collisions but you know you just live with that. Um I mean in some sense the the opt when you minimize the loss collisions are sort of accounted for. If there's like two um words that have nothing to do with each other then the weight is going to somehow represent the maybe the average of the two. And also note that often we use fast fast text classification with k equals2 classes. Is this document good or is it bad? And in that case uh this is just normal ba basically um binary classification linear classification. Okay. So of course it doesn't take too much imagination to imagine well if you can do linear classification you can use fancier models such as bird or even llama. The only thing is that it's going to be slower and therefore you run into this trade-off where if you use a giant model to do classification, maybe you should use those flops to actually just train the model. So that's the only uh concern because you know the web is very large and remember you were filtering the web down quite a bit. So you're going to spend a lot of um compute on let's say you're filtering like down to 1% of the web, right? So that means um you know you have your classifier that amount of compute you spend has to be a hundth of you know taking a forward pass through the data set. Okay. So the final method I'm going to talk about is this thing called data selection for um language models using importance resampling. Um and here are the basic ideas that you have your R it's a raw data set you have your uh you know uh target data and then I'm going to build an importance weight estimator um which is um the analog of um the language engram model or the fast text classifier and then I'm going to use that to um get a bunch of um documents. Okay. So first just a kind of a quick refresher of importance um resampling I guess um so you have um this shows up in many contexts like you know particle filtering um um and you know Monte Carlo methods um so you have a target distribution so it's a distribution not a data set where you want to ideally draw samples from this but you only have a proposal distribution Q where you can draw samples so for example if you have a vocab of 0 1 2 3 2 3 um let's say this is your target distribution and your proposal distribution is uh is this. Now what you do is uh you sample from Q because that's the only thing you can sample from. So let's say you get a bunch of samples. Okay. Um so notice that um Q has a lot of more mass on zero. So you get a lot of more zeros. Um and then what you're going to do is you compute weights over your samples by looking at this importance um you know ratio P over Q. So you're trying to make up for the fact I sample from Q so I want to divide that out or I really want P and um you normalize and get you you get a distribution um over your your samples. And now you resample um and you uh that sort of balances out the distribution. So now you see that there's more threes because P had more probability distribution on mass on three. Okay. So this is a fairly elementary um but uh it's a building block. Now then now we go to um how we do data selection. So we have a target data set not a distribution. And we have a data set which is small and we also have this um raw data set which we're going to call the proposal data set dq which is large. So um you could let's just fit a distribution p to dp fit a distribution q to dq and door do the importance resampling um that I we talked about just uh you know two minutes ago. Now the main problem is that um DP is really small. Remember target distributions is high quality data. You don't have that much of it which is the whole point. You're trying to get more of it. So you don't have that much of it and it's too small to estimate a good model. So again this the the idea is you just use hash engram. So this is something that we already saw with fast text as well. you uh you know take whatever text you have and then you essentially okay so you basically hash all the um we're doing unograms for now so you hash each unogram so and you um um you basically count the number estimate the probability of every um hashed engram okay so in this case um this the cat and the hat hashes to this. Notice that there's some hash collisions the and the hat hatch into the same value but you know whatever this is all crude um and then you just estimate the probability of each of these um the probabilities. Okay. So then to evaluate the probability of a new text um you um you hash um the new text and you multiply the probabilities together. Okay, so I got a little bit unlucky and the probability is gets zeroed out. You can do some smoothing if you want. So it turns out in Python hash is non deterministic. So every time I run this I'm going to get something something else. Um and the paper shows that this indeed uh helps a bit. The gains are not you know super massive but you know compared to doing fast text which is the heristic classification um the gains on the glue benchmark using bird style models um there is some um lift and the the comparison with fastex. So the main idea here is that modeling a whole distribution is in some sense a more principled approach right because you want to sample data from um your distribution and you have a different distribution Q and now you're trying to essentially compensate um and so this could be better for you know diversity because you're trying to match the distribution whereas fast text you're essentially trying to classify whether something is is in the distribution or not and it doesn't have any guarantees about matching the distribution. Um okay but it's it's also fast like fast text and um both can be improved by having better um models. So instead of using linear models based on engrams you can increase n you can um use uh neural approaches as well. Okay, so just to wrap up this uh part um so we look at engram models, linear classifiers and importance resampling. Um but the general framework I think is uh is something that maybe worth taking away which is that the setup given a target data set and a raw data set find a subset that's similar to the target and all these methods have the basic recipe which is estimate some sort of model based on the raw data and the target data set which gives you a scoring function and then keep examples in the raw data set based on that scoring function. usually high scores are kept. So just to walk through this in the KLM case um you know the score is simply the probability um of or I guess you know maybe the perplexity um if you want normalized by the number of tokens under the target. Okay so R is not used here and then you keep examples with score greater than threshold. Um and you can you can um you know randomize a little bit around the threshold if you if you want. So you can also use a discriminative classifier where the score is what's the probability that this example came from the target as opposed to the raw data set and then you keep examples where the score is of greater than some threshold. And then importance resampling says the score is the ratio of you know two generative models which could be engram models in this in this case. Um p of uh you know the target distribution over the raw distribution and to use a score um this is really the importance weights you resample with probability proportional to that. Okay, so at a high level all of them are doing essentially the same thing. You have you fit some sort of distribution that tells you what looks like target as opposed to raw and then you apply that to the raw data set. Okay. Uh maybe I'll pause here in case there's any questions. Yeah. So um on sort of like a high level philosophical thing we have an idea of what good data is but we've kind of not talked about what that's actually like what what like a good document looks like. So what this is doing is like making sure that the words fit next to each other when you have like another like that you know one comes after the other but it doesn't ensure that they it makes any sense on like a larger level or if you want that you just increase the N and the engram. Yeah. Yeah. So the question seems I think is that if you're using an engram model, it's only looking at local contexts and maybe that's not great for assessing quality because if you shuffle everything around, it still looks good to the engram model. So there's definitely um that danger that it's very easy to adversarily game, right? If you can construct you can definitely construct um examples that the engram model thinks is high. I mean I showed you one the the happens to be you know pretty high. Um but somehow on average um you know you're doing okay and in some sense maybe if a document looks like grammatical English and it looks like news or you know some scientific paper um all you want to make sure is that that model assigns high probability to that. If you get some, you know, bad examples, it's not the the end of the the world. Yeah. So, we should think about these as just like filtering out like true nonsense web pages that you show. Yeah, I would say this is uh filtering out true nonsense is I think a good way to to look at it. As I go through some examples, I think maybe it'll become a bit clearer that um this is these classifiers are not just used for quality and for some of the other use cases. I think it's much more maybe compelling why it should work. Okay. So, um so the same data filtering machinery can be used on different tasks. So, I'm going to talk about language identification, quality filtering, and toxicity filtering. So, language identification. Um, the idea is that you want to find text of a specific language. Maybe it's English, maybe it's Japanese. Um, and you could ask, well, you know, um, you could just train on all the languages. Why don't you do that? Um, the problem is that it's, um, can often be difficult to, um, you know, do the curation and processing of high quality data in a given language. And and there's also, you know, the the problem that if you train, let's say you only care about, you know, one language. Now, if you have data from all the other languages, you're going to be spending less compute and tokens on any given languages on the given language. Um, so this was a and that could hurt your performance in that language. For example, you know, Bloom, which was trained in about 20 uh 22, I believe, um was only trained on 30% English. Um and as a result, the English performance maybe was not as good as it could have been. It was strong on other languages. So there in a compute limited, you know, regime, you're definitely at risk of um compromising the quality of the of of the language you care about. Now that said, if you have enough capacity and you're training huge models, then you can definitely train multilingual models and there's could be even positive transfer between languages. So all the kind of frontier models are are trained on um uh heavily multilingual sometimes even hundreds of lang like 100 plus languages. Um so you know one simple way to do this is fast text is um a toolkit for training simple classifiers. It also comes with some off-the-shelf um models and one of the ones that typically gets used is they have a language identification um um model and this is just off-the-shelf classifier. It supports a lot of different languages and and it was trained on um basically multilingual sites including Wikipedia which has a lot of different languages as translation sites um and you know for some reason Southeast European news. Um so DOMA uses this um they basically run the fast text classifier and keep pages with English uh probability greater than 0.5. you know. Um so let's try it out. So there's this model um language ID that you can just and download. Um and then let's see how well it does. Um so the quick fox jumps over the uh lazy dog. Let's see where are the predictions. Um okay. I don't know why. Okay, there we go. So um okay so this says English this is the label English and the probability of English is 71. Okay so I mean you would have guessed it would be higher. It looks pretty English to me. Um if you duplicate the the sentence the probability isn't you know changing which is good. Um it you know saying the same thing doesn't make it more English. Um um there's some informal English. Um which this is kind of weird. This is I guess more English than the quick round fox. Um German um actually gets classified fairly uh oh sorry this is uh German now. The label is German. Okay. That is German. Um math gets classified pretty weakly as as English. Um code um does is um highest is like Russian. Um, this is hello is apparently, you know, Italian. This is French. Bonjour. Um, that's that's good. Um, and this is, I guess, a tricky one where I put in some Spanish and also some English. And I guess I favor the the Spanish in this case. Okay. So, this gives you a sense. I mean, it's always good to play with these classifiers, right? Because just because it's a classifier you can download and everyone uses doesn't mean it like works all the time. Um, so I think obviously for short sentences, um, it's less reliable because there's just less information about what's happening. Low resource languages is tough. Um, there's a worry that for things that don't really or dialects of English, um, this could, um, be viewed as not English. Um, I think if you want to distinguish between similar languages, that's also can be easily confused. and code switching. I don't even know what ground truth is, but it'll do hopefully one of the uh languages. Um so just talk through one case study open web math was this uh this paper um from two years ago. Um and the goal here is to curate a large corpus of mathematical text where here I'm saying that you know let's assume that math is a as a language. Um so first they use some rules to filter then they train canlam on this uh large data set of proofs called proof pile and kept it if the perplexity was below some threshold and they they trained this fastest classifier which we talked about to predict mathematical writing. Um this is a kind of hacky but um the they set the threshold of a classifier if it's um identified as math based on rules to be relatively low and if it's not identified as math according to rules it's um it's uh needs to be higher. Um and as a result they produced 15 billion tokens and they trained some models that do better that models that were trained on 20 times more data that weren't really specialized. So this is kind of one of um nice examples of the the utility of data filtering, right? You could train on all this data, but if you care about let's say a particular domain like math, you can just go out and get more of it and target that data set. And if you train on that, you can be much more data efficient. Okay, so let's talk about quality filtering. So quality filtering obviously is something that is um doesn't really I mean it's a catch-all for um a bunch of things and what is quality um so remember that some papers uh explicitly don't use modelbased quality filtering um but more recent papers have just kind of given in and say well um I mean it just works a lot better um um so GBD3 um we talked a little bit about this last time trained a quality classifier by taking positive samples from the highquality sources that they had or rather the non-common crawl sources they had and negatives are just common crawl. They train a linear classifier um and keep documents stochcastically based on this this score which is something that just turns numbers um basically sort of sharpens the you know the the threshold um and uh the llama first llama paper um they use pages referenced by Wikipedia so not Wikipedia as positives negatives are just sampled from common crawl and then they keep documents that are classified positive. I don't think there was actually sampling in that which is interesting. Um 51 was another paper that um their philosophy was kind of you know interesting. Um they really wanted high quality data that looked like textbooks and they wanted to train a very small model. Um so they trained on synthetic data but also filter data. So for the filter data what they did is they took the Python subset of the stack. Remember the stack is this pre-processed data set coming from GitHub of code and then they defined a prompt which effectively asked um GP4 to determine the educational value for a student whose goal is to learn basic coding concepts and then they prompted GP4 and classified 100k um documents from this Python subset and that became the positive examples. They train a random forest classifier on these examples where the embeddings fed into a random classifier were from some pre-trained cogen model and then they selected data from R that is classified by the um you know classifier. So they run the classifier then over all of our and then get um that data. So this is kind of interesting because there is a random force classifier and in this case where does t come from? Well t doesn't exist ap t comes from prompting gpd4 with some prompt. Okay, so this is something that you'll see more increasingly as the models get better is that instead of you know looking at sources like well maybe books one is great or maybe web Wikipedia is great. Um now if you have a good model you can just ask the model to um for the type of data you want. maybe I want more chemistry data or I want more mathematical data that you know does a lot of proofs or elementary math or something and you just let the language model um uh a fairly sophisticated language model give you t and now once you have t you turn the recipe that we've been talking about so far so the f1 results um again looks uh good so they had a baseline where they just took the python subset of a stack which was this R and the performance on human eval which is a coding data set was only 12% after 96k steps of training and then they trained the exact same model architecture on this new fancy data set and it was already at 17 after 36 um so they declared success. Okay, so these are some examples of of uh quality filtering. Um any questions about quality filtering? Yeah. LM and not using like Yeah. So the the the point is that you can why can you get away with um using GP4 here because you're using it only on um to create a thousand 100k examples um and to create and then using that to distill a classifier which is much faster and the 100k is much less than the size of R which is you know hundreds of tens or hundreds of millions Don't worry. Okay. So, let's talk a bit about toxicity filtering. Um, so I'm going to talk about how the DOMA paper um does toxicity filtering. So, there's this data set called Jigsaw toxic comments. Um and this came out of a project where they were trying to help people have better discussions online. So they buil wanted to build a classifier that could um identify when there was problematic content. So they took the data uh comments on the Wikipedia talk pages and had annotators annotate them with you know toxic severe toxic obscene threat insult ID hate. Um and so the Dolma folks trained two fastex classifiers. one um which is basically um uh you know correspond to hate and the other one is correspond to NSFW um and so here's some examples of the you know of the you know data set um so let's download the model um and try it out. So the first example um you know are you threatening me for dispute neutrality? This gets classified as um safe for work um because I mean it seems fine. Um and the second one is flagged as NSFW um you know and these sentences are both uh fine. Okay. So, just give you a a sense, you know, I don't want to show too many of these examples, but um you get the idea. You can play it on with your own time. Um okay, so that's all I'll say about uh filtering. So, now let's talk about dduplication. So, there's two types of duplicates. Exact uh duplicates. And the thing to realize is that the web actually just inherently has a lot of exact duplicates based on u mirroring. So if you just look at project Gutenberg um you know you'll see that you know the same exact site gets mirrored on a bunch of different URLs. So if you're just crawling as common crawl does um it has no way knowing that these are mirrors. It just get you the exact same content. Um and then there's neoduplicates which is basically the same text that differ by a few tokens. And we're going to show you algorithms that deal with um you know both. So neoduplicates um when does this happen? Well, you have terms of service and licenses like the MIT license. This thing shows up I don't know how many times on the web. It's just copy and pasted everywhere. Uh maybe there's even I don't know maybe people take it and actually change some things or they mess up copy and pasting and um remove a comma or something. Um that's all possible. Um and then there's uh cases if you look at these these data sets um you know near duplicates are it just gives you an idea of what new duplicates might look like. Um, sometimes you have this article where like someone's just seemed to have paraphrased best actor in a negative role for most impactful character, but everything else seems the same. Um, this is just like missing a a comma here. Not really sure how where that comma went off to, but um, you know, there could be like just annotator or processing errors. Um, and then this one's kind of interesting. I mean there's clearly this very uh kind of ad um like a text which um they just replace Canada with USA um and some other flight. So this probably is generated from some sort of template system where they slotted in entities. Um so you know these narrow duplicates are obviously different but obviously if you train on tons of um you know this once you have this document this one doesn't give you that much value. Let's just put it that way. Um so in extreme case so if you look at C4 uh remember C4 was filtered based on the set of rules that looked for sentences that looked like English. um you'll see this um sentence which is indeed English that shows up an astonishing 61,000 times and for whatever reason if you actually trace down where this comes from um not the MIT license oh I have to what okay do I should I accept these cookies okay fine um there's this like random product from Amazon that has graffiti mass drawings And um you know there's this text and apparently this is for whatever reason I you know I don't even know how there's 61,000 copies of this but that's that's what it is. And and so the idea is that this text isn't necessarily bad right? It's perfectly good English but you don't want to like take 61,000 epochs over this. I mean it's just you know kind of pointless. So dduplication is sort of complementaryary to quality filtering. Quality filtering says this piece of data I don't want to ever train on it. Dduplication says well this is might be fine but I only want a small number of these rather than 61,000 uh copies. So there's been work uh showing that dduplication makes language models better. Um the first most obvious thing is that you can train more efficiently, right? Right? Duplication reduces the number of tokens and if you haven't thrown away information then you um can save a lot of uh effort and then the second is that um you can also avoid memorization. So we haven't really talked about this but language models trained on data can memorize that data which is bad for um you know copyright or privacy reasons because it can regurgitate the training set and dduplication is one tool. It's not definitely not perfect, but it can help mitigate uh some of these um risks. Um okay, so here's how to think about dduplication in terms of design space. So first you have to think about what is the unit that you're dduplicating? What is an item? Is it a sentence? Is it a paragraph? Is it a document? The second question is how do you match? Do you what is considered a duplicate? Is it exact match? Is it a fraction of common sub items? Um and then the final thing is what action uh do you take? Do you remove all the instances or do you remove all but one? Okay. So, so the key challenge and this is a um algorithmic challenge is that fundamentally dduplication is about comparing items to other items. Right? Quality classification. You can take an item, you can classify it as high quality or not. Dduplication is fundamentally a parise thing. And remember, these data sets are large. So you can't if you're trying to do something quadratic, that's kind of a no-go. So you need some sort of linear time algorithm to scale um but still do something that's pair-wise. Um so the building block of all this is hash functions. So hash functions just as a review is a function that maps an item which could be a sentence could be a document to a hash value which is an integer or string and the value is much smaller than item and the thing that's relevant about hash functions is that you could potentially get hash collisions. So which means that you have two distinct items that map to the same hash value. So in general if you've I'm sure you've all I guess all of you have used dictionaries or hashts hash collisions are generally bad but later we'll see that they're actually can be good in some limited doses. U there's many types of hash functions um which tradeoff between efficiency and collision resistance which means uh don't collide. There's cryptographic hash functions which are used for security sensitive applications like in in Bitcoin. You really don't want collisions because otherwise that means someone might steal your um your money or it could compromise um things. And then there's things where um but these are slow um but then there's hash functions which are generally used for hash tables that are not collision resistant but you know fast. And so we're generally going to go with the latter category um because we want things to be fast and um this the we're not doing cryptography here. Okay. So we're going to use this thing called murmur hash which takes strings and returns you know values but you can use a bunch of other hashes as well. Um okay so let's start with exact dduplication. So here's a simple example. the item is just a string and I want exact match and I want to remove all but one. So here's a bunch of items. Um and what I'm going to first do is uh compute um a mapping from hash value to the list of items with that hash. Um so and then once I have that I keep one item from each uh group. So each group correspond to a hash value. Um and in the case of exact dup um exact match um assuming there's no hash collisions then um you basically do a exact ddup. So hello shows up twice here and then now it shows up once and of course hello uppercase exclamation point is just a completely distinct item because we're doing exact. So this the nice thing is that this is very you know simple. It's clear what you're doing. It's high precision, so you're never gonna throw away anything that you didn't um uh that you didn't need or you needed. Um but the con is that you it doesn't work for near duplicates. Um and the other thing is that's nice is that you know this is very simple to implement. We kind of wrote this in a map reduce way and you can scale it and paralyze it fairly easily. And this is something that uh C4 uses actually to prepare their data set. So their unit is of dduplication is three sentence spans. They use exact match and they remove all all but one. Um so one thing that is always kind of bothered me but I know no one else seems to care is uh that um when you you're looking at three sentence spans, right? So you have this document and then you have this three sentence span um and if it's marked as a duplicate you're just going to you know you know do surgery and take that those sentences out and so the resulting document might not even be coherent but then people say you know who cares and move on. Um okay so there's another um way to do uh dduplication that is less map reduceuced but more kind of like hasht I think the you know bloom filters are you know some of you might have seen but just uh just to go through it because I think it's kind of a cool method um so this is you know efficient and um it's approximate so you get it's more efficient but it's um you know obviously It's not always doing the exact thing but you know we're we're very have not we we are not you know being too picky here. Okay. So it's uh the nice thing is a very memory efficient. Um you can update it but you can't delete. Um and it's it's basically a representation of set where if you ask for set membership and it returns no then it's definitely a no. But if it's returns yes then you know most likely if you set your hyperparameters right it's yes but you know there's a small probability that you have of of no like you have a false positive um and then you can you know set your uh your parameters to drive down the false positive rate if you like. Okay so just to uh walk through what this looks like. So suppose you have five items, the cat and the hat. And um later we'll test out with is some items that don't show up in the the set. Um the first thing is you're going to define a hash function that maps to m bins. m is going to be eight here. Um so to build the bloom filter um we're going to do you know something fairly simple. you just um you make uh you know a bit array um with a number of bins and then go through every item hash it the hashes to two so I update the second item um cat hashes to seven update the seventh item in the hat okay so I just um you know populate this bit array and the bit array I don't keep track of the item so I have no idea if I have a false positive or not um Okay, so let's just sanity check that um every item that I put in is actually in the item. So to query it, you just uh compute whether that item um hashed is in the table. Um and indeed everything should pass. Um and now um let's look at the nonite items and see um how they fare. And you'll see that you know some of the non items you get a zero which is what you want. Um but some of the non items the bloom feather says oh actually it's in the in the set. So the number of mistakes here is is four. And if you compute the false positive rate which is the number of mistakes over the total number of times um the procedure produced um you know a a positive then the false positive rate is in this case you know 444 which is pretty bad. Um now of course this is a bin 8 and if the bin were to grow then this would get better. In fact it grows the error probability is one over the number of bins. Okay. So, but you look at this and say, well, that's not really good because if I want the error probability to be, you know, um 10 to the minus 10, then I need a lot of bins and that's not going to hurt on memory. So, there's a more clever solution here, which is to use more hash functions. Okay, so let's say I use two hash functions and I'm going to essentially I still have the same, you know, bit array, but for every item now I'm going to use hash it twice. So the under seed zero gets hashed to two. I'm going to pop it in. And then um the with seed one is going to hash to five. So I'm going to pop it in there. So every item gets hashed into K not necessarily distinct but hey K slots. So then I go through the cat and then I fill up this um you know table. Okay. So now I um when I do the querying I take an element and I going to return one if the table set was set to one for all K hash functions. Okay. because if I put it in, I have to set K bits. And so if I'm testing, I need to check the K locations and um check that they were all set. Um so indeed all of those pass. And now let's look at the um so the number of um you know mistakes now is three um and the false positive rate um decreased. So, you know, that's great. So, of course, this is a toy example, but you know, you can really drive down the false positive rate with um modest memory um you know, uh spend. Okay, maybe I'll uh pause there before I go to um analyzing this a bit more, you know, formally. Okay, so let's let's think about what happens more generally. Um, so let's say we have a thousand bins. Um, we have 10 cache functions and we're going to hash um 100 items. The question is, you know, what is the false positive rate? And the way to think about this is that I'm going to consider a test input that's not in the set um that would is going to hash into a particular location like I. Okay. And then I'm going to so um I'm going to now consider putting items the n items into the boom filter and see what is the probability that it hits I. If anything hits I then that um is is a is a bad sign. Okay. So let's warm up here. So let's say I'm just n is one. I'm just inserting one element and k is also one. So I'm only using one hash function. So now the question is what is a chance that there's a hash collision um between i and whatever element I'm putting in. And the answer to that is just one over the number of bins. Right? Assuming everything's kind of independent. um um that's uh going to be you know 0.001. Okay. So now um I'm still n is still one um but I'm get to use k hash functions. So then I essentially if um I'm going to uh then the the criteria is I have to miss not just one time you know but you know k times. Okay. So this is a probability I miss once. So one minus that is the probability I I hit and um and then that raised to k times is the probability I I um have hit you know um on k and then y minus that is the probability I um you know uh have to miss k times. So now the probability here uh goes down you know uh a little bit. Um okay so now um if I'm inserting n items so I'm asking the probability that the test bin um is is one after n. So now instead of missing k times I have to miss k times n times. So then basically it's this expression but just k * n. Um and finally um because the test item is something that um I am you know also hashing. So I get sort of k chances you know to miss. So the false positive rate actually this is what kind of helps the K really helps is that I can um you know drive down the false positive rate with with with K. So that becomes um you know f goes from 63 to 01. Um the you can actually look at the expression f here and compute the optimal value of k um given a fixed you know ratio um as this is a sort of asmtoic calculation and then you find that um this results in k is scaling as order m over n. So the number of hash functions you should use is on order of number of bins over um the number of items you're hashing. And if you do that then f uh um turns out to be um you know 0.5 and the false positive rate is 0.5 to the to the k. So you see that in this particular case I wasn't optimal um because the optimal value is actually k= 6 and the false positive rate is 0.08 as opposed to um 0.0 sorry 0.008 rather than 0.01. Okay. So you can read more about this. There's some lecture notes that allow you to trade off the compute um the memory and the false positive rate. Okay, so the bloom filter has some hyperparameters that allow you to control um whether the um you know what your desired false positive rate is, how much memory you have, how much compute you're willing to put in and so on. Yeah. went down. So you're saying that the optimal K, you know, went down. Um, and so why does the F go down? Um, so the, um, let's see. So if a K goes down, um I think it's not monotonic, right? So if you use a very um I think if you use a very large value of K, that's actually pretty bad because then you just fill up the you know the bloom filter. Um and if you use too small, that's also not very good either. So I think there's a sort of optimum and whether it goes down or up depends on which side you're on. Okay, so in DOMA um they set the false positive rate to um 10 um to the minus5 and they use a bloom filter to do their exact uh dduplication um and they did it at the paragraph uh level. Okay, so that was exact dduplication and the problem with exact dduplication is that it doesn't capture some of these near misses where morally it's a duplicate but um we are not able to detect it. So now how do you do approximate set membership? Um and to talk about approximate set membership you need a notion of a similarity measure. So one common one that's used is jakard similarity. So the jakard of two sets A and B is the ratio of their intersection over the union. So as an example if I have 1 2 3 4 and 1 2 3 5. Um if you compute the jakard um the the intersection has size three the union has size five and therefore the jakard is 6. Okay. So we're going to say that two documents are near duplicates if their jakard similarity exceeds some some threshold. And generally the threshold will be you know fairly high like n9 um if you want to only you know match if you're like missing a comma or something. Now notice that there's nothing semantic about this. you could be um you know missing a word like not and of course that changes the meaning completely or some content word but um this is just trying to get documents that look fairly superficially um similar. Now the the algorithmic challenge is how do you find near duplicates in linear time. Okay. So to work up to that we're going to try to rewrite the jakard similarity which again is a pair-wise. You can't compute jakard of one element. You can only compute it if you have two elements. And I'm going to try to write it in terms of a hash function which you can compute in terms of one element. Okay. Okay, so there there's a cache function called minash um which has the nice property that the probability of a hash collision is exactly jakard ab right so normally remember you think of hash collisions as you know to be avoided at all you know costs you you really don't like them but here um it's not that you want more hash collisions it's just that hash collisions are something that you want to control for you want just the right level of hash collisions governed by the similarity. So the prob again to just state it probability the similarity is equal to the probability of um collision. So the more similar two things are the more they should collide. So that's the the intuition. Um okay so so minash is defined as follows. You take a set and you hash um all these elements. Remember these return numbers and you just take the minimum element. So you know it might at first glance if you haven't seen this it's like kind of not obvious why uh just taking the min actually gives you this property of expectation equal to the chard. Um but the argument is actually fairly you know simple. So the way to think about it is that you have your five items and you have two sets and I'm going to look at um this sort of carous matrix representation. Um so A has 1 2 3 4 B has 1 2 3 5 um and now the random hash function um induces a permutation over the items. So permutation might be three, two, one, four, five for example. This hash function right there. And now let's look at which item is first in A. Um and which item is first in B. Okay. Um so that corresponds to the you know the item corresponding to the minimum hash and the item each item has the same probability as um you know being first because it's the hash function you're assuming has you know it's it's random. So if one, two or three are first then the the minash is actually going to be the same right because the minimum um if these are you know first um the minimum here is um equal to the minimum you know here right but if the minimum if the first uh the minimum hash is four or five then the first in a will not be the first in um B and the min hash will be different. Okay. And then now you can say see that the probability of the min hash being equal is exactly 1 2 or three which is identically exactly the intersection. Okay. So if you're in the intersection and that item is first then you're going to get a collision. And if you're not in the intersection, then that one being first, you'll not get a collision because you're going to get the min for one of them and you're going to get some other value for the other one. Okay. All right. So, um you can check this empirically if you don't believe me. Um so, generate a thousand. Um I guess we're running this on A and B. Um and checking what's the probability that the you get a collision and it turns out the estimated jakard is 6. It's not exactly 6. I think there's some rounding but you know it's it's pretty close. Okay. So now we've taken this chicard similarity which is paraphase function and reduced it to some probabistic um you know uh unary function um just on on item. Um, but this is actually, you know, we're far from done because now we can hash our items, but a collision doesn't tell us that these two are similar, right? Just like more likely that more similar things will be hashed together, but we want something a bit stronger here. So, this is where locality sensitive hashing comes. Um and so right now so far we just have a hash function that um takes two um you know hashes sets and the sets will collide if with probability jakard. Um but we really want A and B to collide if and only if ideally if uh the jakard of A and B is exceeding some threshold. So somehow you have to sharpen the probabilities, right? You want to say that if the jakard is over the threshold, then with almost like probability one, they're going to be hashed together. And if it's below this threshold, then with basically probability zero or very small. Um so you know, how do we do this? Um it's sort of taking kind of the same trick, which is use more hash functions. Um, and here the construction is you're going to break up your N hash functions into B bands of R hash functions. So, for example, if you have 12 hash functions, um, and three bands and each band would have four hash functions. So, you have H1 through H4, H5 through H8, H9 through H12. Okay? And then we're just going to declare that A and B are going to collide if for some band all of hash functions returning the same value. Okay, so if I have A and B, I'm going to hash uh using all 12 hash functions for both A and B using the minash. And now I'm going to say if this band, for example, if the hash values for H1, H2, H3, H4 all agree, then they're going to collide. or this band or this band. Okay, so there's a sort of or and or structure which is the key to you know making sharpening the probabilities around the threshold. Okay, so um now we just have to you know do some analysis to calculate what is the probability that a and b uh collide. Okay, so similar I'm going to use sim to refer to the jakard. similarity. Um so um okay so I'm going to say let's say I have jakard similarity of uh 0.8 eight. In that case, I'm going to have five bands. Each band has um 10 hash functions. So, a total of 50 hash functions. Um so, what is the probability of a fixed band you know matching? So, probability of fixed band matching is simply you know 0.8 to the R, right? Um because the probability of hash a single hash function is just a jacard which is 0.8. And now what is the probability of that sum band matches? Well, it's a probability that um you know the that one fixed this is the probability that a fixed band um doesn't match and this is the probability that you know all of them don't match and then so this is the probability that at least one matches. Okay. So now you see the probability of collision is you know 04 um you know three. Okay. So it's kind of reshaping this probab umility. Before if you have one hash function if B and R is one then um the probability would be of collision would be 08 and now it's uh you know.3. Okay. So you know that's but let's um I guess here's a picture you should have in your head. So on the x-axis is similarity and what we want is the mapping from similarity to probability of collision to look something like that. If the threshold is the desired threshold is you know 0.5 then I want everything down here to be you know near zero and everything up here to be near one. So let's see uh what this looks like. I'm going to set um um for various similarity values. I'm going to set B equals 10, R equals 10. And notice that what this happens is that it squashes down some of these low probabilities and then it sharpens some of these high probabilities. Okay. Um but you know, maybe I want to make this a little bit sharper. Um, so what I'm going to do is to increase R from 10 to 20. And what that does is that moves the curve to the right um to make it harder to match and also sharpens the threshold. So if I do that then you'll see that now these probabilities go up and now um these probabilities are much smaller. Right? So in before um even a 7 probability uh some 7 similarity had um you know a probability of 0.24 which is you know kind of high. I really don't want this. So I want to squash this. So in squashing that I can now get it down to you know 07 which is you know pretty good I guess. Um, but now I've also squashed uh some of these probabilities and I the goal isn't to shift everything to the right otherwise I might as well do exact do duplication. So now there's another trick. If I increase B which is the number of buckets or bins um then I can um essentially get these probabilities like 0.9 to be in the 90s uh now and these probabilities are still fairly low. Okay. So this picture shows that as you increase B I get to shift this curve to the left. So increasing R will sharpen the curve and move things to the right and then increasing B will um shift the curve to the left so that you can with appropriate setting of B and R you can get your like really sharp sigmoid. Okay. So just an example of what this looks like in practice. So there's this um paper that we saw last time that uh uses a near duplicate uh near dduplication um using minash LSH um and uh their values um b= 20 and r equals uh 450. So so this r is large which means that they want a really um sharp threshold. So um there's a formula which you can um compute which um says the threshold is um.99. Okay. So that means um for every 100 words you can only allow basically one word to be you know different. So this is fairly you know strict dduplication and uh the probability that a um you know fixed band matches is um you know this to the r which is point um 05 and then the probability of collision is um 0.64. So one thing that's kind of interesting is that um I'm computing the probability of collision at the threshold. So in some sense you know that should be somewhere in between 0 and one. And it turns out that it converges to 1 - 1 over e which is you know around you know 6ish. Okay. So around the threshold, you know, you could go either way, but as soon as you go above the threshold, you have very high probability of being in a collision and below the threshold, you have very low probability of being uh in a collision. Okay. So um so with that, I'm going to you know summarize. Um actually maybe I'll ask uh if there's any questions about dduplication. Um so dduplication makes you run faster, avoids memorization. You can do exact dduplication um with bloom filters or you can use minash LSH to do approximate dduplication. Yeah. Um so I have a question like are most of the duplicates functions are like based on exactly like duplicate but if I like rewrote the text like other since like synthetic data is like authorized I was wondering how do we dduplicate for that use case? Yeah. So given that synthetic data is on the rise, how did you duplicate using um something that's like uh you know paraphrase um sensitive? So there are some papers uh for example 70 duplicate didn't mention that look at essentially embedding. So um that uh can allow you to get a more semantic notion of um similarity or did you um and and this all fits into the same you know framework because you know in some sense LH LSH was devised to find approximate nearest neighbors and so all you have to do is have a embedding and that can you embed all the documents and you can find nearest neighbors in that space. Um obviously there's a higher cost if you're going to run an embedding to um to embed your documents and also you know we are sort of working in a regime where documents are you know the the near duplicates are like really near duplicates. Um I think you have to be very careful if you're doing some sort of more fuzzy duplication. you can throw out a lot of your data um if you are too permissive. Uh another question. So we know that all the better. I wonder if there's any situation where you actually have to duplicate data since it's like so high quality or you want to be repeated. Yeah. So the question is uh well if the data is high quality maybe you do want duplicates and that's actually right. Um so in a lot of language modeling papers there's sort of the mid-training regime where you have a bunch of high quality sources. You actually want to take multiple epochs over high quality data. Um so the dduplication is more I would say for kind of the pre-training where you have all this stuff and you have 60,000 copies of some random thing you just want to like clean that up. But um I think the optimal thing to do is well clearly not I don't know what the optimal thing to do is but um it's probably if you have documents that occur a lot of times you could also signal that there's more importance to it and maybe the right thing to do is like taking the number of the count and I don't know taking the square root or log or something so that it shows up um doesn't proportionally show up in your training data. But it's you acknowledge that it's um important enough to go take more epochs over it. Okay. So let's summarize. Um so this lecture was mostly about giving you algorithmic tools. So we look at engram models uh linear classifiers and importance resampling as ways to do filtering. And with filtering um you can do language identification, quality filtering, toxicity filtering, anything I guess where you have a targets data set and you want to get more of that data set, then you can apply these tools. Or if you don't have a target data set and you have a strong language models, you can prompt that language model to synthesize a or synthesize or um filter down a you know a lower quality data set to get to t and then you get a fast classifier that uh you're and you're off to the races. And the second we looked at dduplication which um is important um for reducing the amount of compute you you're spending on you know kind of you're wasting essentially um and the algorithmic tool here is is hashing u because hashing is what allows you to take these kind of pair-wise notions of similarity and collision and turn them into unary functions which allows you to have this linear time, you know, property. And, you know, we saw some sort of clever methods where, um, basically using multiple hash functions um, and using these kind of andor constructions allows you to kind of shape your your criteria in in ways like the LSH example. So now you have all the tools um, you know, now in some sense if you ask how how you teach data, this is kind of only the beginning. um the you really have to spend time with the data looking at it, filtering and training models and that's how you build um your intuitions over what works and what doesn't. So hopefully you'll be able to do that uh a bit in assignment uh 4. Okay, that's all for the data lecture and next week uh we're going to start doing um reinforcement learning and you know alignment which is will be our last unit.