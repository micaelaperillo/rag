let's imagine three servers with logs that looked like this where the numbers I'm writing are the term numbers of the command that's in that log entry so we don't really care what the actual commands are then I got a number the log slots and so let's imagine that the presumably the the next term is term six although you can't actually tell that from looking the evidence on the board but it must be at least six or greater let's imagine that server s3 is chosen as the leader for term six and at some point s3 the new leader is going to want to send out a new log entry so let's suppose it wants to send out its first log entry per term six so we're sort of thinking about the append entries our PCs that the leader is going to send out to carry the first log entry for term six really should be under slot thirteen the rules in Figure two say that an append entries are bc actually as - as well as the command that the client sent in to the leader that we want to replicate on the logs of all the followers there's this append entries RPC also contains this previous log index field and a previous log term field and when we're sending out an end pend entries for where this is the first entry we're leaders supposed to put information about the previous slot the slot before the new information sending out so in this case the log index of the previous entry is 12 and the term of the command in the leaders log for the previous entry is by so sends out this information to the followers and the followers before they accept a upend entries are supposed to check you know they know they've received an append entries that for some log entries that start here and the first thing they do is check that there are previous the receiving followers check that their previous log entry matches the previous information that follow that the leader sent out so for a server to of course it doesn't match the server to has a entry here all right but it's an entry from term for not from turn five and so the server twos going to reject this append entries and sort of send a false reply back leader and server one doesn't even have anything here so server ones gonna also reject the append entries in the leader and so so far so good right the terrible thing that that has been averted at this point is you know the bad thing we absolutely don't want to see is that server to actually stuck the new blog entry in here which would break sort of inductive proofs essentially that the figure to scheme relies on and hide the fact that server two actually had a different log so instead of accepting log entry server two projects this RPC the leader sees is two rejections and leader is maintaining this next index field one for each follower so it has a next index for server two and the leader has a next index for server one presumably if the should have said this before if the server sending out information about slot thirteen here that must mean that the server's next index is for both of these other servers this started out as thirteen and that would be the case at the server if this leader had just restarted because the figure two rules say that next index starts out at the end of the new leaders log and so in response to errors the leaders supposed to decrement its next in steal so it does that for both got errors from boat deca mr. Calvin resends and this time the server is going to send out append entries with previous log index equals 11 and previous log term equals 3 and this new append entries has it has a different previous log index but it's the content in the log entries that the server is going to send out this time include you know all the entries after that the new previous log index is sending out so server 2 now the previous log index 11 it looks there and it sees a ha you know the term is 3 same as what the reader is sending me so server 2 is actually going to accept this append entries and figure 2 rules say oh if you accept a pendent we supposed to delete everything in your log after where the append entry starts and replace it with whatever's in the append entries so server tune is going to do that now it's he just went to 5 6 server 1 still has a problem cuz it has nothing at slot 11 middle would return another error the server will now backup its server 1 next index 2 11 it'll send out its log starting here with the previous index and term referring now to this slot and this one's actually acceptable server 1 it'll adopt it'll accept the new log entries and send a positive response back to the server and now they're all now they're all caught up and the presumably the server also when it sees that followers accepted and append entries that had a certain number of log entries it actually increments this next index could be 14 4 alright so the net effect of all this backing up is that the server has used a backup mechanism to detect the point at which the followers logs started to be equal to this servers and then sent each of the followers starting from that point that a complete remainder of the server's log after that last point at which they were equal any questions all right just to repeat discussion we've had before and we'll probably have again you notice that we erased some blog entries here which are now su erase that I forget what they were 4 & 5 so there were some well actually that was mostly remember we erased this log entry here this used to say for um server - the question is why was it ok for the system to forget about this client command right this thing we erased corresponds to some client command which are now throwing away I talked about this yesterday what's the rationale here yeah so it's not a majority of the servers and therefore whatever previous leader it was who sent this out couldn't have gotten acknowledgments from a majority of servers therefore that previous leader couldn't have decided it was committed couldn't have executed it and applied it to the application state could never have sent a positive reply back to the client so because this isn't done a majority of servers we know that the client who send in and has no reason to believe it was executed couldn't have gotten a reply because one of the rules is the server only sends over the leader only sends a reply to a client after it commits and executes so the client had no reason to believe it was even received by any server and then and the rules of figure to basically say the client if he gets no response after a while it supposed to resend the request so we know whatever request this was it threw away we've never executed never included in any state already and the clients gonna resend it by-and-by yes well it's always deleting suffix of the followers log I mean in the end the sort of backup answer to this is that the leader has a complete log so all its fails it can just send us complete log to the follower and indeed if you know if you've just started up the system and something very strange happened even at the very beginning then you may end up actually you know maybe in some of the tests for lab two you may end up backing up to the very first entry and then having the leader essentially send the whole log but because the leader has this whole law we know it could sort of it's got all the information that's required to feel everybody's logs if it needs to okay all right so in this example which I guess are now erased we elected s3 as the leader and the question is could we you know who can we who are we allowed to elect this leader right cool you know that all right if you read the paper you know the answer is not just anyone it turns out it matters a lot for the correctness the correctness of the system that we don't allow just anyone to be the leader like for example the first node whose timer goes off may in fact not be an acceptable leader and so it turns out raft has some rules that applies about oh yes you can be leader or you can't be leader and to see why this is true let's sort of set up a straw man proposal that maybe raft should accept should use the server with the longest log as the leader right you know some alternate universe that could be true and it is actually true in systems with different designs just not in raft so the question we're investigating is why not use the cervical longest law as leader and this would involve changing the voting rules in raft have a voters only vote for nodes that have longer logs all right so the example that's going to be convenient for showing why this is a bad idea so let's imagine we have three servers again and now the log set setups are server Wan has entries for terms five six and seven server two four five and eight and server three also four five and eight that's the first question of course to avoid spending our time scratching our heads about utter nonsense is to make sure that convince ourselves that this configuration could actually arise because if it couldn't possibly arise then may be a waste of time to figure out what would happen if it did arise so anybody wanna propose a sequence of events whereby this set of logs could have arisen how about an argument that it couldn't have arisen oh yeah okay so well maybe we'll back up sometime all right so server one wins is wins the election at this point and it's in term six sends out yeah it receives a client request sends out the first append entries and then that's fine actually everything's fine so far nothing's wrong all right well a good bet for all these things is then it crashes right or it receives the client requests in term six it appends the client requests to its own log which it does first and it's about to send out a pen entries but it crashes yes it didn't send out any pen entries and then you know we need then it crashes and restarts very quickly there's a new election and gosh server one is elected again as the as the new leader it receives in term seven and receives a client request appends it to its log and then it crashes right and then after after a crashes we have a new election maybe server 2 gets elected this time maybe server 1 is down now so off the table if server 2 is elected at this point suppose server 1 is still dead what term is server what server two venues yeah eights the right answer so why eight and not remember this you know this is now gone why eight and not six that's absolutely right so not written on the board but in order for server one to have been elected here it must have votes from majority of nodes which include at least one of server-to-server three if you look at the vote request code and figure two if you vote for somebody you're you're supposed to record the term in persistent storage and that means that either server 2 or server 3 are both new about term six and in fact term seven and therefore when sever one dies and they cannot elect a new leader at least one of them knows that the current term was eight if that one and only that one actually if there's only one of them only that one could win an election because it has the higher terminal birth they both know about term eight sorry if they both know about term seven then they'll both and either one of them will try to be leader and term eight so that fact of that the next term must be term a dis is insured by the property of the majorities must overlap and the fact that current term is updated by vote request and is persistent and guarantee did not be lost even if there were some crashes here so the next term is going to be eight server two or server three will win the leadership election and let's just imagine that whichever one it is sends out append entries for a new client requests the other one gets it and so now we have this configuration right so I was a bit of a detour we're back to our original question of in this configuration suppose server one revives we have an election would it be okay to use server one would it be okay to have the rule be the longest log wins the longest log gets to be the leader yeah obviously not right because server was a leader did it's going to force its log on to the to followers by the append entries machinery that we just talked about a few minutes ago if we live server one to be the leader it's gonna you know sent out a pen entries whatever backup overwrite these aids tell the followers to erase their log entries for term a to accept to overwrite them with this six and seven log entries and then to proceed now with identical to server ones so of course why are we upset about this yeah yeah exactly it was already committed right it's not a majority of servers has already committed probably executed quite possibly a reply sent to a client so we're not entitled to delete it and therefore server one cannot be allowed to become leader and force its log onto servers two and three everybody see why that's bad idea for rapid and because of that this can't possibly have been rule for elections of course shortest log didn't work too well either and so in fact if you read forward to section something five point four point one draft actually has a slightly more sophisticated election restriction that the request vote handling RPC handling code is supposed to check before it says yes before votes yes for a different peer and the rule is we only vote you vote yes for some candidate who send us over request votes only if candidate has a higher term in the last log entry or same last term same charming the last log entry and a log length that's greater than or equal to the the server that received that received the boat request and so if we apply this here if server two gets a vote request from server one there our last log entry terms or seven the server one's gonna send out a request votes with a last entry term whatever of 7 server twos is eight so this isn't true server service we didn't get a request from somebody with a higher term in the last entry and or the last entry terms aren't the same either said the second Clause doesn't apply either so neither server to new serve nor server three is going to vote for server one and so even if it sends out this vote requests first because this has a shorter election timeout nobody's going to vote for it except itself so I don't think it's one vote it's not a majority if either server two or server three becomes a candidate then either of them will accept the other because they have the same last term number and their logs are each greater than or equal to in length and the others so either of them will vote for for the other one will server one vote for either of them yes because either server 2 or server 3 has a higher term number in the last entry so you know what this is doing is making sure that you can only become a candidate if or it prefers candidates that knew about higher that have log entries some higher terms that is it prefers candidates that are more likely to have been receiving log entries from the previous leader and you know this second part says well we were all listening to the previous leader then we're going to for the server that has saw more requests from the very last leader any questions about the election restriction okay final thing about sending out log entries is that this rollback scheme at least as I described it and it's as its described in Figure two rolls back one log entry at a time and you know probably a lot of fun that's okay but there are situations maybe in the real world and definitely in the lab tests where backing up one entry at a time is going to take a long long time and so the real-world situation where that might be true is if they if a follower has been down for a long time and missed a lot of upend entries and the leader restarts and if you follow the pseudocode in Figure two if a leader restarts is supposed to set its next index to the end of the leaders log so if the follower has been down and you know miss the last thousand log entries and leader reboots the leader is gonna have to walk back off one at a time one RPC at a time all thousand of those log entries that the follower missed and there's no you know particular reason why this would never happen in real life it could easily happen at somewhat more contrived situation that the tests are definitely explorers is if a follower is if we say we have five servers and there's there's a leader but the leaders got trapped with one follower in a network partition but the leader doesn't know it's not leader anymore and it's still sending out append entries to its one follower and none of which are committed while in the other majority partition the system is continuing as usual the ex leader and follower in that Minority partition could end up putting in their logs you know sort of unlimited numbers of log entries for a stale term that will never be committed and need to be deleted and overwritten eventually when they rejoin the main group that's maybe a little less likely in the real world but you'll see it happen and the test set up so in order to be able to back up faster that paper has somewhat a vague description of a faster scheme towards the end of section 5.3 it's a little bit hard to interpret so I'm gonna try to explain what their ideas about how to back up faster a little bit better and the general idea is to be able to to have the follower send enough information to the leader that the leader can jump back an entire terms worth of entries that have to be deleted per append entries so it leader may only have to send one in a pennant and append entries per term in which the leader and follower disagree instead of one per entry so there's three cases I think are important and the fact is that you can probably think of many different log backup acceleration strategies and here's one so I'm going to divide the kinds of situations you might see into three cases so this is fast backup case one I'm just going to talk about one follower and the leader and not worry about the other nodes the same we have two server one which is the follower and server 2 which is the leader so this is one case and here we need to backup over a term where that term is entirely missing from the leader another case so in this case we need to back up over some entries but their entries for a term that the leader actually knows about so apparently the this followers saw a couple of entry a couple of the very Flass few append entries sent out by a leader that was about to crash but the new leader didn't see them we still need to back up over them and a third case is where the followers entirely missing the following the leader agree but the followers is missing the end of the leaders log and I believe you can take care of all three of these with three pieces of extra information in the reply that a follower sends back to the leader in the case in the append entries so we're talking about the append entries reply if the follower rejects the append entries because the logs don't agree there's three pieces of information that will be useful and taking care of three street cases I'll call them X term which is the term of the conflicting entry I remember the leader sent this previous log term and if the follower rejects it because it has something here but the terms wrong so it'll put the followers term for the conflicting entry here or you know I'm negative one or something it doesn't have anything in the log there it'll also send back the index of the conflicting but the index are the first entry with that term and finally if there wasn't any log entry there at all the follower will send back on the length of its law like the followers log so for case one the way this helps if the it's a leader sees that the leader doesn't even have an entry with X term of term X term at all in its log so that's this case where the leader didn't have turn five and if the leader can simply back up to the beginning of the followers run of entries with X term that is the the leader can set its next index to this X index thing which is the first entry the followers run of items from term five alright so if the leader doesn't have X term at all it should back up to X back the follower up to X index the second case you can detect the fault the leader can detect if X term is valid and the leader actually has log entries of term X term that's the case here where the you know the disagreement is here but the leader actually has some entries that term in that case the leader should back up to the last entry it has that has the contesta followers term for the conflicting term in it that is the last entry that a leader has for term for in this case and if neither of these two cases hold that is the well actually if the follower indicates by maybe setting X term to minus one it actually didn't have anything whatsoever at the conflicting log and index because it's log is too short then the leader should back up its next index to the last entry that the follower had at all and start sending from there and I'm telling you this because it'll be useful for doing a lab and if you miss some of my description it's it's in electronics then any questions about this backing up business Jack I think that's true yeah yeah yeah maybe binary search I'm not ruling out other solutions I mean that you know after reading the papers non description of how to do it I like cook this up and there's probably other ways to do this probably better ways and faster ways of doing it like I'm I'm sure that if you're willing to send back more information or have a more sophisticated strategy like binary search you can do a better job yeah well you you almost certainly need to do something experience suggests that in order to pass the tests you'll need to do something to as well probably not me although I that's not quite true like one of the solutions I've written over the years actually does the stupid thing and still passes the tests but because the tests you know the one of the sort of unfortunate but inevitable things about the tests we give you is that they have a bit of a real time requirement that is the tests are not willing to wait forever for your solution to produce an answer so it is possible to have a solution that's you know technically correct but takes so long that the tester gives up and unfortunately you know we will the tester will fail you if your solution doesn't finish the test and whatever the time limit is and therefore you do actually have to pay some attention to performance in order you know your solution has to be both correct and have enough performance to finish before the tester gets bored and sometimes out on you which is like 10 minutes or I don't know what it is and unfortunately it's relatively this stuff's complex enough that it's not that hard to write a color correction that's not fast enough yes so the way you can tap the leader can tell the difference is that the follower we're supposed to send back the term number it sees in the conflicting entry you we have case one if the leader does not have that term in its log so here the follower will set X term to five to five because this is this is going to be the this is gonna be the conflicting entry the follower says this X term to five the leader observes oh I do not have term five in my log and therefore this case one and you know it should back up to the beginning like it doesn't follower hasn't leader has none of those and term five entry so it should just get rid of all of them in the follower by backing up to the beginning which is X index yeah yeah because the leaders gonna back up its next index to here and then send an append entries that starts here and the rules a figure to say ah the follower just has to replace its log so it is gonna get rid of the fives okay alright the next thing I want to talk about is persistence you'll notice in Figure two that the state in the upper left-hand corners sort of divided and summer marked persistent and some are marked volatile and what's going on here is that the the distinction between persistence and volatile you know only matters if a server reboots crashes and restarts because the persistent what the persistent means is that if you change one of those items it's marked persistent you're supposed to the server supposed to write it to disk or to some other non-volatile storage like as or battery-backed something or whatever that will ensure that if the server restarts that it will be able to find that information and sort of reload it into memory and that's to allow us to allow servers to be able to pick up where they left off if they crash and restart now you might think that it would it would be sufficient and simpler to say well if a server crashes then we just throw it away and or we need to be able to throw it away and replace it with a brand-new empty server and bring it up to speed right and of course you do actually it is vital to be able to do that right because if some server suffers a failure of some catastrophic failure like it's you know disk melts or something you absolutely need to be able to replace it and you cannot count on getting anything useful off its disk if something bad happened to its disk so we absolutely need to be able to replace completely replace servers that have no state whatsoever you might think that's sufficient to handle any difficulties but it's actually not it turns out that another common failure mode is power failure of you know the entire cluster where they all stop executing at the same time right and in that case we can't handle or we can't handle that failure by simply throwing away the servers and replacing them with new hardware that we buy from Dell we actually have to be able to get off the ground we need to be able to get a copy of the state back in order to keep executing if we want our service to be fault tolerant and therefore in order at least in order to handle the situation of simultaneous power failure we have to have a way for the server's to sort of save their state somewhere where it will be available when the power returns and that's one way of viewing what's going on with persistence it said that's the state that's required to get a server going again I'm after either a single power failure or power failure of the entire cluster alright so figure two this three items only three items are persistent so there's a log that's like all the log entries current term and voted for and by the way you know one of us server reboots it actually has to make an explicit check to make sure that these data are valid on its disk before it rejoins the raft cluster I have to have some way of saying oh yeah I actually do have some save persistent state as opposed to a bunch of zeros that that are not valid all right so the reason why log has to be persisted is that at least according to figure two this is the only record of the application state that is figure two doesn't really have a notion fears two does not say that we have to persist the application state so if we're running a database or you know a test and set service like for vmware ft the actual database or the actual value of the test and set flag isn't persistent according to figure two only the logins and so when the server restarts the only information available to reconstruct the application state is the sequence of commands in the log and so that has to be persisted that's what about current term why does current term have to be persistent yeah so they're both about ensuring that there's only one that each term has at most one leader so yeah so voted for the specific you know potential damaging case is that if a server receives a boat request and votes for server one and then it crashes and if it didn't persist this the identity of who had voted for and in my crash we start get another boat request for the same term from server two and say gosh I haven't voted for anybody because my voted for is blank now I'm gonna vote for server 2 and now our servers voted for server 1 and for server 2 in the same term and that might allow two servers since both server and server to voted for themselves they both may think they have a majority out of three and they're both going to become leader now we have two simultaneous servers for the same term so this that's why I voted for it has to be persistent current term is gonna be a little more subtle but we actually talked before about how you know again we don't want to have more than one server for a term and if we don't know what term number it is then we can't necessarily then it may be hard to ensure that there's only one server for a term and I think maybe in this example ya if s if server 1 was down and server 2 and server 3 we're gonna try to elect a new server they need evidence that the correct turn numbers 8 and not 6 right because if they if they forgot about current term and it was just server 2 and server 3 voting for each other and they only had their log to look at they might think the next term should be term 6 they did that they start producing stuff for term 6 but now there's gonna be a lot of confusion because we have two different term sixes and so that's the reason my current term has to be persistent to preserve evidence about term numbers that have already been used these have to be persisted pretty much every time you change them right so certainly the safe thing to do is every time you add an entry of log or change current term are said voted for you need you probably need to persist that and in a real raft server that would mean writing it to the disk so you'd have some set of files that recorded this stuff you can probably be a little bit you may be can cut some corners if you observed that you don't need to persist these things until you communicate with the outside world so there may be some opportunity for a little bit of batching by saying well we don't have to persist anything until we're about to reply to an RPC or about to send out an RPC I mean that may allow you to avoid a few persisting x' the reason that's important is that writing stuff to disk is can be very expensive it's a if it's a mechanical hard drive that we're talking about then writing anything you know if the way we're persisting is writing files on the disk writing anything on the disk cost you about 10 milliseconds because you either have to wait for the disk to spin for the point you want to write to spin under the head which disk only rotates about once every 10 milliseconds or worse that you may actually have to seek to move the arm the right track right so these per systems can be terribly terribly expensive and if for sort of any kind of straightforward design they're likely to be the limiting factor in performance because they mean that doing anything anything whatsoever on these graph servers takes ten milliseconds a pop and 10 milliseconds as far longer than it takes to say send an RPC or almost anything else you might do 10 milliseconds each means you can just never if you persist data to a mechanical drive you just can never build a raft service it can serve more than 100 requests per second because that's what you get it at 10 milliseconds per operation and you know this is this cost so this is really all about cost of synchronous just updates and it comes up in many systems like file systems the file systems that are running in your laptops are that the designers spend a huge amount of time sort of trying to navigate around the performance problems of synchronous disk up they think of as disk writes because in order for stuff to get safe on your disk in order to update the file system on your laptop's disk safely there turns out the file system has to like be careful about how it writes and needs to sometimes wait for the disk to finish writing so this is a like a cross-cutting issue in all kinds of systems certainly comes up in draft if you want it to build a system they could serve more than a hundred quests per second then there's a bunch of options one is you can use a solid-state drive or some kind of flash or something solid eight drives can do a write to the flash memory in maybe a tenth of a millisecond so that's a factor of a hundred for you or if you're even more sophisticated maybe you can build yourself battery backed DRAM and do the persistence into battery back DRAM and then if the server reboots hope that reboot was took shorter than the amount of time the battery lasts and that this stuff you persisted is still in the RAM and the reason I mean if you have money and sophistication the reason to favor that is you can write DRAM you know millions of times per second and so it's probably not going to be a performance bottleneck anyway so that this problem is why and it's sort of marking a persistent versus volatile and figure 2 is like has a lot of significance for performance as well as crash recovery and correctness any questions about persisting yeah yes alright so your question is basically you're writing code say go code for your raft implementation or you're trying to write a real rafterman implementation and you actually want to make sure that when you persist your an update to the law or the current term or whatever that it in fact will be there after a crash and reboot like what's the recipe for what you have to do to make sure it's there and your observation that if you call you know on a UNIX or Linux or whatever Mac if you call right you know the right system call is how you write to a disk file you simply call right as you pointed out it is not the case that after the write returns the data is safe on disk and will survive a reboot it almost certainly isn't almost certainly not on disk so the you know the particular piece of magic you need to do is on unix at any rate you need you need to call right so you cannot write some file you've opened that's going to contain the stuff that you want to write and then you got a call this F st. call which on most systems the guarantee is that F sync doesn't return until all the data you've previously written into this file is safely on the surface on the media in a place on a place where it will still be there if there's a crash so so this thing is some then this call is an expensive call and that's why it's a separate that's why Wright doesn't write the disk only F sync does is because it's so expensive you would never want to do it unless you really wanted to persist some stuff some data okay so you can use more expensive disk hardware the other trick people play a lot is to try to batch that is if you can if client requests are if you have a lot of client requests coming in maybe you should accept a lot of them and not reply to any of them for a little bit we call a lot of them accumulate and then persist you know a hundred log entries at a time from your hundred clients and you know only then send out the append entries good because you do actually have to persist this stuff to disk if you receive a client request you have to persist the new entry to disk before you send the append entries our PCs the followers because you're not allowed if the leader you know the leader it's essentially promising to commit that that request and can't forget about it and indeed the followers have to persist the new log entry to their disk before they reply to the append entries because they were apply to the append entries it's also a promise to preserve and eventually commit that log entry so they can't be allowed to forget about it if they crash other questions about persistence all right well final you know a little detail about persistence is that some of the stuff in figure two is not persistent and so it's worth scratching your head a little bit about why commit index lasts apply next index and match index why it's fair game for them to be simply thrown away if the server crashes and restarts like why wasn't you know commit index or last apply it like geez last applied is the record of how much we've executed right if we throw that away aren't we gonna execute log entries twice and is that correct how about that why is why is it safe to throw away last applied yes I am we're all about simplicity and safety here with raft so that's exactly correct the the reason why all that other stuff can be non-volatile as you mentioned I mean sorry volatile the reason why those other fields can be volatile and thrown away is that we can the leader can reconstruct sort of what's been committed by inspecting its own log and by the results of append entries that it sends out to the followers I mean initially the leader if it if everybody restarts because they experienced a power failure initially the leader does not know what's committed what's executed but when it sends out log and append entries it'll sort of gather back information and essentially from the followers about What's in how much of their logs match the leaders and therefore how much must have been committed before the crash another thing in the 4-2 world which is not the real world another thing about figure two is that figure two assumes that the application state is destroyed and thrown away if there's a crash in a restart so the figure two world assumes that while log is persistent that the application state is absolutely not persistent required not to be consistent in figure 2 because the in figure 2 the log is preserved persisted from the very beginning of the system and so what's going to happen if you sort of play out what the various rules in figure 2 after a leader restart is that the leader will eventually re execute every single log entry that is handed to the application you know starting with log entry one after a reboot it's the raft is gonna hand the application every log entry starting from one and so that will after a restart the application will completely reconstruct its state from scratch by a replay from the beginning of the time of the entire log after each restart and again that's like a sort of straightforward elegant plan but obviously potentially very slow which brings us to the next topic which is log compaction and and snapshots and this has a lot to do with lab 3b actually you'll see log compaction and snapshots in vlog 3b in lab 3b and so the problem that log compaction and snapshotting is solving a raft is that indeed for a long-running system that's been going for weeks or months or years if we just follow the figure 2 rules the log just keeps on growing may end up you know millions and millions of entries long and so requires a lot of memory to store if you store it on disk like if you have to persist it every time you persist the log it's using up a huge I may not space on disk and if a server ever be starts it has to reconstruct its state by replaying these millions and millions of log entries from the very beginning which could take like hours for a server to run through its entire log and we execute it if it crashes and restarts all of which is like similar what kind of wasted because before it crashed it had already had applications state and so in order to cope with this wrath has this idea of snapshots and the sort of idea behind snapshots is to be able to save or ask the application to save a copy of its state as of a particular log entry so we've been mostly kind of ignoring the application but the fact is that you know if we have a suppose we're building a key value store under BRAF you know the log is gonna contain a bunch of you know putting gets or read and write request so maybe a law contains you know a put that some client wants to set X to one and then another one where it says X to 2 and then you know y equals 7 or whatever and if there's no crashes as the raft is executing along there's going to be this if the layer above Rath there's going to be this application and the application if it's a key value store databases it's going to be meeting this table and as raft hands it one command after our next the applications going to update its table so you know after the first command it's going to set X to one and it's stable after the second command it's going to update its table you know one interesting fact is that for most applications the application state is likely to be much smaller than the corresponding log right at some level we know that the the you know the log and the state are the log in that and the state as of some point in the log are kind of interchangeable right they both sort of implied the same thing about the state of the application but the log may contain a lot of you know a lot of multiple assignments 2x they use up a lot of space in the log but are also to effectively compact it down to a single entry in the table and that's pretty typical of these replicated applications but the point is that instead of storing the log which may go to be huge we have the option of storing instead the table which might be a lot smaller and that's what the snapshots are doing so when raft feels that it's log has gotten to be too large you know more than a megabyte or ten megabytes or whatever some arbitrary limit raft will ask the application to take make a snapshot of it the application state as of a certain point in the log so if we add if raft asked the application for a snapshot reference it would pick a point in the log that the snapshot referred to and require the application to produce a snapshot as at that point this is extremely critical because the because what we're about to do is throw away everything before that point so if there's not a will to find point that corresponds to a snapshot then we can't safely throw away the log before that point so that means that Rath is gonna have you know ask for snaps on the snap so it's basically just the table it's just about a database server and we also need to annotate the snapshot with the entry number that are corresponds to you so it's basically you know if the entries are 1 2 3 this snapshot corresponds to just after log index 3 with the snapshot in hand if we persist it to disk rats persistent to disk raft never again will need this part of the logs and it can simply throw it away as long as it persists a snapshot as of a certain in debt log index plus the log after that index as long as that's persisted to disk we never going to need to log before that and so this is what RAF does the rocks ask the application for snapshot gets the snapshot saves it to disk with the log after that it just throws away this log here right and so it really operates or the sort of persistence story is all about pairs of a snapshot in the log after that after the point in the log associated with snapshot I don't see this yes no it's still it's it's you know there's these sort of phantom entries one two three and this you know suffix of the log is indeed viewed as still the it's maybe the right way to think of it is still there's just one log except these entries are sort of phantom entries that we that we can view as being kind of there in principle but since we're we never need to look at them because we have the snapshot the fact that they just happened not to be stored anywhere is neither here nor there but it's but yeah you should think of it as being stole the same log it's just not just threw away their early entries did this that's a maybe a little bit too glib of an answer because the fact is that figure two talks about the log in ways that makes it that if you just follow figure to you sometimes still need these earlier entries and so you'll have to reinterpret figure two a little bit in light of the fact that sometimes it says blah blah blah a log entry where the log entry doesn't exist okay okay and so what happens on a restart so the restart story is a little more complicated in it than it used to be with just a log what happens on a restart is that there needs to be away for raft to give the latest for graph to find the latest snapshot log pair on its disk and hand the snapshot to the application because we no longer are able to replay you know all the log entries so there must be some other way to initialize the application basically not only is the application have to be able to produce a snapshot of application state but but it has to be able to absorb a previously made snapshot and sort of reconstruct it stable in memory from a snapshot and so this now even though raft is kind of managing this whole snapshotting stuff the snapshot contents are really the property to the application and RAF doesn't even understand what's in here only the application does because it's all full of application specific information so after a restart the application has to be able to absorb the latest snapshot that raft found so for just this simple it would be simple unfortunately this snapshotting and in particular the idea that the leader might throw away part of its log introduces a major piece of complexity and that is that if there's some follower out there whose log ends before the point at which the leaders log starts then unless we invent something new we need monney install snapshot unless we invent something new that follower can never get up-to-date right because if the followers you know if there's some follower whose log only is the first two log entries we no longer have the log entry three that's required to send it to that follower in an append entries RPC to allow its log to catch up to the leaders now we could avoid this problem by having the leader never drop part of its log if there's any follower out there that hasn't caught up to the point at which the leader is thinking about doing a snapshot because the leader knows through next index well actually leader doesn't really know but the leader could know in principle how far each follower had gotten and the leader could say well I'm just never gonna drop the part of my log before the end of the follower with the shortest log and that would be okay they might actually just be a good idea period the reason why that's maybe not such a great idea is that of course if a follower shut down for a week you know it's not gonna be acknowledging log entries and that means that the leader can't reduce its memory use by snapshotting so the way the raft designs chosen to go is that the leader is allowed to throw away parts of its logs that would be needed by some follower and so we need some other scheme that append entries to deal with the gap between the end of some followers log in the beginning of the leaders log and so that solution is the install snapshot RPC and the deal is that when a leader we have some follower whose log is that you know just powered on its log as short the leaders gonna send it and append entries and you know it's gonna be forced the leaders gonna be forced to backup and at some point the leader you know failure or fail dependent recalls will cause the leader to realize it it's reached the beginning of the actual log its doors and at that point instead of sending in append entries the leader will send its current snapshot plus current law well send its current snapshot to the follower and then presumably immediately follow it with an append entries that has the leaders current law questions yeah I'm the sad truth this is like this is adds significant complexity here Jarrell I'm three partially because of the kind of cooperation that's required between raff this is sort of a little bit of a violation of modularity it requires a good deal cooperation like for example when an install snapshot comes in it's delivered to raft but raft really requires the application to absorb the snapshot so they have to talk to each other more than they otherwise might yes the question is that this is the way the snapshot is created dependent on the application it's absolutely it so the snapshot creation function is part of the application as part of like the key value server so raffle you know somehow call up to the application and say geez you know I really like a snapshot right now in the application because only the application understands what it's status and you know the inverse function by which an application reconstructs a state from a snapshot files also totally application dependent where there's intertwining because of course every snapshot has to be labeled with a point in a log that it corresponds to talking about rule six and figure thirteen okay so yeah the question here is that and you will be faced with this in lab three that because the RPC system isn't perfectly reliable and perfectly sequenced and RBC's can arrive out of order or not at all or you may send an RPC and get no response and think it was lost but actually was delivered and was the reply that was lost all these things happen including to send to whatever install snapshot our pcs and the leaders almost certainly sending out many our pcs concurrently you know both append entries and install snapshots that means that you can get things like install snapshot our pcs from deep in the past almost anything else right and therefore the the follower has to be careful you know has to think carefully about an install snapshot that arrives and the yeah I think the specific thing you're asking is that if follower receives that an install snapshot that appears to be completely redundant that is the install snapshot contains information that's older than the information the follower already has what should the follower do and rule six and figure thirteen says something but I think equally valid response to that is that the follower can ignore a snapshot that clearly is from the past I don't really understand that rule six okay I want to move on to sort of somewhat more conceptual topic for a bit so far we haven't really tried to nail down anything about what it meant to be correct what I meant for a replicated service already any other kind of service to be behaving correctly and the reason why and you know whatever for most of my life I managed to get by without worrying too much about precise definitions of correctness but the fact is that you know if you're trying to optimize something or you're trying to think through some weird corner case it's often handy to actually have a more or less formal way of deciding is that behavior correct or not correct and so you know for here what we're talking about is you know clients are sending in requests to the to our replicated service with our PC maybe they'll be sending who knows well maybe the service is crash it can be starting and you know loading snapshots or whatever the client sends in a request and gets a response like is that response correct how are we supposed to how are we supposed to tell whether response a would be correct or response B so we need a notion we need a pretty formal notion of distinguishing oh that's okay from now that would be a wrong answer and for this lab the our notion of correctness is linearize ability and I mentioned strong consistency and some of the papers I mentioned strong consistency and basically equivalent to linearize ability linearize ability is a sort of a formalization of more or less of the behavior you would expect if there was just one server and it didn't crash and it executed the command client requests one at a time and you know nothing funny ever happened so it has it has a definition and the definition I'll write out the definition then talk about it so so an execution history is linearizable linearizable and this is in the notes if there exists a total order so an execution history is a sequence of client requests maybe many requests from many clients if there's some total order of the operations in the history it matches the real-time order of requests so if one request if client sends out a request and gets a response and then later in time another client sends out a request and I get a response those two requests are ordered because one of them's started after the other one finished so it's linearizable history is linearizable if there exists an order of the operations in the history that matches real-time for non concurrent requests that is for a request to didn't overlap in time and each read you can think of it as each read sees the value from the most immediately preceding right to the the same piece of data most recent right in the order all right this is the definition let me illustrate what it means by running through an example so first of all the history is a record of client operations so this is a definition that you can apply from outside this definition doesn't appeal in any way to what happens inside the implementation or how the implementation works it's something that we can if we see a system operating and we can watch the messages that come in and out we can answer the question was that execution that we observe linearizable so let me let me write out of history and talk about why it is or isn't linearizable all right so here's an example the new eyes ability talks about operations that start at one point and end at another and so this corresponds to the time at which a client sends a request and then later receives a reply so let us suppose that our history says that at at some particular time this time some client sent a write request for the data item named X and asked for it to be set to 1 and then time passed and at the second vertical bar is when that client got a reply through send a request at this point you know time pass who knows what's happening when the client got a reply there and then later in time that client or some other client doesn't really matter sends a write request again for item X and value 2 and gets a response to that right meanwhile some client sends a read for X and gets value 2 and sent the request there and got the response with value 2 there and there's another request that we observed it's a part of the history request was sent to read value X and it got value 1 back and so when we have a history like this you know the question were that you asked about this history is is this a linearizable history that is did the machinery did the service did the system that produced this history and was that a linearizable system or did it produce a linearizable history in this case if this history is not linear inaudible then then Lisa we're talking about I have 3 we know we have a problem there must be some some bug ok so we need to analyze this to figure out if it's linearizable there's linear linearize ability requires us to produce an order you know one by one order of the four operations in that history so we know we're looking for an order and there's two constraints on the order one is if one operation finished before another started then the one that finished first has to come first in the history the other is if some read sees a particular written value then the read must come after the write in the order all right so we want to order so we're gonna produce an order that has four entries the two rights and the two leads I'm gonna draw with arrows that constraints implied by those two rules and then our order is gonna have to obey these constraints so one constraint is that this write finished before this write started and therefore one of the ordering constraints is that this write must appear in the total order before this write this read saw the value of two so in the total order the most recent right that this read must come after this right and this write must be the most recent right so that means that in the total order we must see the right of X - 2 and then after it the read of X it yields - and this this read of X of 1 if we assume that the X didn't already have the value 1 there there must be in this relationship and that is the read must come after the right and this read also must become for this right and maybe there's some other restrictions - anyway we can take these we can take this set of arrows and flatten it out into an order and that actually works so the order that's the total order that demonstrates that this history is linearizable is first the right of x - 1 then the read of x yielding 1 then the right of x - 2 and the read of x that yields 2 alright so the fact that there is this order that does obey the ordering constraints shows that this history is linearize ability and doesn't you know if we're worried about the system that produced this history whether it's a but that system is linearizable then this particular example we saw it doesn't contradict the presumption that the system is linearizable any questions about what I just did each read sees you know read of X the value it sees must be them value written by the most the most recent proceeding right in the order so you know in this case in this case we're totally ok with this order because this read the value it saw is indeed the value written by the most recent write in this order and this read the value it sighs I mean in informally it's that reads can't real should not be yielding stale data if I write something in Rita back gosh I should see the value I wrote and that's like a formalization of the notion that oh yes oh yeah yeah all right let me let me he's right up example that's not indeed linearizable so example two let's suppose our history is we had a right of X value one right back with value two and so this one we also want to write out the arrows and so we know what the constraints are on any total order we might find the right of X to one because of time because it finished in real time before the right x to started and must come before in any satisfying order we produce the right of Ecsta two has to come before the right before the read of X that yields two so we have this arrow the read of X had to finished before the read of X to one started so we have this arrow and the read of X to one because it saw value one has to come after the right of X - 1 and more crucially before the right of X 2 - right so we can't have this read of X yielding one if it's immediately preceded by I'll write out X - 2 so we also have this arrow like this and because there's a cycle in these constraints there's no order that can obey all these constraints and therefore this history is not linearizable and so the system that produced it is is not a linearizable system you know would be linearizable the history was missing any one of these three and I would break the cycle yes maybe I'm not sure because suppose or I don't know how to incorporate very strange things like supposing somebody red 27 you know it doesn't really if there's no right of 27 a read of 27 doesn't at least the way I've written out the rules doesn't sort of well there may be some sort of anti dependency that you would construct okay um I will continue this discussion next week