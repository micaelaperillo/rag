all right today today we're going to talk about spark spark say essentially a successor to MapReduce you can think of it as a kind of evolutionary step in MapReduce and one reason we're looking at it is that it's widely used today for data center computations that's turned out to be very popular and very useful one interesting thing it does which will pay attention to is that it it generalizes the kind of two stages of MapReduce the map introduced into a complete notion of multi-step data flow graphs that and this is both helpful for flexibility for the programmer it's more expressive and it also gives the system the SPARC system a lot more to chew on when it comes to optimization and dealing with faults dealing with failures and also for the from the programmers point of view it supports iterative applications application said you know loop over the data effectively much better than that produced us you can cobble together a lot of stuff with multiple MapReduce applications running one after another but it's all a lot more convenient in and SPARC okay so I think I'm just gonna start right off with an example application this is the code for PageRank and I'll just copy this code with a few a few changes from some sample source code in the in the spark source I guess it's actually a little bit hard to read let me just give me a second law try to make it bigger all right okay so if this is if this is too hard to read is there's a copy of it in the notes and it's an expansion of the code and section 3 to 2 in the paper a page rank which is a algorithm that Google uses pretty famous algorithm for calculating how important different web search results are what PageRank is trying to do well actually PageRank is sort of widely used as an example of something that doesn't actually work that well and MapReduce and the reason is that PageRank involves a bunch of sort of distinct steps and worse PageRank involves iteration there's a loop in it that's got to be run many times and MapReduce just has nothing to say about about iteration the input the PageRank for this version of PageRank is just a giant collection of lines one per link in the web and each line then has two URLs the URL of the page containing a link and the URL of the link that that page points to and you know if the intent is that you get this file from by crawling the web and looking at all the all collecting together all the links in the web's the input is absolutely enormous and as just a sort of silly little example for us from when I actually run this code I've given some example input here and this is the way the impro would really look it's just lines each line with two URLs and I'm using u1 that's the URL of a page and u3 for example as the URL of a link that that page points to just for convenience and so the web graph that this input file represents there's only three pages in it one two three I could just interpret the links there's a link from one two three there's a link from one back to itself there's a web link from two to three there's a web link from two back to itself and there's a web link from three to one just like a very simple graph structure what PageRank is trying to do it's you know estimating the importance of each page what that really means is that it's estimating the importance based on whether other important pages have links to a given page and what's really going on here is this kind of modeling the estimated probability that a user who clicks on links will end on each given page so it has this user model in which the user has a 85 percent chance of following a link from the users current page following a randomly selected link from the users current page to wherever that link leads and a 15% chance of simply switching to some other page even though there's not a link to it as you would if you you know entered a URL directly into the browser and the idea is that the he drank algorithm kind of runs this repeatedly it sort of simulates the user looking at a page and then following a link and kind of adds the from pages importance to the target pages importance and then sort of runs this again and it's going to end up in the system like page rank on SPARC it's going to kind of run this simulation for all pages in parallel it or literately the and the idea is that it's going to keep track the algorithms gonna keep track of the page rank of every single page or every single URL and update it as it sort of simulates random user clicks I mean that eventually that those ranks will converge on kind of the true final values now because it's iterative although you can code this up in rapid MapReduce it's a pain it can't be just a single MapReduce program it has to be multiple you know multiple calls to a MapReduce application where each call sort of simulates one step in the iteration so you can do in a MapReduce but it's a pain and it's also kind of slope because MapReduce it's only thinking about one map and one reduce and it's always reading its input from the GFS from disk and the GFS filesystem and always writing its output which would be this sort of updated per page ranks every stage also writes those updated per page ranks to files in GFS also so there's a lot of file i/o if you run this as sort of a sequence of MapReduce applications all right so we have here this sum there's an a PageRank code that came with um came a spark I'm actually gonna run it for you I'm gonna run the whole thing for you this code shown here on the input that I've shown just to see what the final output is and then I'll look through and we're going to step by step and show how it executes alright so here's the you should see a screen share now at a terminal window and I'm showing you the input file then I got a hand to this PageRank program and now here's how I read it I've you know I've downloaded a copy of SPARC to my laptop it turns out to be pretty easy and if it's a pre compiled version of it I can just run it just runs in the Java Virtual Machine I can run it very easily so it's actually doing downloading SPARC and running simple stuff turns out to be pretty straightforward so I'm gonna run the code that I show with the input that I show and we're gonna see a lot of sort of junk error messages go by but in the end support runs the program and prints the final result and we get these three ranks for the three pages I have and apparently page one has the highest rank and I'm not completely sure why but that's what the algorithm ends up doing so you know of course we're not really that interested in the algorithm itself so much as how we execute arc execute sit all right so I'm gonna hand to understand what the programming model is and spark because it's perhaps not quite what it looks like I'm gonna hand the program line by line to the SPARC interpreter so you can just fire up this spark shell thing and type code to it directly so I've sort of prepared a version of the MapReduce program that I can run a line at a time here so the first line is this line in which it reads the or asking SPARC to read this input file and it's you know the input file I showed with the three pages in it okay so one thing there notice here is is that when Sparky's a file what is actually doing is reading a file from a GFS like distributed file system and happens to be HDFS the Hadoop file system but this HDFS file system is very much like GFS so if you have a huge file as you would with got a file with all the URLs all the links and the web on it on HDFS is gonna split that file up among lots and lots you know bite by chunks it's gonna shard the file over lots and lots of servers and so what reading the file really means is that spark is gonna arrange to run a computation on each of many many machines each of which reads one chunk or one partition of the input file and in fact actually the system ends up or HDFS ends up splitting the file big files typically into many more partitions then there are worker machines and so every worker machine is going to end up being responsible for looking at multiple partitions of the input files this is all a lot like the way map works mapreduce okay so this is the first line in the program and you may wonder what the variable lines actually hold so in printed the result of lines but with the lines points - it turns out that even though it looks like we've typed a line of code that's asking the system to read a file in fact it hasn't read the file and won't read the file for a while what we're really building here with this code what this code is doing is not causing the input to be processed instead what this code does is builds a lineage graph it builds a recipe for the computation we want like a little kind of lineage graph that you see in Figure three in the paper so what this code is doing it's just building the lineage graph building the computation recipe and not doing the computation when the computations only gonna actually start to happen once we execute what the paper calls an action which is a function like collect for example to finally tell mark oh look I actually want the output now please go and actually execute the lineage graph and tell me what the result is so what lines holds is actually a piece of the lineage graph not a result now in order to understand what the computation will do when we finally run it we could actually ask SPARC at this point we can ask the interpreter to please go ahead and tell us what you know I actually execute the lineage graph up to this point and tell us what the results are so and you do that by calling an action I'm going to call collect which so just prints out all the results of executing the lineage graph so far and what we're expecting to see here is you know all we've asked it to do so far the lineage graph just says please read a file so we're expecting to see that the final output is just the contents of the file and indeed that's what we get and what what this lineage graph this one transformation lineage graph is results in is just the sequence of lines one at a time so it's really a set of lines a set of strings each of which contains one line of the input alright so that's the first line of the program the second line is is collect essentially just just-in-time compilation of the symbolic execution chain yeah yeah yeah yeah that's what's going on so what collect does is it actually huge amount of stuff happens if you call collect it tells SPARC to take the lineage graph and produce java bytecodes that describe all the various transformations you know which in this case it's not very much since we're just reading a file but so SPARC well when you call collect SPARC well figure out where the data is you want by looking HDFS it'll you know just pick a set of workers to run to process the different partitions of the input data it'll compile the lineage graph and we reach transformation in the lineage graph into java bytecodes it sends the byte codes out to the all the worker machines that spark chose and those worker machines execute the byte codes and the byte codes say oh you know please read tell each worker to read it's partition at the input and then finally collect goes out and fetches all the resulting data back from the workers and so again none of this happens until you actually wanted an action and we sort of prematurely run collect now you wouldn't ordinarily do that I just because I just want to see what the the output is to understand what the transformations are doing okay if you look at the code that I'm showing the second line is this map call so the leave so line sort of refers to the output of the first transformation which is the set of strings correspond to lines in the input we're gonna call map we've asked the system call map on that and what map does is it runs a function over each element of the input that is in this case or each line of the input and that little function is the S arrow whatever which basically describes a function that calls the split function on each line split just takes a string and returns a array of strings broken at the places where there are spaces and the final part of this line that refers to parts 0 & 1 says that for each line of input we want to at the output of this transformation be the first string on the line and then the second string of the line so we're just doing a little transformation to turn these strings into something that's a little bit easier to process and again at a curiosity I'm gonna call collect on links one just to verify that we understand what it does and you can see where as lines held just string lines links one now holds pairs of strings of from URL and to URL one for each link and when this executes this map executes it can execute totally independently on each worker on its own partition of the input because it's just considering each line independently there's no interaction involved between different lines or different partitions these are it's running if these this map is a purely local operation on each input record so can run totally in parallel on all the workers on all their partitions ok the next line in the program is this called the distinct and what's going on here is that we only want to count each link once so if a given page has multiple links to another page we want to only consider one of them for the purposes of PageRank and so this just looks for duplicates now if you think about what it actually takes to look for duplicates in a you know multi terabyte collection of data items it's no joke because the data items are in some random order and the input and what distinct needs to do since an e sirup replace each duplicated input with a single input distinct needs to somehow bring together all of the items that are identical and that's going to require communication remember that all these data is spread out over all the workers we want to make sure that any you know that we bring we sort of shuffle the data around so that any two items that are identical or on the same worker so that that worker can do this I'll wait a minute there's three of these I'm gonna replace it these three with a single one I mean that means that distinct when it finally comes to execute requires communication it's a shuffle and so the shuffle is going to be driven by either hashing the items the hashing the items to pick the worker that will process that item and then sending the item across the network or you know possibly you could be implemented with a sort or the system sort of sorts all the input and then splits up the sorted input overall the workers I'd actually don't know which it does but anyway I'm gonna require a lot of computation in this case however almost fact nothing whatsoever happens because there were no duplicates and sorry whoops links to all right so anyone collect and the links to which is the output a distinct is basically except for order identical two links one which was the input to that transformation and the orders change because of course it has to hash or sort or something all right the next the next transformation is is grouped by key and here what we're heading towards is we want to collect all of the links it turns out for the computation with little C we want to collect together all the links from a given page into one place so the group by key is gonna group by it's gonna move all the records all these from two URL pairs it's gonna group them by the from URL that is it's gonna bring together all the links that are from the same page and it's gonna actually collapse them down into the whole collection of links from each page is gonna collapse them down into a list of links into that pages URL plus a list of the links that start at that page and again this is gonna require communication although spark I suspect spark is clever enough to optimize this because the distinct already um put all records with the same from URL on the same worker the group by key could easily and may well just I'm not have to communicate at all because it can observe that the data is already grouped by the from URL key all right so let's print links three let's run collect actually drive the computation and see what the result is and indeed what we're looking at here is an array of couples where the first part of each tuple is the URL the from page and the second is the list of links that start at that front page and so you can see the YouTube has a link to you two and three you three as a link to just u 1 and u 1 has a link to u 1 & u 3 okay so that's link 3 now the iteration is going to start in a couple lines from here it's gonna use these things over and over again each iteration of the loop is going to use this this information in links 3 in order to sort of propagate probabilities in order to sort of simulate these user clicking I'm from from all pages to all other link to two pages so this length stuff is these links data is gonna be used over and over again and we're gonna want to save it it turns out that each time I've called collect so far spark has re-execute 'add the computation from scratch so every call to collect I've made has involved spark rereading the input file re running that first map rerunning the distinct and if I were to call collect again it would rerun this route by key but we don't want to have to do that over and over again on sort of multiple terabytes of links for each loop iteration because we've computed it once and it's gonna state this list of links is gonna stay the same we just want to save it and reuse it so in order to tell spark that look we want to use this over and over again the programmer is required to explicitly what the paper calls persist this data and in fact modern spark the function you call not persist if you want to sleep in a memory but but it's called cash and so links for is just identical the links we accept with the annotation that we'd like sparked keep links for in memory because we're gonna use it over and over again ok so that the last thing we need to do before the loop starts is we're gonna have a set of page ranks for every page indexed by source URL and we need to initialize every pages rank it's not really ranks here it's kind of probabilities we're gonna initialize all the probabilities to one so they all start out with a probability one with the same rank but we're gonna well we're gonna actually you code that looks like it's changing ranks but in fact when we execute the loop in the code I'm showing it really produces a new version of ranks for every loop iteration that's updated to reflect the fact that the code algorithm is kind of pushed page ranks from each from each P to the page is that it links to so let's print ranks also to see what's inside it's just a mapping from URL from source URL to the current page rank value for every page ok not gonna start executing inside the spark allow the user to request more fine-grained scheduling primitives than cache that is to control where that is stored or how the computations are performed well yeah so cache cache is a special case of a more general persist call which can tell spark look I want to you know save this data in memory or I want to save it in HDFS so that it's replicated and all survived crashes so you got a little flexibility there in general you know we didn't have to say anything about the partitioning in this code and spark will just choose something at first the partitioning is driven by the partitioning of the original input files but when we run transformations that had to shuffle had to change the partitioning like distinct it does that and group by key does that spark will do something internally that if we don't do any we don't say anything it'll just pick some scheme like hashing the keys over the available workers for example but you can tell it look you know I it turns out that this particular way of partitioning the data you know use a different hash function or maybe partitioned by ranges instead of hashing you can tell it if you like more clever ways to control the partitioning okay so I'm about to start the first thing the loop does and I hope you can see the the code on line 12 we actually gonna run this join this is the first statement of the first iteration of the loop with this joint is doing is joining the links with the ranks and what that does is pull together the corresponding entries in the links which said for every URL what is the point what does it have links to and I'm sort of putting together the links with the ranks and but the rank says is for every URL what's this current PageRank so now we have together and a single item for every page both what its current PageRank is and what links it points to because we're gonna push every pages current PageRank to all the pages it appoints to and again this joint is uh is what the paper calls a wide transformation because it doesn't it's not a local the I mean it needs to it may need to shuffle the data by the URL key in order to bring corresponding elements of links and ranks together now in fact I believe spark is clever enough to notice that links and ranks are already partitioned by key in the same way actually that assumes that it cleverly created links well when we created ranks its assumes that it cleverly created ranks using the same hash scheme as used when it created links but if it was that clever then it will notice that links and ranks are passed in the same way that is to say that the links ranks are already on the same workers or sorry the corresponding partitions with the same keys are already in the same workers and hopefully spark will notice that and not have to move any data around if something goes wrong though in links and ranks are partitioned in different ways then data will have to move at this point to join up corresponding keys in the two and the two rdd's alright so JJ contained now contains both every pages rank and every pages list of links as you can see now we have a even more complex data structure it's an array with an element per page with the pages URL with a list of the links and the one point over there is the page you choose current rank and these are all all this information is any sort of a single record that has all this information for each page together where we need it alright the next step is that we're gonna figure out every page is gonna push a fraction of its current page rank to all the pages that it links to it's kind of sort of divided up its current page rank among all the pages it links to and that's what this contribs does you know basically what's going on is that it's a one another one call to map and we're mapping over the for each page were running map over the URLs that that pages points to and for each page it points to we're just calculating this number which is the from pages current rank divided by the total number of pages that points to so this sort of figured you know creates a mapping from link name to one of the many contributions to that pages new page rank and we can sneak peek it what this is gonna produce I think is a much simpler thing it just as a list of URLs and contributions to the URLs page ranks and there's there's more there's you know more than one record for each URL here because there's gonna for any given page there's gonna be a record here for every single link that points to it indicating this contribution of from whatever that link came from to this page to this pages new updated PageRank what has to happen now is that we need to sum up for every page we need to sum up the PageRank contributions for that page that are in contribs so again we going to need to do a shuffle here it's gonna be a wide a transformation with a wide input because we need to bring together all of the elements of contribs for each page we need to bring together and to the same worker to the same partition so they can all be summed up and the way that's done the bay PageRank does that is with this reduced by key call would reduce spike he does is it first of all it brings together all the records with the same key and then sums up the second element of each one of those records for a given key and produces as output the key which is a URL and the sum of the numbers which is the updated PageRank there's actually two transformations here the first ones is reduced by key and the second is this map values which and and this is the part that implements the 15% probability of going to a random page and the 85% chance of following a link all right let's look at ranks by the way even though we've assigned two ranks here um what this is going to end up doing is creating an entirely new transformation I'm so not it's not changing the value is already computed or when it comes to executing this it won't change any values are already computed it just creates a new a new transformation with new output and we can see what's gonna happen in indeed we now have member ranks originally was just a bunch of pairs of URL PageRank now again we appears if you are I'll page rank another different we'd actually updated them sort of changed them by one step and I don't know if you remember the original PageRank values we saw but these are closer to those final output that we saw then the original values of all one are okay so that was one iteration of the algorithm when the loop goes back up to the top it's gonna do the same join flat map and reduce by key and each time it's again you know what the loop is actually doing is producing this lineage graph and so it's not updating the variables that are mentioned in the loop it's really creating essentially appending new transformation nodes to the lineage graph that it's building but I've only run that Elite once after the loop and then now this is what the real code does the real code actually runs collect at this point and so they were in the real PageRank implementation only at this point with the computation even start because of the call to collect here and I go off and read the end burden we're on the input through all these transformations and shuffles for the wide dependencies and finally collect the output together on the computer that's running this program by the way the computer that runs the program that the paper calls it the driver the driver computer is the one that actually runs this scallop program that's kind of driving the spark computation and then the program takes this output variable and runs it through a nice nicely formatted print on each of the records in the collect up okay so that's the kind of style of programming that people use for Scala and I mean for for spark went one thing to note here relative to MapReduce is that this program well you know and look looks a little bit complex but the fact is that this program is doing the work of many many MapReduce or doing an amount of work that would require many separate MapReduce programs in order to implement so you know it's 21 lines and maybe you used two MapReduce programs that are simpler than that but this is doing a lot of work for 21 lines and it turns out that this is you know this is sort of a real algorithm to so it's like a pretty concise and easy program easy to program way to express vast Big Data computations you know people like pretty successful okay so again just want to repeat that until the final collect or this code is doing is generating a lineage graph and not processing the data and the the lineage graph that it produces actually the paper I'm just copied this from the paper this is what the lineage graph looks like it's you know this is all that the program is producing it's just this graph until the final collect and you can see that it's a sequence of these processing stage where we read the file to produce links and then completely separately we produce these initial ranks and then there's repeated joins and reduced by key pairs each loop iteration produces a join and a each of these pairs is one loop iteration and you can see again that the loop is appended more and more nodes to the graph rather than what it is not doing in particular it is not producing a cyclic graph the loop is producing all these graphs are a cyclic another thing to notice that you wouldn't have seen a MapReduce is that this data here which was the data that we cashed that we persisted is used over and over again and every loop iteration and so it sparks going to keep this in memory and it's going to consult it multiple times alright so it actually happens during execution what is the execution look like so again the the assumption is that the data the input data starts out kind of pre partitioned by over in HDFS we assume our one file it's our input files already split up into lots of you know 64 megabyte or whatever it may happen pieces in HDFS spark knows that when you started you actually call collect the start of computation spark knows that the input data is already partitioned HDFS and it's gonna try to split up the work the workers in a corresponding way so if it knows that there's I actually don't know what the details are a bit it might actually try to run the computation on the same machines that store the HDFS data or it may just set up a bunch of workers to read each of the HDFS partitions and again there's likely to be more than one partition per per worker so we have the input file and the very first thing is that each worker reads as part of the input file so this is the read their file read if you remember the next step is a map where the each worker supposed to map a little function that splits up each line of input into a from two linked tupple um but this is a purely local operation and so it can go on in the same worker so we imagine that we read the data and then in the very same worker spark is gonna do that initial map so you know I'm drawing an arrow here's really an arrow from each worker to itself so there's no network communication involved indeed it's just you know we run the first read and the output can be directly fed to that little map function and in fact this is that that initial map in fact spark certainly streams the data record by record through these transformations so instead of reading the entire input partition and then running the map on the entire input partition SPARC reads the first record or maybe the first just couple of records and then runs the map on just sort of all I'm each record in fact runs each record of E if it was many transformations as it can before going on and reading the next little bit from the file and that's so that it doesn't have to store yes these files could be very large it isn't one half so like store the entire input file it's much more efficient just to process it record by record okay so there's a question so the first node in each chain is the worker holding the HDFS chunks and the remaining nodes in the chain are the nodes in the lineage oh yeah I'm afraid I've been a little bit confusing here I think the way to think of this is that so far all this happen is happening on it on individual workers so this is worker one maybe this is another worker and each worker is sort of proceeding independently and I'm imagining that they're all running on the same machines that stored the different partitions of the HTTPS fob but there could be Network communication here to get from HDFS to the to the responsible worker but after that it's very fast kind of local operations all right and so this is what happens for the with the people called the narrow dependencies that is transformations that just look consider each record of data independently without ever having to worry about the relationship to other records so by the way this is already potentially more efficient than MapReduce and that's because if we have what amount to multiple map phases here they just string together in memory whereas MapReduce if you're not super clever if you run multiple MapReduce is even if they're sort of degenerate map only MapReduce applications each stage would reduce input from G of s compute and write its output back to GFS then the next stage would be compute right so here we've eliminated the reading writing in it you know it's not a very deep advantage but it sure helps enormous Li for efficiency okay however not all the transformations are narrow not all just sort of read their input record by record kind of with every record independent from other records and so what I'm worried about is the distinct call which needed to know all instances all records that had a particular key similarly group by key needs to know about all instances that have a key join also it's gotta move things around so that takes two inputs needs to join together all keys from both inputs so that this all records from both inputs that are the same key so there's a bunch of these non-local transformations which the paper calls wide transformations because they potentially have to look at all partitions of the input that's a lot like reduce in MapReduce serve example distinct exposing we're talking about the distinct stage you know the distinct is going to be run on multiple workers also and no distinct works on each key independently and so we can partition the computation by key but the data currently is not partitioned by key at all actually isn't really partitioned by anything but just sort of however HDFS have my distorted so four distinct we're gonna run distinct on all the word partition and all the workers partitioned by key but you know any one worker needs to see all of the input records with a given key which may be spread out over all of the preceding workers for the preceding transformation and all of all of the you know they're all for the workers are responsible for different keys but the keys may be spread out over workers for the preceeding transformation now in fact the workers are the same typically it's gonna be the same workers running the map is running running the distinct but the data needs to be moved between the two transformations to bring all the keys together and so what sparks actually gonna do it's gonna take the output of this map hash the each record by its key and use that you know mod the number of workers to select which workers should see it and in fact the implementation is a lot like your implementation of MapReduce the very last thing that happens in in the last of the narrow stages is that the output is going to be chopped up into buckets corresponding to the different workers for the next transformation where it's going to be left waiting for them to fetch I saw the scoop is that each of the workers run the sort of as many stages all the narrows stages they can through the completion and store the output split up into buckets when all of these are finished then we can start running the workers for the distinct transformation whose first step is go and fetch from every other worker the relevant bucket of the output of the last narrow stage and then we can run the distinct because all the given keys are on the same worker and they can all start producing output themselves all right now of course these Y transformations are quite expensive the now transformations are super efficient because we're just sort of taking each record and running a bunch of functions on it totally locally the Y transformations require pushing a lot of data impact essentially all of the data in for PageRank you know you get terabytes of input data that means that you know it's still the same data at this stage because it's all the links and then in the web so now we're pushing terabytes and terabytes of data over the network to implement this shuffle from the output of the map functions to the input of the distinct functions so these wide transformations are pretty heavyweight a lot of communication and they're also kind of computation barrier because we have to wait all for all the narrow processing to finish before we can go on to the so there's wide transformation all right that said the there are some optimizations that are possible because SPARC has a view SPARC creates the entire lineage graph before it starts any of the data processing so smart can inspect the lineage graph and look for opportunities for optimization and certainly running all of if there's a sequence of narrow stages running them all in the same machine by basically sequential function calls on each input record that's definitely an optimization that you can only notice if you sort of see the entire lineage graph all at once another optimization that spark does is noticing when the data has all has has already been partitioned due to a wide shuffle that the data is already partitioned in the way that it's going to be needed for the next wide transformation so in the in our original program let's see I think we have two wide transformations in a row distinct requires a shuffle but group by key also it's gonna bring together all the records with a given key and replace them with a list of for every key the list of links you know starting at that URL these are both wide operators they both are grouping by key and so maybe we have to do a shuffle for the distinct but spark can cleverly recognize a high you know that is already shuffled in a way that's appropriate for a group by key we don't have to do in other shuffle so even though group by key is in principle it could be a wide transformation in fact I suspect spark implements it without communication because the data is already partitioned by key so maybe the group by key can be done in this particular case without shuffling data without expense of course it you know can only do this because it produced the entire lineage graph first and only then ran the computation so this part gets a chance to sort of examine and optimize and maybe transform the graph so that looks topic actually any any questions about lineage graphs or how things are executed I feel free to interact the next thing I want to talk about is fault tolerance and here the you know these kind of computations they're not the fault tolerance are looking for is not the sort of absolute fault tolerance you would want with the database what you really just cannot ever afford to lose anything what you really want is a database that never loses data here the fault tolerance we're looking for is more like well it's expensive if we have to repeat the computation we can totally repeat this computation if we have to but you know it would take us a couple of hours and that's irritating but not the end of the world so we're looking to you know tolerate common errors but we don't have to certainly don't have to having bulletproof ability to tolerate any possible error so for example spark doesn't replicate that driver machine if the driver which was sort of controlling the computation and knew about the lineage graph of the driver crashes I think you have to rerun the whole thing but you know any only any one machine only crashes maybe every few months so that's no big deal another thing to notice is that HDFS is sort of a separate thing SPARC is just assuming that the input is replicated in a fault-tolerant way on HDFS and indeed just just like GFS HDFS does indeed keep multiple copies of the data on multiple servers if one of them crashes can soldier on with the other copy so the input data is assumed to be to be relatively fault tolerant and what that means that at the highest level is that spark strategy if one of the workers fail is just to recompute the whatever that worker was responsible for to just repeat those computations they were lost with the worker on some other worker and on some other machine so that's basically what's going on and it you know it might take a while if you have a long lineage like you would actually get with PageRank because you know PageRank with many iterations produces a very long lineage graph one way that spark makes it not so bad that it has to be may have to be computer everything from scratch if a worker fails is that each workers actually responsible for multiple partitions at the input so spark can move those parts move give each remaining worker just one of the partitions and they'll be able to basically paralyzed the recomputation that was lost with the failed worker by running each of its partitions on a on a different worker in parallel so if all else fails spark just goes back to the beginning from being input and just recomputes everything that was running on that machine however and for now our dependencies that's pretty much the end of the story however there actually is a problem with the wide dependencies that makes that story not as attractive as you might hope so this is a topic here is failure one failed node 1 failed worker in a lineage graph that has wide dependencies so the a reasonable or a sort of sample graph you might have is you know maybe you have a dependency graph that's you know starts with some power dependencies but then after a while you have a wide dependency so you got transformations that depend on all the preceding transformations and then some small narrow ones all right and you know the game is that a single workers fail and we need to reconstruct the Maeby's field before we've gone to the final action and produce the output so we need to kind of reconstruct recompute what was on this field work the the damaging thing here is that ordinarily as spark is executing along it you know it executes each of the transformations gives us output to the next transformation but doesn't hold on to the original output unless you unless you happen to tell it to like the links data is persisted with that cache call but in general that data is not held on to because now if you have a like the PageRank lineage graph maybe dozens or hundreds of steps long you don't want to hold on to all that data it's way way too much to fit in memory so as the SPARC sort of moves through these transformations it discards all the data associated with earlier transformations that means when we get here and if this worker fails we need to we need to restart its computation on a different worker now so we can be the input and maybe do the original narrow transformations they just depend on the input which we have to reread but then if we get to this y transformation we have this problem that it requires input not just from the same partition on the same worker but also from every other partition and these workers so they're still alive have in this example have proceeded past this transformation and therefore discarded the output of this transformation since it may have been a while ago and therefore the input did our recomputation needs from all the other partitions doesn't exist anymore and so if we're not careful that means that in order to rebuild this the computation on this field worker we may in fact have to re execute this part of every other worker as well as well as the entire lineage graph on the failed worker and so this could be very damaging right if we're talking about oh I mean I've been running this giant spark job for a day and then one of a thousand machines fails that may mean we have to we know anything more clever than this that we have to go back to the very beginning on every one of the workers and recompute the whole thing from scratch no it's gonna be the same amount of work is going to take the same day to recompute a day's computation so this would be unacceptable we'd really like it so that if if one worker out of a thousand crashes that we have to do relatively little work to recover from that and because of that spark allows you to check point to make periodic check points of specific transformation so um so in this graph what we would do is in the scallop program we would call I think it's the persist call actually we call the persist call with a special argument that says look after you compute the output of this transformation please save the output to HDFS and so everything and then if something fails the spark will know that aha the output of the proceeding transformation was safe th th d fs and so we just have to read it from each DFS instead of recomputing it on all for all partitions back to the beginning of time um and because HDFS is a separate storage system which is itself replicated in fault-tolerant the fact that one worker fails you know the HDFS is still going to be available even if a worker fails so I think so for our example PageRank I think what would be traditional would be to tell spark to check point the output to check put ranks and you wouldn't even know you can tell it to only check point periodically so you know if you're gonna run this thing for 100 iterations it actually takes a fair amount of time to save the entire ranks to HDFS because again we're talking about terabytes of data in total so maybe we would we can tell SPARC look only check point ranks to HDFS every every 10th iteration or something to limit the expanse although you know it's a trade-off between the expensive repeatedly saving stuff to disk and how much of a cost if a worker failed you had to go back and redo it Bertha's a question when we call that does act as a checkpoint you know okay so this is a very good question which I don't know the answer to the observation is that we could call cash here and we do call cashier and we could call cashier and the usual use of cash is just to save data in memory with the intent to reuse it that's certainly why it's being called here because we're using links for but in my example it would also have the effect of making the output of this stage available in memory although not on not an HDFS but in the memory of these workers and the paper never talks about this possibility and I'm not really sure what's going on maybe that would work or maybe the fact that the cash requests are merely advisory and maybe evicted if the workers run out of space means that calling cash doesn't give you it isn't like a reliable directed to make sure the data really is available it's just well it'll probably be available on most nodes but not all nodes because remember even a single node loses its data and we're gonna have to do a bunch of recomputation so III I'm guessing that persists with replication is a firm directive to guarantee that the data will be available even if there's a failure I don't really know it's a good question alright okay so that's the programming model and the execution model and the failure strategy and by the way just a beat on the failure strategy a little bit more the way these systems do failure recovery is it's not a minor thing as as people build bigger and bigger clusters with thousands and thousands of machines you know the probability that job will be interrupted by at least one worker failure it really does start to approach one and so the the designs recent designs intended to run on big clusters have really been to a great extent dominated by the failure recovery strategy and that's for example a lot of the explanation for why SPARC insists that the transformations be deterministic and why the are these its rdd's are immutable because you know that's what allows it to recover from failure by simply recomputing one partition instead of having to start the entire computation from scratch and there have been in the past plenty of proposed sort of cluster big data execution models in which there really was mutable data and in which computations could be non-deterministic make if you look up distributed shared memory systems those all support mutable data and they support non-deterministic execution but because of that they tend not to have a good failure strategy so you know thirty years ago when a big cluster was for computers none of this mattered because the failure probability was little very low and so many different kinds of computation models seemed reasonable then but as the clusters have grown to be hundreds and thousands of workers really the only models that have survived are ones for which you can devise a very efficient to failure recovery strategy that does not require backing all the way up to the beginning and restarting the paper talks about this a little bit when it's criticizing I'm distributed shared memory and it's a very valid criticism I bet it's a big design constraint okay so the sparks not perfect for all kinds of processing it's really geared up for batch processing of giant amounts of data bulk bulk data processing so if you have terabytes of data and you want to you know chew away on it for for a couple hours smart great if you're running a bank and you need to process bank transfers or people's balance queries then SPARC is just not relevant to that kind of processing known or to sort of typical websites where I log into you know I access Amazon and I want to order some paper towels and put them into my shopping cart SPARC is not going to help you maintain this part the shopping cart SPARC may be useful for analyzing your customers buying habits sort of offline but not for sort of online processing the other sort of kind of a little more close to home situation that spark in the papers not so great at is stream processing i SPARC definitely assumes that all the input is already available but in many situations the input that people have is really a stream of input like they're logging all user clicks on their web sites and they want to analyze them to understand user behavior you know it's not a kind of fixed amount of data is really a stream of input data and you know SPARC as in describing the paper doesn't really have anything to say about processing streams of data but it turned out to be quite close to home for people who like to use spark and and now there's a variant of SPARC called spark streaming that that is a little more geared up to kind of processing data as it arrives and you know sort of breaks it up into smaller batches and runs in a batch at a time to spark so it's good for a lot of bad stuff but that's certainly on to be thing right to wrap up the UH you should view spark as a kind of evolution after MapReduce and I may fix some expressivity and performance sort of problems or that MapReduce has what a lot of what SPARC is doing is making the data flow graph explicit sort of he wants you to think of computations in the style of figure three of entire lineage graphs stages of computation and the data moving between these stages and it does optimizations on this graph and failure recovery is very much thinking about the lineage graph as well so it's really part of a larger move and big data processing towards explicit thinking about the data flow graphs as a way to describe computations a lot of the specific win and SPARC have to do with performance part of the prepend these are straightforward but nevertheless important some of the performance comes from leaving the data in memory between transformations rather than you know writing them to GFS and then reading them back at the beginning of the next transformation which you essentially have to do with MapReduce and the other is the ability to define these data sets these are Dedes and tell SPARC to leave this RDD in memory because I'm going to reuse it again and subsequent stages and it's cheaper to reuse it than it is to recompute it and that sort of a thing that's easy and SPARC and hard to get at in MapReduce and the result is a system that's extremely successful and extremely widely used and if you deserve real success okay that that's all I have to say and I'm happy to take questions if anyone has them you