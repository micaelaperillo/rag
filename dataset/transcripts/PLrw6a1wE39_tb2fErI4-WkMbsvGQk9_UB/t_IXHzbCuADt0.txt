today I want to do two things I want to finish the discussion of zookeeper and then talk about crack the particular things that I'm most interested in talking about a bad zookeeper are the design of its API that allows the zookeeper to be a general-purpose service that really bites off significant tasks that distributed systems need so why is this you know why is that a good API design and then the really more specific topic of mini transactions turns out this is a worthwhile idea to know so they got API and I'm just just a recall zookeepers based on raft and so we can think of it as being and indeed it is fault tolerant and does the right thing with respect to partitions it has this sort of performance enhancement by which reads can be processed at any replica and therefore the reads can be stale so we just have to keep this in mind as we're analyzing various uses of the zookeeper interface on the other hand zookeeper does guarantee that every replicas process the stream of rights in order one at a time with all replicas executing the rights in the same order so that the replicas advance sort of in their states of all than exactly the same way and that all of the operation reads and writes produced by a generated by a single client or processed by the system also in order both in the order that the client issued them in and successive operations from a given client always see the same state or later in the right stream as the previous read operation right any operation from that client okay so before I dive into what the API looks like and why it's useful it's worth just thinking about what kinds of problems zookeeper is aiming to solve or could be expected to solve so for me a totally central example of motivation of why you would want to use ooh keeper this is it as an implementation of the test and set service that vmware ft required in order for either server to take over when the other one failed it was a bit of a mystery in the vmware paper what is this test instant service how is it may you know is it fault tolerant does it itself tolerate partitions but zookeeper actually gives us the tools to write a fault tolerant test and set service of exactly the kind that vmware ft needed that is fault tolerant and does do the right thing under partitions that's sort of a central kind of thing that zookeepers doing there's also a bunch of other ways that turns out people use it suki was very successful people use it for a lot of stuff one kind of thing people use is just to publish just configuration information for other servers to use like for example the IP address of the current master for some set of workers this is just config configuration information another classic use of zookeepers to elect a master you know if we want to have a when the old master fails we need to have everyone agree on who the new master is and only elect one master even if there's partitions you can elect a master using zookeeper primitives if the master for small amounts of stated anyway if whatever master you elect needs to keep some state it needs to keep it up-to-date like maybe you know informations such as who the primary is for a given chunk of data like you'd want in GFS the master can store its state in zookeeper it knows new keepers not going to lose it if the master crashes and we elect a new master to replace it that new master can just read the old master state right out of zookeeper and rely on it actually being there other things you might imagine maybe you know MapReduce like systems workers could register themselves by creating little files and zookeeper and again with systems like MapReduce you can imagine the master telling the workers what to do by writing things in zookeeper like writing lists of work in zookeeper and then worker sort of take those work items one by one out of zookeeper and delete them as they complete them but people use zookeeper for all these things question yeah exactly yeah so the question is oh how people use zookeeper and in generally yeah you you would if you're running some big data center and you run all kinds of stuff in your data center you know web servers storage systems MapReduce who knows what you might fire up a zookeeper one zookeeper cluster because this general purpose can be used for lots of things so you know five or seven zookeeper replicas and then as you deploy various services you would design the services to store some of the critical state in your one zookeeper cluster alright the API zookeeper looks like a filesystem some levels it's got a directory hierarchy you know there's a root directory and then maybe you could maybe each application has its own sub directory so maybe the application one keeps its files here in this directory app two keeps its files in this directory and you know these directories have files and directories underneath them one reason for this is just because you keeper is like just mentioned is a design to be shared between many possibly unrelated activities we just need a naming system to be able to keep the information from these activities distinct so they don't get confused and read each other's data by mistake within each application it turns out that a lot of convenient ways of using zookeeper will involve creating multiple files let's see a couple examples like this in a few minutes okay so it looks like a filesystem this is you know not very deep it doesn't it's not actually you know you can't really use it like a file system in the sense of mounting it and running LS and cat and all those things it's just that internally it names objects with these path names so you know one this x y&z here few different files you know when you talk to me you send an RPC - zookeeper saying you know please read this data you would name the data you want maybe add up to slash X there's just a sort of hierarchical naming scheme these these files and directories are called Z nodes and it turns out it's there's three types you have to know about that helps you keep or solve various problems for us there's just regular Z nodes where if you create one it's permanent until you delete it there's a femoral Z nodes where if a client creates an ephemeral Z node zookeeper will delete that ephemeral Z node if it believes that the client has died it's actually tied to client sessions so clients have to sort of send a little heartbeat in every once a while into the zookeeper into zookeeper say oh I'm still alive I'm still alive so zookeeper won't delete their ephemeral files and the last characteristic files may have is sequential and that means when you ask to create a file with a given name what you actually end up creating is a file with that name but with a number appended to the main and zookeeper guarantees never to repeat a number if multiple clients try to create sequential files at the same time and also to always use montt increasing numbers for the for the sequence numbers that are pens to filenames and we'll see all of these things come up in examples at one level the operations the RPC interface that zookeeper exposes is sort of what you might expect for your files was to create RPC where you give it a name really a full path name some initial data and some combination of these flags and interesting semantics of create is that it's exclusive that is when I send a create into zookeeper ask it to create a file so you keep your responds with a yes or no if that file didn't exist and I'm the first client who wants to create it zookeeper says yes and creates the file the file already exists zookeeper says no or returns an error so clients know it's exclusive create and clients know whether they were the one client if multiple clients are trying to create the same file which we'll see in locking samples the clients will know whether they were the one who actually managed to create the file there's also delete and one thing I didn't mention is ever easy note has a version as a current version number that advances as its modified and delete along with some other update operations you can send an a version number saying only do this operation if the files current version number is the version that was specified and that'll turn out to be helpful if you're worried about in situations where multiple clients might be trying to do the same operation at the same time so you can pass a version saying only delete there's an exists call oh does this path named Xenu exist an interesting extra argument is that you can ask to watch for changes to whatever path name you specified you can say does this path name exist and whether or not exists it exists now if you set this watch if you pass in true for this watch flag zookeeper guarantees to notify the client if anything changes about that path name like it's created or deleted or modified and furthermore the the check for whether the file exists and the setting of the watch point of the watching information in the inside zookeeper or atomic so nothing can happen between the point at which the point in the write stream which zookeeper looks to see whether the path exists and the point in the write stream at which zookeeper inserts the watch into its table and then that's like very important for for correctness we also get D then you get a path and again the watch flag and now the watch just applies to the contents of that file there's set data again path the new data and this conditional version that if you pass an inversion then zookeeper only actually does the right if the current version number of the file is equal to the number you passed in okay so let's see how we use this the first maybe almost this first very simple example is supposing we have a file in zookeeper and we want to store a number in that file and we want to be able to increment that number so we're keeping maybe a statistics count and whenever a client you know I know gets a request from a web user or something it's going to increment that count in zookeeper and more than one client may want to increment the count that's the critical thing so an example so one thing to sort of get out of the way is whether we actually need some specialized interface in order to support client coordination as opposed to just data this looks like a file system could we just provide the ordinary readwrite kind of file system stuff that dated that typical storage systems provide so for example some of you have started and you'll all start soon Ladd 3 in which you build a key value store where the two operations are the only operations are put key value and so one question is can we do you know all these things that we might want to do with zookeeper can we just do them with lab 3 with a key with a key value put get interface so supposing for my I want to implement this count thing maybe I could implement the count with just lab threes key value interface so you might increment the count by saying x equals get you know whatever key were using and then put that key an X plus 1 why why is this a bad answer yes yes oh it's not atomic that is absolutely the root of the problem here and you know the abstract way of putting it but one way of looking at it is that of two clients both want to increment the counter at the same time they're both gonna read they're both gonna use get to read the old value and get you know ten those gonna add one to ten and get 11 and I was gonna call put with 11 so now we've increased the counter by one but two clients were doing it so surely we should have ended up increasing it by two so that's why the lab three cannot be used for even this simple example furthermore in the sort of zookeeper world where guests can return stale data is not lab 3 or gets are not allowed to return stale data but in zookeeper reads can be stale and so if you read a stale version of the current counter and add one to it you're now writing the wrong value you know if 30 values 11 but you're get returns a stale value of 10 you add 1 to that and put 11 that's a mistake because we really should have been putting 12 so zookeeper has this additional problem that we have to worry about that that gets don't return the latest data ok so how would you do this in zookeeper here's how I would do this in zookeeper it turns out you need to do you need to wrap this code Siemens in a loop because it's not guaranteed to succeed the first time so we're just gonna say while true we're gonna call get data to get the current value of the counter and the current version so we're gonna say X V equals I'm get data and we need to say final name I don't care what the file name is we just say that nice now we get the well we get a value and a version number possibly not fresh possibly stale but maybe fresh and then we're gonna use a conditional put a conditional setting and if set data is a set data operation return true meaning it actually did set the value we're gonna break otherwise just go back to the top of the loop otherwise so what's going on here is that we read some value and some version number maybe still maybe fresh out of the replicas the set data we send actually did the zookeeper leader because all rights go to the leader and what this means is only set the value to X plus one if the version with the real version the latest version is still is V so if we read fresh data and nothing else is going on in the system like no other clients are trying to increment this then we'll read the latest version latest value we'll add one to the latest value specify the latest version and our set data will be accepted by the leader and we'll get back a positive reply to our request after it's committed and we'll break because we're done if we got stale data here or this was fresh data but by the time our set data got to the leader some other clients set data and some other client is trying to increment their set data got there before us our version number will no longer be fresh in either those cases this set data will fail and we'll get an error response back it won't break out of the loop and we'll go back and try again and hopefully we'll succeed this time yes yes so the question is could this it's a while loop or we guaranteed is ever going to finish and no no we're not really guaranteed that we're gonna finish in practice you know so for example if our replicas were reading from is cut off from the leader and permanently gives us stale data then you know maybe this is not gonna work out but you know but in real life well in real life the you know leaders pushing all the replicas towards having identical data to the leader so you know if we just got stale data here probably when we go back you know maybe we should sleep for 10 milliseconds or something at this point but when we go back here eventually we're gonna see the latest data the situation under which this might genuinely be pretty bad news is if there's a very high continuous load of increments from clients you know if we have a thousand clients all trying to do increments the risk is that maybe none of them will succeed or something I think one of them will succeed because I think one of the most succeed because you know the the first one that gets its set data into the leader will succeed and the rest will all fail because their version numbers are all too low and then the next 999 will put and get data's in and one of them will succeed so it all have a sort of N squared complexity to get through all of the all other clients which is very damaging but it will finish eventually and so if you thought you were gonna have a lot of clients you would use a different strategy here this is good or load situations yes if they fit in memory it's no problem if they don't fit memory it's a disaster so yeah when you're using zookeeper you have to keep in mind that it's yeah it's great for 100 megabytes of stuff and probably terrible for 100 gigabytes of stuff so that's why people think of it as storing configuration information rather than their we old data of your big website yes I mean it's sort of watch into this sequence yet that could be so if we want if we wanted to fix this to work under high load then you would certainly want to sleep at this point where I'm not well the way I would fix this my instinct I'm fixing this would be to insert asleep here and furthermore double the amount of it sort of randomized sleep whose span of randomness doubles each time we fail and that's a sort of tried and true strategies exponential back-off is a it's actually similar to raft leader election it's a reasonable strategy for adapting to an unknown number of concurrent clients so okay tell me what's right okay so we're getting data and then watching its true so yes so if somebody else modifies the data before you call set data maybe you'll get a watch notification um the problem is the timing is not working in your favor like the amount of time between when I received the data here and when I send off the message to the leader with this new set data is zero that's how much time will pass here roughly and if some other client is sent in increment at about this time it's actually quite a long time between when that client sends in the increment and when it works its way through the leader and is sent out to the followers and actually executed the followers and the followers look it up in their watch table and send me a notification so I think it won't give you any read result or if you read at a point if you're gonna read at a point that's after where the modification occurred that should raise the watch you'll get the notification of the watch before you get the read response but in any case I think nothing like this could save us because what's gonna happen is all thousand clients are gonna do the same thing whatever it is right they're all gonna do again and set a watch and whatever they're all gonna get the notification at the same time they're all gonna make the same decision about well they're all not gonna get to watch because none of them has done the put data yet right so the worst case is all the clients are starting at the same point they all do a get they all get version one they all set a watch point they don't get a notification because no change has occurred they all send a set data RPC to the leader all thousand of them the first one changes the data and now the other 999 and get a notification when it's too late because they've already sent the set data so it's possible that watch could help us here but sort of straightforward version of watch I have a feeling if you wanted the the mail we'll talk about this in a few minutes but the anon heard the second locking example absolutely solves this kind of problem so we could adapt to the second locking example from the paper to try to cause the increments to happen one at a time if there's a huge number of clients who want to do it other questions about this example okay this is an example of a what many people call a mini transaction all right it's transactional in a sense that wow there's you know a lot of funny stuff happening here the effect is that once it all succeeds we have achieved an atomic read-modify-write of the counter right the difficulty here is that it's not atomic the reading the right the read the modifying the right are not atomic the thing that we have pulled off here is that this sequence once it finishes is atomic right we actually man and once we have to be on the pass through this that we succeeded we managed to read increment and write without anything else intervening we managed to do these two steps atomically and you know this is not because this isn't a full database transaction like real databases allow fully general transactions where you can say start transaction and then read or write anything you like maybe thousands of different data items whatever who knows what and then say end transaction and the database will cleverly commit the whole thing as an atomic transaction so real transactions can be very complicated zookeeper supports this extremely simplified version of you know when you're sort of one we can do it atomic sort of operations on one piece of data but it's enough to get increment and some other things so these are for that reason since they're not general but they do provide atomicity these are often called mini transactions and it turns out this pattern can be made to work with various other things too like if we wanted to do the test and set that vmware ft requires it can be implemented with very much this setup you know maybe the old value if it's zero then we try to set it to one but give this version number you know nobody else intervened and we were the one who actually managed to set it to one because the version number hadn't changed but i'm leader got our request and we win somebody else changes to one after we read it then the leader will tell us that we lost so you can do test and set with this pattern also and you should remember this is the strategy okay alright next example I want to talk about is these locks and I'm talking about this because it's in the paper not because I strongly believe that this kind of lock is useful but they have they have an example in which a choir has a couple steps one we try to create we have a lock file and we try to create the lock file now again some file with a femoral set to true and so if that succeeds then or not we've acquired the lock the second step that doesn't succeed then we want to wait for whoever did acquire the lock what if this isn't true that means the lock file already exists I mean somebody else has acquired the lock and so we want to wait for them to release the lock and they're gonna release the lock by deleting this file so we're gonna watch yes alright so we're gonna watch we're gonna gonna call exists and watching is true now it turns out that um okay and and and if the file still exists right which we expect it to because after all they didn't exist presumably would have returned here so if it exists we want to wait for the notification we're waiting for this watch notification call this three and a step for go to what so the usual deal is you know we call create you know maybe we win if it fails we wait for whoever owns a lock to release it we get the watch notification when the file is deleted at that point this wait finishes and we go back to Mon and try to recreate the file hopefully we will get the file this time okay so we should ask ourselves questions about possible interleavings of other clients activities with our four steps so one we know for sure we know of already if another client calls create at the same time then the zookeeper leader is going to process those two to create rpcs one at a time in some order so either mike reid will be executed first or the other clients create will be executed first minds executed first i'm going to get a true back in return and acquire the lock and the other client is guaranteed to get a false return and if there are pcs processed first they'll get the true return and i'm guaranteed to get the false return and in either case the file will be created so we're okay if we have simultaneous executions of one another question is well you know if I if create doesn't succeed for me and I'm gonna call exists what happens if the lock is released actually between the create and the exists so this is the reason why I rap I have a knife around me around the exists is because it actually might be released before I call exists because it could have been acquired quite a long time ago by some other client and then if the file doesn't exist at this point then this will fail and I'll just go directly back to this go to one and try again similarly and actually more interesting is what happens if the whoever holds it now releases it just as I call exist or as the replica I'm talking to is in the middle of processing my exists requests and the answer to that is that the whatever replica I'm looking at you know it's log or guaranteed that rights occur in some order right so the repla I'm talking to it's it's log its proceeding in some way and my exists call is guaranteed to be executed between two log entries in the right stream right this is a this is a read-only request and you know the problem is that somebody's delete request is being processed at about this time so somewhere in the log is going either is going to be the delete request from the other client and the rep and you know this is my mind the replica that I'm talking to zookeeper replicas I'm talking to his log my watch my exists RPC is either processed completely processed here in which case the replica sees oh the file still exists and the replica inserts the watch information into its watch table at this point and only then executes the delete so when the delete comes in were guaranteed that my watch request is in the replicas watch table and it will send me a notification right or my exist requests is executed here at a point after the delete happen the file doesn't exist and so now the call returns true and no well actually a watch table entry is entered but we don't care right so it's quite important that the rights are sequenced and that reads happen at definite points between rights yes well okay so yes so this is where the exists is executed the file doesn't exist at this point exists returns false we don't wait we go to one we create the file and return we did install a watch here that watch will be triggered it doesn't really matter because we're not really waiting for it but the watch will be triggered by this created we're not waiting for it but yeah okay so the file doesn't exist we go to one somebody else has created the file we try to create the file that fails we install another watch and it's a dis watch that we're not waiting for so this way does not a wait for anything to happen although it doesn't really matter in the moment it's not harmful to to to break out of this loop early it's just wasteful anyway we've all the history this code leaves watches sort of in the system and I don't really know what does my new watch on the same file override my old watch I'm not actually sure okay I'm finally this example and the previous example suffle suffer from the herd effect we also heard effect we talked about I mean what we were talking about when we were worrying about oh but if clients I'll try to increment this at the same time gosh that's going to have N squared complexity as far as how long it takes to get to all thousand clients this lock scheme also suffers from the herd effect in that if there are a thousand clients trying to get the lock then the amount of time that's required to sort of grant the lock to each one of the thousand clients is proportional to a thousand squared because after every release all of the remaining clients get triggered by this watch all of the remaining clients go back up here and send in a create and so the total number create our pcs generated is basically a thousand squared so this suffers from this herd the whole herd of waiting clients is beating on zookeeper another name for this is that it's a non scalable lock or yeah okay and so the paper is a real deal and we'll see it more and in other systems and soon enough serious end of problems the paper actually talks about how to solve it using zookeeper and the interesting thing is that Zook it's actually expressive enough to be able to build a more complex lock scheme that doesn't suffer from this hurt effect that even of a thousand clients are waiting the cost of one client giving up a lock and another acquiring it is order 1 instead of order n and this is the because it's a little bit complex this is the pseudocode in the paper in section 2.4 it's on page 6 if you want to follow along so this is and so this time there is not a single lock file there's no yes it is just a name that allows us to all talk about the same lock so it's just a name know now I've acquired the lock and I can do I can whatever the lock was protecting you know maybe only one of us at a time should be allowed to give a lecture in this lecture hall if you want to give a lecture in this lecture hall you first have to acquire the lock called 34 100 the that turns out it's yes it's a Z node and zookeeper but it like nobody cares about its contents we just need it to be able to agree on a name for the lock that's the sense in which that's piyah this it looks like a file system but it's really a naming system alright so step one is we create a sequential file and so yeah we give it a prefix name but what it actually creates is you know if this is the 27th file sequential file created with with prefix F you know maybe we get F 27 or something and and in the sequenced in the sequence of writes that zookeeper is it's working through successive creates get ascending guaranteed ascending never descending always ascending sequence numbers when you create a sequential file there was an operation I left off from the list it turns out you can get a list of files you can get a list of files underneath you give the name of Zeno that's actually a directory with files in it you can get a list of all the files that are currently in that directory so we're gonna list the files let's start with that you know maybe list f star we get some list back we create a file with the system allocated us a number here we can look at that number if there's no lower numbered file in this list then we win and we get the lock so if our sequential file is the lowest number file with that name prefix we win so no lower number we've quired the lock and we can return if there is one then again what we want to wait for then what's going on is that these sequentially numbered files are setting up the order in which the lock is going to be granted to the different clients so if we're not the winner of the lock what we need to do is wait for the previously numbered with the client who created the previously numbered file to release to acquire and then release the lock and we're going to release the lock the convention for releasing the locking in this system is for remove the file to remove your sequential file so we want to wait for the previously numbered sequential file to be deleted and then it's our turn and we get the lock so we need to call exists so we're gonna say if the call exists mostly to set a watch point so it's you know next lower number file and we want to have a watch get that file still exist we're gonna wait and then so that's step 5 and then finally we're gonna go back to we're not going to create the file again because it already exists we're gonna go back to listing the yeah the files so this is a choir releases just I delete if I acquire the lock I delete my the file I created complete with my number yes why do you need to list the files again that's a good question so the question is we got the list of files we know the next lower number file there's a guarantee of the sequential file creation is that once filed 27 is created no file with a lower number will ever subsequently be created so we now know nothing else could sneak in here so how could the next lower number file you know why why do we need to list again why don't we just go back to waiting for that same lower numbered file thing Britney guess the answer I mean the the the way this code works the answer to the question is whoever was the next lowered person might have either acquired him at least the lock before we noticed or have died and this went and these are transient files sorry or whatever they're called ephemeral there's an ephemeral file you know even if we're 27th in line number 26 may have died before getting the lock if number 26 dies the system automatically deletes their ephemeral files and so if that happened now we need to wait for number 25 that is the next you know it if all files you know 2 through 27 and and we're 27 if they're all they are and they're all waiting there's a lock if if the one before is dies before getting the lock now we need to wait for the next next lower number file not because the next lower one is has gone away so that's why we have to go back and relist the files in case our predecessor in the list of waiting clients turned out to die yes if there's no lower numbered file than you have acquired the lock absolutely yes how does this not suffer from the herd effect suppose we have a thousand clients waiting and currently client made through the first five hundred and client five hundred holds the lock every client waiting every client is sitting here waiting for an event but only the client that created file five hundred and one he's waiting for the vision of file five hundred so everybody's waiting for the next lower number so five hundred is waiting for 499 twenty nine nine but everybody everybody's waiting for just one file when I release the lock there's only one other client the next higher numbered client that's waiting for my file so when I release the lock one client gets a notification one client goes back and lists the files one client and one client now has the lock so the sort of expense you know no matter how many clients that are the expense of one of each release and acquire is a constant number of our PCs where's the expense of a release and acquire here is that every single waiting client is notified and every single one of them sends a write request than the create request into zookeeper oh you're free to get a cup of coffee yeah I mean this is you know what the programming interface looks like is not our business but this is either and there's there's two options for what this actually means as far as what the program looks like one is there's some thread that's actually in a synchronous wait it's made a function call saying please acquire this lock and the function hold doesn't return until the locks finally acquired or the notification comes back of much more sophisticated interface would being one in which you fire off requests a zookeeper and don't wait and then separately there's some way of seeing well as you keep your said anything recently or I have some go routine whose job it is just wait for the next whatever it is from zookeeper in the same sense that you might read the apply Channel and just all kinds of interesting stuff comes up on the apply channel so that's a more likely way to structure this but yeah you're totally either through threading or some sort of event-driven thing you can do something else while you're waiting yes yes or if the person before me has neither died nor released it's a file before me exists that means either that client is still alive and still waiting for the lock or still alive and holds the lock we don't really know it does it as long as that client 500 still live if if this exists fails that means one of two things either my predecessor held the lock and is released it and deleted their file or my predecessor didn't hold the lock they exited and zookeeper deleted their file because it was an ephemeral file so there's two reasons to come out of this to come out of his weight or four they exist to return false and that's why we have to like we check everything you know you really don't know what the situation is after the exists completes that might that yeah maybe maybe that could need to work that sounds reasonable and it preserves the sort of scalable nature of this and that each require release only involves a few clients two clients alright this pattern to me actually first saw this pattern a totally different context and scalable locks for threading systems I go this end in for most of the world this is called a scale of a lock I find it one of those interesting constructions I've ever seen now and so like I'm impressed that zookeeper is able to express it and it's a valuable construct having said that I'm a little bit at sea about why zookeeper about why the paper talks about locks at all because these locks these locks are not like threading locks and go because in threading there's no notion of threads failing at least if you don't want them there to be there's no notions of threads just sort of randomly dying and go and so really the only thing you're getting out of a mutex it's really the case and go that when you use it if everybody uses mutexes correctly you are getting atomicity for the sequence of operations inside the mutex that you know if you take out a lock and go and you do 47 different read and write a lot of variables and then release the lock if everybody follows that locking strategy nobody's ever going to see some sort of weird intermediate version of the data as of halfway through you're updating it right just makes things atomic no argument these locks aren't really like that because if the client that holds the lock fails it just releases the lock and somebody else can pick up the lock so it does not guarantee atomicity because you can get partial failures and distributed systems where you don't really get partial failures of ordinary threaded code so if the current lock holder had the lock and needed to update a whole bunch of things that were protected by that lock before releasing and only got halfway through updating this stuff and then crashed then the lock will get released you'll get the lock and yet when you go to look at the data it's garbage because it's just whatever random seed it was in the middle of updated so there's these locks don't by themselves provide the same atomicity guarantee that threading locks do and so we're sort of left to imagine for ourselves by the paper or why you would want to use them or why this is the sort of some of the main examples in the paper so I think if you use locks like this then you sort in a distributed system then you have two general options one is everybody who acquires a lock has to be prepared to clean up from some previous disaster right so you acquire this lock you look at the data you try to figure out gosh if the previous owner of a lot crashed you know when I'm looking at the data you know how can I fix the data to make up how can I decide if the previous owner crashed and what do I do to fix up the data and you can play that game especially if the convention is that you always update in a particular sequence you may be able to detect where in that sequence the previous holder crashed assuming they crashed but it's a you know it's a tricky game the requires thought of a kind you don't need for like thread locking um the other reason maybe these locks would make sense is if there's sort of soft locks protecting something that doesn't really matter so for example if you're running MapReduce jobs map tasks reduce tasks you could use this kind of lock to make sure only one task only one worker executed each task so workers gonna run test 37 it gets the lock for task 37 execute it marks it as executed and releases it well the way not produce works it's actually proof against crashed workers anyway so if you grab a lock and you crash halfway through your MapReduce job so what the next person who gets the lock you know because your lock will be released when you crash the next version who gets it will see you didn't finish the task and just we execute it and it's just not a problem because of the way MapReduce is defined so you could use these locks or some kind of soft lock thing although anyway and you know maybe the other thing which we should be thinking about is that some version of this be used to do things like elect a master but if what we're really doing here is electing a master you know we could use code much like this and that would probably be a reasonable approach yeah oh yeah yeah yeah so the picking of paper talk that remember the text in the paper were says it's going to delete the ready file and then do a bunch of updates to files and then recreate the ready file that would that is a fantastic way of sort of detecting and coping with the possibility that the previous lock held or the previous master or whoever it is crashed halfway through because gosh the ready file has never be created Inigo program yeah sadly that is possible and you know either okay so the question is nothing about zookeeper but if you're writing threaded code and go a thread acquires a lock could it crash while holding the lock halfway through whatever stuff it's supposed to be doing while holding a lock and the answer is yes actually there are there are ways for an individual thread to crash and go oh I forget where they are maybe divide by zero certain panics anyway you can do it and my advice about how to think about that is that the program is now broken and you've got to kill it because in threaded code the way the thing about locks is that while the lock is held the invariants in the data don't hold so there's no way to proceed if the lock holder crashes there's no safe way to proceed because all you know is whatever the invariants were that the lock was protecting no longer hold so and so and if you do want to proceed you have to leave the lock marked as held so that no one else will ever be able to acquire it and you know unless you have some clever idea that's pretty much the way you have to think about it in a threaded program because that's kind of the style with which people write threaded lock programs if you're super clever you could play the same kinds of tricks like this ready flag trick now it's super hard and go because the memory model says there is nothing you can count on except if there's a happens before relationship so if you play this game of writing changing some variables and then setting a done flag that doesn't mean anything unless you release a lock and somebody else acquires a lock and only then can anything be said about the order in which or in even whether the updates happen so this is very very hard it rivairy hard and go to recover from a crash of a thread that holds the lock here is maybe a little more possible okay okay okay that's all I want to talk about with zoo keeper it's just two pieces of high bid one is at these clever ideas for high performance by reading from any replica but the they sacrifice a bit of consistency and the other interesting thing uninteresting take-home is that they worked out this API that really does let them be a general-purpose sort of coordination service in a way that simpler schemes like put get interfaces just can't do so they worked out a set of functions here that allows you to do things like write mini transactions and build your own locks and it all works out although requires care okay now I want to turn to today's paper which is crack the the reason why we're reading a crack paper it's a couple reasons one is is that it's it does replication for fault tolerance and as we'll see the properties you get out of crack or its predecessor chain replication are very different in interesting ways from the properties you get out of a system like raft and so I'm actually going to talk about so crack is sort of an optimization to an older scheme called chain replication chain replications actually fairly frequently used in the real world there's a bunch of systems that use it crack is an optimization to it that actually does a similar trick - zookeeper where it's trying to increase weed throughput by allowing reads to two replicas to any replicas so that you get you know number of replicas factor of increase in the read performance the interesting thing about crack is that it does that while preserving linearise ability unlike zookeeper which you know it seemed like in order to be able to read from any replica they had to sacrifice freshness and therefore snot linearizable crack actually manages to do these reads from any replica while preserving strong consistency I'm just pretty interesting okay so first I want to talk about the older system chain replication teen replication is a it's just a scheme for you have multiple copies you want to make sure they all seen the same sequence of right so it's like a very familiar basic idea but it's a different topology then raft so the idea is that there's a chain of servers and chain replication and the first one is called the head last one's called the tail when a right comes in when a client wants to write something say some client it sends always Albright's get sent to the head the head updates its or replaces its current copy of the data that the clients writing so you can imagine be go put key value store so you know if everybody started out with you know version a of the data and under chain replication when the head process is the right and maybe we're writing value B you know the head just replaces its a with a B and passes the right down the chain as each node sees the right it replaces over writes its copy the data the new data when the right gets the tail the tail sends the reply back to the client saying we completed your right that's how rights work reads if a client wants to do a read it sends the read to the tail the read request of the tail and the tail just answers out of its current state so if we ask for this whatever this object was the tail which is I hope current values be weeds are a good deal simpler okay so it should think for a moment like why to chain chain replication so this is not crack just to be clear this is chain replication chain replication is linearizable you know in the absence of failures what's going on is that we can essentially view it as really than the purposes of thinking about consistency it's just this one server the server sees all the rights and it sees all the reads and process them one at a time and you know a read will just see the latest value that's written and that's pretty much all there is to it from the point of view look if there's no crashes what the consistency is like pretty simple the failure recovery the a lot of the rationale behind chain replication is that the set of states you can see when after there's a failure is relatively constrained because of this very regular pattern with how the writes get propagated and at a high level what's going on is that any committed write that is any rate that could have been acknowledged to a client to the writing client or any rate that could have been exposed in a read that'll neither of those will ever happen unless that write reached the tail in order for it to reach the tail it had to a pass through them in process by every single node in the chain so we know that if we ever exposed to write ever acknowledged write ever use it to a read that means every single node in the tail must know about that right we don't get these situations like if you'll call figure seven figure eight and RAF paper where you can have just hair-raising complexity and how the different replicas differ if there's a crash here you know either that it is committed or it before the crash should reach some point and nowhere after that point because the progress of rights has always menu so committed rights are always known everywhere if a right isn't committed that means that before whatever crash it was that disturb the system the rate of got into a certain point everywhere before that point and nowhere after point there's really the only two setups and at a high level failure recovery is relatively simple also if the head fails then to a first approximation the next node can simply take over his head and nothing else needs to get done because any rate that made it as far as the second node while it was the head that failed so that right will keep on going and we'll commit if there's a rate that made it to the head before a crash but the head didn't forward it well that's definitely not committed nobody knows about it and we definitely didn't send it an acknowledgment to the writing client because the write didn't get down here so we're not obliged to do anything about a write it only reached a crashed head before it failed I may be the client where we sinned but you know not our problem if the tale fails it's actually very similar the tale fails the next node can directly take over because everything the tale knew then next the node just before it also knows because the tale only hears things from the node just before it and it's a little bit complex of an intermediate node fails but basically what needs to be done is we need to drop it from the chain and now there may be rights that it had received that the next node hasn't received yet and so if we drop a note out of the chain the predecessor may need to resend recent rights to the to its new successor right that's the recovery in a nutshell that's for why this construction why this instead of something else like why this verse is wrapped for example the performance reason is that in raft if you recall we you know if we have a leader and a bunch of you know some number of replicas right with the leader it's not in a chain we got these the replicas are all directly fed by the leader so if a client right comes in or a client read for that matter the the leader has to send it itself to each of the replicas whereas in chain replication the leader on the head only has to do once and these cents on the network are actually reasonably expensive and so that means the load on a raft leader is going to be higher than the load on a chain replication leader and so that means that you know as the number of client requests per second that you're getting from clients goes up a raft leader will hit a limit and stop being able to get faster sooner than a chain replication head because it's doing more work than the chain replication had another interesting difference between chain replication and raft is that the reeds in raft are all also required to be processed by the leaders the leader sees every single request from clients where's here the head sees everybody sees all the rights but only a tail sees the reed requests so there may be an extent to which the load is sort of split between the head and the tail rather than concentrated in the leader and and as I mentioned before the failure different sort of analysis required to think about different failure scenarios is a good deal simpler and chain replication than it is and raft and as a big motivation because it's hard to get this stuff correct yes yeah so if the tale fails but its predecessor had seen a right that the tale hadn't seen then the failure of that Hale basically commits that right is now committed because it's reached the new tale and so he could respond to the client it probably won't because it you know it wasn't a tail when it received the right and so the client may resend the right and that's too bad and so we need duplicate suppression probably at the head basically all the systems were talking about require in addition to everything else suppression of duplicate client requests yes pink psyche setting in you want to know who makes the decisions about how to that's a outstanding question the question is or rephrase the question a bit if there's a failure like or suppose the second node stops being able to talk to the head can this second node just take over can it decide for itself gosh the head seems to thought away I'm gonna take over his head and tell clients to talk to me instead of the old head but what do you think that's not like a plan with the usual assumptions we make about how the network behaves that's a recipe for split brain right if you do exactly what I said because of course what really happened was that look the network failed here the head is totally alive and the head thinks its successor has died you know the successors actually alive it thinks the head has died and they both say well gosh that other server seems to have died I'm gonna take over and the head is gonna say oh I'll just be a sole replica and I you know act as the head and the tail because the rest of the change seems to have gone away and second I'll do the same thing and now we have two independent split brain versions of the data which will gradually get out of sync so this construction is not proof against network partition and has not does not have a defense against split brain and what that means in practice is if it cannot be used by itself it's like a helpful thing to have in our back pocket but it's not a complete replication story so it's it's very commonly used but it's used in this stylized way in which there's always an external Authority you know not not this chain that decides who's that sort of makes a call on who's alive and who's dead and make sure everybody agrees on a single story about who constitutes the change there's never any disagreement some people think the change is this no and some people think the chain is this other node so what's that's usually called as a configuration manager and its job is just a monitor aliveness and every time it sees of all the servers every time Isis every time the configuration manager thinks the server's dead it sends out a new configuration in which you know that this chain has a new definition had whatever tail and that's server that the configuration manager thinks is that may or may not be dead but we don't care because everybody is required to follow then your configuration and so there can't be any disagreement because there's only one party making these decisions not going to disagree with itself of course how do you make a service that's fault tolerant and doesn't disagree with itself but doesn't suffer from split brain if there's network partitions and the answer to that is that the configuration manager usually uses wrath or paxos or in the case of crack zookeeper which itself of course is built on a raft like scheme so so you to the usual complete set up in your data center is it you have a configuration manager it's it's based on or after PACs or whatever so it's fault tolerant and does not suffer from split brain and then you split up your data over a bunch of change if you know room with a thousand servers in it and you have you know chain a you know it's these servers or the configuration manager decides that the change should look like chain a is made of server one server to server three chain be you know server for server 5 over 6 whatever and it tells everybody this whole list it's all the clients know all the servers know and the individual servers opinions about whether other servers are alive or dead are totally neither here nor there if this server really does die then then the head is required to keep trying indefinitely until I guess a new configuration from the configuration manager not allowed to make decisions about who's alive and who's dead what's that oh boy you've got a serious problem so that's why you replicated using raft make sure the different replicas are on different power supplies the whole works but this this construction I've set up here it's extremely common and it's how chain replication is intended to be used how cracks intend to be used and the logic of it is that like chain require replication if you don't have to worry about partition and split brain you can build very high speed efficient replication systems using chain replication for example so these individual you know data replication and we're sharding the data over many chains individually this these chains can be built to be just the most efficient scheme for the particular kind of thing that you're replicating you may read heavy right heavy whatever but we don't have to worry too much about partitions and then all that worry is concentrated in the reliable non split-brain configuration manager okay so your question is why are we using chain replication here instead of raft okay so it's like a totally reasonable question um the the it doesn't really matter for this construction because even if we're using raft here we still need one party to make a decision with which there can be no disagreement about how the data is divided over our hundred different replication groups right so all you know and I need kind of big system you're splitting your sharding or splitting up the data somebody needs to decide how the data is assigned to the different replication groups this has to change over time as you get more or less Hardware more data or whatever so if nothing else the configuration manager is saying well look you know the keys start with a or B goes here or then C or D goes here even if you use Paxos here now there's also this smaller question if we didn't eat you know what should we use for replication should be chain replication or paxos or raft or whatever and people do different things some people do actually use Paxos based replication like spanner which I think we're gonna look at later in the semester has this structure but it actually uses Paxos to replicate rights for the data you know the reason why you might not want to use PAC so so raft is that it's arguably more efficient to use this chain construction because it reduces the load on the leader and that may or may not be a critical issue the a reason to favor rafter Paxos is that they do not have to wait for a lagging replica this chain replication has a performance problem that if one of these replicas is slow because even for a moment you know because every rate has to go through every replica even a single slow replica slows down all offer all right operations and I can be very damaging you know if you have thousands of servers probably did any given time you know seven of them are out to lunch or unreliable or slow because somebody's installing new software who knows what and that so it's a bit damaging to have every request be sort of limited by the slowest server whereas brafton paxos well it's so rad for example if one of the followers is so it doesn't matter because that leader only has to wait for a majority it doesn't have to wait for all of them you know ultimately they all have to catch up but raft is much better resisting transient slowdown and some Paxos based systems although not really raft are also good at dealing with the possibility that the replicas are in different data centers and maybe far from each other and because you only need a majority you don't have to necessarily wait for acknowledgments from a distant data center and so that can also leads people to use paxos raft like majority schemes rather than chain replication but this is sort of a it depends very much on your workload and what you're trying to achieve but this overall architecture is in I don't know if it's Universal but it's extremely common like intentional topologies okay the for a for a network that's not broken the usual assumption is that all the computers can talk to each other through the network for networks that are broken because somebody stepped on a cable or some routers misconfigured any crazy thing can happen so absolutely due to miss configuration you can get a situation where you know these two nodes can talk to the configuration manager and the configuration managers think sir they're up but they can't talk to each other so yes and and that's a killer for this right because other configuration manager thinks that are up they can't talk to each other boy it's just like it's a disaster and if you need your system to be resistant to that then you need to have a more careful configuration manager you need logic in the configuration manager that says gosh I'm only gonna form a chain out of these services not only I can talk to that but they can talk to each other and sort of explicitly check and I don't know if that's common I mean I'm gonna guess not but if you were super careful you'd want to because even though we talked about network partition that's like a abstraction and in reality you can get any combination of who can talk to who else and some are may be very damaging okay I'm gonna wrap up and see you next week