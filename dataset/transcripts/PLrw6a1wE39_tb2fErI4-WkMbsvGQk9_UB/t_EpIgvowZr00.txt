I'd like to get started today we're gonna talk about GFS the Google file system paper we read for today and this will be the first of a number of different sort of case studies we'll talk about in this course about how to be build big storage systems so the larger topic is big storage the reason is the storage is turned out to be a key abstraction you might you know if you didn't know already you might imagine that there could be all kinds of different you know important abstractions you might want to use for distributed systems but it's turned out that a simple storage interface is just incredibly useful and extremely general and so a lot of the thought that's gone into building distributed systems has either gone into designing storage systems or designing other systems that assume underneath them some sort of reasonably well behaved big just distributed storage system so we're going to care a lot about how the you know how to design a good interface to a big storage system and how to design the innards of the storage system so it has good behavior you know of course that's why we're reading this paper just to get a start on that the this paper also touches on a lot of themes that will come up a lot in a tube for parallel performance fault tolerance replication and consistency and this paper is as such things go reasonably straightforward and easy to understand it's also a good systems paper it sort of talks about issues all the way from the hardware to the software that ultimately uses the system and it's a successful real world design so it says you know academic paper published in an academic conference but it describes something that really was successful and used for a long time in the real world so we sort of know that we're talking about something that is it's a good a good useful design okay so before I'm gonna talk about GFS I want to sort of talk about the space of distributed storage systems a little bit set the scene so first why is it hard it's actually a lot to get right but for a 2/4 there's a particular sort of narrative that's gonna come up quite a lot for many systems often the starting point for people designing these sort of big distributed systems or big storage systems is they want to get huge aggregate performance be able to harness the resources of hundreds of machines in order to get a huge amount of work done so the sort of starting point is often performance and you know if you start there a natural next thought is well we're gonna split our data over a huge number of servers in order to be able to read many servers in parallel so we're gonna get and that's often called sharding if you shard over many servers hundreds or thousands of servers you're just gonna see constant faults right if you have thousands of servers there's just always gonna be one down so we defaults are just every day every hour occurrences and we need automatic weekend of humans involved and fixing this fault we need automatic fault-tolerant systems so that leads to fault tolerance the among the most powerful ways to get fault tolerance is with replication just keep two or three or whatever copies of data one of them fails you can use another one so we want to have tolerance that leads to replication if you have replication two copies the data then you know for sure if you're not careful they're gonna get out of sync and so what you thought was two replicas of the data where you could use either one interchangeably to tolerate faults if you're not careful what you end up with is two almost identical replicas of the data that's like not exactly replicas at all and what you get back depends on which one you talk to so that's starting to maybe look a little bit tricky for applications to use so if we have replication we risk weird inconsistencies of course clever design you can get rid of inconsistency and make the data look very well-behaved but if you do that it almost always requires extra work and extra sort of chitchat between all the different servers and clients in the network that reduces performance so if you want consistency you pay for with low performance I which is of course not what we originally hoping for of course this is an absolute you can build very high performance systems but nevertheless there's this sort of inevitable way that the design of these systems play out and it results in a tension between the original goals of performance and the sort of realization that if you want good consistency you're gonna pay for it and if you don't want to pay for it then you have to suffer with sort of anomalous behavior sometimes I'm putting this up because we're gonna see this this loop many times for many of the systems we look we look at people are we're rarely willing to or happy about paying the full cost of very good consistency ok so you know with brought a consistency I'll talk more later in the course about more exactly what I mean by good consistency but you can think of strong consistency or good consistency as being we want to build a system whose behavior to applications or clients looks just like you'd expect from talking to a single server all right we're gonna build you know systems out of hundreds of machines but a kind of ideal strong consistency model would be what you'd get if there was just one server with one copy of the data doing one thing at a time so this is kind of a strong consistency kind of intuitive way to think about strong consistency so you might think you have one server we'll assume that's a single-threaded server and that it processes requests from clients one at a time and that's important because there may be lots of clients sending concurrently requests into the server and see some current requests it picks one or the other to go first and excuse that request to completion then excuse the nets so for storage servers or you know the server's got a disk on it and what it means to process a request is it's a write request you know which might be writing an item or may be increment and I mean incrementing an item if it's a mutation then we're gonna go and we have some table of data and you know maybe index by keys and values and we're gonna update this table and if the request comes in and to read we're just gonna you know pull the write data out of the table one of the rules here that sort of makes this well-behaved is that each is that the server really does execute in our simplified model excuse to request one at a time and that requests see data that reflects all the previous operations in order so if a sequence of writes come in and the server process them in some order then when you read you see the sort of you know value you would expect if those writes that occurred one at a time the behavior this is still not completely straightforward there's some you know there's some things that you have to spend at least a second thinking about so for example if we have a bunch of clients and client one issues a write of value X and wants it to set it to one and at the same time client two issues the right of the same value but wants to set it to a different the same key but wants to set it to a different value right something happens let's say client three reads and get some result or client three after these writes complete reads get some result client four reads X and get some also gets a result so what results should the two clients see yeah well that's a good question so these what I'm assuming here is that client one inclined to launch these requests at the same time so if we were monitoring the network we'd see two requests heading to the server at the same time and then sometime later the server would respond to them so there's actually not enough here to be able to say whether the client would receipt would process the first request first which order there's not enough here to tell which order the server processes them in and of course if it processes this request first then that means or it processes the right with value to second and that means that subsequent reads have to see to where is it the server happened to process this request first and this one's second that means the resulting value better be one and these these two requests and see what so I'm just putting this up to sort of illustrate that even in a simple system there's ambiguity you can't necessarily tell from trace of what went into the server or what should come out all of you can tell is that some set of results is consistent or not consistent with a possible execution so certainly there's some completely wrong results we can see go by it you know if client 3 sees a 2 then client 4 I bet had better see it too also because our model is well after the second right you know climb trees these are two that means this right must have been second and it still had better be it still has to have been the second right one client 4 goes to the date so hopefully all this is just completely straightforward and just as expected because it's it's supposed to be the intuitive model of strong consistency ok and so the problem with this of course is that a single server has poor fault tolerance right if it crashes or it's disk dies or something we're left with nothing and so in the real world of distributed systems we actually build replicated systems so and that's where all the problems start leaking in is when we have a second copying data so here is what must be close to the worst replication design and I'm doing this to warn you of the problems that we will then be looking for in GFS all right so here's a bad replication design we're gonna have two servers now each with a complete copy of the data and so on disks that are both gonna have this this table of keys and values the intuition of course is that we want to keep these tables we hope to keep these tables identical so that if one server fails we can read or write from the other server and so that means that somehow every write must be processed by both servers and reads have to be able to be processed by a single server otherwise it's not fault tolerant all right if reads have to consult both and we can't survive the loss of one of the servers okay so the problem is gonna come up well I suppose we have client 1 and client 2 and they both want to do these right say one of them gonna write one and the other is going to write two so client 1 is gonna launch it's right x1 2 both because we want to update both of them and climb 2 is gonna launch it's write X so what's gonna go wrong here yeah yeah we haven't done anything here to ensure that the two servers process the two requests in the same order right that's a bad design so if server 1 processes client ones request first it'll end up it'll start with a value of 1 and then it'll see client twos request and overwrite that with 2 if server 2 just happens to receive the packets over the network in a different order it's going to execute client 2's requests and set the value to 2 and then then it will see client ones request set the value to 1 and now what a client a later reading client sees you know if client 3 happens to reach from this server and client for happens to reach from the other server then we get into this terrible situation where they're gonna read different values even though our intuitive model of a correct service says they both subsequent reads hefty you're the same value and this can arise in other ways you know suppose we try to fix this by making the clients always read from server one if it's up and otherwise server two if we do that then if this situation happened and four why oh yeah both everybody reads might see client might see value too but a server one suddenly fails then even though there was no right suddenly the value for X we'll switch from 2 to 1 because if server 1 died it's all the clients assistant server 2 no but just this mysterious change in the data that doesn't correspond to any right which is also totally not something that could have happened in this service simple server model all right so of course this can be fixed the fix requires more communication usually between the servers or somewhere more complexity and because of the cost of inevitable cost to the complexity to get strong consistency there's a whole range of different solutions to get better consistency and a whole range of what people feel is an acceptable level of consistency in an acceptable sort of a set of anomalous behaviors that might be revealed all right any questions about this disastrous model here okay that's what you're talking about GFS a lot of thought about doing GFS was doing is fixing this they had better but not perfect behavior okay so where GFS came from in 2003 quite a while ago actually at that time the the web you know was certainly starting to be a very big deal and people are building big websites in addition there had been decades of research into distributed systems and people sort of knew at least at the academic level how to build all kinds of highly parallel fault tolerant whatever systems but there been very little use of academic ideas in industry but starting at around the time this paper was published big websites like Google started to actually build serious distributed systems and it was like very exciting for people like me who were I'm a kid I'm excited this to see see real uses of these ideas where Google was coming from was you know they had some vast vast data sets far larger than could be stored in a single disk like an entire crawl copy of the web or a little bit after this paper they had giant YouTube videos they had things like the intermedia files for building a search index they also apparently kept enormous log files from all their web servers so they could later analyze them so they had some big big data sets they used both to store them and many many disks to store them and they needed to be able to process them quickly with things like MapReduce so they needed high speed parallel access to these vast amounts of data okay so what they were looking for one goal was just that the thing be big and fast they also wanted a file system that was sort of global in the sense that many different applications could get at it one way to build a big storage system is to you know you have some particular application or mining you build storage sort of dedicated and tailored to that application and if somebody else in the next office needs big storage well they can build their own thing right but if you have a universal or kind of global reusable storage system and that means that if I store a huge amount of data si you know I'm crawling the web and you want to look at my crawled web web pages because we're all using we're all playing in the same sandbox we're all using the same storage system you can just read my files you know maybe access controls permitting so the idea was to build a sort of file system where anybody you know anybody inside Google could name and read any of the files to allow sharing in order to get a in order to get bigness and fastness they need to split the data through every file will be automatically split by GFS over many servers so that writes and reads would just automatically be fast as long as you were reading from lots and lots of reading a file from lots of clients you get high aggregate throughput and also be able to for a single file be able to have single files that were bigger than any single disk because we're building something out of hundreds of servers we want automatic feel your recovery we don't want to build a system where every time one of our hundreds of servers a fail some human being has to go to the machine room and do something with the server or to get it up and running or transfers data or something well this isn't just fix itself um there were some sort of non goals like one is that GFS was designed to run in a single data center so we're not talking about placing replicas all over the world a single GFS installation just lived in one one data center one big machine run so getting this style system to work where the replicas are far distant from each other is a valuable goal but difficult so single data centers this is not a service to customers GFS was for internal use by applications written by Google engineers so it wasn't they weren't directly selling this they might be selling services they used GFS internally but they weren't selling it directly so it's just for internal use and it was tailored in a number of ways for big sequential file reads and writes there's a whole nother domain like a system of storage systems that are optimized for small pieces of data like a bank that's holding bank balances probably wants a database that can read and write an update you know 100 byte records that hold people's bank balances but GFS is not that system so it's really for big or big is you know terabytes gigabytes some big sequential not random access it's also that has a certain batch flavor there's not a huge amount of effort to make access be very low latency the focus is really on throughput of big you know multi megabyte operations this paper was published at s OSP in 2003 the top systems academic conference yeah usually the standard for papers such conferences they have you know a lot of very novel research this paper was not necessarily in that class the specific ideas in this paper none of them are particularly new at the time and things like distribution and sharding and fault tolerance were you know well understood had to had to deliver those but this paper described a system that was really operating in in use at a far far larger scale hundreds of thousands of machines much bigger than any you know academics ever built the fact that it was used in industry and reflected real world experience of like what actually didn't didn't work for deployed systems that had to work and had to be cost effective also like extremely valuable the paper sort of proposed a fairly heretical view that it was okay for the storage system to have pretty consistency we the academic mindset at that time was the you know the storage system really should have good behavior like what's the point of building systems that sort of return the wrong data like my terrible replication system like why do that why not build systems return the right data correct data instead of incorrect data now with this paper actually does not guarantee return correct data and you know the hope is that they take advantage of that in order to get better performance I'm a final thing that was sort of interesting about this paper is its use of a single master in a sort of academic paper you probably have some fault-tolerant replicated automatic failure recovering master perhaps many masters with the work split open um but this paper said look you know you they can get away with a single master and it worked fine well cynically you know who's going to notice on the web that some vote count or something is wrong or if you do a search on a search engine now you're gonna know that oh you know like one of 20,000 items is missing from the search results or they're in the wrong order probably not so there was just much more tolerance in these kind of systems than there would like in a bank for incorrect data it doesn't mean that all data and websites can be wrong like if you're charging people for ad impressions you better get the numbers right but this is not really about that in addition some of the ways in which GFS could serve up odd data could be compensated for in the applications like where the paper says you know applications should accompany their data with check sums and clearly mark record boundaries that's so the applications can recover from GFS serving them maybe not quite the right data all right so the general structure and this is just figure one in the paper so we have a bunch of clients hundreds hundreds of clients we have one master although there might be replicas of the master the master keeps the mapping from file names to where to find the data basically although there's really two tables so and then there's a bunch of chunk servers maybe hundreds of chunk servers each with perhaps one or two discs the separation here's the master is all about naming and knowing where the chunks are and the chunk servers store the actual data this is like a nice aspect of the design that these two concerns are almost completely separated from each other and can be designed just separately with separate properties the master knows about all the files for every file the master keeps track of a list of chunks chunk identifiers that contain the successive pieces that file each chunk is 64 megabytes so if I have a you know gigabyte file the master is gonna know that maybe the first chunk is stored here and the second chunk is stored here the third chunk is stored here and if I want to read whatever part of the file I need to ask the master oh which server hole is that chunk and I go talk to that server and read the chunk roughly speaking all right so more precisely we need to turns out if we're going to talk about how the system about the consistency of the system and how it deals with false we need to know what the master is actually storing in a little bit more detail so the master data it's got two main tables that we care about it's got one table that map's file name to an array of chunk IDs or chunk handles this just tells you where to find the data or what the what the identifiers are the chunks are so it's not much yet you can do with a chunk identifier but the master also happens to have a a second table that map's chunk handles each chunk handle to a bunch of data about that chunk so one is the list of chunk servers that hold replicas of that data each chunk is stored on more than one chunk server so it's a list chunk servers every chunk has a current version number so this master has a remembers the version number for each chunk all rights for a chunk have to be sequence ooh the chunks primary it's one of the replicas so master remembers the rich chunk servers the primary and there's also that primary is only allowed to be primary for a certain least time so the master remembers the expiration time of the lease this stuff so far it's all in RAM and the master so just be gone if the master crashed so in order that you'd be able to reboot the master and not forget everything about the file system the master actually stores all of this data on disk as well as in memory so reads just come from memory but writes to at least the parts of this data that had to be reflected on this writes have to go to the disk so and the way it actually managed that is that there's all the master has a log on disk and every time it changes the data it appends an entry to the log on disk and checkpoint so some of this stuff actually needs to be on disk and some doesn't it turns out I'm guessing a little bit here but certainly the array of chunk handles has to be on disk and so I'm gonna write env here for non-volatile meaning it it's got to be reflected on disk the list of chunk servers it turns out doesn't because the master if it reboots talks to all the chunk servers and ask them what chunks they have so this is I imagine not written to disk the version number any guesses written to disk not written to disk requires knowing how the system works I'm gonna vote written to disk non-volatile we can argue about that later when we talk about how system works identity the primary it turns out not almost certainly not written to disk so volatile and the reason is the master is um reboots and forgets therefore since it's volatile forgets who the primary is for a chunk it can simply wait for the 62nd lease expiration time and then it knows that absolutely no primary will be functioning for this chunk and then it can designate a different primary safely and similarly the lease expiration stuff is volatile so that means that whenever a file is extended with a new chunk goes to the next 64 megabyte boundary or the version number changes because the new primary is designated that means that the master has to first append a little record to his log basically saying oh I just added a such-and-such a chunk to this file or I just changed the version number so every time I change is one of those that needs to writes right it's disk so this is paper doesn't talk about this much but you know there's limits the rate at which the master can change things because you can only write your disk however many times per second and the reason for using a log rather than a database you know some sort of b-tree or hash table on disk is that you can append to a log very efficiently because you only need you can take a bunch of recent log records they need to be added and sort of write them all on a single write after a single rotation to whatever the point in the disk is that contains the end of the log file whereas if it were a sort of b-tree reflecting the real structure of this data then you would have to seek to a random place in the disk and do a little right so the log makes a little bit faster to write there to reflect operations on to the disk however if the master crashes and has to reconstruct its state you wouldn't want to have to reread its log file back starting from the beginning of time from when the server was first installed you know a few years ago so in addition the master sometimes checkpoints its complete state to disk which takes some amount of time seconds maybe a minute or something and then when it restarts what it does is goes back to the most recent checkpoint and plays just the portion of a log that sort of starting at the point in time when that check one is created any questions about the master data okay so with that in mind I'm going to lay out the steps in a read and the steps in the right where all this is heading is that I then want to discuss you know for each failure I can think of why does the system or does the system act directly after that failure um but in order to do that we need to understand the data and operations in the data okay so if there's a read the first step is that the client and what a read means that the application has a file name in mind and an offset in the file that it wants to read some data front so it sends the file name and the offset to the master and the master looks up the file name in its file table and then you know each chunk is 64 megabytes who can use the offset divided by 64 megabytes to find which chunk and then it looks at that chunk in its chunk table finds the list of chunk servers that have replicas of that data and returns that list to the client so the first step is so you know the file name and the offset the master and the master sends the chunk handle let's say H and the list of servers so now we have some choice we can ask any one of these servers pick one that's and the paper says that clients try to guess which server is closest to them in the network maybe in the same rack and send the read request to that to that replica the client actually caches cassia's this result so that if it reads that chunk again and indeed the client might read a given chunk in you know one megabyte pieces or 64 kilobyte pieces or something so I may end up reading the same chunk different points successive regions of a chunk many times and so caches which server to talk to you for giving chunks so it doesn't have to keep beating on the master asking the master for the same information over and over now the client talks to one of the chunk servers tells us a chunk handling offset and the chunk servers store these chunks each chunk in a separate Linux file on their hard drive in a ordinary Linux file system and presumably the chunk files are just named by the handle so all the chunk server has to do is go find the file with the right name you know I'll give it that entire chunk and then just read the desired range of bytes out of that file and return the data to the client I hate question about how reads operate can I repeat number one the step one is the application wants to read it a particular file at a particular offset within the file a particular range of bytes in the files and one thousand two two thousand and so it just sends a name of the file and the beginning of the byte range to the master and then the master looks a file name and it's file table to find the chunk that contains that byte range for that file so good [Music] so I don't know the exact details my impression is that the if the application wants to read more than 64 megabytes or even just two bytes but spanning a chunk boundary that the library so the applications linked with a library that sends our pcs to the various servers and that library would notice that the reads spanned a chunk boundary and break it into two separate reads and maybe talk to the master I mean it may be that you could talk to the master once and get two results or something but logically at least it two requests to the master and then requests to two different chunk servers yes well at least initially the client doesn't know for a given file what chunks need what chunks well it can calculate it needs the seventeenth chunk but but then it needs to know what chunk server holds the seventeenth chunk of that file and for that it certainly needs for that it needs to talk to the master okay so all right did I'm not going to make a strong claim about which of them decides that it was the seventeenth chunk in the file but it's the master that finds the identifier of the handle of the seventeenth chunk in the file looks that up in its table and figures out which chunk servers hold that chunk yes how does that or you mean if the if the client asks for a range of bytes that spans a chunk boundary yeah so the the well you know the client will ask that well the clients linked with this library is a GFS library that noticed how to take read requests apart and put them back together and so that library would talk to the master and the master would tell it well well you know chunk seven is on this server and chunk eight is on that server and then why the library would just be able to say oh you know I need the last couple bites of chunk seven and the first couple bites of chunk eight and then would fetch those put them together in a buffer and return them to the calling application well the master tells it about chunks and the library kind of figures out where it should look in a given chunk to find the date of the application wanded the application only thinks in terms of file names and sort of just offsets in the entire file in the library and the master conspire to turn that into chunks yeah sorry let me get closer here you see again so the question is does it matter which chunk server you reach room so you know yes and no notionally they're all supposed to be replicas in fact as you may have noticed or as we'll talk about they're not you know they're not necessarily identical and applications are supposed to be able to tolerate this but the fact is that you make a slightly different data depending on which replicas you read yeah so the paper says that clients try to read from the chunk server that's in the same rack or on the same switch or something all right so that's reads the rights are more complex and interesting now the application interface for rights is pretty similar there's just some call some library you call to mate you make to the gfs client library saying look here's a file name and a range of bytes I'd like to write and the buffer of data that I'd like you to write to that that range actually let me let me backpedal I only want to talk about record of pens and so I'm going to praise this the client interface as the client makes a library call that says here's a file name and I'd like to append this buffer of bytes to the file I said this is the record of pens that the paper talks about so again the client asks the master look I want to append sends a master requesting what I would like to pen to this named file please tell me where to look for the last chunk in the file because the client may not know how long the file is if lots of clients are opinion to the same file because we have some big file this logging stuff from a lot of different clients may be you know no client will necessarily know how long the file is and therefore which offset or which chunk it should be appending to so you can ask the master please tell me about the the server's that hold the very last chunk current chunk in this file so unfortunately now the writing if you're reading you can read from any up-to-date replica for writing though there needs to be a primary so at this point on the file may or may not have a primary already designated by the master so we need to consider the case of if there's no primary already and all the master knows well there's no primary so so one case is no primary in that case the master needs to find out the set of chunk servers that have the most up-to-date copy of the chunk because know if you've been running the system for a long time due to failures or whatever there may be chunk servers out there that have old copies of the chunk from you know yesterday or last week that I've been kept up to kept up to date because maybe that server was dead for a couple days and wasn't receiving updates so there's you need to be able to tell the difference between up-to-date copies of the chunk and non up-to-date so the first step is to find you know find up-to-date this is all happening in the master because the client has asked the master told the master look I want up end of this file please tell me what chunk service to talk to so a part of the master trying to figure out what chunk servers the client should talk to you so when we finally find up-to-date replicas and what update means is a replica whose version of the chunk is equal to the version number that the master knows is the most up-to-date version number it's the master that hands out these version numbers the master remembers that oh for this particular chunk you know the trunk server is only up to date if it has version number 17 and this is why it has to be non-volatile stored on disk because if if it was lost in a crash and there were chunk servers holding stale copies of chunks the master wouldn't be able to distinguish between chunk servers holding stale copies of a chunk from last week and a chunk server that holds the copy of the chunk that was up-to-date as of the crash that's why the master members of version number on disk yeah if you knew you were talking to all the chunk servers okay so the observation is the master has to talk to the chunk servers anyway if it reboots in order to find which chunk server holds which chunk because the master doesn't remember that so you might think that you could just take the maximum you could just talk to the chunk servers find out what trunks and versions they hold and take the maximum for a given chunk overall the responding chunk servers and that would work if all the chunk servers holding a chunk responded but the risk is that at the time the master reboots maybe some of the chunk servers are offline or disconnected or whatever themselves rebooting and don't respond and so all the master gets back is responses from chunk servers that have last week's copies of the block and the chunk servers that have the current copy haven't finished rebooting or offline or something so ok oh yes if if the server's holding the most recent copy are permanently dead if you've lost all copies all of the most recent version of a chunk then yes No okay so the question is the master knows that for this chunk is looking for version 17 supposing it finds no chunk server you know and it talks to the chunk servers periodically to sort of ask them what chunks do you have what versions you have supposing it finds no server with chunk 17 with version 17 for this this chunk then the master will either say well either not respond yet and wait or it will tell the client look I can't answer that try again later and this would come up like there was a power failure in the building and all the server's crashed and we're slowly rebooting the master might come up first and you know some fraction of the chunk servers might be up and other ones would reboot five minutes from now but so we ask to be prepared to wait and it will wait forever because you don't want to use a stale version of that of a chunk okay so the master needs to assemble the list of chunk servers that have the most recent version the master knows the most recent versions stored on disk each chunk server along with each chunk as you pointed out also remembers the version number of the chunk that it's stores so that when chunk slivers reported into the master saying look I have this chunk the master can ignore the ones whose version does not match the version the master knows is the most recent okay so remember we were the client want to append the master doesn't have a primary it figures out maybe you have to wait for the set of chunk servers that have the most recent version of that chunk it picks a primary so I'm gonna pick one of them to be the primary and the others to be secondary servers among the replicas set at the most recent version the master then increments the version number and writes that to disk so it doesn't forget it the crashes and then it sends the primary in the secondaries and that's each of them a message saying look for this chunk here's the primary here's the secondaries you know recipient maybe one of them and here's the new version number so then it tells primary secondaries this information plus the version number the primaries and secondaries alright the version number to disk so they don't forget because you know if there's a power failure or whatever they have to report in to the master with the actual version number they hold yes that's a great question so I don't know there's hints in the paper that I'm slightly wrong about this so the paper says I think your question was explaining something to me about the paper the paper says if the master reboots and talks to chunk servers and one of the chunk servers reboot reports a version number that's higher than the version number the master remembers the master assumes that there was a failure while it was assigning a new primary and adopts the new the higher version number that it heard from a chunk server so it must be the case that in order to handle a master crash at this point that the master writes its own version number to disk after telling the primaries there's a bit of a problem here though because if the was that is there an ACK all right so maybe the master tells the primaries and backups and that their primaries and secondaries if they're a primary secondary tells him the new version number waits for the AK and then writes to disk or something unsatisfying about this I don't believe that works because of the possibility that the chunk servers with the most recent version numbers being offline at the time the master reboots we wouldn't want the master the master doesn't know the current version number it'll just accept whatever highest version number adheres which could be an old version number all right so this is a an area of my ignorance I don't really understand whether the master update system version number on this first and then tells the primary secondary or the other way around and I'm not sure it works either way okay but in any case one way or another the master update is version number tells the primary secondary look your primaries and secondaries here's a new version number and so now we have a primary which is able to accept writes all right that's what the primaries job is to take rights from clients and organize applying those rights to the various chunk servers and you know the reason for the version number stuff is so that the master will recognize the which servers have this new you know the master hands out the ability to be primary for some chunk server we want to be able to recognize if the master crashes you know that it was that was the primary that only that primary and it secondaries which were actually processed which were in charge of updating that chunk that only those primaries and secondaries are allowed to be chunk servers in the future and the way the master does this is with this version number logic okay so the master tells the primaries and secondaries that there it they're allowed to modify this block it also gives the primary a lease which basically tells the primary look you're allowed to be primary for the next sixty seconds after sixty Seconds you have to stop and this is part of the machinery for making sure that we don't end up with two primaries I'll talk about a bit later okay so now we were primary now the master tells the client who the primary and the secondary czar and at this point we're we're executing in figure two in the paper the client now knows who the primary secondaries are in some order or another and the paper explains a sort of clever way to manage this in some order or another the client sends a copy of the data it wants to be appended to the primary in all the secondaries and the primary in the secondaries write that data to a temporary location it's not appended to the file yet after they've all said yes we have the data the client sends a message to the primary saying look you know you and all the secondaries have the data I'd like to append it for this file the primary maybe is receiving these requests from lots of different clients concurrently it picks some order execute the client request one at a time and for each client a pen request the primary looks at the offset that's the end of the file the current end of the current chunk makes sure there's enough remaining space in the chunk and then tells then writes the clients record to the end of the current chunk and tells all the secondaries to also write the clients data to the end to the same offset the same offset in their chunks all right so the primary picks an offset all the replicas including the primary are told to write the new appended record at at offset the secondary's they may do it they may not do it I'm either run out of space maybe they crashed maybe the network message was lost from the primary so if a secondary actually wrote the data to its disk at that offset it will reply yes to the primary if the primary collects a yes answer from all of the secondaries so if they all of all of them managed to actually write and reply to the primary saying yes I did it then the primary is going to reply reply success to the client if the primary doesn't get an answer from one of the secondaries or the secondary reply sorry something bad happened I ran out of disk space my disk I don't know what then the primary replies no to the client and the paper says oh if the client gets an error like that back in the primary the client is supposed to reissue the entire append sequence starting again talking to the master to find out the most grease the chunk at the end of the file I want to know the client supposed to reissue the whole record append operation ah you would think but they don't so the question is jeez you know the the primary tells all the replicas to do the append yeah maybe some of them do some of them don't right if some of them don't then we apply an error to the client so the client thinks of the append in happen but those other replicas where the pen succeeded they did append so now we have replicas donor the same data one of them the one that returned in error didn't do the append and the ones they returned yes did do the append so that is just the way GFS works yeah so if a reader then reads this file they depending on what replica they be they may either see the appended record or they may not if the record append but if the record append succeeded if the client got a success message back then that means all of the replicas appended that record at the same offset if the client gets a no back then zero or more of the replicas may have appended the record of that all set and the other ones not so the client got to know then that means that some replicas maybe some replicas have the record and some don't so what you which were roughly read from you know you may or may not see the record yeah oh that all the replicas are the same all the secondaries are the same version number so the version number only changes when the master assigns a new primary which would ordinarily happen and probably only happen if the primary failed so what we're talking about is is replicas that have the fresh version number all right and you can't tell from looking at them that they're missing that the replicas are different but maybe they're different and the justification for this is that yeah you know maybe the replicas don't all have that the appended record but that's the case in which the primary answer no to the clients and the client knows that the write failed and the reasoning behind this is that then the client library will reissue the append so the appended record will show up you know eventually the a pendel succeed you would think because the client I'll keep reissuing it until succeeds and then when it succeeds that means there's gonna be some offset you know farther on in the file where that record actually occurs in all the replicas as well as offsets preceding that word only occurs in a few of the replicas yes oh this is a great question the exact path that the right data takes might be quite important with respect to the underlying network and the paper somewhere says even though when the paper first talks about it he claims that the client sends the data to each replica in fact later on it changes the tune and says the client sends it to only the closest of the replicas and then the replicas then that replica forwards the data to another replica along I sort of chained until all the replicas had the data and that path of that chain is taken to sort of minimize crossing bottleneck inter switch links in a data center yes the version number only gets incremented if the master thinks there's no primary so it's a so in the ordinary sequence there already be a primary for that chunk the the the the master sort of will remember oh gosh there's already a primary and secondary for that chunk and it'll just it won't go through this master selection it won't increment the version number it'll just tell the client look up here's the primary with with no version number change my understanding is that if this is this I think you're asking a you're asking an interesting question so in this scenario in which the primaries isn't answered failure to the client you might think something must be wrong with something and that it should be fixed before you proceed in fact as far as I can tell the paper there's no immediate anything the client retries the append you know because maybe the problem was a network message got lost so there's nothing to repair right you know now we're gonna message got lost we should be transmitted and this is sort of a complicated way of retransmitting the network message maybe that's the most common kind of failure in that case just we don't change anything it's still the same primary same secondaries the client we tries maybe this time it'll work because the network doesn't discard a message it's an interesting question though that if what went wrong here is that one of that there was a serious error or Fault in one of the secondaries what we would like is for the master to reconfigure that set of replicas to drop that secondary that's not working and it would then because it's choosing a new primary in executing this code path the master would then increment the version and then we have a new primary and new working secondaries with a new version and this not-so-great secondary with an old version and a stale copy of the data but because that has an old version the master will never never mistake it for being fresh but there's no evidence in the paper that that happens immediately as far as what's said in the paper the client just retries and hopes it works again later eventually the master will if the secondary is dead eventually the master does ping all the trunk servers will realize that and will probably then change the set of primaries and secondaries and increment the version but only only later the lease the leases that the answer to the question what if the master thinks the primary is dead because it can't reach it right that's supposing we're in a situation where at some point the master said you're the primary and the master was like painting them all the service periodically to see if they're alive because if they're dead and wants to pick a new primary the master sends some pings to you you're the primary and you don't respond right so you would think that at that point where gosh you're not responding to my pings then you might think the master at that point would designate a new primary it turns out that by itself is a mistake and the reason for that the reason why it's a mistake to do that simple did you know use that simple design is that I may be pinging you and the reason why I'm not getting responses is because then there's something wrong with a network between me and you so there's a possibility that you're alive you're the primary you're alive I'm peeing you the network is dropping that packets but you can talk to other clients and you're serving requests from other clients you know and if I if I the master sort of designated a new primary for that chunk now we'd have two primaries processing rights but two different copies of the data and so now we have totally diverging copies the data and that's called that error having two primaries or whatever processing requests without knowing each other it's called squid brain and I'm writing this on board because it's an important idea and it'll come up again and it's caused or it's usually said to be caused by network partition that is some network error in which the master can't talk to the primary but the primary can talk to clients sort of partial network failure and you know these are some of the these are the hardest problems to deal with and building these kind of storage systems okay so that's the problem is we want to rule out the possibility of mistakingly designating too I'm Aries for the same chunk the way the master achieves that is that when it designates a primary it says it gives a primary Elyse which is basically the right to be primary until a certain time the master knows it remembers and knows how long the least lasts and the primary knows how long is least lasts if the lease expires the primary knows that it expires and will simply stop executing client requests it'll ignore or reject client requests after the lease expired and therefore if the master can't talk to the primary and the master would like to designate a new primary the master must wait for the lease to expire for the previous primary so that means master is going to sit on its hands for one lease period 60 seconds after that it's guaranteed the old primary will stop operating its primary and now the master can see if he doesn't need a new primary without producing this terrible split brain situation oh so the question is why is designated a new primary bad since the clients always ask the master first and so the master changes its mind then subsequent clients will direct the clients to the new primary well one reason is that the clients cash for efficiency the clients cash the identity of the primary for at least for short periods of time even if they didn't though the bad sequence is that I'm the prime the master you ask me who the primary is I send you a message saying the primary is server one right and that message is inflate in the network and then I'm the master I you know I think somebody's failed whatever I think that primary is filled I designated a new primary and I send the primary message saying you're the primary and I start answering other clients who ask the primary is saying that that over there is the primary while the message to you is still in flight you receive the message saying the old primaries the primary you think gosh I just got this from the master I'm gonna go talk to that primary and without some much more clever scheme there's no way you could realize that even though you just got this information from the master it's already out of date and if that primary serves your modification requests now we have to and and respond success to you right then we have two conflicting replicas yes again you've a new file and no replicas okay so if you have a new file no replicas or even an existing file and no replicas the you'll take the path I drew on the blackboard the master will receive a request from a client saying oh I'd like to append to this file and then well I guess the master will first see there's no chunks associated with that file and it will just make up a new chunk identifier or perhaps by calling the random number generator and then it'll look in its chunk information table and see gosh I don't have any information about that chunk and it'll make up a new record saying but it must be special case code where it says well I don't know any version number this chunk doesn't exist I'm just gonna make up a new version number one pick a random primary and set of secondaries and tell them look you are responsible for this new empty chunk please get to work the paper says three replicas per chunk by default so typically a primary and two backups okay okay so the maybe the most important thing here is just to repeat the discussion we had a few minutes ago the intentional construction of GFS we had these record a pens is that if we have three we have three replicas you know maybe a client sends in and a record a pen for record a and all three replicas or the primary and both of the secondaries successfully append the data the chunks and maybe the first record in the trunk might be a in that case and they all agree because they all did it supposing another client comes in says look I want a pen record B but the message is lost to one of the replicas the network whatever supposably the message by mistake but the other two replicas get the message and one of them's a primary and my other secondaries they both depend of the file so now what we have is two the replicas that B and the other one doesn't have anything and then may be a third client wants to append C and maybe the remember that this is the primary the primary picks the offset since the primary just gonna tell the secondaries look in a right record C at this point in the chunk they all right C here now the client for be the rule for a client for B that for the client that gets us error back from its request is that it will resend the request so now the client that asked to append record B will ask again to a pen record B and this time maybe there's no network losses and all three replicas as a panel record be right and they're all lives there I'll have the most fresh version number and now if a client reads what they see depends on the track which replicas they look at it's gonna see in total all three of the records but it'll see in different orders depending on which replica reads it'll mean I'll see a B C and then a repeat of B so if it reads this replica it'll see B and then C if it reads this replica it'll see a and then a blank space in the file padding and then C and then B so if you read here you see C then B if you read here you see B and then C so different readers will see different results and maybe the worst situation is it some client gets an error back from the primary because one of the secondaries failed to do the append and then the client dies before we sending the request so then you might get a situation where you have record D showing up in some of the replicas and completely not showing up anywhere in the other replicas so you know under this scheme we have good properties for for appends that the primary sent back a successful answer for and sort of not so great properties for appends where the primary sent back of failure and the records the replicas just absolutely be different all different sets of replicas yes my reading in the paper is that the client starts at the very beginning of the process and asked the master again what's the last chunk in this file you know because it might be might have changed if other people are pending in the file yes so I can't you know I can't read the designers mind so the observation is the system could have been designed to keep the replicas in precise sync it's absolutely true and you will do it in labs 2 & 3 so you guys are going to design a system that does replication that actually keeps the replicas in sync and you'll learn you know there's some various techniques various things you have to do in order to do that and one of them is that there just has to be this rule if you want the replicas to stay in sync it has to be this rule that you can't have these partial operations that are applied to only some and not others and that means that there has to be some mechanism to like where the system even if the client dies where the system says we don't wait a minute there was this operation I haven't finished it yet so you build systems in which the primary actually make sure the backups get every message if the first right abhi failed you think the sea should go with the beers well it doesn't you may think it should but the way the system actually operates is that the primary will add C to the end of the chunk and the after V yeah I mean one reason for this is that at the time the right Percy comes in the primary may not actually know what the fate of B was because we met multiple clients submitting a pen's concurrently and you know for high performance you want the primary to start the append for B first and then as soon as I can got the next stop set tell everybody did you see so that all this stuff happens in parallel you know by slowing it down you could you know the primary could sort of decide that B it totally failed and then send another round of messages saying please undo the right of B and there'll be more complex and slower I'm you know again the the justification for this is that the design is pretty simple it you know it reveals some odd things to applications and the hope was that applications could be relatively easily written to tolerate records being in different orders or who knows what or if they couldn't that applications could either make their own arrangements for picking an order themselves and writing you know sequence numbers in the files or something or you could just have a if application really was very sensitive to order you could just not have concurrent depends from different clients to the same file right you could just you know close files where order is very important like say it's a movie file you know you don't want to scramble bytes in a movie file you just write the Moot file you write the movie to the file by one client in sequential order and not with concurrent record depends okay all right the somebody asked basically what would it take to turn this design into one which actually provided strong consistency consistency closer to our sort of single server model where there's no surprises I don't actually know because you know that requires an entire new complex design it's not clear how to mutate GFS to be that design but I can list for you lists for you some things that you would want to think about if you wanted to upgrade GFS to a assistance did have strong consistency one is that you probably need the primary to detect duplicate requests so that when this second becomes in the primary is aware that oh actually you know we already saw that request earlier and did it or didn't do it and to try to make sure that B doesn't show up twice in the file so one is you're gonna need duplicate detection another issues you probably if a secondary is acting a secondary you really need to design the system so that if the primary tells a secondary to do something the secondary actually does it and doesn't just return error right for a strictly consistent system having the secondaries be able to just sort of blow off primary requests with really no compensation is not okay so I think the secondaries have to accept requests and execute them or if a secondary has some sort of permanent damage like it's disk got unplugged by mistake this you need to have a mechanism to like take the secondary out of the system so the primary can proceed with the remaining secondaries but GFS kind of doesn't either at least not right away and so that also means that when the primary asks secondary's to append something the secondaries have to be careful not to expose that data to readers until the primary is sure that all the secondaries really will be able to execute the append so you might need sort of multiple phases in the rights of first phase in which the primary asks the secondaries look you know I really like you to do this operation can you do it but don't don't actually do it yet and if all the secondaries answer with a promise to be able to do the operation only then the primary says alright everybody go ahead and do that operation you promised and people you know that's the way a lot of real world systems strong consistent systems work and that trick it's called two-phase commit another issue is that if the primary crashes there will have been some last set of operations that the primary had launched started to the secondaries but the primary crashed before it was sure whether those all the secondaries got there copied the operation or not so if the primary crashes you know a new primary one of the secondaries is going to take over as primary but at that point the second the new primary and the remaining secondaries may differ in the last few operations because maybe some of them didn't get the message before the primary crashed and so the new primer has to start by explicitly resynchronizing with the secondaries to make sure that the sort of the tail of their operation histories are the same finally to deal with this problem of oh you know there may be times when the secondaries differ or the client may have a slightly stale indication from the master of which secondary to talk to the system either needs to send all client reads through the primary because only the primary is likely to know which operations have really happened or we need a least system for the secondaries just like we have for the primary so that it's well understood that when secondary Canon can't legally respond a client and so these are the things I'm aware of that would have to be fixed in this system tor added complexity and chitchat to make it have strong consistency and you're actually the way I got that list was by thinking about the labs you're gonna end up doing all the things I just talked about as part of labs two and three to build a strictly consistent system okay so let me spend one minute on there's actually I have a link in the notes to a sort of retrospective interview about how well GFS played out over the first five or ten years of his life at Google so the high-level summary is that the most is that was tremendously successful and many many Google applications used it in a number of Google infrastructure was built as a late like big file for example BigTable I mean was built as a layer on top of GFS and MapReduce also so widely used within Google may be the most serious limitation is that there was a single master and the master had to have a table entry for every file in every chunk and that men does the GFS use grew and they're about more and more files the master just ran out of memory ran out of RAM to store the files and you know you can put more RAM on but there's limits to how much RAM a single machine can have and so that was the most of the most immediate problem people ran into in addition the load on a single master from thousands of clients started to be too much in the master kernel they see if you can only process however many hundreds of requests per second especially the right things to disk and pretty soon there got to be too many clients another problem with a some applications found it hard to deal with this kind of sort of odd semantics and a final problem is that the master that was not an automatic story for master failover in the original in the GFS paper as we read it like required human intervention to deal with a master that had sort of permanently crashed and needs to be replaced and that could take tens of minutes or more I was just too long for failure recovery for some applications okay excellent I'll see you on Thursday and we'll hear more about all these themes over the semester