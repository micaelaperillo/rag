all right hello everyone I'm today going to talk about this paper about how Facebook uses memcache I'm in order to handle enormous load the reason we're reading this paper is that it's an experience paper there's not really any new concepts or ideas or techniques here but it's kind of what a real live company ran into when they were trying to build very high capacity infrastructure there's a couple of ways you could read it one is a sort of cautionary tale about what goes wrong if you don't take consistency seriously from the start another way to read it is that it's an impressive story about how to get extremely high capacity from using mostly off-the-shelf software another way to read it is that it's a kind of illustration or the fundamental struggle that a lot of setups face between trying to get very high performance which you do by things like replication and how to give consistency for which techniques like replication are really the enemy and so you know we can argue about whether we like their design or we think it's elegant or a good solution but we can't really argue with how successful they've been so we do need to take them seriously and for me actually this paper which I first read quite a few years ago it's been I thought about it a lot and it's been sort of a source of sort of ideas and understanding about problems at many points all right so before talking about Facebook proper you know they're an example of a pattern that you see fairly often or that many people have experienced in which they're trying to build a website to do something and you know typically people who build websites are not interested in building high performance you know high performance storage infrastructure they're interested in building features that will make their users happy or selling more advertisements or something you know so they they're not gonna start by spending a main year of effort or a whole lot of time building cool infrastructure they're gonna start by building features in that they'll sort of make infrastructure better only to the extent that they really have to because you know that's the best use of their time alright so a typical starting scenario in a ways when a some website is very small is you know there's no point in starting with anything more than just a single machine right you know maybe you started you only have a couple users sitting in front of their browsers and you know they talk over the internet here with your single machine your single machine is gonna maybe run the Apache web server now maybe you write the scripts that produce web pages using PHP or Python or some other convenient easy to program sort of scripting style language and Facebook uses PHP you need to store your data somewhere or you can just download sort of standard database and Facebook happen to use my sequel my school is a good choice because it implements the sequel query language is very powerful and acid transactions provides durable storage so this is like a very very nice set up I am will take you a long way actually but supposing supposing you get successful you get more and more users you know you're gonna get more and more load more and more people gonna be viewing your website and running whatever PHP stuff you're with site provides and so at some point almost certainly the first thing that's going to go wrong is that the PHP scripts are gonna take up too much CPU time that's usually the first bottleneck people encounter if they start with a single server so what you need is some way to get more horsepower for your PHP scripts and so that takes us to kind of architecture number two for websites in which you know you have lots and lots of users right or more users than before you need more CPU power for your PHP scripts so you run a bunch of front end servers whose only job is to run the web servers that users browsers talk to and these are called front end servers so these are going to run a patch either webserver and the PHP scripts now you know you users are going to talk to different servers at different times maybe your users Quadra each other they message each other they need to see each other's posts or something so all these front-end servers are going to need to see the same back-end data and in order to do that you probably can't just stick at least for a while you can just stick with one database server so you gonna have a single machine already my sequel that handles all of the database all queries and updates reads and writes from the front end servers and if you possibly can it's wise to use a single server here because as soon as you go with two servers and somehow your data over multiple database servers like gets much more complicated and you need to worry about things like do you need distributed transactions or how it has the PHP scripts decide which database server to talk to and so again you can get a long way with this second architecture you have as much CPU power as you like by adding more front end servers and up to a point a single database server will actually be able to absorb the reason rights of many front ends but you know maybe you're very successful you get even more users and so the question is what's gonna go wrong next and typically what goes wrong next is that the database server since you can always add more CPU more web servers you know what inevitably goes wrong is that after a while the database server runs out of steam okay so what's the next architecture this is web architecture 3 and the kind of standard evolution of big websites here we have the same if you know now thousands and thousands of users lots and lots of front ends and now we basically we know we're gonna have to have multiple database servers so now behind the front ends we have a whole rack of database servers each one of them running my sequel again but we're going to shard the data we're driven now to sharding the data over the database server so you know maybe the first one holds keys you know a through G & G through second one holds keys G through Q and you know whatever the charting happens to be and now the front-end you know you have to teach your PHP scripts here to look at the data they need and try to figure out which database server they're going to talk to it you know in different times for different data they're going to talk to different servers so this is sharding and of course the reason why this gives you a boost is that now the all the work of reading and writing has split up hopefully hopefully evenly split up between these servers since they hold different data now replicas rating word charting the data and they can execute in parallel and have big parallel capacity to read and write data it's a little bit painful the PHP code has to know about the sharding if you change the setup of the database servers that you add a new database server or you realize you need to split up the keys differently you know now you need a you're gonna have to modify the software running on the front ends or something in order for them to understand about how to cut over to the new sharding so there's some there's some pain here there's also if you need transactions and you know many people use them if you need transactions but the data involved in a single transaction is on more than one database server you're probably going to need two-phase commit or some other distributed transaction scheme it's also a pain and slow all right well you can you can get fairly far with this arrangement however it's quite expensive my sequel or sort of you know fully featured database servers like people like to use it's not particularly fast it can probably perform a couple hundred thousand reads per second and far fewer rights and you know web sites tend to be read heavy so it's likely that you're gonna run out of steam for reads before writes that traffic will be that we load on the web servers will be dominated by reads and so after a while you know you can slice the data more and more thinly over more and more servers but two things go wrong with that one is that the some sometimes you're you have specific keys that are hot that are used a lot and no amount of slicing really helps there because each key is only on a single server so that keeps very popular that servers can be overloaded no matter how much you partition or shard the data and the other problem with adding was shorting adding lots and lots of my sequel database servers for sharding is that it's really an expensive way to go as it turns out and after a point you're gonna you're going to start to think that well instead of spending a lot of money to add another database server running my sequel I could take the same server run something much faster on it like as it happens memcache D and get a lot more reads per second out of the same Hardware using caching than using databases so the next architecture and this is now starting to resemble what Facebook is using the next architecture still need users we still have a bunch of front end servers running web servers in PHP and by now maybe a vast number of front end servers we still have our database servers because you know we need us a system that will store data safely on disk for us and we'll provide things like transactions for us and so you know probably want a database for that but in between we're gonna have a caching layer that's this is where memcache D comes in and of course there's other things you could use that the memcache but memcache D happens to be an extremely popular caching scheme the idea now is you have a whole bunch of these memcache servers and when a front-end needs to read some data the first thing it does is ask one of the memcache servers look do you have the data I need so it'll send a get request with some key to one of the memcache servers and the memcache server will check it's got just a table in memory it's in fact memcache is extremely simple it's far far simpler than your lab 3 for example it just has just as a big hash table on memory it checks with that keys in the hash table if it is it sends back the data saying oh yeah here's the value I've cashed for that and if we if the front end hits in this memcache server great I can then produce the webpage with that data in it if it misses in the webserver though the front-end has to then rear equesticle irrelevant database server and the database server will say oh you know here's the here's the data you need and at that point in order to cash it in for the next front-end that needs it the front end we'll send a put with the data it fashion the database into that memcache server and because memcache runs at least 10 and maybe maybe more than 10 times faster for weeds than the database for a given amount of hardware it really pays off to use a fair amount some of that hardware for memcache as well as for the database servers so people people use this arrangement a lot and it just saves them money because memcache is so much faster for weeds than a database server still need to send writes to the database because you want right to an updates to be stored durably on the database as this can still be there if there's a crash or something but you can send the Reese to the cache very much more quickly ok so we have a question the question is why wouldn't the memcache server actually hit the put on behalf of the front-end and cache the response before responding the front-end so that's a great question you could imagine a caching layer that you would send a get to it and it would if it missed the memcache layer would would forward the request to the database babies respond the memcache memcache would add the data to its tables and then respond and the reason for this is that memcache is like a completely separate piece of software that it doesn't know anything about databases and it's actually not even necessarily used and combined in conjunction with the database although it often is so we can't bake in knowledge of the database into memcache and sort of deeper reason is that the front ends are often not really storing one for one database records in memcache almost always or very frequently what's going on is that the front-end will issue some requests to the database and then process the results somewhat you know maybe take a few steps to turning it into HTML or sort of collect together you know results from multiple careers on multiple rows in the database and cached partially processed information in memcache just to save the next reader from having to do the same processing and for that reason memcache it doesn't really does not understand the relationship between what the friends would like to see see cached and how did you ride that data from the database that knowledge is really only in the PHP code on the front end so therefore even though we could be architectural a good idea we can't have this integration here sort of direct contact between memcache and the database although it might make the cache consistency story much more straightforward and yes this is it's this and answer the next question that is the difference between a lookaside cash and a look through cash the fact the lookaside business is that the front end sort of looks asides to the cash to see if the data is there and if it's not it makes its own arrangements for getting the deed on amiss you know a look through cash my forward request of the database and directly and handle the response now part of the reason for the popularity in memcache is that it is it is a lookaside cash that is completely neutral about whether there's a database or what's in the database or the relationship between stuff in memcache and what's in the end items in the database all right so this is very popular arrangement very widely used it's cost effective because memcache is so much faster in the database it's a bit complex every website that makes serious you so this faces the problem that if you don't do something the data that's stored in the caches will get out of sync with the data in the database and so everybody has to have a story for how to make sure that when you modify something in the database you do something to memcache to you know take care of the fact that memcache may then be storing stale data that doesn't reflect the updates and a lot of this papers about what Facebook story is for that although other people had other plans this arrangements also potentially a bit fragile it allows you to scale up to far more users then you could have gone with databases alone because memcache is so fast but what that means is that you're gonna end up with the system that's sustaining a load that's far far higher you know orders of magnitude higher than what the databases could handle and thus if anything goes wrong for example if one of your memcache servers were to fail and meaning that the front ends would now have to contact the database because they didn't hit they couldn't use this to store data you're gonna be increasing a load in the databases dramatically right because memcache do you know supposing it has a you know hit rate of 99 percent or whatever it happens to be you know memcache is gonna be absorbing almost all the reads the database backends only going to be seeing a few percent of the total reads so any failure here is gonna increase that few percent of the reads to maybe you know I don't know 50 percent of the reads or whatever which is a huge huge order of magnitude increase so as Facebook does once you've got to rely on this caching layer you need to be set up pretty serious measures to make sure that you never expose the database layer to the full anything like the full load that the caching layer is seeing and you know you see in facebook they have quite a bit of thought put into making sure the databases don't ever see anything like the full load okay so far this is generic now I want to sort of switch to a big picture of what Facebook describes in the paper for their overall architecture of course they have lots of users every user as a friendless and status and posts and likes and photos but Facebook's very easy or e nted towards showing data to users and a super important aspect of that is that fresh data is not absolutely necessary in that circumstance you know suppose the reads are you know due to caching supposed to reads yield data that's a few seconds out of date so you're showing your users data not the very latest data but the data from a few seconds ago you know what the users are extremely unlikely to notice except in special cases right if I'm looking at a news feed of today's you know today's news you know if I see the news from a few times ago versus the news from now a big deal nobody's gonna notice nobody's gonna complain you know that's not always true for all data but for a lot a lot of the data that they have to deal with sort of super up-to-date consistency in the sense of like linearise ability is not actually important what is important is that you don't cache stale data indefinitely you know what they can't do is by mistake have some data that they're showing users that's from yesterday or last week or even an hour ago those users really will start to notice that so they don't care about consistency like second-by-second but they care a lot about not not being in cannot chewing stale data from more than well more than a little while ago the other situation in which they need to provide consistency is if a user updates their own data or if a user updates almost any data and then reads that same data that the human knows that they just updated it's extremely confusing for the user to see stale data if they know they just changed it and so in that specific case the Facebook design is also careful to make sure that if a user changes data that that user will see the change data ok so Facebook has multiple data centers which they call regions and I think at the time this paper was written they had two regions their sort of primary region was on the west coast California and their sort of secondary region was in the East Coast and the two data centers look pretty similar you set of database servers running my sequel the sharted date over these my sequel database servers they had a bunch of memcache D servers which we'll see they are actually arranged in independent clusters and then they had a bunch of front ends again sort of a separate arrangement in each data center and there's a couple reasons for this one is that their customers were scattered all over the country and it's nice just for a performance that people on the East Coast can talk to a nearby data center and people on the west coast can also talk to a nearby deficit it just makes internet delays less now the the data centers were not symmetric each of them held a complete copy of all the data they didn't sort of shard the data across the data centers so the West Coast I think was a primary and it sort of had the real copy of the data and the East Coast was a secondary and what that really means is that all rights had to be sent to the relevant database and the primary day to be Center so you know any right gets sent you know here and they use a feature of my sequel they serve asynchronous log replication scheme to have each database in the primary region send every update to the corresponding database in secondary region so that with a lag of maybe even a few seconds these database servers would have identical content the secondary database servers would have identical content to the primaries reads though we're local so these front ends when they need to find some data I'm in general would talk to memcache memcache in that data center and if they missed in memcache they talked to the they'd read from the database in that same data data center um again though the databases are complete replicas all the data's on both of these these in both of these regions that's the overall picture the next thing I want to talk about is a few details about how they how they use you know with this leukocyte caching actually looks like so there's really there's reads and writes and this is just what's shown in Figure two for a read which is executing on a front-end the first thing if you read any data that might be cached the first thing that code in the front-end does is makes this get library call with the key of the data they want and get just generates an RPC to the relevant memcache server so they hash this library routine hashes on the client hashes the key to pick the memcache server and sends an RPC to that mcat server them casually reply yes here's your data or or maybe it'll point nil saying I don't have that data it's not cached so if if V is nil then the front-end will issue whatever sequel query is required to fetch the data from the database and then make another RPC call - memk2 the relevant memcache server to install the fetch data in the memcache server so this is just the routine I talked through before it's kind of what lookaside caching does and for right you know V is the writing we have a key and a value no Rana right and so library routine on an each front end we're gonna send the the new data to the database and you know I as I mentioned before the Keene the value may be a little bit different you know what's stored in the database is often in a somewhat different form from what's stored in memcache see but we'll imagine for now the same and once the database has the new data then the right library routine sends an RPC to memcache detailing it look you got to delete this key so I want to write the writer is invalidating the key in memcache do you know what that means is that the next front-end that tries to read that key from memcache D is gonna get nil back because it's no longer cached and will fetch the updated value from the database and install it into memcache all right so this is an invalidation in particular it's not you could imagine a scheme that would send the new data to memcache T at this point but it doesn't actually do that instead of gliese it and actually in the context of facebook scheme the real reason why this delete is needed is so that we'll see their own rights because in fact in their scheme the mem cat the my sequel server the database servers also send deletes one of you and the front end writes something in the database the database with the mix squeal mechanism the paper mentions well send the relevant deletes to the memcache servers that that might hold this key so the data the database servers will actually invalidate stuff in memcache by-and-bye may take them a while um but because that might take a while the front ends also delete the key said that a front end won't see a stale value for data that it just updated okay all sort of the background of this is pretty much how everybody uses memcache G there's nothing yet really very special here now eventually you know the paper is all about on the surface all about solving consistency problems and indeed those are important but the reason why they got where they ran into those consistency problems is in large part because they you know modify the design or set up a design that had extremely high performance because they had extremely high load and say they were desperate to get performance and kind of struggled along behind the performance improvements in order to retain a reasonable level of consistency and because the performance kind of came first for them I'm actually going to talk about their performance architecture before talking about how they fix the consistency okay sorry there's been a bunch of questions here that I haven't seen let me take a peek okay so one question this means that the replicated updates from the primary my sequel database to the secondary must also issue deletes - yeah so this is I think a reference to the previous or architecture slide the observation is that yes indeed when a front-end sends a write to the database server today every server updates its data on disk and it will send an invalidate a delete to whatever memcache server there is in the local region the local data center that might have had the key that was just updated the database server also sends a sort of representation of the update to the corresponding database serve in the other region which process it applies the right to its disk data on disk it also using them excuse sort of log reading apparatus figures out which memcache server might hold the key that was just updated and sends it delete also to that memcache server so that the if it's the key is cache is invalidated in in both data centers okay so another question what would happen if we delete first in the right and then send to the database so that's or with reference to this thing here would what if we did to the feet first you know if you do delete first then you're increasing the chances that some other clients so supposing you delete and then send to database right in here if another client reads that same key they're gonna miss at this point they're gonna fetch the old data from the database and they're gonna then insert it cash and then you're going to update it leaving memcache for a while at least with stale data and then if this the writing client reads it again it may see the stale data even though it just updated it doing the delete second um you know these over the possibility that somebody will read during this period of time and see steal data but they're not worried about stale data in general they're really most worried in this context about clients reading their own rights so on balance even though there's a consistency problem by the way I'm doing the delete second ensures that clients will be their own rights in either case eventually the database server as I'm just mentioned will send a delete for the written keys another question I'm confused on how writing the new value shows stale data but deleting doesn't let me see I'm not really sure what the question is asking the if it's with reference to this code once the writes done okay maybe the question is it's really we didn't do delete at all so that when a client a front web for an end did or wanted to update him David would just tell the database but not explicitly delete the data from memcache the problem with this is that if the client sent this write to the database and then immediately read the same data that read would come out of the memcache and because memcache still has the old data you know memcache hasn't seen this right yet a client that updated some data and then read it you know updates it in the database but it reads the data if the stale data from memcache and then a client might update some data but still see the old data and if you delete it from memcache then if a client if you do do this delete than a client that writes some data and deletes it from memcache and then reads it again it'll miss in memcache because of the delete and they don't have to go to the database and read the data and the database will give it fresh data okay so the question is how come why do we delete here gosh why don't we just instead of this delete have the client just directly since it knows the new data just send a set RPC - memcache T and this is a this is a good question and so here we're doing I have an invalidate scheme this would often be called an update scheme and let me try to cook up an example that shows that while this could probably be made to work this update scheme it doesn't work it doesn't work out of the box and you wouldn't you need to do some careful design in order to make it work so this wasn't client wants posing up now we have two clients reading and writing the same key interleaved so let's say client one tells the database you know sends X plus plus to the database right just incrementing X and then of course or let me say it's going to increment X from zero to one so set X to one and then after that client one is going to call set of our key which is X and the value one and write that at the memcache D supposing meanwhile client two also wants to increment X so it's going to read this latest value in the database and almost certainly these are in fact transactions so what if we were doing increment what client won't won't be sending would be some sort of increment transaction on the database for correctness because the database does support transactions so we're going to imagine the client to increments the value of x to to sends that increment to the database and client two also is going to do this set so it's going to set X to be two but now what we're left with is the value of one in memcache D even though the correct values and the databases to which is to say if we do this update was set even though it does save us some time right cuz now we're saving somebody a miss in the future because we directly said instead of delete we also run the risk if the if it's popular data of leaving stale data in the database it's not that you couldn't get this to work somehow but it does require some careful thought to fix this problem all right so that was why they use invalidate and instead of update okay so I was going to about performance they this sort of route of how they get performance is through parallel parallelization parallel execution and for a storage system just at a high level there's really two ways that you can get a good performance one is by partition which is sharding that is you take your data and you split it up over you know into ten pieces over ten servers and those ten servers can run independently hopefully the other way you can use extra hardware to get higher performance despite replication at is have more than one copy of the data and you kind of for a given amount of hardware you can kind of choose whether to partition your data or replicate it in order to use that hardware and there's you know from memcache see what we're talking about here is is splitting the data over the available memcache servers by hashing the key so that every key sort of lives on one memcache server and from memcache what we would be talking about here is having each front-end just talk to a single memcache server and send all its requests there so that each memcache server serves only a subset of the front ends and sort of serves all their needs and Facebook actually uses a combination of both partition and replication for partition the things that are in its favor one is that it's memory efficient because you only store a single copy of each item Abita where's in replication you're gonna store every piece of data maybe on every server on the sort of partition is that it's as long as your keys are sort of equally roughly equally popular works pretty well but if there's some hot a few hot keys partition doesn't really help you much once you get those partition enough that those hot keys are on different servers you know once the if there's a single hot key for example no amount of partitioning helps you because no matter of how much you partition that hot key is still sitting on just one server the problem partition is that it doesn't mean that the front if front ends need to use lots of data lots of different keys it means in the end each front-end is probably going to talk to lots of partitions and at least if you use protocols like TCP that keep state there's significant overhead to as you add more and more sort of N squared communication for a replication it's fantastic if if your problem is that a few keys are popular because now you know you're making replicas of those those hotkeys and you can serve each replica the same key in parallel it's good because there's fewer this there's not n squared communication each front-end maybe only talks to one memcache server but the bad thing is it's there's a copy of data in every server you can cache far fewer distinct data items with replication then with partition so there's less total data can be stored so these are just generic for pros and cons of these two main ways of using extra hardware to get higher performance alright so I want to talk a bit about there when one sort of context in which they use partition and replication is at the level of different regions so I just want to talk through why it is that they decided to have separate regions and kind of separate complete data center with all the data in each of the regions so I before I do that there's a question why can't we cache the same amount of data with replication ok so supposing you have 10 machines each with a gigabyte of RAM and you can use these 10 machines each with a gigabyte of RAM for either replication or in a partitioning scheme if you use a partitioning scheme where each server stores different data from the other servers that you can store a total of 10 gigabytes of distinct data objects on your 10 servers each with a gigabyte of RAM so with partition you know each byte of ram is used for different data so you can look at the total amount of RAM you have that's how much distinct data you know different data items you can store with replication you know assuming your users are more or less looking at the same stuff each each replicas each cache replicas will end up storing roughly the same stuff as all the other caches so your 10 you have 10 gigabytes of RAM still they and your 10 machines but each of those machines stores roughly the same data so would you end up with this 10 copies of the same gigabyte of items so in a can this particular example if you use replication you snoring attempt as many distinct data items and you know that may actually be a good idea depending on you know sort of way your data is like but it does mean that replication gives you less total data that's cached and you know you can see there's points in the paper word that they mention this tension nominally they don't come down on one side of the other because they use both replication and charting okay okay so the highest level at which they're playing this game is between regions and so it at this high level each region has a complete replica of all the data right they have a each region as a complete set of database servers each database database corresponding database servers for the same data and assuming users are looking at more or less the same stuff that means the memcache servers in the different regions are also storing more or less basically replicating where we have yours replicating in both the database servers and the memcache servers and the point again one point is to you want a complete copy of the site that's close to West Coast users in the internet load early in the internet and another copy of the complete website this close to users on the East Coast close on the internet again and the Internet's pretty fast but coast to coast is you know 50 milliseconds or something which if you do if users have to wait too many 50 millisecond intervals they'll start to notice that amount of time another reason is that the you wanna a reason to applicate the data between the two regions is that these front ends to even create a single web page for user requests often dozens or hundreds of distinct data items from the cache or the databases and so the speed the latency the delay at which a front-end can fetch these hundreds of items from that from the look from the memcache key is quite important and so it's extremely important to have the front and only talk to only read local memcache servers and local databases so that you can do the hundreds of queries it needs to do for a web page very rapidly so if we have partitioned the data between the two regions then a front-end you know if I'm looking at my friends and some of my friends are on the East Coast and some on the west coast that means if we partitioned that would might require the front ends to actually make many requests you know 50 milliseconds each to the other data center and users would users would see this kind of latency and be very upset so so the reason to another reason to replicate is to keep the front ends always close to the data to all the data they need of course this makes writes more expensive because now if a front-end and the secondary region needs to write in estes send the data all the way across the internet the reads are far far more frequent than right so it's a good trade-off although the paper doesn't mention it it's possible that another reason for complete replication between the two sites is so that if the primary site goes down perhaps they could switch the whole operation to the secondary site but I don't know if they had that in mind okay so this is the story between Regents is basically a story of replication between the two data centers all right now within a data center within a region so in each region there's a single set of database servers so at the database level the data is charted and not replicated inside each region however at the memcache level they actually use replication as well as charting so they had this notion of clusters so a given regions actually supports multiple clusters of front-ends and database servers so here I'm going to have two clusters in this region this cluster has a you know a bunch of front ends and a bunch of memcache servers and these are completely independent almost completely independent so that a front-end and cluster one sends all its reads to the local memcache servers and misses it needs to go to the one instead of database servers and similarly each front-end in this cluster talks only to memcache servers in the same cluster so why do they have this multiple clusters why not just have you know essentially a single cluster a single set of front end servers and a single set of memcache server is shared by all those front ends one is that if you did that and and that would mean you know if you need to scale up capacity you sort of be adding more and more memcache servers in front ends to the same cluster you don't get any win therefore in performance for popular Keys you know so there the data sort of this memcache service is sort of a mix you know most of it is maybe only used by a small number of users but there's some stuff there that lots and lots of users need to look at and by using replication as well as sharding they get you know multiple copies of the very popular keys and therefore they get sort of parallel serving of those keys between the different clusters another reason to not want to increase the size of the cluster individual cluster too much is that all the data within a cluster is spread over partitioned over all the memcache servers and any one front end is typically actually going to need data from probably every single memcache server eventually and so this means you have a sort of n-squared communication pattern between the front ends and the memcache servers and to the extent that they're using TCP for the communication that involves a lot of overhead a lot of sort of connection state for all the different TCP so they wanted to limit so you know this is N squared CCP's they want to limit the growth of this and the way to do that is to make sure that no one cluster gets to be too big so this N squared doesn't get too large and well related to that is this in caste congestion business they're talking about the if a frontman needs data from lots of memcache servers it's actually it's gonna send out the requests more or less all at the same time and that means this front-end is gonna get the responses from all the memcache servers to query it more or less the same side time and that may mean dozens or hundreds of packets arriving here all at the same time which if you're not careful we'll cause packet losses that's in caste congestion and in order to limit how bad that was that you had a bunch of techniques they talked about but one of them was not making the clusters too large so that the number of memcache has given front-end tend to talk to and they might be contributing to the same caste never got to be too large and a final reason the paper mentions is that it's or behind this is is a big network in the data center and it's hard to build networks that are both fast like many bits per second and can talk to lots and lots of different computers and by splitting the data center up into these clusters and having most of the communication go on just within each cluster that means they need a smaller they need you know a modest size fast Network for this cluster and a modest size you know reasonably fast network for this cluster but they don't have to build a single network that can sort of handle all of the traffic between among all the computers of the giant cluster so it limits how expensive underlying network is on the other hand of course they're replicating the data and the two clusters and for items that aren't very popular and aren't really going to benefit from the performance win of having multiple copies this it's wasteful to sit on all this RAM and you know we're talking about hundreds or thousands of servers so the amount of money they spent on RAM for the memcache services is no joke so in addition to the pool of memcache servers inside each cluster there's also this regional pool of memcache servers that's shared by all the clusters in a region and into this regional pool they then modify the software on the front end so that the software on the front end knows aha this key the data for this skis actually not use that often instead of storing it on a memcache server my own cluster I'm going to store this not very popular key in the appropriate memcache server of the regional pool so this is the regional pool and this is just sort of an admission that some data is not popular enough to want to have lots of replicas of it they can save money by only cashing a single copy all right so that's how they get that's this kind of Carol replication versus partitioning strategy they use inside each inside each region a difficulty they had that they discuss is that when they want to create a new cluster in a data center they actually have a sort of temporary performance problem as they're getting that cluster going so you know supposing they decide to install you know couple hundred machines to be a new cluster with the front end new front ends new memcache errors and then they fire it up and you know maybe cause half the users to start using the new cluster I'm gonna have to use the old cluster well in the beginning there's nothing in these memcache servers and all the front end servers are gonna miss on the memcache servers and have to go to the databases and at least at the beginning until these memcache service gets populated with all the sort of data that's used a lot this is gonna increase the load on the database servers absolutely enormous leap because before we added the new clusters maybe the database servers only saw one percent of the reads because maybe these memcache servers have a hit rate of say 99 percent for reads the only one percent of all that means go to the database servers before we added the new cluster if we add a new cluster with nothing in the memcache servers and send half the traffic to it it's gonna get a hundred percent miss rate initially right and so that'll mean you know we gone from and so the overall miss write will now be 50 percent so we've gone from these database servers serving one percent of the reads to them serving 50 percent of the reads so at least in this imaginary example we've been quite firing up this new cluster we may increase the load on the databases by a factor of 50 and chances are the database servers were running you know reasonably Coast's the capacity and certainly not a factor of 50/50 under capacity and so this would be the absolute end of the world if they just fired up a new cluster like that and so instead they have this cold start idea in which a new cluster is sort of marked by some flag somewhere as being in this cold start state and in that situation when a front end and the new cluster misses that actually first first it has its own local memcache if that says no I don't have the data then the front end we'll ask the corresponding memcache in another cluster in some warm cluster that already has the data for the data if it's popular data chances are it'll be cached my friend and will get its data and then it will install it in the local memcache and it's only if both local memcache and the warm memcache don't have the data that this is front end and the new cluster will read from the database servers and so this is it and so they run in this kind of cold mode for a little while the paper I think mentions a couple hours until the memcache servers source and the new clusters start to have all the popular data and then they can turn off this cold feature and just use the local cluster memcache alone it's alright so another another load problem that the paper talks about if they ran into and this is a load problem again deriving from this kind of look aside caching strategies is called the thundering herd and the the scenario is that supposing we have some piece of data there's lotsa memcache servers but there's some piece of data stored on this memcache server there's a whole bunch of front ends that are ordinarily reading that one piece of very popular data so they're all sending constantly sending get requests for that data the memcache server has it in the cache it answers them and you know their memcache server is conserve like millions to million requests per second so we're doing pretty good and of course there's some database server sitting back here that has the real copy of that data but we're not bothering it because it is cached well suppose some front-end comes along and modifies this very popular data so it's going to send a write to the database with the new data and then it's gonna send a delete to the memcache server because that's the way rights work so now we've just deleted this extremely popular data we have all these front ends constantly sending gets for that data they're all gonna miss all at the same time they're all gonna now having missed send a read request to the front end database all at the same time and so now this front-end database is faced with maybe dozens or hundreds of simultaneous requests for this data so the Loews here is gonna be pretty high and it's particularly disappointing because we know that all these requests are for the same key so the database is going to do the same work over and over again to respond with the latest written copy of that key until finally the front ends get around to installing the new key in memcache and then people start hitting again and so this is the Thundering hurt what we'd really like is a single you know if a miss if there's a right and the leads and a miss happens in memcache we'd like what we'd like is the for the first front end that misses to fetch the data and install it and for the other front ends just like take a deep breath then wait until the new data is cached and that's just what their design does if you look at the if this thing called Elise which is different from the pieces were used to but they call Elise and we start from scratch in the scenario again let's see so now suppose we have a popular piece of data the first front end that asks for a data that's missing memcache Devo will send back an error saying oh no I don't have the data in my cache but it will install Elise which is a bit unique number it'll pick a least number install it in a table and the send this lease token back to the front end and then other front ends that come in and ask for the same Keith they'll simply get a just be asked to wait you know a quarter of a second or whatever some reasonable amount of time by the memcache D because the memcache key will see a haha I've already issued the lease for that key now there's at least potentially a V Sparky the server will notice it's already issued at least for the can tell these ones to wait so only one of the server's guess Elise this server then asks for the data from the database when against the responds back mom then it sends the put for the new data with a key and the value of God and the least proved that it was the one who was allowed to write the data memcache people looking for these today aha yeah you are the the person whose lease was granted and it'll actually do the install by and by these other friends who are told the wait will reissue their reads now that it will be there and so we all if all goes well get just one request to the database instead of dozens or hundreds and I think it's the sense in which is the lease is if the front-end fails at an awkward moment and doesn't actually request the data from the database or doesn't get around it installing it memcache D eventually memcache D will delete the lease cuz it times out and the next front end to ask will get a new lease and will hope that it will talk to the database and install new data so yes they answer the question the lease does up a time out in case the first front end fails yes yes okay so these leases are the their solution to the Thundering Herd problem um another problem they have is that if one of these memcache servers fails the most natural you-know-whats if they don't do anything special if the memcache server fails the front ends will send a request they'll get back a timeout and network will say jeez that you know I couldn't contact that host never got a response and what the real I BRE software does is it then sense it requests the database so if a memcache server fails and we don't do anything special the database is now going to be exposed directly to the reads all of these reefs and I'm catch server West serving this is the memcache server may well have been serving you know a million reads per second that may mean that the database server would be then exposed to those million reads per second then it's nowhere near fast enough to deal with all those weeds now Facebook they don't really talk about in the paper but they do have automated machinery to replace a failed memcache server but that takes a while to sort of set up a new server a new memcache server and redirect all the front-end to the new server instead of the old server so in the meantime they need a sort of temporary solution and that's this gutter idea so let's say the scoop is that we have our front ends we have the sort of ordinary set of memcache servers the database the one of the memcache service has failed we're kind of waiting until the automatic memcache server replacement system replaces this memcache server in the meantime friends are sending requests to it they get a sort of server did not respond error from the network and then there's a presumably small set of gutter servers whose only purpose in life is to eye they must be idle except when a real memcache server fails and when the front end gets an error back saying that get couldn't contact the memcache server it'll send the same request to one of the gutter servers and though the paper doesn't say I imagine that the front end will again hash the key in order to choose which gutter server to talk to and if the gutter server has the value that's great otherwise the front end server will contact the database server to read the value and then install it in the memcache server in case somebody else answer asks for the same data so while this means down the gutter servers will handle basically handle its request and so they'll be a miss you know handled by lease leases the Thundering Herd they'll be at least I need a Miss on each of the items that was a no-fail memcache server so there will be some load in the database server but then hopefully quickly this memcache server will I get all the data that's listen use and provide good service and then by and by this will be replaced and then the friends will know to talk to a different replacement server and because they don't and this is today's question I think that they don't send deletes to these gutter servers because since a gutter server could have taken over for anyone and maybe more than one of the ordinary memcache service it could actually have cache the caching any key so that would mean that and there may be you know friends talking to it that would mean that whenever a front-end needs it to delete a key from memcache or when the squeal on the database sends a delete for any key to the relevant memcache server yeah you know the the natural design would be that it would also send a copy of that delete to every one of the gutter servers and the same for front ends that are deleting data they would delete from the memcache stores but they would also have to leave potentially from any MCAD gutter server that would double the amount of defeats that had to be sent around even though most of the time these gutter servers aren't doing anything and don't cache anything and it doesn't matter and so in order to avoid all these extra deletes they actually fix the gutter servers so that they delete Keys very rapidly instead of hanging on to them until they're explicitly deleted that was answer to the question all right so I wanna talk a bit about consistency all this at a super high level you know the consistency problem is that there's lots of copies of the data for any given piece of data you know there's a copy in the primary database there's a copy in the corresponding database server of each of the secondary regions there's a copy of that key in each local cluster in one of the memcache keys in each local cluster there may be copies of that key and the gutter servers and there may be copies of the key in the memcache servers and the gutter memcache servers at each other region so we have lots and lots of copies of every piece of data running around when a write comes in you know the stuff has to happen on all those copies and furthermore the writes may come from multiple sources the same key may be written at the same time by multiple front ends and this region may be by friends and other regions too and so it's this concurrency and multiple copies and sort of multiple sources of writes since there's multiple front ends it creates a lot of opportunity for not just for there to be stale data but for data stale data to be left in the system for long periods of time and so I want to I want to illustrate what are those problems actually in a sense we've already talked a bit about this when somebody asked why the front ends don't update why do they delete instead of updating so that's certainly one instance of the kind of weather multiple sources of data and so we have trouble enforcing correct order but here's another example of a race an update race that if they hadn't done something about it would have left data indefinitely stale data and definitely in memcache it's going to be a similar flavor to the previous example so supposing we have client one he wants to read a key but memcache says it doesn't have the data it's a Miss so C one's gonna read the data from from the database and let's say it gets back some value that you want meanwhile client to wants to update this data so it sends you know its rates he equals v2 and sends that to the database and then you know the rule for writes the code for writes that we saw is that the next thing we do is delete it from the database from memcache d.c c2 is going to delete ah the key from the database oh that's a Friday you know it was actually c2 doesn't really know what's in memcache d but the leading was ever there is always safe because certainly not gonna cause a stale data to be deleting won't cause her to be stilled Leena um and this is the sense that the paper claims that delete is idempotent said delete it's always safe to kabhi but if you recall the pseudocode for what a read does if you miss and you read the data from the database you're supposed to just insert that data into memcache so client 1 you know may have been slow and finally gets around to sending a set RPC two memcache T but it read version 1 and read a you know what is now an old outdated version of the data from the database but it's going to set that into set this into memcache and yeah you know one other thing that happened is that we know the database is is whenever you write something I'm database that sends deletes to memcache D so of course maybe at this point the database will also have sent a delete for k2m caste and so now we get to deletes but it doesn't really matter right these lease may already have happened by the time client one gets around to updating this key and so at this point indefinitely the memcache D will be cashing a stale version of of this data and there's just no mechanism anymore or this is them if the system worked in just this way there's no mechanism for the memcache D to ever see to ever get the actual correct value it's gonna store and serve up stale data for key K forever and they because they ran into this and while they're okay with data being somewhat out-of-date they're not okay with data being out of date forever because users will eventually notice that they're seeing ancient data and so they had to solve this they had to make sure that this scenario didn't happen they actually solved this this problem also with the lease mechanism at the same lease mechanism that we describe for the Thundering hoard although there's an extension to the lease mechanism that makes this work so what happens is that when memcache descends back a Miss indication seeing the data wasn't in the cache it's gonna grant the lease so we get the Miss indication plus this lease which is basically just a big unique number and the memcache server is gonna remember that the association between this lease and this key it knows that somebody out there with a lease to update this key the new rule is that when the when the memcache server gets a delete from either another client or from the database server the memcache server is gonna as well as deleting the item is going to invalidate this lease so as soon as either these deletes come in assuming that Elyse arrived first the memcache server is gonna believe this lease from its table about leases this set is the lease back from the front end now when the set arrives the memcache server will look at the lease and say wait a minute I you don't have a lease for this key all right invalid if these fit this key I'm gonna ignore this set so because the lease has been because one of these if one of these deletes came in before the set this sees to be invalid in invalidated and the memcache server would ignore this set and that would mean that the key would note just stay missing from memcache and the next client that tried to read that key you'll get a Miss would read the fresh data now from the database and would install it in memcache and presumably the second time around the second readers lease would be valid um you may and indeed you should ask what happened that the order is different so supposing these deletes instead of happening before the set these deletes were instead to have happen after the set I want to make sure this scheme still works then and so how things would play out then is that since if these Dilys were late happened after the set the memcache server wouldn't delete these from its table of visas Solis would still be there when the set came and yes indeed we would still then it would accept the setting we would be setting key to a stale value but our assumption was this time that the deletes had been late and that means the Dilys are yet to arrive and when they when these deletes arrive then this stay on theta will be knocked out of the cache and so the stale date will be in the cache a little bit longer but we won't have this situation where stale data is sitting in the cache indefinitely and never deleted any questions lissa machinery okay um to wrap up you it's certainly fair to view this system a lot of the complexity of the system as stemming from the fact that it was sort of put together out of pieces that didn't know about each other like it would be nice for example memcached he knew about the database I'm understand memcache D and the database kind of cooperated in a consistency scheme and perhaps if Facebook could have at the very beginning you know predicted the how things would play out on what the problems would be and if they have had enough engineers to work on it they could have from the beginning built a system that could provide both all the things they needed high-performance multi data center replication partition and everything and they're having companies that have done that so the example I know of that sort of most directly comparable to the system in this paper is that if you care about this stuff you might want to look at it is Yahoo's peanuts storage system which in a sort of designed from scratch and you know different different in many details but it does provide multi-site replication with consistency and good performance so it's possible to do better but you know all the issues are present that's just had a more integrated perhaps elegant set of solutions the takeaway so for us from this paper one is that for them at least and for many big operations caching is vital absolutely vital for to survive high load and the caching is not so much about reducing latency it's much more about hiding enormous load from relatively slow storage servers that's what a cache is really doing for Facebook is hiding sort of concealing almost all the load from the database servers another takeaway is that you always in big systems you always need to be thinking about caching versus control versus sorry partition versus replication I mean you need ways of either formally or informally sort of deciding how much your resources are going to be devoted to partitioning and how much to replication and finally ideally you'd be able to do a better job in this paper about from the beginning integrating the different storage layers in order to achieve good consistency okay that is all I have to say please ask me questions if you have