all right everybody let's get started today the paper four days I'm is aunt Aurora paper which is all about how to get a high-performance reliable database going as a piece of cloud infrastructure and itself built out of infrastructure that Amazon itself makes available so the reason why we're reading this paper is that first of all it's a very successful recent cloud service from Amazon a lot of their customers use it it shows sort of in its own way an example of a very big payoff from clever design table one which sort of summarizes the performance shows that relative to some other system which is not very well explained the paper claims to get a thirty five times speed up in transaction throughput which is extremely impressive this paper also kind of explores the limits of how well you can do for performance and fault tolerance using general-purpose storage because one of the themes of the papers they basically abandoned general-purpose storage they switch from a design in which they were using their Amazon's own general-purpose storage infrastructure decided it was not good enough and basically built totally application-specific storage furthermore the paper has a lot of little tidbits about what turned out to be important in this and the kind of cloud infrastructure world so before talking about aurora i want to spend a bit of time kind of going over the back history or what my impression is about the story that led up to the design of aurora because it's you know the sort of m f-- way that amazon has in mind that you ought to build that their cloud customers ought to build databases on amazon's infrastructure so in the beginning amazon had basically their very first offering cloud offering to support people who wanted to build websites but using Amazon's hardware and in Amazon's machine room their first offering was something called ec2 for elastic cloud apparently too and the idea here is that Amazon had big machine rooms full of servers and they ran virtual machine monitors on their servers and they'd rent out virtual machines to their customers and their customers would then you know rent a bunch of virtual machines and run web servers and databases and whatever ever all else they needed to run inside these ec2 instances so the picture of one physical server looked like this Amazon we control the virtual machine monitor on this hardware server and then there'd be a bunch of guests a bunch of ec2 instances each one rented out to a different cloud customer each of these would just run a standard operating system like Linux and then you know a web server or maybe a database server and these were relatively cheap relatively easy to set up and as a very successful service so one little detail that's extremely important for us is that initially the way you get storage the way you've got storage if you rented an ec2 instance was that every one of their servers had a disk attached a physical disk attached and each one of these instances that they rented to their customers will get us you know a slice of the disk so they said locally attached storage and you got a bit of locally attached storage which itself just look like a hard drive an emulated hard drive to the virtual machine guests ec2 is like perfect for web servers for stateless web servers you know your customers with their web browsers would connect to a bunch of rented ec2 instances that ran a web server and if you added all of a sudden more customers you could just instantly rent more ec2 instances from Amazon and fire up web servers on them and sort of an easy way to scale up your ability to handle web load so it was good for web servers but the other main thing that people ran in ec2 instance this was databases because usually a website is constructed of a set of stateless web servers that anytime they need to get out permanent data go talk to a back-end database so what you would get is is maybe a bunch of client browsers in the outside world outside of Amazon's web infrastructure and then a number of ec2 web server instances as many as you need it to run the sort of logic of the website this this is now inside Amazon and then also some also typically one ec2 instance running a database your web servers would talk to your database instance and ask it to read and write records in the database unfortunately ec2 wasn't perfect was it nearly as well-suited to running a database as it was to running web servers and the most immediate reason is that the storage or the sort of main easy way to get storage for your ec2 database instance was on the locally attached disk attached to whatever a piece of hardware your database instance was currently running on in fact hardware crashed then you also lost access to whatever what is on its hard drive so if it's a hardware that it was actually implementing a web server crashed no problem at all because there's really keeps no state itself you just fire up a new web server on a new ec2 instance if the ec2 instance it's a hardware running it crashes have become unavailable you have a serious problem if the data is stored on the locally attached disk so initially at least there wasn't sort of a lot of help for doing this one thing that did work out well is that Amazon did provide this sort of large scheme for storing large chunks of data called s3 and you could take snapshots you could take Prius periodic snapshots if you need a basis state and stored in s3 and use that for sort of backup disaster recovery but you know that style of periodic snapshots means you're gonna lose updates that happen between the periodic backups all right so the next thing that came along that's that's relevant to the sort of Aurora database story is that in order to provide their customers with disks for their ec2 instances that didn't go away if there was a failure that is more sort of fault tolerant long-term storage was guaranteed to be there Amazon introduced the service called EBS and this stands for elastic block store so with EBS is is a service that looks to an ec2 instances it looks to one of these instances one of these guest virtual machines just as if it were a hard drive an ordinary way you could format it as a hard drive but a file system like ext3 or whatever Linux file system you like on this on this thing that looks to be guest just like a hard drive but the way it's actually implemented is as a replicated pair of storage servers so this is the local this is one of local storage with Mike if when EBS came out then you could you could rent an e BS volume which this thing that looks just like an ordinary hard drive but it's actually implemented as a pair so these are EBS servers a pair of EBS servers each with an attached hard drive so if your software here maybe you're running a database now and your databases mount's one of these EBS volumes as its storage when the database server doesn't write what that actually means is that the right to send out over the network and using chain replication which we talked about last week you're right is you know first written to the EBS server one on the first CBS server that's backing your volume and then the second one and finally you get the reply and similarly when you do a read I guess some chain replication you'll be the last of the chain so now database is running on ec2 instances had available a storage system that actually would survive the crash of or the you know death of the hardware that they were running on if this physical server died you could just get another ec2 instance fire up your database and have it attached to the same old EBS volume that the sort of previous version of your database was attached to and it would see all the old data just as it had been left off by the previous database just like you moved a hard drive from one machine to another so EBS was like really a good deal for people who need it to keep permanent state like people running databases one thing to that is sort of important for us about EBS is that it's really it's not a system for sharing at any one time only one ec2 instance only one virtual machine can mount a given EBS volume so the EBS volumes are implemented on a huge fleet of you know hundreds or whatever storage servers with disks at Amazon and they're all you know everybody's EBS volumes are stored on this big pool of servers but each one of each PPS volume can only be used by only one ec2 instance only one customer all right still EBS was a big step up but it had still has some problems so there's still some things that are not quite as perfect as it could be one is that if you run a database on EBS it ends up sending large volumes of data across the network and this is uh we're now starting to sort of sneak up on figure two in the the paper where they start complaining about how many just how many writes it takes if you run a database on top of a network storage system so there's the database on EBS ended up generating a lot of network traffic and one of the kind of things in the paper that the paper implies is that they're as much network limited as they are CPU or storage limited that is they pay a huge amount of attention to reducing the Aurora paper sends a huge amount of attention for reducing the network that the database generates and seems to be worrying less about how much CPU time or disk space is being consumed that's a sort of a hint at what they think is important the other problem with EBS is not very fault tolerant it turns out that for performance reasons they I'm done would always put both of the EBS both of the replicas of your EBS volume in the same data center and so we have a single server crashed if you know one of the two EBS servers that you're using crashed it's okay because you switch to the other one but there was just no story at all for what happens if an entire data center went down and and apparently a lot of customers really wanted a story that would allow their data to survive an outage of an entire data center maybe it lost his network connection it was a fire in the building or a power failure to the whole building or something people really wanted to have at least the option if they're willing to pay more of having their data stored in a way they hid they could still get at it I'm even if one data center goes down and the way that Amazon described this there is that both an instance and its EBS to EBS replicas are in the same ability veil ability zone and an Amazon jargon an availability zone is a particular data center and the way they structure their data centers is that there's usually multiple independent data centers in more or less the same city or relatively close to each other and all the multiple availability zones maybe two or three that are near by each other are all connected by redundant high speed networks so there's always payers or triples of nearby availability availability centers and we'll see the buy that's important in a little bit but at least for EBS in order to keep the sort of costs of using chain replication down they required the two replicas to be in the same availability zone all right um before I dive into more into how Aurora actually works it turns out that the details of the design in order to understand them we first have to know a fair amount about the sort of design of typical databases because what they taken is sort of the main machinery of a database my sequel as it happens and split it up in an interesting way so we need to know sort of what it but it is a database does so we can understand how they split it up so this is really a kind of database tutorial really focusing on what it takes to implement transactions crashed recoverable transactions so what I really care about is transactions and crash recovery and there's a lot else going on in databases but this is really the part that matters for this paper so first what's a transaction you know transaction is just a way of wrapping multiple operations on maybe different pieces of data and declare in that that that's entire sequence of operations should appear a Tomic to anyone else who's reading or writing the data so you might see transposing we're running a bank and we want to do transfers between different accounts maybe you would say well we would see code or you know see a transaction looks like this is you have to clear the beginning of the sequence of instructions that you want to be atomic in the in transaction maybe we're going to transfer money from account Y to account X so we might see where I'll just pretend X is a bank balance Jordan the database you might see the transaction looks like oh can I add $10 to X's account and deduct the same ten dollars from my account and that's the end of the transaction I want the database to just do them both without allowing anybody else to sneak in and see the state between these two statements and also with respect to crashes if there's a crash at this point somewhere in here we're going to make sure that after the crash and recovery that either the entire transactions worth the modifications are visible or none of them are so that's the effect we want from transactions there's additionally people expect database users expect that the database will tell them tell the client that submitted the transaction whether the transaction really finished and committed or not and if a transaction is committed we expect clients expect that the transaction will be permanent will be durable still there even if the database should crash and reboot um one thing it's a bit important is that the usual way these are implemented is that the transaction locks each piece of data before it uses it so you can view the they're being locks x and y for the duration of the transaction and these are only released after the transaction finally commits that is known to be permanent this is important if you for some of the things that you have to if you some of the details in the paper really only makes sense if you realize that the database is actually locking out other access to the data during the life of a transaction so how this actually implemented it turns out the database consists of at least for the simple database model where the databases are typically written to run on a single server with you know some storage directly attached and a game that the Aurora paper is playing is sort of moving that software only modestly revised in order to run on a much more complex network system but the starting point is we just assume we have a database with a attached to a disk the on disk structure that stores these records is some kind of indexing structure like a b-tree maybe so there's a sort of pages with the paper calls data pages that holds us you know real data of the of the database you know maybe this is excess balances and this is wise balance these data pages typically hold lots and lots of records whereas X and y are typically just a couple bites on some page in the database so on the disk there's the actual data plus on the disk there's also a right ahead log or wal and the right ahead logs are a critical part of why the system is gonna be fault tolerant inside the database server there's the database software the database typically has a cache of pages that it's read from the disk that it's recently used when you execute a transaction what that actually executes these statements what that really means is you know what x equals x plus 10 turns into the runtime is that the database reads the current page holding X from the disk and adds 10 to it but so far until the transaction commits it only makes the modifications in the local cache not on the disk because we don't want to expose we don't want to write on the disk yet and the part possibly expose a partial transaction so while then when the database but before because the database wants to sort of pre to clear the complete transaction so it's available to the software after a crash and during recovery before the database is allowed to modify the real data pages on disk its first required to add log entries that describe the transaction so it has to in order before it can commit the transaction it needs to put a complete set of log ahead entries in the right ahead log on disk I'm describing all the data bases modification so let's suppose here that x and y start out as say 500 and y starts out as 750 and we want to execute this transaction before committing and before writing the pages the database is going to add at least typically 3 log records 1 this that says well as part of this transaction I'm modifying X and it's old value is 500 make more room here this is the on dis log so each log entry might say here's the value I'm modifying here's the old value and we're adding and here's the new value say five ten so that's one log record another 4y may be old value is 750 we're subtracting 10 so the new value is 740 and then when the database if it actually manages to get to the end of the transaction before crashing its gonna write a commit record saying and typically these are all tagged with some sort with a transaction ID so that the recovery software eventually will know how this commit record refers to these log records yes in a simple database will be enough to just store the new values and say well it is a crash we're gonna just reapply all the new values the reason most serious databases store the old as well as a new value is to give them freedom to even for a long-running traction for a long-running transaction even before the transaction is finished it gives the database the freedom to write the updated page to disk with the new value 740 let's say from the from an uncompleted transaction as long as it's written the log record to disk and then if there's a crash before the commit the recovery software always say aha well this transaction never finished therefore we have to undo all of its changes and these values these old values are the values you need in order to undo a transaction that's been partially written to the data pages so the aurora indeed uses undo redo logging to be able to undo partially applied transactions okay so if the database manages to get as far as getting the transactions log records on the disk and the commit record marking is finished then it is entitled to apply to the client we said the transactions committed the database can reply to the client and the client can be assured that its transaction will be sort of visible forever and now one of two things happens the database server doesn't crash then eventually so it's modified in its cache these these X&Y records to be 510 and 740 eventually the database will write it's cached updated blocks to their real places on the disk over writing you know these be tree nodes or something and then the database can reuse this part of the log so databases tend to be lazy about that because they like to accumulate you know maybe there'll be many updates to these pages in the cache it's nice to accumulate a lot of updates before being forced to write the disk if the database server crashes before writing the day writing these pages to the disk so they still have their old values then it's guaranteed that the recovery software when you restart that debase scan the log see these records for the transaction see that that transaction was committed and apply the new values to the to the stored data and that's called a redo it basically does all the rights in the transaction so that's how transactional databases work in a nutshell and so this is a sort of very extremely abbreviated version of how for example the my sequel database works that an Aurora is based on this open source software thing called database called my sequel which does crash recovery transaction and crash recovery in much this way ok so the next step in Amazon's development a better and better database infrastructure for its cloud customers is something called RDS and I'm only talking about RDS because it turns out that even though the paper doesn't quite mention it figure 2 in the paper is basically a description of RDS so what's going on and RDS is that it was a first attempt to get a database that was replicated in multiple availability zones so that if an entire data center went down you could get back your database contents without missing any rights so that deal with RDS is that there's one you have one ec2 instance that's the database server you just have one you just want to running one database it stores its data pages and log just basically with this instead of on the local disk its stores them in EBS so whenever the database does a log write or page write or whatever those rights actually go to these two EBS volumes EBS replicas in addition so and so this is in one availability zone in addition for every write that the database software does Amazon would transparently without the database even realizing necessarily this was happened also send those rights to a special set up in a second availability zone in a second machine room - just going from figure 2 to apparently a separate computer or ec2 instance or something whose job was just a mirror writes that the main database did so this other sort of mirroring server would then just copy these rights to a second pair of EBS servers and so with this set up with this RDS set up and that's what figure - every time the database appends to the log or writes to one of its pages it has to the data has to be sent to these two replicas has to be sent on the network connection across the other availability zone on the other side of town sent to this mirroring server which would then send it to it's two separate EBS replicas and then finally this reply would come back and then only then with the right be finished with a DAT bc AHA my writes finished I can you know count this log record it was really being appendage of the log or whatever so this RDS arrangement gets you betcha better fault tolerance because now you have a complete up-to-date copy of the database like seeing them all the very latest writes in a separate availability zone even if you know fire burns down this entire data center boom you can weaken you can run the database in a new instance and the second availability zone and lose no data at all yes um I don't know how to answer that I mean that is just not what they do and my guess is that it would be that for most EVs customers it would be too painfully slow to forward every right across two separate data center I'm not really sure what's going on but I think the main answers they don't do that and this is sort of a a little bit of a workaround for the way EBS works too kind of tricky BS and actually producing and sort of using the existing EBS infrastructure unchanged I stableman chose this turns out to be extremely expensive or anyway it's expensive as you might think you know we're writing fairly large volumes of data because you know even this transaction which seems like it just modifies two integers like maybe eight bytes or I don't know what sixteen who knows only a few bytes of data are being modified here what that translates to as far as the database reading and writing the disk is I actually these log records are that also quite small so this these two log records might themself only be dozens of bytes long so that's nice but the reads and writes of the actual data pages are likely to be much much larger than just a couple of dozen bytes because each of these pages is going to be you know eight kilobytes or 16 kilobytes or some relatively large number the file system or disk block size and it means that just to read and write these two numbers when it comes time to update the data pages there's a lot of data being pushed around on to the disk a locally attached disk now it's reasonably fast but I guess what they found is when they start sending those big 8 kilobyte writes across the network that that used up too much network capacity to be supported and so this arrangement this figure 2 arrangement evidently was too slow yes so in this in this figure to set up the you know unknown to the database server every time it called write erode its EBS disk a copy of every write went over across availabilities zones and had to be written to the was written to the both of these EBS servers and then acknowledged and only then did the write appear to complete to the database so I really had to wait for all the fall for copies to be updated and for the data to be sent on the link across to the other availability zone and you know as far as table one it's concerned that first performance table the reason why the reason why the slow the mirrored my sequel line is much much slower than the Aurora line is basically that it sends huge amounts of data over these relatively slow Network links and that was the problem that was the performance problem they're really trying to fix so this is good for fault tolerance because now we have a second copy and another availability zone but it was bad news for performance all right the way Aurora and the next step after this is Aurora and to set up there the high level view is we still have a database server although now it's running custom software that Amazon supplies so I can rent an Aurora server from Amazon but it's not I'm not running my software on it I'm renting a server running Amazon's Aurora database software on it rent an Aurora database server from them and it's it's just one instance it sits in some availability zone and there's two interesting things about the way it's set up first of all is that the data you know it's replacement basically for EBS involves six replicas now - in each of three availability zones for super fault tolerance and so every time the database complicated we'll talk but basically when the database writes or reads when the database writes it's we're not sure exactly how its managed but it more or less needs to send a write one way or another writes have to get sent to all six of these replicas the key to making and so this looks like more replicas gosh you know why isn't it slower why isn't it slower than this previous scheme which only had four replicas and the answer to that is that what's being the only thing being written over the network is the log records so that's really the key to success is that the data that goes over these links in the sense of the replicas it's just the log records log entries and as you can see you know a log entry here you know at least and this is a simple example now it's not quite this small but it's really not vastly more than a couple of dozen bytes needed to store the old value and the new value for the piece of data we're writing so the log entries tend to be quite small whereas when the database you know we had a database that thought it was writing a local disk and it was updating its data pages these tended to be enormous like doesn't really say in the paper I don't think that eight kilobytes or more so this set up here was sending for each transaction was sending multiple 8 kilobyte pages across to the replicas whereas this set up is just sending these small log entries to more replicas but the log entries are so very much smaller than 8k pages that it's a net performance win okay so that's one this is like one of their big insights is just in the log entries of course a fallout from this is that their storage system is now not very general purpose this is a storage system that understands what to do with my sequel log entries right it's not just you know EBS was a very general purpose just emulated to disk you read them right block's doesn't understand anything about anything except for blocks this is a storage system that really understands that it's sitting underneath the database so that's one thing they've done is ditched general-purpose storage and switched to a very application specific storage system the other big thing I'll also go into in more detail is that they don't require that the rights be acknowledged by all six replicas in order for the database server to continue instead the database server can continue as long as a quorum and which turns out to be for as long as any four of these servers responds so if one of these availability zones is offline or maybe the network connection to it is slow or maybe even just these servers just happen to be slow doing something else at the moment we're trying to write the database server can basically ignore the two slowest or the two most dead of the server's when it's doing it right so it only requires acknowledgments from any four out of six and then it can continue and so this quorum scheme is the other big trick they use to help them have more replicas in more availability zones and yet not pay a huge performance penalty because they never have to wait for all of them just the four fastest of the six replicas so the rest of the lecture is gonna be explaining first quorums and then log entries and then this idea of just sending log entries basically table one summarizes the result if you look at table one by switching from this architecture in which they send the big data pages to four places to this Aurora schema sending just the log entries to six replicas they get a amazing 35 times performance increase over some other system you know this system over here but by playing these two tricks and paper is not very good about explaining how much of the performance is due to quorums and how much is due to just sending log entries but anyway you slice it 35 times improvement performance is very respectable and of course extremely valuable to their customers and to them and it's like transformative I am sure for many of Amazon's customers all right okay so the first thing I want to talk about in in detail is their quorum arrangement what they actually mean by quorums so first of all the quorums is all about the arrangement of fault-tolerant of this fault-tolerant storage so it's worth thinking a little bit about what their fault tolerance goals were so this is like fault tolerance goals they wanted to be able to do rights even if one reads and writes even if one availability zone was completely dead so they're gonna write you know even with they wanted to be able to read even if there was one dead availability zone plus one other dead server and the reason for this is that an availability zone might be offline for quite a while because maybe it's you know was suffered from a flood or something and while it's down for a couple of days or a week or something well people prepare the damage from the flood we're now reliant on just you know the servers and the other two availability zones if one of them should go down we still we don't want it to be a disaster so they're going to be able to write with one even with one dead availability zone they furthermore they wanted to be able to read with one dead availability zone plus one other dead server so they wanted to be able to still read you know and get the correct data even if there was one dead availability zone plus one other server and the live availability zones were dead so you know they we have to sort of take take it for granted that they know what their they know their own business and that this is really you know kind of a sweet spot for how fault-tolerant you want to be um and in addition I already mentioned they want to be able to taller to sur ride out temporarily slow replicas I think from a lot of sources it's clear that the if you read and write EBS for example you don't get consistently high performance all the time sometimes there's little glitches because maybe some part of the network is overloaded or something is doing a software upgrade or whatever and it's temporarily slow so they want to be able to just keep going despite transient transiently slow or maybe briefly unavailable storage servers and a final requirement is that if something if a storage server should fail it's a bit of a race against time before the next storage server fails sort of always the case and it's not the statistics are not as favorable as you might hope because typically you buy basically because server failure is often not independent like the fact that one server is down often means that there's a much increased probability that another one of your servers will soon go down because it's identical Hardware may be bought from the same company came off the same production line one after another and so a flaw and one of them is extremely likely to be reflected in a flaw and another one so people always nervous off there's one failure boy there could be a second failure very soon and in a system like this well it turns out in these quorum systems you know you can only recover it's a little bit like raft you can recover as long as not too many of the replicas fail so they really needed to have fast we replicate them that is of one server seems permanently dead we'd like to be able to generate a new replica as fast as possible from the remaining replicas I mean a fast food replication these are the main fault tolerance goals the peeper lays out and by the way this discussion is only about the storage servers and you know what their failure character is too excited you know the failures how to recover and it's a completely separate topic what to do if the database server fails and Aurora has a totally different set of machinery for noticing a database servers fail creating a new instance running in a new database server on the new instance which is intense it's not what I'm talking about right now we'll talk about it a little bit later on right now it's just gonna build a storage system that's a lot that's where the storage system is fault tolerant okay so they use this idea called quorums and for a little while now I'm going to describe the sort of classic quorum idea which is dates back to the late 70s so this is quorum replicate quorum replication I'm gonna describe to you this or abstract quorum idea they use a variant of what I'm gonna explain and the idea of behind quorum quorum systems is to be able to build storage systems that provide fault tolerance storage using replications and guarantee that even if some of the replicas fail your that reads will still see the most recent writes and typically quorum systems are sort of simple readwrite systems put get systems and they don't typically directly support more complex operations just you can read you could have objects you can read an object or you can overwrite an entire object and so the idea is you have n replicas if you want to write or you have to get you have to in order to write you have to make sure your write is acknowledged by W where W is less than n of the replicas so W right you have to send each right to these W are the replicas and if you want to do a read you have to get input read information from at least our replicas and so a typical setup that's so well first of all the key thing here is that W and our have to be set relative to end so that any quorum of W servers that you manage to send a right to must necessarily overlap with any quorum of our servers that any future reader might read from and so what that means is that our plus W has to be greater than n so that any W servers must overlap in at least one server with any our servers and so you might have three we can imagine there's three servers s1 s2 s3 each of them holds I say we just have one object that we're updating we send out a write maybe we want to set the value of our object to 23 well in order to do a write we need to get our new value on to at least W of the of the replicas let's say for this system that R and W are both equals 2 and n is equal to 3 that's the setup to do a write we need to get our new value onto a quorum onto a beast to the server so maybe we get our right onto these two so they both now know that the value of the of our data object is 23 if somebody comes along and reads or read it also requires that the reader check with at least a read quorum of the servers so that's also 2 in this set up so you know that quorum could include a server that didn't see the right but it has to include at least one other in order to get to so that means the any future read must for example consult both this server that didn't see the write plus at least one that did that is a requirement of right form must overlap in at least one server so any read must consult a server that saw any previous right now what's cool about this well actually there's still one critical missing piece here the reader is gonna get back our results possibly are different results because and the question is how does a reader know which of the our results it got back from the our servers in its forum which one actually uses the correct value something that doesn't work is voting like just voting by popularity of the different values it gets back it turns out not to work because we're only guaranteed that our reader overlaps of the writer in at most one server so that could mean that the correct value is only represented by one of the servers that the reader consulted and you know in a system with say six replicas you know you might have Reaper might be four you might get back for answers and only one of them is the answer that is the correct answer from the server in which you overlap with the previous right so you can't use voting and instead these quorum systems need version numbers so every right every time you do a right you need to accompany your new value with you know an increasing version number and then the reader it gets back a bunch of different values from the read quorum and it can just use them only the highest version number I'm said that means that this 21 here you know maybe s2 had a old value of 20 each of these needs to be tagged with a version number so maybe this is version number three this was also version number three because it came from the same original right and we're imagining that this server that didn't see the right is gonna have version number two then the reader gets back these two values these two version numbers fix the version were the highest the value with the highest version number and in Aurora this was essentially about well never mind about Aurora for a moment okay furthermore if you can't talk to if you can't actually contact a quorum or a read or write you really just have to keep trying those are the rules so keep trying until the server's are brought back up or connected again so the reason why this is preferable to something like chain replication is that it can easily ride out temporary dead or disconnected or slow servers so in fact the way it would work is that if you want to read or write if you want to write you would saying your newly written about you would send the newly written value plus its version number to all of the servers to all n of the servers but only wait for W of them to respond and similarly if you want to read you would in a quorum system you would send the read to all the servers and only wait for a quorum for R of the servers to respond and that and because you only have to wait for are out of n of them that means that you can continue after the fastest are have responded or the fastest W and you don't have to wait for a slow server or a server that's dead and there's not any you know the machinery for ignoring slow or dead servers is completely implicit there's nothing here or about oh we have to sort of make decisions about which servers are up or down or like the leaders or anything it just kind of automatically proceeds as long as the quorum is available so we get very smooth handling of dead or slow servers in addition there's not much leeway for it here well actually you even in this simple case you can adjust the R and W to make either reads to favor either reads or writes so here we could actually say that well the right forum is three every write has to go to all three servers and in that case the read quorum can be want so you could if you wanted to favored reads with this setup you could have read equals one write equals three memories are much faster they only have to wait for one server but then return the writes are slow if you wanted to favor right you could say that Oh any reader has to be from all of them but a writer only has to write one so I mean the only one server might have the latest value but readers have to consult all three but they're guaranteed that their three will overlap with this of course these particular values makes writes not fault tolerant and here reads not fault tolerant because all the server's have to be up so you probably wouldn't want to do this in real life you might have you would have as Knowle Rohrer does a larger number of servers and sort of intermediate numbers of vinum right corns Aurora in order to achieve its goals here of being able to write with one debt availability zone and read with one dead availability zone plus one other server it uses a quorum system with N equals 6 w equals 4 and R equals 3 so the W equals 4 means that it can do a write with one dead availability zone if this availability zone can't be contacted well these other four servers are enough to complete right the reform of 3 so 4 plus week so 7 so they definitely guaranteed overlap a read quorum of 3 means that even if one availability is zone is dead plus one more server the three remaining servers are enough to serve a read now in this case we're three servers are now down the system can do reads and as you know can reconstruct the confine the current state of the database but it can't do writes without further work so if they were in a situation where there was three dead servers there they have enough of a quorum to be able to read the data and reconstruct more cop more replicas but until they've created more replicas to basically replace these dead ones they can't serve as rights and also the quorum system as I explained before allows them to ride out these transient slow replicas all right as it happens as explained before what the rights in Aurora aren't really over writing objects as in a sort of classic quorum system what Aurora in fact its rights never overwrite anything its rights just append log entries to the current law so the way it's using quorums is basically to say well when the database sends out our new log record because it's executing some transaction it needs to make sure that that log record is present on at least four of the store of its storage servers before it's allowed to proceed with the transaction are committed so that's really the meaning of its other Wars right porins is that each new log record has to be appended to the storage and at least for the replicas before the write can be considered to to have completed and when a when Aurora gets to the end of a transaction before it can reply to the client until the client tell the client a hi you know your transaction is committed and finished and durable Aurora has to wait for acknowledgments from a write quorum for each of the log records that made up that transaction and in fact because because if there were a crash in a recovery you're not allowed to recover one transaction if preceding transactions don't aren't also recovered in practice Aurora has before Aurora can acknowledge a transaction it has to wait for a write quorum of storage servers to respond for all previously committed transaction and the transaction of interest and then can respond to the client okay so these these storage servers are getting incoming log records that's what rights look like to them and so what do they actually do you know they're not getting new data pages from the database server they're just getting log records that just describe changes to the data pages so internally one of these one of these storage servers it has internally it has copies of all that data of all the data pages at some point in the database data pages evolution so it has maybe in its cache on its disk a whole bunch of these pages you know page 1 page 2 so forth when a new write comes in the storage server would win a new log rec over in a new write arrives carrying with it just a log record what has to happen some day but not right away is that the changes in that log record the new value here has to be applied to the relevant page but we don't at the source of it doesn't have to do that until someone asks just until the database server or the recovery software asks to see that page so immediately what happens to a new log record is that the log records are just appended to lists of log records that effect each page so for every page that the storage server stores if it's been recently modified by a log record by a transaction what the storage server will actually store is an old version of the page plus the string of the sequence of log records that have come in from trend from the database server since that page was last brought up to date so if nothing else happens the storage server just stores these old pages plus lists of log records if the database server later you know fix the page from its cache and then needs to read the page again for a future transaction it'll send a read request out to one of the storage servers and say look you know I need a copy I need an updated copy a page one and at that point the storage server will apply these log records to the page you know do do these writes of new data that are implied that are described in the log records and then send that updated page back to the database server and presumably maybe then like a racist list and just store the newly updated page although it's not quite that simple all right so the storage servers just store these strings of log records plus old log page versions now the database server as I mentioned sometimes needs to read pages so by the way one thing to observe is that the database server is writing log records but it's reading data pages so there's also different my corns poram system in the sense that the sort of things that are being read and written are quite different in addition it turns out that in ordinary operation the database server knows doesn't have to send quorum reads because the database server tracks for each one of the storage servers how far how much of the prefix of the log that storage server is actually received so the database server is keeping track of these six numbers so so first of all log entries are numbered just one two three four five the database server sends that new log entries to all the storage servers the storage servers that receive them respond saying oh yeah I got log entries 79 and furthermore you know I have every log entry before 79 also the database server keeps track of these numbers how far each server has gotten or what the highest sort of contiguous log entry number is that each of the servers has gotten so that way when the database server needs to do a read it just picks a storage server that's up to date and sends the read request for the page it wants just to that storage server so the the database server does have to do quorum writes but it basically doesn't ordinarily have to do quorum reads and knows which of these storage servers are up to date and just reads from one of them so the reason I keep ur than they would be in a that just reads one copy of the page and doesn't have to go through the expense of a quorum read now it does sometimes use quorum reads it turns out that during crash recovery you know if the crash during crash recovery of the database server and so this is different from a crash recovery of the storage service if the database server itself sir crash in me because the it's running in an ec2 instance on some piece of hardware some real piece of hardware may be that piece of hardware suffers a failure the database server crashes there's some monitoring infrastructure at Amazon that says oh wait a minute you know the database the Aurora database server over running for a customer or whatever just crashed and Amazon will automatically fire up a new ec2 instance start up the database software and that ec2 instance and sort of tell it look your data is sitting on this particular volume this set of storage systems please clean up any partially executed transactions that are evident in the logs stored in these storage servers and continue so we have to and that's the point at which Aurora uses quorum logic for weeds because this database server when the old when the previous database server crashed it was almost certainly partway through executing some set of transactions so the state of play at the time of the crash was well it's completed some transactions and committed them and their log entries are on a quorum plus it's in the middle of executing some other set of transactions which also may have log entries on on a quorum but because a database server crashed midway through those transactions they can never be completed and for those transactions that haven't completed in addition there may be you know we may have a situation in which you know maybe log entry this server has log on three hundred and the Surrey has logon 302 and there's a hundred and four somewhere but no you know for I as yet uncommitted transaction before the crash made me know server got a copy of log entry 103 so after a crash and remember the new database service recovering it does quorum reads to basically find the point in the log the highest log number for which every preceding log entry exists somewhere in the storage service so basically it finds the first missing the number of the first missing log entry which is 103 and says well and so we're missing a log entry we can't do anything with a log after this point because we're like missing an update so the database server does these quorum reads it finds a hundred and three is the first entry that's MIT that's I can't you know I look at my quorum the server's I can reach and 103 is not there and the database server will send out a message to all the server saying look please just discard every log entry from 103 onwards and those mussels necessarily not include log entries from committed transactions because we know a transaction can't commit until all of its entries are on a right corner so we would be guaranteed to see them so we're only discarding log entries from uncommitted transactions of course so we're sort of cutting off the log here at login 302 these log entries that we're preserving now may actually include log entries from uncommitted transactions from transactions that were interrupted by the crash and the database server actually has to detect those which you can by seeing a hope you know a certain transaction there's it has update entries in the log but no commit record the database server will find the full set of those uncompleted transactions and basically issue undo operations I sort of knew log entries that undo all of the changes that that that those uncommitted transactions made and you know that's the point at which Aurora needs this these old values in the log entries so that a server that's doing recovery after a crash can sort of back out of partially completed transactions all right one another thing I'd like to talk about is how Aurora deals with big databases so so far I've explained the storage setup as if the database just has these six replicas of its storage and if that was all there was to it basically a database couldn't be you know each of these just a computer with a disk or two or something attached to it if this were the way the full situation then we couldn't have a database that was bigger than the amount of storage that you could put on a single machine there's the fact that we have six machines doesn't give us six times as much usable storage because each one I'm storing a replica of the same old data again and again and you know so I want to use solid-state drives or something we can put you know terabytes of storage on a single machine but we can't put you know hundreds of terabytes on a single machine so in order to support customers who need like more than ten terabytes who need to have vast databases Amazon is happy Amazon will split up the databases data onto multiple sets of six replicas so and the kind of unit of sharding the unit of splitting up the data I think is 10 gigabytes so a database that needs 20 gigabytes of data will use two protection groups these these PG things to its data you know sit on half of it will sit on the six servers of protection Group one and then they'll be another six servers you know possibly a different set of six storage servers because Amazon's running and like a huge fleet of these storage servers that are jointly used by all of its Aurora customers the second ten gigabytes of the databases 20 gigabytes of data we'll be replicated on another set of you know typically different I'll you know there could be overlap between these but typically just a different set of six server so now we get 20 gigabytes a day done and we have more of these as a database goes bigger one interesting piece of fallout from this is that while it's clear that you can take the data pages and split them up over multiple independent protection groups maybe you know odd numbered data pages from your b-tree go on PG one and even number pages go on PG - it's good you can shard split up the data pages it's not immediately obvious what to do with a log all right how do you split up the log if you have two of these two protection groups or more in a mantra tection group and the answer that amazon does is that that that Aurora uses is that the database server when it's sending out a log record it looks at the data that the log record modifies and figures out which protection groups store that data and it sends each log record just to the protection groups that store data that's mentioned that's modified in the log entry and so that means that each of these protection groups store some fraction of the data pages plus all the log records that apply to those data pages see these protection groups stores a subset of a log that's relevant to its pages so a final maybe I erase the photons requirements but a final requirement is that if a if ass one of these storage servers crashes we want to be able to replace it as soon as possible right because you know if we wait too long then we risk maybe three of them are four of them crashing and a four of them crash then we actually can't recover because then we don't have a reform anymore so we need to regain replication as soon as possible if you think about any one storage server sure this this do which server is storing 10 gigabytes for you know my databases protection group but in fact the physical thing you know the physical setup of any one of these servers is that it has a you know maybe a one or two or something terabyte disk on it that's storing 10 gigabyte segments of a hundred or more different Aurora instances so what's what's on this physical machine is you know 10 terabyte era byte or 10 terabytes or whatever of data in total so when there's a when one of these storage servers crashes it's taking with it not just the 10 gigabytes from my database but also 10 gigabytes from a hundred other people's databases as well and what has to be replicated is not just my 10 gigabytes but the entire terabyte or whatever or more that's stored on this servers solid-state drive and if you think through the numbers you know maybe we have 10 gigabit per second network interfaces if we need to move 10 terabytes across a 10 gigabyte per second network interface from one machine to another it's gonna take I don't know a thousand seconds ten thousand seconds maybe ten thousand seconds and that's way too long right we don't want to have to sit there and wait you know it we don't want to have a strategy in which the way we weak we can reconstruct this is to find is to have another machine that was replicating everything on it and had that machine send 10 terabytes to a replacement machine we're gonna be able to reconstruct the data far faster than that and so the actual setup they use is that if I have a particular storage server it stores many many segments you know replicas of many 10 gigabyte protection groups so maybe this protection group maybe this segment that it's storing data for the other envy for this one the other replicas are you know these five other machines all right so these are all storing segments of protection group a and so you know there's a whole bunch of other ones that we're also storing so I mean we may be this particular machine also stores a replica for protecting group B but the other copies of the data for B are going to be put on a disjoint set of servers right so now there's five servers that have the other copies of B and so on for all of the segments that this server that are sitting on this storage servers hard drive for you know many many different Aurora instances so that means that this machine goes down the replacement strategy is that we pick if we're say we're storing a hundred of these segments on it we pick a hundred different storage servers each of which is gonna pick up one new segment that is each of which is going to now be participating in one more protection group so one one we miss like one server to be replicate on for each of these ten gigabytes segments and now we have you know maybe 100 sort of different segment servers and you know I probably storing other stuff but they have a little bit of free disk space and then for each of these we pick one machine one of the replicas that we're going to copy the data from one of the remaining replicas so maybe for a we're going to copy from there for B from here you know if we have five other copies with C we pick a different server for C and so we have we copy a from this server to that server and B like this and C like this and so now we have a hundred different 10 gigabyte copies going on in parallel across the network and assuming you know we have enough servers that these can all be disjoint and we have plenty of bandwidth in switching network that connects them now we can copy our terabyte or 10 terabytes or whatever of data and total in parallel with a hundredfold parallelism and the whole thing will take you know 10 seconds or something instead of taking a thousand seconds if there were just two machines involved anyway so this is this is the strategies they use and it means that they can recover you know for machine dies they can recover in parallel from one machine's death extremely quickly if lots of machines diets doesn't work as well but they can recover from single they can be replicate from single machine crashes extremely quickly alright so a final thing that the paper mentions if you look at figure three you'll see that not only do they have this main database but they also have replica databases so for many of their customers many of their customers see far more read-only queries than they see readwrite queries that is if you think about a web server if you just view a web page on some website then chances are the web server you connected to has to read lots and lots and stuff in order to generate all the things that are shown on the page to you maybe hundreds of different items have to be read out of the database or so out of some database but the number of writes for a typical web page view is usually much much smaller maybe some statistics have to be updated or a little bit of history for you or something so you might have a hundred to one ratio of reads to writes that is you may typically have a large large large number of straight read only database queries now with this set up the writes can only go through the one database server because we really can only support one writer for this storage strategy and I think you know one place where the rubber really hits the road there is that the log entries have to be numbered sequentially and that's easy to do if all the writes go through a single server and extremely difficult if we have lots of different servers all sort of writing in an uncoordinated way to the same database so the writes really have to be go through one database but we could set up and indeed Amazon does set up a situation where we have read only database replicas that can read from these storage servers and so the full glory of figure three is that in addition to the main database server that handles the write requests there's also a set of read-only databases and they say they can support up to 15 so you can actually get a lot of you know if your senior we'd have you workload a lot of it can be you know most of it can be sort of hived off to a whole bunch of these read-only databases and when a client sends a read request to read only database what happens is the read only database figures out you know what data pages it needs to serve that request and sends reads into the directly into the storage system without bothering the main readwrite database so the the read-only replica database ascend page requests read requests directly the storage servers and then they'll be no cache those pages so that they can you know respond to future read requests right out of their cache of course they need to be able to update those caches and for that reason Aurora also the main database sends a copy of its log to each of the read-only databases and that's the horizontal lines you see between the blue boxes and figure three that the main database sends all the log entries do these mean only databases which they use to update their cached copies to reflect recent transactions in the database and it means it does mean that the read only database is lag a little bit behind the main database but it turns out for a lot of read-only workloads that's okay if you look at a web page and it's you know 20 milliseconds out of date that's usually not a big problem there are some complexities from this like one problem is that we don't want these relay databases to see data from uncommitted transactions yet and so in this stream of log entries the database may need to be sort of denotes which transactions have committed and they're read-only databases are careful not to apply uncommon uncommitted transactions to their caches they wait till the transactions commit the other complexity that these read-only replicas impose is that the the the these structures he of these andhe structures are quite complex this might be a b-tree it might need to be rebalanced periodically for example I'm the rebalancing is quite a complex operation in which a lot of the tree has to be modified in atomically and so the tree is incorrect while it's being be balanced and you only allowed to look at it after the rebalancing is done if these read-only replicas directly read the pages out of the database there's a risk they might see the be tree that the database that's being stored here in these data pages they may see the bee tree in the middle of a rebalancing or some other operation and the data is just totally illegal and they might crash or just malfunction and when the paper talks about mini transactions and the vdl verses vcl distinction what it's talking about is the machinery by which the database server can tell the storage servers look this complex sequence of log entries must only be revealed all or nothing' atomically to any read-only transactions that's what the mini transactions and VDL are about and basically the read when a read only database asks to see data a data page from a storage server the storage server is careful to either show it data from just before one of these sequence many transaction sequences of log entries or just after but not in the middle all right so that's the all the technical stuff I have to talk about just to kind of summarize what's interesting about the paper and what can be learned from the paper one thing to learn which is just good in general not specific to this paper but everybody in systems should know is the basics of how transaction processing databases work and the sort of impact that the interaction between transaction processing databases and the storage systems because this comes up a lot it's like a pervasive you know the performance and crash recoverability complexity of running a real database just comes up over and over again in systems design another thing to learn this paper is this idea of quorums and overlap the technique of overlapping read/write quorums in order to always be able to see the latest data but also get fault tolerance and of course this comes up in raft also raft has a strong kind of quorum flavor to it another interesting thought from this paper is that the database and the storage system are basically Co designed as kind of an integrated there's integration across the database layer and the storage layer or nearly redesigned to try to design systems so they have you know good separation between consumers of services and the sort of infrastructure services like typically storage is very general-purpose not aimed at a particular application just you know because that's a pleasant design and it also means that lots of different uses can be made of the same infrastructure but here the performance issues were so extreme you know they would have to get a 35 times performance improvement by sort of blurring this boundary this was a situation in which general-purpose storage was actually really not advantageous and they got a big win by abandoning that idea and a final set of things to get out of the papers all the interesting sometimes kind of implicit information about what was valuable to these Amazon engineers who you know really know what they're doing about what concerns they had about cloud infrastructure like the amount of worry that they put into the possibility of an entire availability zone might fail it's an important tidbit the fact that transient slowness of individual storage servers was important is another thing that actually also comes up a lot and finally the implication that the network is the main bottleneck because after all they were it went to extreme lengths to send less data over the network but in return the storage servers have to do more work and they put it they're willing to you know 6 copies the data and have 6 CPUs all replicating the execution of applying these redo log entries apparently CPU is relatively cheap for them whereas the network capacity was extremely important all right that's all I have to say and see you next week