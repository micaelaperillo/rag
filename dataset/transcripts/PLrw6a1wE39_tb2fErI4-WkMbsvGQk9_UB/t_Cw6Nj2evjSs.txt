I'd like to like to talk about farms day and optimistic concurrency control which is the main interesting technique that uses the reason we're talking about farm it's this the last paper in the series about transactions and replication and sharding and this is still an open research area where people are totally not satisfied with performance or in the kind of performance versus consistency trade-offs that are available and they're still trying to do better and in particular this particular paper is motivated by the huge performance potential of these new RDMA NICs so you may be wondering since we just read about spanner how farm differs some spanner both of them after all replicate and they use two-phase commit for transactions of that level they seem pretty similar spanner as a is a deployed systems been used a lot for a long time its main focus is on Geographic replication that is to be able to have copies on there like east and west coasts and different data centers and be able to have reasonably efficient transactions that involve pieces of data in lots of different places and the most innovative thing about it because in order to try to solve the problem of how long it takes to do two-phase commit over long distances is that it has a special optimization path for read-only transactions using synchronized time and the performance you get out of spanner if you remember is that a read/write transaction takes 10 to 100 milliseconds depending on how close together the different data centers are farm makes a very different set of design decisions and targets a different kind of workload first of all it's a research prototype so it's not by any means a finished product and the goal is to explore the potential of these new RDMA high speed networking hardware so it's really still an exploratory system it assumes that all replicas are in the same data center absolutely it doesn't wouldn't make sense the replicas were in even in different data centers let alone on East Coast versus West Coast so it's not trying to solve a problem that spanner is about what happens if an entire data center goes down can I so get out my data really that's does the extent that it has fault tolerance is for individual crashes or maybe try to recover after a whole data center loses power and gets restored again it uses this RDMA technique which I'll talk about but already may turns out to seriously restrict the design options and because of this farm is forced to use optimistic concurrency control on the other hand the performance they get is far far higher than spanner farm can do a transit a simple transaction in 58 microseconds and this is from figure 7 and section 6.3 so this is 58 microseconds versus to 10 milliseconds that the spanner takes is that's about a hundred times faster than spanner so that's maybe the main huge differences that farm us how much higher performance but is not aimed at Geographic replication so this you know farms performance is extremely impressive like how much faster than anything else another way to look at it is that spanner and farm target different bottlenecks and span are the main bottleneck the people worried about is the speed of light and network speed of light delays and network leaves between data centers whereas in farm the main bottlenecks that the design is worried about is is CPU time on the server's because they kind of wished away the speed of light and network delays by putting all the replicas in the same data center all right so sort of the background of how this fits into the 684 sequence the setup and farm is that you have it's all running in one datacenter there's a sort of configuration manager this which we've seen before and the configuration managers in charge of deciding which rep which servers should be the primary in the backup before each shard of data and if you read carefully you'll see that they use zookeeper in order to help them implement this configuration manager but it's not not the focus of the paper at all instead the interesting thing is that the data is sharded split up by key across a bunch of primary backup payers so I mean one shard goes on you know primary one server primary one backup one another short one primary to backup two and so forth and that means that anytime you update data you need to update it both on the primary and on the backup and these are not these primaries these replicas are not maintained by PACs or anything like it instead all the replicas of the data are updated whenever there's a change and if you read you always have to read from the primary the reason for this replication of course is fault tolerance and the kind of fault tolerance they get is that as long as one replicas of a given shard is available then that shard will be available so they only require one living replica not a majority and the system as a whole if there's say a data center white power failure it can recover as long as there's at least one replicas of every shard in the system another way of putting that is if you they have F plus one replicas then they can tolerate up to F failures for that shard in addition to the primary backup copies of each sort of data there's transaction code that runs it's maybe most convenient to think of the transaction code is running as separate clients in fact they run the transaction code in their experiments on the same machines as the actual farm storage servers but I'll mostly think of them as as being a separate set of clients and the clients are running transactions and the transactions need to read and write data objects that are stored in the in the sharded servers in addition these transaction these clients each client not only runs the transactions but also acts as that transaction coordinator for two-phase commit okay so it's the basic set up the way they get performance because this really this is a paper all about how you can get high performance and still have transactions one way they get high performances with sharding these are the ingredients in a sense the main way is through sharding in experiments they shard their data over 90 ways for 90 servers or maybe it's 45 ways and not just if as long as the operations and different shards are more or less independent of each other that just gets you an automatic 90 times speed up because you can run whatever it is you're running in parallel on 90 syrups this huge went from shorter sharding um another trick they play in order to get good performance as the data all has to fit in the RAM of the servers they don't really store the data on disk it all has to fit in RAM and that means of course you can get out of pretty quickly another way that they get high performance is they need to tolerate power failures which means that they can't just be using RAM because they need to recover the data after a power failure and RAM loses contents on a power failure so they have a clever non-volatile Ram scheme for having the contents of RAM the data survived power failures this is in contrast to storing the data persistently on disk i'm is much faster than disk um another trick they play is they use this RDMA technique which essentially clever network interface cards that allow that accept packets that instruct that then that we're interface card to directly read and write the memory of the server without interrupting the server I know that trick they play is what you often call kernel bypass which means that the application level code can directly access the network interface card without getting the kernel involved okay so these are all the sort of clever tricks we're looking at out pour it that they used to get high performance and I'll talk about we've already talked about sharding a lot but I'll talk about the rest in this lecture okay so first I'll talk about non-volatile Ram I mean this is really a topic that doesn't doesn't really affect the rest of the design directly as I said all the data and for farm is stored in RAM when you update it when a client transaction updates a piece of data what that really means is it reaches out to the relevant servers that store the data and causes those servers to modify the whatever object is the transaction is modifying to object modify it right in RAM and that's as far as the writes get they don't go to disk and this is you know contrast to your raft implementations for example which spent a lot of time persisting data to disk there's no persisting and in farm this is a big wind writing stuff in RAM write a write to ram takes about 200 nanoseconds whereas a raid even to a solid state drive which is pretty fast a right to a stall seek drive takes about a hundred microseconds and a write to our hard drive takes about ten milliseconds so being able to write to ram is worth many many orders of magnitude and speed for transactions that modify things but of course iran loses its content and a power failure so it's not persistent by itself as a side you might think that writing modifications to the RAM of multiple servers that if you have replica servers and you update all the replicas that that might be persistent enough and so after all if you have F 1 F +1 replicas you can tolerate up to F failures and the reason why just simply writing to Ram on multiple servers is not good enough is that a site-wide power failure will destroy all of your servers and thus violating the assumption that the failures are in different servers are independent so we need a scheme that it's gonna work even if power fails to the entire data center so what what forum does is it it puts a battery a big battery in every rack and runs the power supply system through the batteries so the batteries automatically take over if there's a power failure and keep all their machines running at least until the battery fails but of course you know the battery is not very big it may only be able to run their their machines for say 10 minutes or something so the battery by itself is not enough to make this the system be able to withstand a lengthy power failure so instead the battery system when it sees that the main power is failed the battery system while it keeps the server's Marling also alerts the server's all the servers and with some kind of interrupt or message telling them look the powers just failed you know you only got 10 minutes left before the batteries fail also so at that point the software on farms servers copies all of rain active stops all processing it for farm first and then copies each server copies all of its RAM to a solid-state drive attached to that server I'm what wished could take a couple minutes and once all the RAM is copied to the solid-state drive then the machine shuts itself down and turns itself off so if all goes well there's a site-wide power failure all the machines save their RAM to disk when the power comes back up in the datacenter all the machines will when they reboot will read the memory image that was saved on disk restored into RAM and but there's some recovery that has to go on but basically they won't have lost any of their persistent state due to the power failure and so what that really means is that the farm is using conventional Ram but it's essentially made the RAM non-volatile being able to survive power failures with the this trick of using a battery having a battery alert the server having the server store the RAM content solid-state drives any questions about the nvram scheme alright this is a is a useful trick but it is worthwhile keeping mind that it really only helps if there's power failures that is if the you know the whole sequence of events only it gets set in train when the battery notices that the main power is failed if there's some other reason causing the server to fail like something goes wrong with the hardware or there's a bug in the software that causes a crash those crashes the non-volatile Ram system is just nothing to do with those crashes those crashes will cause the machine to reboot and lose the contents of its RAM and it won't be able to recover them so this NVRAM scheme is good for power failures but not other crashes and so that's why in addition to the NVRAM farm also has multiple copies multiple replicas of each shard all right so this NVRAM scheme essentially eliminates persistence rates as a bottleneck in the performance of the system leaving only as performance bottlenecks the network and the CPU which is what we'll talk about next ok so there's a question if the datacenter power fails and farm lose everything for solid-state drive would it be possible to carry all the data to a different data center and continue operation there in principle absolutely in practice I think would be would all certainly be easier to restore power to the data center then to move the drives the problem is there's no power and the power in the dated old data center so you'd have to physically move the drives and the computers maybe just the drives to the new data center so this was if you wanted to do this it might be possible but it's certainly not it's not what the farm designers had in mind they assumed the power be restored okay so that's NVRAM and at this point we can just ignore nvram for the rest of the design it doesn't it doesn't really interact with the rest of the design except that we know we're have to worry about writing data to disk all right so as I mentioned the remaining bottlenecks once you eliminate having a great data to disk for persistence in remaining bottlenecks have to do with the CPU and the network in fact in farman and indeed a lot of the systems that i've been involved with the a huge bottleneck has been the cpu time required to deal with network interactions so now we're can CPU are kind of joint bottlenecks here farm doesn't have any kind of speed of light network problems it just has the problems or it just spends a lot of time eliminating bottlenecks having to do is getting network data into and out of the computers so first as a background I want to lay out what the conventional architecture is for getting things like remote procedure call packets between applications and on different computers just so that can we have an idea of why this approach that farm takes is more efficient so typically what's going on is on one computer that maybe wants to send a procedure call message you might have an application and then the application is running in user space there's a user kernel boundary here the application makes system calls into the kernel which are not particularly cheap in order to send data and then there's a whole stack of software inside the kernel involved is sending data over the network there might be what's usually called a socket layer that does buffering which involves copying the data which takes time there's typically a complex TCP the protocol stack that knows all about things like retransmitting and sequence numbers and check sums and flow control there's quite a bit of processing there at the bottom there's a piece of hardware called the network interface card which is has a bunch of registers that the kernel can talk to to configure it and it has hardware required to send bits out over the cable onto the network and so there's some sort of network interface card driver in the kernel and then all self respecting that we're gonna price cards use direct memory access to move packets into and out of host memory so there's going to be things like queues of packets that the network interfaces card has D made into memory the waiting for the kernel to read and outgoing hues the packets that the kernel would like then that we're going to face to car to send as soon as convenient all right so you want to send a message like an RPC request let's go down from the application through the stack network interface card sends the bits out on a cable and then there's the reverse stack on the other side isn't network interface Hardware here in the kernel then organ or face might interrupt the kernel kernel runs driver Code which hands packets to the TCP protocol which writes them into buffers waiting for the application to read them at some point the application gets around reading them makes system calls into the kernel copies the data out of these buffers into user space this is a lot of software it's a lot of processing and a lot of fairly expensive CPU operations like system calls and interrupts and copying data as a result so classical Network communication is relatively slow it's quite hard to build an RPC system with the kind of traditional architecture that can deliver more than say a few hundred thousand or BC messages per second and that might seem like a lot but it's orders of magnitude too few for the kind of performance that farm is trying to target and in general that couple hundred thousand our pcs per second is far far less than the speed that the actual network hardware like Network wire in the network interface card is capable of typically these cables run at things like 10 gigabits per second it's very very hard to write our PC software that can generate small messages of the kind that databases often need to use it's very hard to write software in this style that can generate or absorb anything like 10 gigabits per second of messages that's millions maybe tens of millions of messages per second ok so this is the plan that farm doesn't use and a sort of a reaction to to this plan instead farm uses - - ideas to reduce the costs of pushing packets around the first one I'll call kernel bypass and the idea here is that instead of the application sending all its data down through a complex stack of kernel code instead the application the kernel configures the protection machinery in the computer to allow the application direct access to network interface card so the application can actually reach out and touch the network interfaces registers and tell it what to do in addition the network interface card when it DMAs and this kernel bypass scheme it DNA's directly into application memory where the application can see the bytes arriving directly without kernel intervention and when the application needs to send data the application can create queues that the network interface card can directly read with DMA and send out over the wire so now we've completely eliminated all the kernel code involved in networking kernels just not involved there's no system calls there's no interrupts the application just directly reason why it's memory that the network interface card sees and of course the same thing on the other side and this is a this is an idea that is actually was not possible years ago with network interface cards but most modern serious network interface cards okay can be set up to do this it does however require that the application you know you know all those things that TCP was doing for you like check sums or retransmission the application would now be in charge if we wanted to do this you can actually do this yourself kernel bypass using a toolkit that you can find on the way up called DP DK and it's relatively easy to use and allows people to write extremely high performance networking applications but and so so form does use this it's applications directly you talk to the neck the neck DM ace things right into application memory we have a student question I'm sorry yes does this mean that farm machines run a modified operating system well I I don't know the actual answer that question I believe farm is runs on Windows some form of Windows whether or not they had to modify Windows I do not know in the sort of Linux world in Linux world there's already full support for this it does require kernel intervention because the kernel has to be willing to give ordinarily application code cannot do anything directly with devices so Linux has had to be modified to allow the allow the kernel to delegate hardware access to applications so it does require kernel modifications those monitor occasions are already in Linux and maybe already in Windows also in addition though this on fairly intelligent Knicks because of course you're going to have multiple applications that want to play this game with a network interface card and so modern NICs actually know about talking to multiple distinct cues so that you can have multiple applications each with its own set of cues and the the Nick knows about so it did it has required modification of a lot of things okay so sort of step one is is Colonel bypass idea step two is even cleverer next and now we're starting to get into hardware that is not in wide use of the moment you can buy it commercially but it's not the default is this RDMA scheme which is remote direct memory access and here this is sort of special kind of network interface cards that support remote support our DMA so now we have an RDM a neck both sides have to have these special network interface cards so I'm drawing these is connected by a cable in fact always there's a switch here that has connections to many different servers and allows any server to talk to any server okay so we have these RDMA necks and we had again we have the applications and application assist memory and now though the application can essentially send a special message through the neck that asks so we have a an application on the source host and maybe we wouldn't call this the destination host can send a special message through the our DMA system that tells this network interface card to directly read or write a byte some bytes of memory probably a cache line of memory in the target applications address space directly so hardware and software on the network interface controller are doing a read and write read or write of the application target applications memory directly and then so we have a sort of request going here that causes the read or write and then sending the result back to really two other incoming queue on the source application and the cool thing about this is that this computer's the CPU this application didn't know anything about the read or write the read or write is executed completely in firmware in the network interface card so it's not there's no interrupts here the application didn't have to think about the request or think about replying network interface card just reads or writes a memory and sends a result back to the source application and this is much much lower overhead way of getting at of all you need to do is read or write memory and stuff in the RAM of the target application this is a much faster way of doing a simple read or write than sending in our PC call even with magic kernel bypass networking it's a question does this mean that already may always require kernel bypass to work at all you know I don't know the answer to that I think I've only ever heard it used in conjunction with kernel bypass cuz you know the people who are interested in any of this or are interested in it only for tremendous performance and I think you would waste you throw away a lot of the performance I'm guessing you throw away a lot of the performance win if you had to send the requests through the kernel okay another question that the the question notes TCP software's TCP supports in order delivery duplicate detection and a lot of other excellent properties which you actually need and so it would actually be extremely awkward if this setup sacrificed reliable delivery or in order delivery and so the answer the question is actually these are DMA NICs run their own reliable sequenced protocol that's like TCP although not TCP between the necks and so when you ask your already am a neck to do a read or write it'll keep you transmitting until if the you know if the request is lost and keep reassurance meaning till it gets a response and it actually tells the originating software did the request succeed or not so you get an acknowledgment back finally so yeah you know in fact have to sacrifice most of TCP is good properties now this stuff only works over a local network I don't believe our DMA would be satisfactory like between distant data centers so there's all tuned up for very low speed of light access okay a particular piece of jargon that the paper uses is one-sided our DMA and that's basically what I've just mentioned when application uses our DMA to read or write the memory of another that's one site our DMA now in fact farm uses our DMA to send messages in an RPC like protocol so in fact sometimes farm directly reads with one sided our DMA but sometimes what farm is using our DMA for is to append a message to an incoming message queue inside the target so sometimes what the what the well actually always with writes what farm is actually doing is using our DMA to write to append a new message to an incoming queue in the target which the target will pull since there's nobody interrupts here the way target the way the destination of a message like this knows I got the messages that periodically checks one of these keys queues and memory to see how have I gotten a recent message from anyone okay so once I did already MA is just to read or write but using our DMA to send a message or append either to a message queue or to a log sometimes farm appends messages or log entries to a log and another server also uses our DMA and you know this memory that's being written into is all non-volatile so all of it the message queues it's all written to disk if there's a power failure the performance of this is the figure 2 shows that you can get 10 million small our DMA reads and writes per second which is fantastic far far faster than you can send messages like our pcs using TCP and the latency of using our DMA to do a simple read or write is about 5 microseconds so again this is you know very very short 5 microseconds is it's slower than accessing your own local memory but it's faster than sort of anything else people do in networks ok so this is sort of a promise there's this fabulous our DMA technology that came out a while ago that at the farm people wanted to exploit you know the coolest possible thing that you could imagine doing with this is using our DMA one sign it already am a reason rights to directly do all the reason writes a records stored in database servers memory so wouldn't be fantastic if we could just never talk to the database server CPU or software but just get at the data that we need you know in five microseconds a pop using direct one-sided our DMA Reiser writes so in a sense this paper is about you know you you start there what do you have to do to actually build something useful so an interesting question by the way is could you in fact implement transactions using one-sided RDMA that is you know anything we wanted to read or write data in server the only use already may and never actually send messages that have to be interpreted by the server software it's worth thinking about in a sense farm is answering that question with a no because that's not really how farm works but but it is absolutely worth thinking how come pure one-sided RDMA couldn't be made to work alright so the challenges to using our DMA in a transactional system that has replication and sharding so that that's the challenge we have is how to combine already made with transactions charting and replication because you need to have sharding and transactions replication to have a seriously useful database system it turns out that all the protocols we've seen so far for doing transactions replication require active participation by the server software that is the server has to be in all the protocols we've seen so far the server's actively involved in helping the clients get at read or write the data so for example in the two-phase commit schemes we've seen the server has to do things like decide whether a record is locked and if it's not walk set the lock on it right it's not clear how you could do that with our DMA the server has to do things like in spanner you know there's all these versions it was the server that was thinking about how to find the latest version similarly if we have transactions in two-phase commit data on the server it's not just data there's committed data there's data that's been written but hasn't committed yet and again traditionally it's the server that sorts out whether data recently updated data is committed yet and that's to sort of protect the clients from you know prevent them from seeing data that's locked or not yet known to be committed and what that means is that without some clever thought RDMA or one-sided pure use of our DME one-sided RDMA doesn't seem to be immediately compatible with transactions and replication and indeed farm while farm does use one-sided it reads to get out directly at data in the database it is not not able to use one-sided rights to modify the data okay so this leads us to optimistic concurrency control it turns out that the main trick in a sense that farm uses to allow it both use RDMA and get transactions is by using optimistic concurrency control so if you remember I mentioned earlier that concurrency control schemes are kind of divided into two broad categories pessimistic and optimistic pessimistic schemes use locks and the idea is that if you have a transaction that's gonna read or write some data before you can read or write the data or look at it at all it must acquire a lock and it must wait for the lock and so you read about two-phase locking for example in that reading from 633 so before you use data you have to lock it and you hold the lock for the entire duration of the transaction and only if the transaction commits or aborts do you release the lock and if there's conflicts because two transactions want to write the same data at the same time or one wants to read and one that monster right they can't do it at the same time one of them has to block or all but one of the transactions that went to you some data missed a block wait for the lock to be released um and of course this locking scheme is the fact that the data has to be locked and that somebody has to keep track of who owns the lock and when the lock is released etcetera this is one thing that makes our DMA it's not clear how you can do rights or even reads using our DMA in a locking scheme because somebody has to enforce the locks I'm being a little tentative about this because I suspect that with more clever our DMA NICs that could support a wider range of operations like atomic test and set you might someday be able to do a locking scheme with pure one-sided RDMA but farm doesn't do it okay so what farm actually uses as an optimistic scheme and here in an optimistic scheme you can use at least you can read without locking you just read the data you don't know yet whether you are allowed to read the data or whether somebody else is in the model middle of modifying it or anything you just read the data and a transaction it uses what it whatever it happens to be and you also don't directly write the data in optimistic schemes instead you buffered so you buffer writes locally and in the client until the transaction finally ends and then when the transaction finally finishes and you want to try to commit it there's a validate what's called a validation stage in which the transaction processing system tries to figure out whether the actual reason rights you did were consistent with serializability that is they try to figure out oh was somebody writing the data while I was reading it and if they were boy we can't commit this transaction because it computed with garbage instead of consistent read values and so if the validation succeeds then you commit and if the validation doesn't succeed if you detect somebody else was messing with the data while you were trying to use it at abort so that means that if there's conflicts if you're reading or writing data and some other transactions also modifying at the same time optimistic schemes abort at that point because the computation is already incorrect at the commit point that is you already read the damage data you weren't supposed to read so there's no way to for example block you know until things are okay instead the transactions already kind of poisoned and just has to abort and possibly be try okay so farm uses optimistic because he wants to be able to use one-sided RDMA to just read whatever's there very quickly so this this design was really forced by use of our DMA this is often abbreviated OCC for optimistic concurrency control all right and then the interesting thing an optimistic concurrency control protocols is how validation works how do you actually detect that somebody else was writing the data while you were trying to use it and that's actually mainly gonna be what I talked about in the rest of this lecture and just again though just to retire this back to the top level of the design what this is doing for farm is that the reads can use one-sided RDMA because and therefore be extremely fast because we're gonna check later whether the reads were okay all right farms a research prototype it doesn't support things like sequel it supports a fairly simple API for transactions this is the API just to give you a tease for what a transaction code might actually look like if you have a transaction it's gotta to clear the start of the transaction because we need to say oh this particular set of Reason rights needs to occur as a complete transaction the code declares a new transaction by calling TX create this is all laid out by the way in the paper I think from 2014 a slightly earlier paper by the same authors you create a new transaction and then you explicitly read those functions to read objects and you have to supply an object identifier an OID indicating what object you want to read then you get back some object and you can modify the object in local memory and we didn't write it you have a copy of it that you've read from the server the TX read back from the server so you know you might increment some field in the object and then when you want to update an object you call this TX right and again you give it the object ID and the new object contents and finally when you're through with all of this you've got to tell the assistant to commit this transaction actually do validation and if it succeeds cause the rights to really take effect and be visible and you call this commit routine the community team runs a whole bunch of stuff in figure 4 which we'll talk about and it returns this okay value and it's required to tell the application oh did the commit succeed or was it aborted so we need the return this okay return valued you know correctly indicate by the transaction succeeded okay there's some questions one is question since OCC aborts if there's contention question is whether retries involve exponential back-off because otherwise it seems like if you just instantly be tried and that there were a lot of transactions all trying to update the same value at the same time they'd all aboard they'd all retry and waste a lot of time and I don't know the answer to that question I don't remember seeing them mentioning exponential back-off in the paper but it would make a huge amount of sense to delay between retries and to increase the delay to give somebody a chance of succeeding this is much like the randomization of the raft collects Tanner's another question is the farm API closer in spirit to a no sequel database yeah you know that's one way of viewing it it really that it doesn't have any of the fancy query stuff like joins for example that sequel has it's really a very low-level kind of readwrite interface plus the transaction support so you you can sort of view it as a no sequel database maybe with with transactions all right this is what a transaction looks like and these are all these are library calls created read/write commit commit as a sort of complex write recall that actually runs the transaction coordinator code first what a rare variant of two-phase commit this described in figure four just repeat that the while the recall goes off and actually reads the relevant server the right call just locally buffers then the new the modified object and it's only in commit that the objects are sent to the servers these object IDs are actually compound identifiers for objects and they contain two parts one is the identify a region which is that all the memory of all the servers is split up into these regions and the configuration manager sort of tracks which servers replicate which region number so there's a reason number in here and then and then you you know you client can look up in a table the current primary and backups for a given region number and then there's an address such as the straight memory address within that region and so the client uses the reason number to pick the primary in the backup to talk to and then it hands the address to the our DMA NIC and tells it look please read at this address in order to get fetch this object alright another piece of detail we have to get out of the way is to look at the server memory layout I'm in any one server there's a bunch of stuff in memory so one part is that the server has in its memory its if it's it's replicating one or more regions it has the actual regions and or what a reason contains is a whole bunch of these objects and each object there's a lot of objects objects sitting in memory each object has in it a header which contains the version number so these are versioned objects but each object only has one version at a time so this is version number and in the high bit let me try again here and the high bit of each version number is a lock flag so in the header of an object there's a lock flag and the high bit and then a version number in a little bit and then the actual data of the object so each object has the same servers memory it's the same layout a lock bit in the high bit and the current version number a little bit and every time the system writes modifies an object it increases the version number and let's see how the lock bits are used in a couple minutes in addition in the server's memory there are pairs of cue pairs of message queues and logs one for every other computer in the system so that means that you know if there's four other servers in the system that are running or if there's four servers that are running transactions there's going to be four logs sitting in memory that can be appended to with our DMA one for each of the other servers and that means that one for each of the other computers can run transactions so that means that the transaction code running on you know so number of them you know it's the transaction code running on computer to when it wants to talk to this server and append to its log which as well see it's actually going to append to server twos log in this servers memory so there's a total N squared of these queues floating around in in in each servers memory and it certainly seems like there's actually one set of logs which are meant to be I would non-volatile and then also possibly a separate set of message queues which are used just for more RPC like communication again one in each server one queue message incoming message cube per other server written with our DMA writes all right actually the next thing to talk about is a year four in the paper this is feet four and this explains the occ commit protocol that farm uses and I'm gonna go through mostly steps one by one and actually to to begin with I'm gonna focus only on the concurrency control part of this it turns out these steps also do replication as well as implement serializable transactions but we'll talk about the replication for fault tolerance a little bit later okay so the first thing that happens is the execute phase and this is the TX reads and TX writes the reason writes that the client transaction is doing and so each of these arrows here what this means is that the transaction runs on computer C and when needs to read something it uses one-sided RDMA we to simply read it out of the relevant primary servers memory so what we got here was a primary backup primary backup primary backup for three different shards and we're imagining that our transaction read something from one object from each of these shards using one-sided RDMA reason that means these blindingly fast five microseconds each okay so the client reads everything it needs to read for the transaction also anything that's going to write it first reads and it has to do it do this read has to first read because it needs to get the version number the initial version number all right so that's the execute phase then when the transaction calls TX commits to indicate that it's totally done the library on the you know the TX commit call on on the client acts as a transaction coordinator and runs this whole protocol which is a kind of elaborate version of two-phase commit the first phase and that's described in terms of rounds of messages so the transaction coordinator sends a bunch of LOC messages and wait for them to reply and then validate messages and waste for the all the replies so the first phase in the commit protocol is the lock fees in this phase what the client is sending is it sends to each primary the identity of the object in for each object for clients written and needs to send that updated object to the relevant primary so it sends the updated objects the primary and as an as a new log entry in the primaries log you know for this client so the client really abusing already made to append to the primaries log and what it's appending is the object ID of the writ of the object wants to write the version number that the client initially read when it read the object and the new value so it appends the object of yours number and new value to the primary logon for the primary beach of the charge that it's written an object in so these I guess what's going on here is that the this transaction wrote two different objects one on primary one and the other on primary to know when this is done when the transaction coordinator gets back the well alright so now the these new log records are sitting in the logs of the primaries the primary though has to actually actively process these log entries because it needs to check and they sort of do a number of checks involved with validation to see if the if this primary is part of the transaction can be allowed to commit so at this point we have to wait for each primary to to poll the this clients log in the primaries memory see that there's a new log entry and process that new log entry and then send a yes-or-no vote to say whether it is or is not willing to do its part of the transaction all right so what does the primary do when it's polling loop sees that an incoming lock log entry from a client first of all if that object with the object ID is currently blocked then the primary rejects this lock message and sends back a message to the client using RDMA saying no that this transaction cannot be allowed to proceed I'm voting no and two-phase commit and that will cause the transaction coordinator to abort the transaction and the other is not locked then the next thing the primary does is check the version numbers it checks to make sure that the version number that the client sent it that is the version number of the client originally read is unchanged and if the version numbers changed that means that between when our transaction read and when it wrote somebody else wrote the object if the version numbers changed and so the version numbers changed again the primary will respond no and forbid the transaction from continuing but if the version number is the same in the lock that's not set and the primary will set the lock and return a positive response back to the client now because the primary's multi-threaded running on multiple CPUs and there may be other transactions there may be other CPUs reading other incoming log cues from other clients at the same time on the same primary there may be races between different transactions or lock the clock record processing from different transactions trying to modify the same object so the primary actually uses an atomic instruction a compare and swap instruction in order to both check check the version number and lock and set the lock a bit on that version number as an atomic operation and this is the reason why the lock of it has to be in the high bits of the version number so that a single instruction can do a compare and swap on the version number and the lock bit okay now one thing to note is that if the objects already locked there's no blocking there's no waiting for the lock to be released the primary simply sends back a know if some other transaction has it locked alright any questions about the lock fees of of Committee all right back in the trend head in the client which is acting his transaction coordinator it waits for responses from all the primaries from the primaries of the shard so for every object that the transaction modified if any of them say no if they need them reject the transaction then the transaction coordinator aborts the whole transaction and actually sends out messages to all the primaries saying I changed my mind I don't want to commit this transaction after all but if they all answered yes of all the primaries answer yes then the transaction coordinator thinks that decides that the transaction can actually commit but the primaries of course don't know whether they all voted yes or not so the transaction coordinator has to notify every ball the primary so yes deed everybody voted yes so please do actually commit this and the way the client does this is by appending another record to the logs of the primaries for each modified object this time it's a commit backup record that it's a pending and the this time the transaction coordinator I'm sorry I did commit primary I'm skipping over valide didn't commit backup for now I'll talk about those later so just ignore those for the moment the transaction coordinator goes on to commit primary sends pens that commit primary to each primaries log and the transaction coordinator only has to wait for the hardware RDMA acknowledgments it doesn't have to wait for the primary just actually process the log record the transaction coordinator it turns out as soon as it gets a single acknowledgment from any of the primaries it can return yes the okay equals true to the transactions signifying that the transaction six succeeded and then there's another stage later on where the once the transaction coordinator knows that every primary knows that the transaction coordinated committed you can tell all the primaries that they can discard all the log entries for this transaction okay now there's one last thing that has to happen the primaries which are looking at the logs their polling the Long's they'll notice that there's a commit primary record at some point and then on the primary that receives the commit primary log entry will it knows that it had locked that object previously and that the object must still be locked so what the primary will do is update the object in its memory with the new contents that were previously sent in the lock message I'm increment the version number associated with that object and finally clear the lock bit on that object and what that means is that as soon as a primary receives and processes a commit primary log message it may since it clears the lock a bit and updates the data it may well expose this new data to other transactions other transactions after this point are free to use it are free to use the object with its new value and new version number all right I'm gonna do an example any questions about the machinery before I start thinking about an example feel free to ask questions any time alright so how about an example let's suppose we have two transactions transaction one and transaction two and they're both trying to do the same thing they both just wanna increment X X is the object sitting off in some servers memory so so both we got two transactions running running through this before we look into what actually happens we should remind ourselves what the valid possibilities are for the outcomes so and that's all about serializability farm guaranteed serialize ability so that means that whatever farm actually does it has to be equivalent to some one at a time execution of these two transactions so we're allowed to see was the results you would see if t1 ran and then strictly afterwards t2 ran or we can see the results that could ensue if t2 ran and then t1 run those are the only possibilities now in fact farm is entitled to abort a transaction so we also have to consider the possibility that one of the two transactions aborted or indeed that they both aborted I since they're doing both doing the same thing there's a certain amount of symmetry here so one possibility is that they both committed and that means two increments happen so one legal possibilities that X is equal to 2 and both then the TX it has to agree with whether things a bit or aborted or committed so that both transactions need to CTX commit returned true in this case another possibility is that only one of them transactions committed and the other aborted and then we want to see only one true and the other false and another possibilities maybe they both aborted we don't think this could necessarily happen but it's actually legal so that X isn't changed and we want both to get false back from TX commit so we better better not see anything other than these three options all right so of course what happens depends on the timing so I'm going to integrate out various different ways that the commit protocol could in early even for convenience I have a handy reminder of what the actual commit protocol is here so one possibility is that they run exactly in lockstep they both send all their messages at the same time they both read at the same time I'm going to assume that X starts out as zero if they both read at the same time that we're going to see zero I assume they both sent out lakh messages at the same time and indeed they accompany their log messages with the value one since they're adding 1 to it and that if they commit if they walk messages say yes then they would if they did both commit at the same time so if if this is the scenario what's going to happen and why you they like to raise their hand and hazard a guess well that's really good field to be since that's a one-sided read can't possibly fail they're both gonna send in fact identical walk messages to whatever primary holds object X and I both send the same version number but a version number they read and the same value so the primaries gonna see to log meant to log messages in two different incoming logs assuming these are running on different clients and exactly what happens now is slightly left up to our imagination by the paper but I think the two incoming log messages could be processed in parallel on different cores on the primary but the critical instruction of the primary is the atomic test and set or compare and swap exactly somebody's volunteer the answer that one of them will get to the compare and swap instruction first and whichever core I guess the compare and swap instruction first it'll set the lock bit on that objects version and will observe the lock a bit wasn't previously set which everyone executes the atomic compare-and-swap second will observe the lock that's already set I mean he's the one of the two will return yes and the other two will fail the lock observe the lock is already set immature no and you know it for symmetry I'm just going to imagine that transaction to the primary sends back a no so the transaction to use client code will abort transaction 1 I've got the lock got a yes back and it will actually commit when it come it's when the primary actually gets the commit message it'll install the updated object you know increments to to clear the lock bit increment the version and return true this is gonna say true because the other primary sent back I know that means that TX commits gonna return false here and the final value would be x equals one that was one of our allowed outcomes but of course it's not the only in are leaving any questions about how this played out or wide executed the way it did okay so there's other possible interleavings so how about how about this one let's imagine that transaction 2 does the beat first she doesn't really matter what the reads are concurrent or not then transaction one doesn't read and then transaction went a little bit faster and it gets its lock message in and a reply and gets a commit back and then afterwards transaction two gets going again and sends a lock message in if it could commit so what happens this time well is this law commissioner is gonna be succeed because there's no reason to believe there's a lock bit is set because the second lock message hasn't even been sent message we'll set the lock the commit message this commit primary message should actually clear the lock a bit so the lock bit will be clear by the time t2 census inserts its lock entry in primaries log so this the primary won't see the lock a bit set at this point yeah so somebody's volunteered that what this primary will see is that the version number so the the lock message contains the version number the transaction to originally read and so the primary is gonna see wait a minute this since commit primary increments of version number the the primary is gonna see that the version number is wrong there's numbers now higher on the real object and so it's actually gonna send back a a no response to the coordinator and the coordinator is gonna abort this transaction and again we're gonna get x equals 1 one of the transactions return true the other returned false which is the same final outcome as before and it is allowed any questions about how this played out a slightly different scenario would be as if and actually okay the slightly different scenario I was gonna think of think of was one in which the commit message was stole it happened after this lock this is essentially the same as the first scenario in which this transaction got the lock set in this transaction observed lock okay everyone one last scenario let's suppose we see this what's going to happen this time [Music] yeah somebody has a right answer at the of course the first transaction will go through because there's no contention in the first transaction the second transaction when it goes to read X will actually see the new version number as incremented by the commit primary processing on the primary so it'll see the new version number the lock that won't be set and so then when it goes to send its lock log entry to the primary lock lock that locked processing code in the primary Co the locks not set and the version is the same hasn't this is the latest version and it all I want to commit and so for this the outcome we're gonna see is x equals 2 because this read not only read the new version um but actually read the new value which was one so this is incorrect here and both calls to TX commit will be true yes that's right succeed it with x equals 2 all right so you know this happened to work out in these cases the intuition behind why optimistic concurrency control provides serializability why it why it basically checks that the execution that did happen is the same as a one at a time execution essentially the intuition is that if there was no conflicting transaction then the version numbers and the lock bits won't have changed if nobody else is messing with these objects you know I'll see the same version numbers at the end of the transaction as we did when we first read the object whereas if there is a conflicting transaction between when we read the object and when we try to commit a change and that conflicting modified something then if it actually started to commit we will see a new version number or a lock a bit set so the comparison of the version numbers and lock bits between when you first read the object and when you finally commit it kind of tells you whether some other commits to the objects snuck in while you were using them all right and you know the cool thing to remember here is that this allowed us to do the reads the use of this optimistic schema which we don't actually check the locks only when we first use the data allowed us to use this extremely fast one sided already ma reads to read the data and get high performance ok so the way I've explained it so far without validate and without commit back up is the way the system works but as I see validate is sort of an optimization for just reading an object but not writing it and commit back up as part of the scheme for fault tolerance I think I'm gonna a few minutes we have left I want to talk about validate so the validate stage is it's an optimization for to treat objects that we're only read by the transaction and I'm not written and it's going to be particularly interesting if it's a straight read-only transaction that modified nothing and you know the optimization is that it's going to be that the transaction coordinator can execute the validate with a one-sided read that's extremely fast rather than having to put something on a log and wait for the primary to see our log entry and think about it so this validates one-sided B is going to be much much faster it's gonna essentially replace lock for objects that would only read it's gonna be much faster basically what's going on here is that the what what the validate does is the transaction coordinator refetch is the object header so you know it would have read an object say this object in the execute phase when it's committing it instead of sending a lock message it be fetches the object hit header and checks whether the version number now is the same as the version number when it first read the object and it also checks if the lock of it is clear so so that's how it works so instead of setting a lock message send this validate message should be much faster for a read-only operation so let me put up another transaction example and run through it how it works let's suppose x and y are initially 0 we have two transactions t1 if X is equal to zero set y equal one and T two says if Y is zero said x equals one but this is a absolutely classic test for strong consistency if the execution is serializable it's going to be either t1 then t2 or t2 and t1 it's got to get to see any you know corrected implementation has to get the same results it's running them one at a time if you run T 1 and then t2 you're gonna get y equals 1 and x equals 0 because the second if statement Y is already 1 the second if statement won't do anything and symmetrically this will give you x equals 1 and y equals 0 and it turns out that if you if they both abort you can get x equals 0 y equals 0 but what you are absolutely not allowed to get is x equals 1 y equals 1 that's not allowed ok so we're looking for how I'm going to use this as a test see what happens with validate and again we're gonna suppose these two transactions execute most so obvious cases they execute it absolutely at the same time and it eat that's the that's the hardest case okay so as we have read of X meet Y why because we wrote it and lock why here I sort of lock X here but since now we're using this read-only a validation optimization that means this one has to validate why this one has to validate X you know it's a red X but didn't write it so it's going to validate it much quicker and maybe it's going to commit and maybe it's and so the question is if we use this validate as I described it that just checks the version number and lock but haven't the version number hasn't changed in the lock but isn't set will we get a a correct answer and no actually both the validation is gonna fail for both because when these LOC messages were processed by the relevant primaries they cause the LOC a bit just to be set initially presumably the the reason okay did a cleared lock bin but when we come to validate even though the client is doing the one-sided read of the object header for X&Y it's gonna see the lock bit that was set by the processing of these lock requests and so they're both gonna see the lock bits set on the object that they merely read and they're both going to abort and neither X nor Y will be modified and so that was one of the legal outcomes that's right somebody somebody notice this indeed both validates will fail another of course sometimes that a transaction can go through and here's a scenario in which it does work out this was transaction one is a little faster validates all right so what's going to happen a transaction one is a little bit faster so this time it's validates gonna succeed because nothing has happened to X between when transaction 1 read it and when it validated so presumably the lock also went through without any trouble because nobody's modified Y here either so the primary answered yes for this the one-sided read revealed an unchanged version number and lock bit here and so transaction one can commit and it will have incremented Y but by this point if this is the order when the primary process is this actually when the primary process is lock of X this will also go through with no problem because nobody's modified X when the primary for Y processes the validate for Y though it's I'm sorry when the client running transaction two refetch is the version number unlocked it for y it's either gonna see this really depends on whether the committee's happen if the commit hasn't happened yet this valid a will see that the lock bit is set because it was set back here if the commit has happened already then the lock bit of will be clear but this validate one-sided reader will see a different version number than was originally seen and it needs somebody it's just this answer so one will commit so that transaction one will commit and transaction to will abort and although I don't have time to talk about it here if there's a straight read-only transaction then there doesn't need to be a locking phase and there doesn't need to be a commit phase pure read-only transactions can be done with just just reading blind reads for the reads sorry one-sided RDMA reads for the reads one-sided already me reads for the validates and so they're extremely fast read-only transactions are and don't require any work any attention by the server so and this is at the heart you know trends these reads and indeed though everything about farm is very streamlined - partially due to our DMA and it uses OCC because it's basically forced to in order to be able to do reads without checking locks there are a few brown downsides though it turns out optimistic concurrency control really works best if there's relatively few conflicts if there's conflicts all the time then transactions will have to board and there's a you know a bunch of other restrictions I already mentioned like on farm like the data must all fit in the RAM and all the computers must mean that the same data center nevertheless this was viewed at the time and still as just a very surprisingly high-speed implementation of distributed transactions like just much faster than any system in sort of in production use and it's true that Hardware involves a little bit exotic and really depends on this non-volatile Ram scheme and it depends on these special RDMA NICs and those are not particularly pervasive now but you do but you can get them and with performance like this it seems likely that they'll both in viewing and already me will eventually be pretty pervasive in data centers so that people can play these kind of games and that's all I have to say about farm happy to take any questions if anybody has some and if not I'll see you next week with a spark which is you may be happy to know absolutely not about transactions I heard everyone bye-bye [Music]