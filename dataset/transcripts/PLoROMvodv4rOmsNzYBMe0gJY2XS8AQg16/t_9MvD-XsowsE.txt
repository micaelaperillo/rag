Welcome back to CS231 lecture 11. Today we're going to talk about large scale distributed training. Um and this is a pretty exciting topic because this is basically how all neural networks get trained in practice today. When you look at large models from from startups, from industries, even academia um really large scale is kind of the new norm in deep learning nowadays. Um and that's actually something that's changed quite a lot in the last 10 years since we started this class. Um 10 years ago it was really well it was actually pretty common to train all models basically on one GPU one device um and it was fairly uncommon to train on multiple devices but as we'll see nowadays the new norm is to train models on tens hundreds thousands even tens of thousands of devices concurrently so we need to develop new algorithms and new ways of thinking in order to do that. So, as a and and then as a as a bit of running example through today's lecture, we're going to be talking a lot about Llama 3 405B. Um, not because this is the best model or the most interesting model, but because this is a model that this is a fairly close to state-of-the-art model that actually shares a lot of the implementation details of how it was trained, the model architecture, everything like that. Um, there's a lot of really amazing powerful models that have been trained in the last couple of years from Google, from OpenAI, from Anthropic, from others, but basically they don't share any details whatsoever about their models anymore. Um there's a very famous quote that just sort of marked a sea change in the industry to me that was in the GPT4 paper um back in 2023. So when they released the GPT4 model they said um given both the competitive landscape and the safety implications of large scale models like GPT4 this report meaning the paper they wrote about GPT4 contains no further details about the architecture including model size hardware training compute data set construction training method or similar and that's basically the new that's basically been the state-of-the-art for um large scale models the last 3 years since this since GPT4 they're not they don't tell you anything about anything they'll tell you nothing about the model you'll be lucky if they'll tell it's a transformer. They might tell you that much. Um, so Llama 3 is sort of notable not because it's the best model out there, but because it's one of the most open models out there. So this is a large language model that was trained by Meta um and released open source about a year ago in April 2024. And unlike OpenAI, the paper actually does share a lot of the details about the the model training, not too much about the data set, a lot about the system infrastructure that was used to train it. Um, so this is something that we and this gives us a sort of a peak into how large scale LLMs are actually trained these days. Um, and by the way, there is a new Llama 4 model that just came out from Meta uh, last month, April 2025. Um, so there are slightly better models out there in open source already. Um, but there's no paper on Llama 4 yet. So I'm excited to read that one hopefully when it comes out in a couple months and see um, what can we learn from the new generation of llama training. But as just as a as a running example through today's lecture, we'll be pointing at a lot of examples from the llama 3 405b model um for this reason. Okay. So there's basically two things that I want to talk about today. Um one is a bit about GPU hardware and the other is how to train on lots of GPUs. So I want to give you a sense both of what actually is the hardware that these things execute on as well as the algorithms that we need to use in order to train on a lot of them. So first we're going to talk a little bit about GPU hardware. So GPU um for those of you that don't know is graphics processing unit. These were specialized co-processors that were originally developed for computer graphics. Um and they turned out to become to be very useful generalizable parallel processors. Um it's actually very fitting to be giving this lecture in this room cuz this is the the Hang auditorium. Jensen Huang is the CEO and founder of Nvidia um which is sort of the biggest company right now and has been for the last couple decades in producing GPUs both for gaming and for uh and for ML. So these things started off basically for graphics because if you think about it when you're doing computer graphics you need to generate a lot of pixels on the screen. You need to process a little lots of little pieces of primitive geometry to produce those pixels. So it's kind of very natural to do a lot of computation all in parallel when you're doing computer graphics. Um so they people quickly figured out that this hardware that had been built intended to use in computer graphics could actually be used for much more general pieces of parallel computation um as well. So researchers kind so in in the early days in the sort of early 2000s researchers figured out how they could contort these graphics cards into doing general generalizable parallel programming. And then moving on towards the end of the 2000s and into the 2010s um Nvidia really picked up this and sort of developed these things, marketed them, built them with the intention of being generalized parallel processors. Um they didn't quite know at the time what exactly they were going to be used for. think I think they had this general idea that parallel processing was going to be important and they really capitalized on deep learning when it came when it started to take off in the early 2010s. Um much to Nvidia's credit, I think they really realized the potential of this research area very early um even in the early 2010s and started putting a ton of resources into making sure that their hardware was really useful um for deep learning training and it's basically been the like the main way that people train deep large scale deep learning models for more than a decade now. um that's starting to change as we'll see a little bit, but um it's their their their chips are kind of the the main one that people use. So I think it's I always like looking inside these things and seeing like what's in them. So this is a picture of the Nvidia H100 which is sort of the the the sort of the mainstay of deep learning training right now today. Um there's a next generation that just came out but it's not really accessible yet. I haven't trained anything on it yet. Um so this is this is kind of the state-of-the-art right now. Inside this Nvidia GP, inside this H100 GPU, in the middle here are these compute cores and surrounding that are 80 GB of HBM memory, high bandwidth memory. Um, so you can see the memory is separated from the compute cores. They need to move they need to talk to each other over this over this bus to move data back and forth from the GPU memory into the cores. Um, and it can do that at at a speed of about 3 terabytes per second, which is a lot of bits moving around. Now, if we dive deeper inside the the GPU cores, um we see that in the middle in that compute core part, we've got a smaller piece of memory about 50 megabytes of L2 cache um that is much much smaller than that 80 GB of um of HBM memory, but it's very very close to this to the GP to the actual computing elements. So, they can be accessed much more quickly from the compute cores. Um and then the real heart of the thing are these 132 streaming multipprocessors or SM. Um these are kind of like independent parallel cores. Um, they're a little bit more powerful in some ways than a typical CPU core because they can do a lot more parallelism, but they're a lot weaker than a typical CP CPU core also in a lot of ways because they tend to have slower clock speeds. They can't do as much instruction prediction, as much branch prediction. So, it's it's really hard to make exact apples to apples comparisons between these GPU cores and the CPU cores. But, I usually think of these um streaming multipprocessors as roughly akin to a CPU core. Um, also for I I know someone's going to go back home and actually count all the little boxes on this screen and you'll see that there are actually 144 of them when I've said there's only 132. Why is that? It's because all GPU hardware uses a process called binning where making these things, they have so many transistors, so many little computing elements, no matter how much money they pour into the process, they just don't come out perfectly. Um, some of them always end up a little bit messed up. So, they kind of plan for that in the development of their products and they say, "We're going to try to make a chip. The full chip in theory has 144, but none of the chips are perfect. But we know we'll get a reasonable number of those that have at least 132 that are functioning. So they tend to use this process of binning. So then they actually only um you know can sell a much more larger proportion of the chips they try to produce by um you know only only promising that 132 of them will be will be um turned on. um then we can dive even deeper inside one of those streaming multipprocessors and then we can see uh even more what's going on inside of these GPUs. So this is just one of the 132 active streaming multipprocessors inside an H100. And there's a couple interesting elements in here to look at. Um first we see we have um 256 kilobytes of L1 cache and register files. So this this continues the trend of the memory hierarchy in the GPU. We saw that you know in general you know you you thought you were learning deep learning, you're actually learning computer architecture. Sorry, it's a surprise. Um, and it turns out that that memory hierarchy is really important for deep learning and for all all kind of high performance computing. And the general trend is that you have larger bits of memory that are farther away from the compute cores. And the closer you get to the compute cores, you have smaller bits of memory but are much much faster. And it's really important to write if you're writing the low low-level algorithms that run on these things, it's very important to be aware of this memory hierarchy and to be very diligent in passing data between different phases of this memor memory hierarchy. And if you're writing performant GPU kernels, you spend a lot of time try trying to optimize that. So just to give you a flavor of that, you know, you see that we see the three levels of memory memory hierarchy in the H100. You've got 256 kilobytes of L1 cache, 50 megabytes of um of L2 cache, and then 80 GB of um HBM memory. So those are the three primary levels of memory hierarchy in the H100. Then we've also got 128 of these FP32 cores. Um these are little arithmetic units that can do sort of generalized floatingoint operations. Um and in particular each one of these 128 FP FP32 cores can compute ax plus b where ax and b are all scalers. It can perform that that bit of computation in one clock cycle. So then if you um add this all up that ax plus b is basically one multiply um one addition and you've got 128 of these cores. Um so you can do this this whole SM can do 256 floatingoint operations per SM uh per clock cycle of the device. Then we'll also see that in red we've got this is where the real magic happens. Um in addition to these uh FP32 cores, there are also these four tensor cores. Um I think the name is a little bit miss a little bit of a misnomer. These are actually matrix cores. Um what these what each of these little tensor cores does is they are special circuits that are designed they only do one thing. they do matrix multiply. So each one of these little tensor cores can do a single chunk of matrix multiply. Um in particular I believe the H100 ones can do a 16 like input matrix A is 16x4. Um input matrix B is 4x8 and then plus a bias matrix of for of a size 16 by 8. So it basically does ax plus b where a x and b are little matrix chunks of this fixed size. And it can do that that one little ma chunk of matrix multiply once per tensor core per clock cycle. Um so then if you kind of multiply all these numbers out you see that um you you know that little matrix multiply of ax plus b of that particular size is 1,024 floatingoint operations where we that's counting each multiply each ad as a single floatingoint operation. We multiply that by the four uh tensor cores in the SM and we see that the entire S the entire SM if it's going through the through the tensor cores can do just over can do 4096 floatingoint operations per SM per clock cycle. Um, and this we need to compare with a 256 that we can get from the FP32 cores. And here we see that just like the tensor cores are where all the magic happens. This is where the main throughput of the device comes from. And if you're writing code that wants to run on these GPUs and make maximum avail make maximum usage of them, you need to make maximum usage of these tensor cores. Another interesting thing about these tensor cores is that they actually operate in mixed precision. um rather than traditional floatingoint numbers which are normally 32-bit, uh the tensor cores tend to use a mix precision procedure where the inputs are usually 16 bit. Um and there's a couple different interesting 16- bit formats that they can use that we can't get into today. Um and they'll do the they'll do the multiplications in this lower precision 16 bit and then do the additions, the accumulations in a higher precision 32-bit. So these tensor cores take a low precision 16- bit input and then produce uh and then do some of the intermediate computation and produce the outputs in a higher precision 32-bit. Um and this is important because um at the PyTorch layer if you forget to cast your model into into 16 bit it will run on the floatingoint coes instead it will be 20 times slower than you expect. So this is uh you know seems like a little bit of minutiae but it becomes very tangible when you mess up that data those data types in your pietorch code. Um, and then not so GPUs are really fast and it's really crazy just how fast how much faster they've gotten over the past decade or 15 years or so. Um, so when I first started my PhD um, and was working on deep learning, the state-of-the-art GPU that we were all using was this K40 GPU um, back which was released back in 2013. And this thing could do just uh, 5 teraflops of FP32 compute um, for the whole device. Uh, all right. So I should explain the graph. So the x- axis is uh time ranging from about uh 2013 up to present day and then the y-axis is the peak throughput of each of these devices measured in terms of uh teraflops per teraflops per second per device. Um and you can see like the graph goes up a lot. Um but there's something salient to notice here is that um from the K40 to the P100 something really amazing happened in the V100 which came out uh towards the end of my PhD in around 20 2016 2017 and that the V100 was the first device that introduced these tensor cores. Um and since then um the the the since then more recent devices have gotten more tensor cores, bigger tensor cores, more of the device area allocated to tensor cores and this has resulted in a gigantic increase in the throughput of these devices over the past 10 or 15 years. Um and the most recent device is this B200 um that was uh you know formally announced. It's slowly rolling out now. Um this one in theory has about you know five uh has about um 83 83.3 uh teraflops per second of FP32 compute and 5,000 teraflops per second in theory of um mixed precision compute on the tensor cores. So if you step back like this is this is like literally we've been living through a 1,000fold increase in computation um over the past you know 12 years. Um and that's just at the per device level. So there one explanation of why AI has gotten so good in the last 10 years. What has happened like this is the answer. Um there's now a source of computation that we're taking advantage of and it's gone up by 1,000x in the last decade. Anytime anything in the world changes by 10,000x you should step up and pay attention cuz that's going to that's going to cause major changes in our technological capabilities. And this 1000x improvement I think is the major driver of improvement in deep learning over the past decade. So the it's it does not have 5,000 does not have 5,000 tensor cores. That's 5,000 teraflops of compute on the tensor cores. Okay. Yeah. So we always try to distinguish between the compute on the tensor cores versus the compute on FP32 cores. Right. So like this is already crazy, right? It's already crazy that there's been a 1,000 increase in like a device that you can hold in your hands. Like I've held a K40 in my hands and I've I've not held I've not had the opportunity to hold a B100, but like they feel like the same physical object. It's like about the same size, about the same weight, like kind of looks the same, but the one from today is 1,000 times faster than the one from 12 years ago. That's insane. Um, but it gets even crazier because we don't train on one GPU, right? I said that when when the K40 first came out in 2013, it actually was common to train a lot of models on just one GPU. But today, we're training not just on one GPU. We're training on thousands, tens of thousands, sometimes hundreds of thousands of GPUs all working together to train one model. So, so, so, so stack that on top of this 1,000fold increase in per device throughput and something truly insane has happened in the past decade. So, then um you know then we can know we've we've looked inside the GPU. Now, from here I want to zoom out and and put that GPU in context not looking at individual devices but thinking about the modern GPU clusters that we build that stitch a lot of these things together. So we've already seen a single H100 GPU. Um we saw right and here the and here we can think of it as another level of memory hier hierarchy. We already saw inside the H100 there were sort of three layers of memory hierarchy as we got closer to the compute elements and as you got farther away from the compute elements the bandwidth the memory bandwidth the ability of the device to move bits around between different parts of the system um gets slower and this trend actually continues if you once you escape the bounds of a single device and imagine these in the context of a full data center. So here we saw a single H100 GPU gets about 3 terabytes of um memory bandwidth. That's sort of the GPU memory talking to its from its own HBM memory to its own compute elements. 3 terabytes per second it can move bits around. Um but these things typically live inside a GPU server. Um almost all GPU servers have um eight devices in one big box. Um and those GPUs can talk to each other. Um and they typically talk to each other at a rate of about 900 GB per second from any one GPU in the server to any other GPU in the server. So you can see that's like a 3x less memory communication bandwidth compared to the GPU talking from in inside one device. Um and here things here we again turn to llama 3. Um a lot of major players don't publish a lot of details on their training clusters but the llama 3 technical report did actually give a lot of details around their training clusters. So from here some of the specifics probably vary a little bit from cluster to cluster. Um but these are now numbers from the llama 3 cluster that was used to train their their their models. Um, so they given one GPU box, they stack two of those box into one server rack. Um, and a server rack, if you haven't seen it, they're, you know, about 6 ft tall, like about the size of a person to just kind of get a mental picture of one of those things. So one server rack has two servers inside of it. Total of 16 GPUs. Then we connect a lot of server racks together into a GPU pod. Um, the Llama 3 cluster has GPU pods that are composed of 192 racks. um and which is a total of 3,72 GPUs. And these things have really high bandwidth connectors between all the different racks. Um and as a result um any pair of GPUs inside that pod can talk to each other at a rate of about 50 GB per second. And now you see this is another sort of 20x decrease in memory traffic between what an individual um server can talk and then what any GPU across an entire rack can talk to each other. So 3072 GPUs seems like a lot of compute, but it's now it's nowhere near enough. Um, so we're going to stack those GPU pods together into a full GPU cluster. Um, so this is actually the full GPU cluster that um, Meta built to train their Llama 3 models. This thing combines eight GPU pods together for a total of 24,576 GPUs. Um, I could not find exact numbers on the memory traffic between these things. Um, but it's definitely less than 50 GB per second. And by the way, this is not the largest GPU cluster in the world by a long shot. Um, it's the long it's the biggest one that I could quickly find precise numbers on, but there are definitely GPU clusters out there in the world that are, you know, 50,000 GPUs, 100,000 GPUs. They exist and people train models on them. Um, and like the way that this works is it sort of scales out naturally. So, you would just sort of cluster together more pods together to create a bigger a bigger cluster. Or you might have another level of hierarchy where you might have sort of a super pod that connects to other super pods to get you another level up. uh how long do they train with that GPU cluster? Um I I don't remember offhand for the llama 3 models, but there's been kind of a rule of thumb um for the past decade is that the longest models that people train are usually on the order of months. Um and that I think has less to do with technology and more to do with people. When it comes to like making like having progress, like making plans, like having people work on things, it's very difficult to have training runs that are that are very very long. Um so the longest training runs, the biggest state-of-the-art models I think are typically measured in months. Um, I would not be surprised if the very very largest models, the GPT 4.5s, the GPT5s, if those are like pushing closer to a year at this point. Um, but it's pretty common to see training runs that are on the order of a couple of months on these really long, really big training clusters. The question is, um, why do you organize servers into a rack rather than in a pod? You got to put them somewhere, right? There's physical constraints on these things. Um, so server racks are kind of a stand have been a standard unit, um, in just like data centers for for decades at this point. So when they when new these new devices came onto the scene of GPUs that that gives you a different kind of server, they're a lot physically bigger. They have a lot more power. Um but you can't redesign the whole data center from scratch overnight. Um so as a result, the server rack has been kind of a standard unit um with standard hardware sizes and everything that the that data centers are typically built around. How much physical space does like a cluster typically? Oh, that's a great question. Um, so a a single you should think of a single server rack as being like around 6 8 ft tall, something like that, like about this big. So maybe like a server rack would be like around the size of this podium and like about as tall as me. Um, then you then you've got 192 racks in a pod. So imagine like 200 of these podiums. How big would that be? And then multiply that by eight. Um, but it's actually that's actually a little bit of an underestimate because you typically organize these things in rows so people can actually walk between them. Um, and there's more hardware that you need to pack into the cluster, not just the train, not just the the compute racks. Um, so in addition to the compute racks that have the physical GPU servers, there'll be other racks that contain networking hardware, we've got a lot of a lot of a lot of bits that need to fly around between all these devices. So there'll be dedicated racks that only hold networking hardware. There will also be dedicated racks that only hold storage hardware um because you need to store the training data somewhere and get that into your devices. So these things uh can take up quite a lot of space. Oh yeah question is um the when you go to these big clusters do the smaller units of compute maintain the higher throughput and yes they do and that's part of this that's part of the secret and the challenge of designing for these systems because you ideally want to take advantage of the fast communication when you can get it but also fall back gracefully to the slower communication on the larger units um as you as you scale up. Oh, how hot does it get? Um pretty hot. Like if if anyway if any of you is a gamer and has a 4090 GPU or 5090 GPU in your in your desktop at home, like a single 4090 GPU if you're playing games will heat up your room, like make you want to open the window. Like it will make a make the room physically warmer. Um so imagine like if that's what a single gaming GPU can do to an averageized room. Yeah, there's some serious cooling requirements for these things once you stack tens of thousands of them in a big in a big data center. Um although another interesting thing is about I mean the cooling gets crazy, right? So a gaming desktop will typically be air cooled, sometimes water cooled, and then like you can design different different cooling systems and you can go nuts on the hardware here to try to optimize all this stuff. All right, so I I think this stuff is super cool. Just like imagine imagining like these GPUs are not just mythical creatures that are floating around in the cloud. These are like actual physical atoms that someone built and stacked up in a room somewhere. And it's really interesting to imagine what they look like. Um and so basically one one one kind of mindset shift when we move to these big GPU clusters is actually thinking not so much about the individual devices about the individual servers. I basically try to think of the entire data center as one big computer. Um and this big computer has in this case has 24,000 GPUs 1.8 pabytes of HBM memory on the GPUs 415 million FP32 cores 13 million tensor cores and this whole thing can do 24 exoflops of compute per second. That's 10. That's 24 * 10 18. That's a lot of flops. It's a lot of flops, but I guarantee you 5 years from today, it will not feel like a lot of flops, which is the even crazier part. Um, and our goal here is actually to think of this entire block of 24,000 GPUs as one giant supercomput. And then the question is, how can we train one neural network for months at a time on this one giant supercomputer and train a really gigantic neural network that's really powerful that can soak up tons and tons of data? Um, and that's basically the the question and the paradigm that we've moved to in deep learning. Um, and by the way, um, I I I keep saying GPU, I keep saying Nvidia because they are sort of the most dominant training architecture and and hardware today, but there are some others that have sprung up. Um, the biggest competitor, I think, right now to to Nvidia training hardware is Google. Google has their own hardware called tensor proing units, TPUs. Um, and these are really good. Um, these are uh they they've gone through six generations of these already. Um, these are the stats of the V5P TPU which you can rent in Google Cloud today. Um, and it's sort of roughly, you know, same order of magnitude, kind of similar specs as the H100 that we just talked about. There are some interesting design decisions in the TPU that are quite different from the GPUs, which I find fascinating, but we just don't have time to get into today. Um, and someone was asking how big are these things. This is an actual picture. Just like GPUs, uh, these TPUs are arranged into pods. Um, and the V5P TPU TPUs can be arranged in pods of up to 8,960 chips. Um, and this is a picture actually of a V2 TPU pod which has only 256 chips. So then that kind of gives you a sense of how big these things are. Each one of those um, then you see there's four racks here. Those racks are kind of like I said maybe about as a little bit taller than me and there's four of them side by side for 256 TPU chips. And now imagine this thing is going to get a lot bigger in the more recent pods that have up to almost 9,000 chips. Uh yes. So Google's Gemini models are almost certainly trained on TPUs. I would be I mean of course they don't tell you, but I would be astounded, absolutely astounded if they um were not. And like I said um the TPUs are actually very good. Um most I I I assume that most large scale Google models are trained on these things and those are very competitive models. Um so this is really good training hardware. Um the difference with Nvidia is you can't buy it. the only way you can access TPUs are either by working at Google or by renting them on Google Cloud. Um, but it is very good hardware and a lot of people are making use of it, but I think it's still a little bit less popular today um than H100 than uh than Nvidia GPUs. Um, and of course other companies obviously know that this is a very important thing. So there's a lot of other companies that are trying to build competitive training hardware, but I think like my honest assessment right now is that probably Nvidia and TPUs are the like the two big ones. they're they're way ahead of everyone else right now today in terms of usability, performance, just like market share, but there are a lot of others that are trying to to catch up here. Um, two notable ones are AMD. Um, AMD has been sort of the second major GPU manufacturer for many decades. They also have um a training accelerator called the MI3 325X. On paper, it actually has really good stats that are pretty comparable to an H100, but it just has not had the same impact as the H100 right now. Um AWS also has their own training chip that they've developed called trrenium. Um I don't know too much about this one. I've never tried to use it myself, but I know that Enthropic uses it for some of their training. I don't know how to what extent their training is entirely tranium versus um GPUs. Um so we should expect to see more. But like today I think G Nvidia GPUs are probably the most dominant and Google TPUs are like right there. They're really good as well, but probably not quite as widely used as GPUs from Nvidia. Okay, so that's basically part one. um what is what are GPUs? How do we arrange them into clusters? Just give you a sense of the physicality of these machines that we're building and training on. Then the second question is how do we actually write algorithms that can make use of this giant GPU cluster with tens of thousands of GPUs? It's going to require us to develop new algorithms, new ways of thinking about our compute, um and new ways of parallelizing and splitting up our neural networks. So the basic strategy here is going to be split up your computation. These things are giant parallel devices. They have a lot of we saw they have a lot of GPUs, a lot of CPU cores, a lot of GPU cores that can all operate independently and they can't talk to each other too much. If you think about what a computer really does like at a high level, a computer basically does two things. It does computation which is taking input bits and computing new output bits from those and it does communication which is taking bits and moving them from one bit of memory in one place to some bit of memory in some other place. And the whole trick is how do we make use of all of these multi- scale multi multiple scales of memory hierarchy across the entire cluster to overlap the communication with the computation and also to split up the computation and parallelize paralyze it so that in the process of training a giant neural network we have useful work for all of those tens of thousands of individ individual GPUs all of those millions of individual compute elements we have useful work for all of them to be doing in parallel and then get them to communicate their work to each other in a way that achieves training a giant neural network all on this giant cluster. So to that end um one way I like to think about it is there's basically five degrees of parallelism that people exploit when training neural networks large scale neural neural networks today. Um a lot of this is specific to transformers because those are the dominant architecture that people are using for large scale training. So if you think about a transformer, a transformer is basically a stack of L layers. And each one of those L layers is operating on a threedimensional tensor of size mini where we're where one dimension is the mini batch dimension. We've got a bunch of sequences all operating in a mini batch. Um a sequence dimension, you know, we're operating on sequences or sets of tokens and a dim dimension. So each of those tokens itself is a vector with some dimension. So our tensor our our our t our um our transformers are operating on these threedimensional tensors and they operate through a stack of layers. So that gives us four axes to parallelize on. Um we can access we can parallelize on the layers axis which is pipeline parallelism. We can we can parallelize on the um batch dimension which is data parallelism. We can split on the sequence dimension which is called context parallelism. And we can split on that dim dimension which is called tensor parallelism. So all of these have kind of funny names, but if you think about it in this way, they're basically all different ways of splitting up your computation across these four axes of compute inside your transformer. And then we're going to step through each of each one of these in more detail because there's a lot of interesting nuances with all of these different meth mechanisms of distributed training. So the first one is data parallelism um or DP. And the the basic idea here is kind of simple. So remember when we're training neural networks, we're always operating on mini batches of samples, right? like we're always, you know, taking a mini batch of of elements. We're computing a loss for every entry in our mini batch depending on what our whatever our training task is. Then we compute a gradient where the gradient is actually typically an average of the gradients of the losses for the individual elements in the mini batch. So um in most neural network architectures, the computation of computing the loss and then computing the gradient is independent for each of the elements in the mini batch. So this is something that seems trivially parallelizable. So the basic idea is that if you have if you can fit a mini batch of n examples on a single GPU and you have access to m GPUs, then we're going to train our model with a giant mini batch of m* n examples where we split up that giant mini batch into a little tiny smaller mini batch of n samples that goes on each GPU. Um, and if you think about mathematically why this makes sense, it's because gradients are linear. So in practice, you know, if you're kind of computing a single scalar loss L, which is going to be the average of some individual losses computed on each of our so these Xig are all the entries across your entire macro batch, I guess we'll call it. And then the W are the weight matric weight matrices of the entire network. Then typically the loss that you're computing um at the end of the forward pass is an average of the losses on each of the individual mini batch elements. And then if you take the gradient of the loss with respect to the weights of the network, that's the that's the that's the thing we need to compute in order to make a weight update, then that is actually going to split. Um and you can because gradients are linear, um you get you get to sort of choose in what order do we want to do the sum, do we want to do the gradient, do we want to do the averaging. So in particular, it becomes convenient to arrange the gradient in this particular formulation. um where there's this inner term that we've highlighted in blue which is um which is basically a normal for backward pass on n elements and these can be be computed in parallel on different on different GPUs and then there's an outer sum where we need to take an average of the gradients across our um m different devices that we are that that we're operating on. Um so that's kind of why what what what's happening from a mathematical perspective and we see that this is perfectly mathematically sound. This is basically exactly the same mathematically as training on um a single device. Um we've just been clever with our algebra and changed the order of doing our averages and our summations. But this is not an approximation. This is exactly the same computation as we would have done on a single larger GPU. So kind of what this looks like at the GPU perspective is that we have um n GPU m GPUs. Here I'm showing m equals 3 because that's all that can sensibly fit on the slide but you know think that this is much larger than three in practice. Then each one of those GPUs actually maintains its own separate copy of the neural network weights um of the optimizer state um and of the gradients. So then what we're going to do each GPU will load in parallel a different mini batch of data. Um here we're showing each GPU loading three a mini batch of three elements. And crucially the the different GPUs need to load different mini batches of data. Um I've had bugs in in my code in students code where they accidentally load the same mini batch in all the GPUs. That's not going to help you. That's not going to be good. Don't don't make that mistake. Um so you it's crucially important that your and that your different GPUs actually load different mini batches of data. Um then each GPU will independently do its own forward pass on its own mini batch of data to compute its own local loss on its own local mini batch of data. And these these these can all operate totally independently. It does not require any communication between GPUs. Then the the each network will do its own backward pass to compute the gradient of its own local loss with respect to all the weights of the model. And again this can happen totally independently because each model remember has its or each GPU has its own independent copy of the model weights. It can do its own forward backward pass um completely independently. But now after the backward pass is done this is where things get tricky. Remember we said we needed to compute an average of those gradients across all the devices that are participating in our training. So then we need communication. So this is where we do um an an all reduce operation and every GPU needs to needs to send its gradients to all the other GPUs and each so like there's sort of two things happening simultaneously. One each GPU needs to broadcast its gradients to all the GPUs and then two each GPU needs to collect the gradients from all the GPUs in the that are participating in the training. Um so this is an all reduce operation. Um and there's uh that this kind of happens in sort of logarithmic time typically depending on in the number of GPUs. Um but at the end of this all reduce operation then each GPU now has um an an average of all the gradients across all the devices. So at this point the communication has happened. Um each GPU now has an identical copy of the gradients that have been all reduced across all the devices. So now um at the beginning of the training iteration we assumed that each GPU had its own independent copy of the model weights. Now at this point each GPU has its own independent but identical copy of the gradients across the entire macro batch of data. So now at this point um each GPU can make a weight update on its own local copy of the weights. And because they started with the same weights and they had this and they applied the same gradient, they're going to have the same weights after the local weight update assuming the arithmetic was deterministic. Um and also by the way um and this is really important, steps four and five can actually happen in parallel. We said that there's two things that there's actually two things here that can happen in parallel. One is the backward pass where each GPU computes its own backward pass to compute gradients and the other is the communication of the gradients across the GPUs and these things in practice will typically happen simultaneously. So that means that each model will start off doing doing backward pass over the last layer in the network and then compute its own local gradient. Now the model will move its compute on to computing backward pass for the second to last layer of the of the of the model. And while the compute elements are busy computing the backward pass on the second to last layer, then the the GPUs will simultaneously be doing an all reduce of the gradients of the last layer. So this means that these things kind of chunk along um communication for layer L+1 and backward pass for layer L and they can just chunk along in parallel so that hopefully by the time we've gotten to the end of the network and by the time the backward pass is done, the the gradients have already been all reduced across all the devices. Um and we can make our weight update all at once without waiting. Um this is really important because like we said with the communication is relatively slow. So the whole trick in these things is figuring out ways to hide the communication costs and do them at the same time as the compute. The question is is four or five going to be the bottleneck? And the answer is yes. Um it depends entirely on how fast your device is, how big is your model, how big is your mini batch, how fast is the inter interconnect between the devices. Um when you get to this lower scale distributed trading the answer is always it depends on your situation and you need to benchmark for your situation. Ah, why not take m different gradient steps on each of them? That's actually a really cool idea. Um, there actually was a popular uh set of algorithms that people used a while back called asynchronous SGD where they would basically do that then basically have a bunch of different model replicas all take a bunch of independent model steps and then try to average them every once in a while. Um, and those were popular like there were some like actually Google used to do this before they developed the TPU pods. Um, and some of their earlier networks in the early 2010s were trained in this way. Um, but one, it tends to just be a lot more unstable. Um, and two, it's very hard to debug and reproduce. Um, so it and it it just tends to work a little bit worse. So it it does feel like a more scalable approach, but in practice, um, if you can do everything synchronously, then your algorithms are easier to debug, easier to understand, easier to reason about. Um, and if you can get basically if you can get away with with synchronous gradient updates, it's probably going to work better. Um, but actually I would personally not be too surprised if we see a resurgence of async SGD methods in the next couple of years at some point because I think they are a lot more friendly to distributed training. There's no one computer that can orchestrate all this stuff. All these things are independent devices with their own independent stuff. There's no there's no driver that can take a god's eye view and and take those steps. All that computation has to happen somewhere. Ah, great question. I I said like as you're overlapping communication and compute, do you need to write code for this or does the hardware do this automatically? You definitely got to write code for this. The hardware is not smart enough to understand what you want to do. Like the hardware, like we said, it's sort of understands these little matrix multiply chunks. It understands pretty low-level stuff. Um anything that you want to do to schedule that communication um the you need to take care of in software. Um but thankfully um for a lot of these common use cases, um PyTorch ships with it for you. So, for example, in this case, there's a there's a PyTorch class called distributed data parallel um that will do this for you and make this uh this sort of happen relatively transparently on top of um otherwise straightforward PyTorch code that you've written. Although that actually that is really interesting to contrast with the individual devices because um if you're programming an individual GPU in CUDA, which is Nvidia's language for programming GPUs, then actually the hardware does take care of a lot of this async transfer for you automatically. Um but at the cluster level, um it typically doesn't. then you typically need to do it in software. So there there actually is a little bit of interesting asymmetry here between parallelism on the individual device level where a lot of that is does happen automatically in hardware versus at the cluster level where it needs to be orchestrated in software. Yeah. So so typically these are these are heterogeneous systems where different parts of the system are written in different programming languages. So there's going to be low-level device kernels that actually are the code that executes inside the GPU and those are typically written in CUDA which is um you know it's a C like language that is Nvidia's language for programming their own GPUs. Um, and but then those individual GPU kernels will get wrapped up and you can call those GPU kernels from Python. And this is basically how PyTorch works. PyTorch is sort of like a collection of a lot of GPU kernels that can do lots of interesting stuff on the GPU and then a lot of C++ and Python code that wraps around those GPU kernels and makes it more user friendly to program. So in this picture, each GPU is computing its own gradients in black um on it by itself and then the gradients in red get computed via an all reduce across all the GPUs in parallel. Oh, the backward pass at the lower layer is dependent on the gradients from the previous layer. Um, but crucially, um, each network is each GPU is only doing backward pass locally on its own mini batch. So then there's basically two different variants of the gradient at each layer that you need to think about. Now there's the local gradients about like the gradient of the local loss of my mini batch with respect to my network weights and there's the global gradient which is the derivative of the total loss of the of the macro batch with respect to the network weights. So each GPU can only in order to compute a backward pass each GPU only needs the local version of its upstream gradient but then the computing the global version of the upstream gradient requires communication. Um so this is data parallelism and there's actually a bit of problem here which is this is a great way to paralyze G GPU computation and this was the first way that people started paralyzing GPU computation in neural network training but we quickly hit a bottleneck on the G on the model size. So here remember that each GPU is keeping its own independent copy of the model parameters and this becomes a bottleneck when you want to have really big models. So in particular um now each weight in your neural network you basically need to keep track of four numbers the weight itself um the gradient of that the the gradient of that weight uh and the optimizer state. So if you're using atom that's typically a beta that's a beta 1 and a beta 2 per parameter in the network. Um and sometimes you'll also have an exponential moving average of the model parameters as well. So typically you'll have you know four to five scalers that you need to keep track of for every weight in your network. Um and if you're training with 16 bit precision which is pretty common these days what some some of these you'll sometimes keep in in higher precision but let's talk about 16 bit as a lower bound then you need two bytes for each number. So that means that we need you know four numbers two bytes we need six we need uh eight bytes per scaler in the in the network um to to keep track of which means that 1 billion parameters um 1 billion model parameters is going to take about 8 GB of GPU memory to store all that stuff and we said the whole GPU only has 80 GB of memory for an H100. So that means that the biggest model you could ever hope to train in this scenario is something like 10 billion parameters and that's not big enough. We want really big models. We don't want to be constrained by the tyranny of our GPU memory size in telling us how big of models we're allowed to train. So we need to fix this somehow. And the fix for this is actually relatively easy. We want to split we need to split the model weights across the different GPUs. So in addition to splitting the batch of data across GPUs, we're also going to split our model weights across the GPUs. And this leads to a variant of data parallelism called fully sharded data parallelism or FSTP. Um, and this is relatively simp conceptually what we're going to do is each model weight in the network, each weight wii, we are going to assign it to a owner GPU. So each weight will be owned by a unique GPU among the MGPUs that we're training on. Um, and that the GPU that owns each weight will also be responsible for managing the global gradients for that weight and the optimizer state for that weight. Um, and typically you would split this up by layer like you're not managing individual scalers. This W you should think of as like the lay like the weight matrix for an entire layer of the neural network. So now what but now so now what's so now the the picture on the right changes a little bit here we're only showing two GPUs because spoiler there's going to be a lot more arrows flying around here in just a moment. So here we're showing a four-layer network that we're distributing across two different GPUs. We've we've assigned the first two network the weights for the first two network layers W1 and W2. Those are owned by GPU1. um the weights W3 and W4 are owned by GPU 3, GPU 2. So that means that you know at the start of each of each batch the network weights are split up across the GPUs in this way. But it's it's still data parallelism. It's still the same basic idea that each GPU is going to load its own independent batch of elements, do a full forward backward pass on that batch to compute its own local gradients. Then I'll reduce the gradients and take a gradient step. Same basic algorithm, but it gets tricky now because the model weights are split up. So here we need to introduce extra communication. So when you're doing fully sharded data parallelism now at the beginning of forward before you start doing the forward pass of the first layer um whoever owns that weight for the first layer needs to broadcast that weight matrix to all the other GPUs that you're training on. So in this case um GPU1 owns W1. So it broadcasts that to GPU 2. So GPU 2 now has a copy of W1. Now that all the GPUs have a copy of W1, they can run a forward pass through the first layer of the network and compute the activations at the first layer of the network. And now um after you run the forward pass um each GPU that does not own that W1 is going to delete its local copy of the W1 weight matrix to save memory. So then after we've run the first after we run the forward pass for the first layer we're back in the state where the model weights are split up across the GPUs. But now all the GPUs have also have an activations in in GPU memory that is the result of running the first layer of the network. And now now it's time to do the second layer and we do the exact same thing. So then the GPU that owns the weight matrix for layer 2 is going to broadcast that to all the GPUs that we're training on. Now they all have their own local copy of W2. They can uh and they can go go forward. Um and by the way, we also have an opportunity to interle computation and communication here as well. So that while we are computing the forward pass for layer I, we can be prefetching the weights for the next layer. So in practice, this will happen in parallel um during the forward pass of an FSDP run. So then we'll be computing for we'll be computing uh layer 2 at the same time we are fetching the weights for layer 3. Um and once we get to layer 3, note that now GPU 1 owns layer 3. So then GPU1 will be broadcasting the weights to all the GPUs that we're training on. And this will repeat until we've gotten to the end of the network. And now at the end of the network, then all models now we now each model has on a full forward pass computed its local loss on its own local mini batch and has all the activations for all the layers in memory all ready for backward. Um and now we need to do the same thing in reverse in order to compute the backward pass. So um at the beginning of the backward pass for the for the last layer whoever owns that last layer weight will broadcast it to all the devices. Um once the devices have that weight then they can perform the backward pass and this whole we'll do a similar kind of procedure in the backward pass. Now there is a little bit of optimization we can do on the la very last layer in the network which is don't delete the me like keep the have the all the GPUs keep the the weights for the last layer in memory. So, this is something that you'll usually do in practice, right? Because at the end, um, all the all the GPUs already have a copy of the last their weights from the forward pass. They'll just keep it in memory and just because they they're they know they're about to reuse it for the backward pass anyway. So, we just won't delete the the the weights from the very last layer. Um, so now there's basically but now there's basically three things that need to happen during the backward pass. Um, one is that you know once the GPUs have computed the backward pass for their last layer of the network now that they have a copy of the weights now at that point each GPU has computed its own local backward gra it own local gradients for its local loss with respect to that last layer weights. Um then we need to we need to communicate those gradients back and we said that the GPU that owns the weight matrix is also going to be responsible for managing the gradients for that weight matrix. So now rather than all reducing the gradients as we did in the data parallelism case, instead we're going to um just just the one weight matrix that owns just the one GPU that owns that one last layer weights is going to gather and take take a sum across all the local gradients across all our devices. So in this case, GPU1 is going to send its last layer local gradient to GPU uh to GPU 2, which will then have the full gradient um DL DW4 of the entire macro batch with respect to the last their weights. What happens during the downtime? You got to get all this stuff happening in parallel. Um so in this so then you know there's basically three things that need to happen during backward. During backward we need to communicate the weights. So whoever whatever whatever GPU owns the layer owns the weights for that layer has to broadcast them. Two we need to all the GPUs once they get that weight need to compute a backward pass for that layer. And then three um after each GPU computes its backward pass then it needs to send the result of that the gradients with respect to the weights of that backward pass back to the GPU that owns it. Um and we can and then after that then the owner then once the owner of the weights has that full gradient then only the owner of the weight matrix can now make a gradient update on that one weight matrix. Um but I I think at this point we actually do not need to communicate the the updated weight matrix because it will get recommunicated to all the GPUs on the next forward pass. Um so that that's a little bit different from the from the from the DP case maybe. Um and then basically all of these things can actually happen in parallel as well. So they will repeat this for every layer of the network and then basically we in the steady state of a very deep network all three of these things will be happening simultaneously. Um so so while we are computing the backward pass for layer L we will be um aggregating the gradients and performing a weight update on the on layer L+1 and we will be pre-fetching the weights for layer L minus one. So I said there's three things that need to happen. we need to get the weight um run the backward pass and then uh update the weight and then aggregate the gradient and update the weight and these things can all happen in parallel. So we'll basically in general be operating on three consecutive layers and doing all three of these things in parallel over the over the course of the backward pass. Right? Right. So then like during the and then as we chunk backwards over the network um then by the time we then hopefully if you were properly able to overlap all that communication and computation then by the time you get you finish your backward pass then all the gradients have already been communicated all the GPUs have already finished doing their update on all the weights and we're ready and also hopefully your data loader that's loading data is al also happening asynchronously usually on the CPU cores of our servers. So then the CPU is like ready with a fresh batch of data to go forward again. So these things are basically parallelization machines. We have a lot of stuff that needs to happen um both within a GPU and across GPUs and we need to overlap all of that as much as possible. So we can always feed the GPUs and have them running on those tensor cores as as densely as possible. Um right so then we're basically ready to do our next batch after after that. So this is great. This is fully sharded data parallelism. Um and this is this can get you a long way. Um but there's actually a slightly uh slightly fancier variant of data parallelism that people sometimes use called hybrid sharded data parallelism or HSDP. Um and in this case we're actually going to imagine conceptually dividing our GPUs into a two-dimensional grid. So in the previous examples we said we had sort of n GPUs and the way that we parallelized our computation was kind of the same. We had sort of one axis of parallelization um in the previous variants of data parallelism. Once we get to hybrid sharded data parallelism, we now are going to have two separate axes of parallelism that we will do at the same time. So the first axis is we will do typical FSTP um fully sharded data parallelism along one axis that we just talked about. Um so we'll have sort of groups of KG GPUs. Um and each group of KGPUs will be doing um fully sharded data parallelism that we just talked about. So within each group of KG GPUs, the model weights will be split across those KGPUs and they will be interle um sending weights and gradients back and forth to each other during the forward and backward passes. But we will have now um m M copies of those K groups operating in parallel. So in this case we have um two groups of four GPUs. So each group of four GPUs you see has the weights split across the four GPUs but we have the entire the entire the entire uh setup duplicated a second time on a second group of two GPUs. Um and the reason and and then when you do this now you know then they do typical data parallelism across the groups. So within a group we're going to do forward backward um and at the end of the backward each group will have computed its own local gradients and then after the backward then each group needs to all reduce the gradients across the groups so that we now have like the full like macro macro batch gradients across the two groups and then each group can make an gradient update independently once they've received the full gradients for the macro macro batch. Um so this is called multi-dimensional parallelism because now there's basically two different axes, two different strategies that we're using to paralyze our computation simultaneously. Um and why this might be useful is because there's different amounts of communication required for these two different kinds of parallelism. So if we think about fully sharded parallelism, you know, we actually need to what do we need to communicate for during fully sharded data parallelism? During FSTP, during the forward pass, remember we were copying the weights all over. So we sort of copy like during the forward pass we end up doing a communication of one full copy of the network weights. Then during the backward pass we need to recommunicate the network weights um and we also need to communicate the gradients. So basically when it comes to when you use fully shredded data parallelism you b like during a single forward backward pass you need to communicate three times the network weights across everything participating in an FSDP group. But when you do normal data parallelism um where each where where you each group keeps its own independent copy of the weights you only need to all reduce the gradients. So that means that across multiple data parallelism groups, you only need to communicate the network weights once over a forward backward pass. And this plays into this idea of multiple levels of hierarchy inside of our GPU clusters. So what you'll what you might do, for example, is have um a GPU server where it has eight GPUs with higher interconnect inside a single machine. Those might be an FSTP group because it requires more communication inside an FSTP group. But then you could have multiple servers that are, you know, on this other axis. So you have sort of one server with a full copy of the model weights then another server with another full copy of the model weights and remember communication across servers is going to be slower than communication inside a server. So then this is our first example of you know take designing algorithms to take to make to take advantage of the network topology that we know our devices are connected into. Question is um would you rather have g like the these things are impossible to tune. It's very very hard to say. Um right so you know and then basically but once you have data parallelism once you have this this DP FSTP and HSTP this is actually a recipe that can take you a long ways. So for example um you know we a model with 100 billion parameters would take 800 GB of memory to store um and and if you split that over 80 GPUs it only takes 10 GB of memory per GPU. So you can have you know a pretty big model once you have FSDP. Um, but there's another problem that the model activations themselves now start to fill up memory. So if we go back to llama 35B, um, it's a transformer with 126 layers, model dimension of 16,000, sequence length 4096. Um, so if you kind of imagine how much GPU memory it takes to just store the hidden states during a forward pass. Um, that's going to be a lot. So that's going to quickly cause your GPU to run out of memory once your models and sequences get really big. So that leads to another trick is called uh activation checkpointing which means that we're actually not going to store all the activations in memory. We're going to recmp compute them during the backward pass. So to to to see how this works like we it's useful to think of your neural network in a different way where there's actually two different layers in the neural each each layer in the in the neural network does two things. It does a forward pass that computes activations for the next layer. Then it has a backward pass that computes gradients that take both the up upstream gradients and the activations. So normally you know how much compute and memory does this all take. Um if we assume that all of these are constant then typically a forward backward pass will take sort of 1 2 3 4 four step string forward you'll remember those activations during the forward pass then 1 2 3 four step string backward. So in a normal forward backward pass it sort of takes um compute and memory for an end layer network. Um but as we just said this is going to run out of memory. So instead what we can do is imagine recomputing the activations during the backward pass. So what that looks like is something like this. So we'll start with the activations. We'll run the first layer and then immediately throw away the activations for the first well run the forward pass for the first layer and then immediately throw away those activations and sort of do this four times. So now we've sort of gone through the network once got the activations at the last layer. At this point we can compute our backward for the last layer. But um now we're kind of out of luck. We we don't have the activations from from A3 to compute the next backward pass. But we can recmp compute them. So we recomp compute them. Now we can do the backward pass. Now recomputee some more. Now do the backward pass. Now recomp compute. Now do another backward pass. So if you kind of add this all up, this ends up being n^2 compute and constant memory for a layer for a network with n layers cuz it's sort of sum n minus 1 n -2 n -3 blah blah blah down to one. That's quadratic time. Um and you can split this up. You know n squed compute is pretty bad for deep networks. So instead let's not recmp compute everything. Let's instead imagine taking a checkpoint of activations every C layers. So we'll only sort of compute like recomp compute within blocks of uh within tinier blocks of the network. Then in that case um if you take C checkpoints where you remember your activations c times over the course of your network then it's going to take n^ squ over c compute and o of c memory. Um and a pretty common thing to do is to set c equal to root n in which case this becomes n root n compute and o of root n memory. Um so this is a pretty common way that you can trade off computation and memory um to train even bigger models. Okay so now at this point once we have FSTP activation checkpointing HSTP we actually this can we can do a lot of damage here. We can start to train some really big models. Um and the recipe for that is basically as following. Um so your scaling recipe that will take you quite a long way from here is first use data parallelism just raw data parallelism roughly up to 128 GPUs um and roughly up to models of around a billion parameters. You can just do normal data parallelism for models of this size. It tends to work pretty well. Um and another thing that you almost always want to set the local batch size per GPU to max out the GPU memory. That's almost always the right thing to do. Um, and then once your model starts to get big, then the model itself will take up a lot of memory inside your GPU. So that'll that will start to give you problems. So you know, it kind of depends on how much memory your GPU has, how fast your interconnects are. But in general, once your model starts to be more than a billion parameters, that's when you want to start thinking about switching from data parallelism to fully sharded data parallelism. Um, and then at this point you can scale up quite a bit, but then you'll run into the memory bottleneck for your activations and that's when you turn on activation checkpointing. Activation checkpointing kind of sucks because it makes everything a lot slower, but it does let you train much bigger models. Um, and once and this this will scale like up to several hundred GPUs and then there's some point usually depending on your cluster topology maybe around 256 GPUs, maybe around 512 GPUs. Once you get to like on the order of multiple hundreds of devices, then FSTP becomes too expensive and you need to and you need to start switching to HSTP. Um, and then this is basically going to let you get up to models that are roughly tens of billions of parameters training on maybe a thousand GPUs. Um, that's on like pretty long sequence lengths. So that's pretty good. Um but if you have maybe more than a thousand GPUs, uh more than 50 billion parameter models, sequence lengths more than more than 10,000 or so, um this is when you need to turn to these more advanced strategies, context parallelism, pipeline parallelism, or tensor parallelism. And then there's a big question. It's like, oh my god, there's a lot of nubs to tune here. Like how am I supposed to optimize this? I need to set the glo the global batch size, the local batch size, the HSTP dimension, the FSDB dimension. Like how much to recomputee? Like I'm lost here. What do I do? There's so many knobs. I don't know what what what what do I do? Um the answer is to optimize a very important metric called model flops utilization MFU. Whenever you get lost in the sea of GPU parallelism like model flops utilization is your guiding light. Follow this. It will tell you what to do um to optimize your training stack. But before we get to model flops utilization, we need to talk about hardware flops utilization. So remember we said in theory an H100 can do 988 989.4 t flops per second of compute on the tensor cores. But that's theoretical. How much can you actually get? Um the question is how much can you actually achieve in practice? Um and that's the metric of hardware flops utilization. You know, you're running some compute on the device. How much compute do you actually realize of that theoretical maximum? Um and this is not hard to do. Like you can write a couple lines of PyTorch code and just like benchmark this. So this is a benchmark that I wrote that I ran on an H100 yesterday. Um and you can see what it does is basically X-axis. It just does dense matrix multiply in in a loop and then times how long did the matrix multiply happen. how long did the matrix multiply take? Um, we can compute how many flops the matrix multiply takes. Then on the x axis, we're plotting the size of our matrix um going from uh 512 up to 32,000. And the y-axis is this hardware flops utilization, which is basically the fraction of the theoretical maximum throughput of the device that we actually realize from these matrix multiplies. And you can see that on this, you know, pretty straightforward PyTorch loop, we're getting about 80% HFU on an H100 once we get to large matrix multiplies of around um 8,000 by 8,000. So that's pretty good. But the problem is that HFU does not account for all the other stuff that your model needs to do, right? We're doing we're maybe doing activation recmputation. We're maybe running some other models on the side. We're maybe doing data loading, data augmentation. There's a lot of other stuff your GPU is doing other than just forward backward on your raw model. And that's where we move from hardware flops utilization to model flops utilization. So model flops utilization is basically saying um what fraction of the GPU's theoretical peak flops are being used for forward backward in my model. Um and this is the thing you always want to optimize for. So then to make this more concrete, you basically compute um you know based on your model architecture, the size of the number of the layers, the size of the layers, you compute how many flops does it take to do a full forward backward pass of your architecture on your mini batch of data. Then you look up somewhere the peak theoretical throughput of the device you're running on. And then you divide those two and that tells you how long should a full forward backward pass take if you were achieving the theoretical maximum throughput of the device. That's like the theoretical fastest you could ever do a forward backward pass on your model. Then um you actually time a forward backward pass of your model. You know your your training loop is doing all this other stuff. It's doing data loading. It's doing it's doing augmentation. It's doing communication. It's doing um maybe activation checkpointing. So it's doing recmp computation during backward. Your training loop is doing a lot of stuff. Just time see how long it actually takes and then divide those two numbers. that gives you a number between zero and one which is like what fraction of that theoretical maximum are you actually achieving in your training loop and that that's your MFU your model FOPS utilization um and again we can kind of benchmark this with some relatively simple PyTorch code here's an example running forward backward on just a like a like a short multi-layer perception with uh with a relu nonlinearity and with really big with really wide MLP layers and a gigantic batch size on a single uh H100 this is getting around 50% MFU Um, but and then in in general whenever you're trying to tune knobs for distributed training, you always want to try to tune whatever knobs you can to maximize MFU because that's the one metric that we typically care about when trying to optimize training throughput. Um, and in general, you know, an MFU these days, generally above 30% is pretty good. If you're way under 30%, you've probably got some gigantic bottleneck somewhere and something is going wrong. Um, and above 40% is pretty pretty excellent. And that's basically state-of-the-art. Um, and here's some numbers that we can pull from a couple papers. In particular, this is that llama llama 3 405b paper that we talked about. Um, in their in their final training step, they have a couple different variants of their of their training phases where they train on between 8,000 and 16,000 GPUs simultaneously. Um, and across that, they're getting MFUs roughly in the low like high30s, low 40s. And that's pretty that's pretty good. Um, you're never going to get really much higher than that on an H100. Um, and actually paradoxically, more recent devices sometimes get worse MFUs. So on on the previous generation devices, the A100's, you could sometimes get MFUs above 50%. And the reason for that is because GPUs are getting faster faster than they are getting faster at communicating. So when we moved from the A100 to the H100, we got roughly a 3x improvement in the theoretical throughput of the compute, but we only got a 2x improvement in the theoretical memory bandwidth. Um, so that that there's there there's this growing gap between making GPUs are getting faster really fast, but it's harder to scale the communication between the GPUs and as a result, we tend to sometimes get worse MFUs actually on more recent generations of devices. So that's um that and I I intentionally spent wanted to spend most of the time on those points because those are the ones that you guys are probably going to use in practice. Um I don't think anyone in this room likely has access to a 10,000 GPU cluster. If you do, like come talk to me after class. I would love to be your friend. Um but uh so like those are the ones that you're likely to encounter in practice like up to many hundreds of GPUs. Um but there are these other ones that I I just like there are slides here that are pretty that I think are pretty nice, but it's okay if we don't go through the full details of these. You can ch you can check it offline. Um so we said context parallelism is basically splitting on the sequence dimension. So we said transformers are operating on sequences. Um and basically the idea is you've got a long sequence, you know, make different GPUs handle different parts of the sequence. Um, and for if you recall your transformer block, this is actually easy for large parts of the transformer because the layer norm, the multi-layer like the the FFN, MLP, um, and the residual connections, those all operate independently across the sequence anyway. So, it's relatively straightforward to ask to chunk up that computation across the sequence dimension. Um, things get I mean it does get a little bit hairy inside the MLP because there are weights in there. So, you have to have some some reduce of the gradients like we did in the data parallelism cases. The attention is where things get hairy for sequence parallelism because if we remember um attention we need to compute the sort of all pairs interaction between every pair of elements inside the sequence. Um the QKB projection is easy um because that's sort of trivially paralyzable over the sequence. But that core attention matrix that actually gets pretty tricky to paralyze. Um the the most like the the first version of this that people developed was called ring attention where you basically take that full attention matrix um chunk it up into blocks and then a have your your your GPUs sort of work on those blocks independently in parallel in the right order to make sure everything works out. Um there's a lot of details in there. You can check out the paper for more details. Um the second which is a little bit conceptually easier is called um ulysus attention. Um where you do parallelism over the heads. So remember in a transformer you're almost always doing multi-head attention where you're sort of computing attention over like multiple attention matrices all in parallel. Um so in ulysus attention we're going to parallelize that computation of that core attention operator paralyze that over heads and then everything else all other parts of the transformer are paralyzed over the sequence dimension. Um and as a as an example like um cont this this context parallelism becomes important once you scale up your sequence length to be quite large. So if we go back to this example of llama 3 pre-training, they actually train the model in two stages. The first stage they go um sequence length of 8,000 with no context parallelism whatsoever. Um and then they have a second stage of training where they crank the sequence length up to 130,000. And then that point at at that point they do 16-way context parallelism. So that means that each of those um 131,000 long sequences has 16 GPUs operating on one sequence in parallel. And that's kind of like saying the batch size is like 1 over 16 because now like each batch each GPU is h is working on like less than one element. Um so that's context parallelism. Pipeline parallelism we're going to split across the layers dimension. Um intuitively what you want to do you have you a network with a bunch of layers and we're going to just divide the layers across the GPUs. That's actually a very intuitive thing to do. The problem is that there's sequential dependencies, right? Because each GPU like it needs the activations from the previous GPU to continue running the forward pass and during the backward pass I need the gradients from the upstream layers in order to compute the backward pass. So we can draw a diagram like this where the the vertical axis are GPUs 1 to 4 the horizontal axis is what happens over the course of time. So then you can see that GPU run GPU 1 runs forward then passes the activations to GPU 2 which passes activations to GPU 3 which passes activations to GPU 4. GPU 4 is lucky it can do forward backward all at once then pass gradients back to GPU 3 back to GPU 2 back to GPU 1. So from this graph like there that's obviously really really bad right because the GPUs are mostly sitting idle um and in fact if you have n GPUs like only you're only getting useful work out of them one overn of the time. So that means that if we had like 8-way pipeline parallelism, your maximum possible MFU at that point is like 12%. Which is terrible. So like that's really bad. Um and by the way, there's a cute name. These are sometimes called the bubble. Um is like that chunk of where GPUs are waiting for work and they they're like waiting around for the communication. So the trick in pipeline parallelism is to shrink the bubble. You want to have less bubble and the way that we do that is running multiple micro batches simultaneously. So now like rather than running running one batch of data through all the GPUs forward and backward, we're going to have multiple batches of data in play simultaneously and shuttle these things across the GPUs um in parallel. So there's a lot of different interesting patterns you can try to design for this, but here's a relatively simple one where we have um four-way pipeline parallelism. So there's four GPUs that are all working in parallel and then we we have four batches of data that are all active at the same time. So then and these batches are colorcoded. So then we see that GPU1 runs forward on the on the blue batch, then forward on the yellow batch, then forward on the green batch, then forward on the red batch. And while GPU1 is going forward on the yellow batch, um we've passed the activations of the blue batch to GPU 1 or to GPU 2 and GPU 2 can now do forward on the blue batch. So these things can all sort of cascade down and happen in parallel. And then the same kind of pattern repeats during the backward pass. We can kind of interle these different mini batch, these different microbatches as we pipeline them through the different GPUs. Um and in this case with um you know four-way pipeline parallelism with four microbatches our max the like the max theoretical MFU is just like the fraction of this graph which is not white and that increases now to 57% which is pretty good. So with pipeline parallelism in theory if you go to like lots and lots of microbatches then your MFU is going to be good because you can do a lot of work in parallel but the more microbatches you have they need to store all the activations in memory. So now we need to do activation checkpointing. And then you think like, oh crap, like how do I tune these things? Should I have more pipeline parallelism? Should I have fewer microbatches? Should I have more aggressive activation checkpointing? And then should I also layer data parallelism on top of that? I don't know. What are you going to do? Maximize MFU. You're going to try to turn tune all of those knobs to maximize the MFU of your of your of your training pipeline. Then the last one is tensor parallelism. So this one um you're going to split on that dimension on that on that um model dimension. So basically what we're going to do is we have a lot of weight matrices in our model. All those weight matrices um are like computing XW equals Y. That's basically what we're doing over and over again inside of our trans inside of our transformer. Now the idea is we'll split each weight matrix across GPUs kind of and now this is different from FSTP because we're actually splitting a single weight matrix across GPUs and now there's no communication. Each GPU now we we do a block matrix multiply. So we um each GPU is now computing a slice of that matrix multiply on the full input. So in this case we split our weight matrix into W1, W2, W3, W4. Um and then each GPU just computes a slice of that matrix multiply to compute a slice of the output. And then a problem is that now you know after you do that forward pass then you need to gather the activations across all the GPUs to do the next to do the next forward pass. Um there's a slight trick which is if you have two of these layers in sequence, you can actually get away with not computing with not uh not gathering in between two layers. So if you have two layers, you can sit down in a quiet place and work through this. You split the first matrix into column-shaped chunks, then you split the second matrix into row-shaped chunks. And if you do all this, then it all kind of works out magically because of the magic and mystery of block matrix multiplication. And you see that the final output you can kind of compute as an inner product like structure of these um of this block of these block matrix multipliers of Y and U. So then you basically can have two layers of matrix multiply um and the that are split across multiple GPUs and then they only need to communicate at the end of every two layers. Um and this actually works out nicely because remember transformers have a two-layer MLP in the FFN. So this is a really nice trick that plays really nicely into the two-layer MLPS that transformers always have. So it's pretty common in big transformers to use tensor parallelism. This this two this like two layer tensor parallelism trick um on the MLP in a transformer. Um so that's basically all of our mechanisms for splitting up computation across GPUs. Which one is the best? The actual answer is all of them. So in practice we're going to use ND parallelism. We saw already an example of two dimensional parallelism with HSTP. In practice, you know, the current state-of-the-art is like four-dimensional parallelism. If we go back to Llama, we see that they are training with their on their biggest training run with 16,000 GPUs. They're using 8-way tensor parallelism, 16-way context parallelism, 16-way pipeline parallelism, and 8way data parallelism all at the same time. Um, and if you're careful in like these different meth mechanisms of parallelism have different communication requirements. So if you're careful in how you arrange these GP how you arrange these different axes of parallelism along your cluster, you can try to take advantage of that varying speeds of communication across your your whole cluster. And that's basically and that's basically a whirlwind tour of large scale distributed training. So the takeaway for today is that an individual GPU is basically a generalizable parallel computing machine. A GPU cluster is a giant massively parallel machine with tens of thousands, maybe hundreds of thousands of individual GPUs that we want to program as one big unit. And then we talked about multiple different mechanisms of paralyzing our computation across big clusters as well as one trick, activation checkpointing for saving memory. And then the one guiding light metric that you're always trying to optimize when you design these pipelines, which is model flops utilization. So, the next time you're going out and training on tens of thousands of GPUs, hope you keep this in mind. Um, and let me know so I can borrow your tens of thousands of GPUs.