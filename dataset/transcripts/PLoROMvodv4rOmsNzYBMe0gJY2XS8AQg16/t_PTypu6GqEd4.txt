Okay, today we'll be talking about different tasks of um core computer vision algorithms and tasks detection and segmentation. We will also be covering topics around visualization and understanding. I will cover the the most important ones. All right. So last le the the previous lecture last time what we discussed was around the topic of um transitioning from sequence to sequence models RNN's through uh two transformers and we saw that transformers were defined by um having some sort of encoder a number of layers which had had multi-headed self attention and layer norm norm as well as some MLP norm uh layers and this was ultimately called something that we now refer to as encoder encoding the sequence and then if we need to decode an image or a language a sequence as the output then similar type of architecture is used for decoder getting the uh encoded tokens as input taking those as input and then generating what is um I'm hoping that you can see my cursor too what is um the uh the the output the desired output we did talk Justin talked extensively quite extensively about the differences of modeling sequences recurrence neural networks RNN's and and their variation ations that we've talked um last week I think on Tuesday uh about and then using convolution also as as another approach but we talked about ultimately that um self attention is what we work with in many of the applications these days they work uh much better than the other other two they do add they are more expensive they do add computation and and memory requirements for um and um but but that comes with much better modeling of the sequence and uh better results in terms of any of the tasks. So until here uh it was it was mostly talking about self attention. We also talked a little bit about cross attention and related topics. And then we got to the topic of vision transformers which is one of the core models that is being used in modern applications computer vision applications. We did go through this um in the last minutes of last lecture and I want to the previous lecture and I want to revisit the topic and after that I'll stop and and and hear any questions or comments you may have regarding the assignments and everything that I talked about so far. We talked about the fact that what we do with um transformers when we want to process images, we split the image into patches basically creating a kind of sequence, right? So the image was split to S bys or um in in this case maybe uh 3x3 patches and each of those patches are then represented by what we call tokens. And tokens are often a linear projection of the the the vector the reshaped version of the image into a vector and um it's it's basically a D-dimensional vector as you can see in this slide. But because we have turned the image into patches what becomes important? What are we losing here? It's it's basically we're losing the location, the position, the 2D position of of the image, right? So that's why we often create or add something that we call positional embedding. And there are many different ways of doing this. You can create a sequence just put numbers of sequence as 1 2 3 and and so on. or you can do uh a 2D version of X and Y u coordinates and adding these two together creates the the new token that goes to the transformer layer the same way all of the self attention layer norm and um everything that we've talked about NMLP everything we talked about last last week. So and then the output layer will will generate the output vectors for us could be used for any application. One of the major applications in computer vision has been classification. We started with image classification. Right? So with image classification what what uh becomes important is to somehow be able to encode or um generate something as the output that is is uh representative of the class. So what we do is often we add one token a special extra input to the transformer which is of the same dimensionality but it's a learnable parameter that in the output space whatever that represents is going to be turned into the class um probability vector. So a cd dimensional vector which is the class probabilities and that's what we often call the the class token. So this is one of the base u and and most standard way of doing uh using vits vision transformers for image classification but transformers are not only used for classification they could be used for for many other tasks that we'll be covering some of those uh today as well but last week we also talked about this other variant of the transformers again tokens And from the tokens we we go with uh the transformer layers. If you remember last time we talked about these multiple um layers of transformers as I said positional embeddings are added and here because images are they we we see the entire image all together. We don't have to do masking like we did for language because language is really a sequence uh that we shouldn't be using the future information for. And then ultimately transformers um give an output of a uh a vector uh patch for for each of the inputs. And the other option for training a transformer is actually to instead of having a separate class token just take the outputs run it pulling layer and then turn that into a probability vector for for C different classes. So I talked about two versions of transformers, right? One of them was we're using a class token and the other one was for we take all of the output tokens. We apply pulling and projection into a vector that represents the class probabilities. How we supervise this? It's the exact same thing that we talked about earlier and that was uh back propagation defining a loss function binary cross entropy the soft max loss and and so on. So this was VITS uh this is VIT in a nutshell but and and and over the years this type of architecture um for many different applications have have remained the same. Um many modern architectures right now use many of these components very similar to what we presented here. But there are some optimizations that we had in the slides last uh last week. But I think I'll I'll just spend quick um quickly a couple of minutes um on on them. But I want you to understand that there are many different tweaks and optimizations for better performance and also making the transformers the training a little bit more stable. One of them was is actually the residual connections. This layer norm is is basically at outside the residual connection. So this means that whatever we get here, we normalize it, right? So this doesn't really mean that we we can't replicate any form of identity function anymore that ResNets really wanted to to do, right? So the solution for that is to bring in the layer normalization. we often put it uh before self attention and the second one right before the MLP layer. So normalization is there but we also preserve our identity function. There are al also other ways of normalizing. There is this RMS norm root mean square normalization which is actually a very um basic type of normalization. It doesn't it doesn't use the for each of the features it doesn't use the the mean value of the feature for normalization but this makes the training a little bit more uh stable. Again there are the these are all empirically shown to be uh better options. Uh although there are some justifications why they work well but most of mostly the reason uh for adopting these is is just um the fact that they are uh they perform they make the trainings more stable. The other option is is to instead of using a simple MLP, we use a uh sugloo MLP where we actually do some sort of this is what we call gated non nonlinearity. Instead of having two vectors of U weight matrices of W1 and W2, we add a third one 1 2 and three. But here we create some sort of gated non nonlinearity which basically what what it uh does is um is is um getting more um trainable uh parameters and not just necessarily trainable parameters but creating a better nonlinearity for a small architecture. Even if we select the hidden layer um value equal to 8 di divided by 3, it keeps the the same size of the network in terms of the number of parameters but it does um learn higher dimensional nonlinearities um in uh in in that layer. The last piece is mixture of extra experts that is often used in even the very modern architectures these days. Instead of having one set of MLP layers, you can have multiple sets of MLP layers. Each of those will be an expert and what we do is you through a a router the tokens will be routed to a of those e experts and in in this uh way we actually have a active experts but then again this what it does is it increases the number of parameters and It helps learning more robust models without increasing too much in the uh on the compute and these again are all parallel MLPS. So we can have multiple experts in parallel. As I said, they are used in all LLMs these days, large language models and um all of the modern LLMs up to the level that we know of know about are using these types of tweaks and this is the summary of all of the tweaks that I just mentioned. This is similar to bias. No, this is this is completely a trainable parameter by itself that you train either a feed forward network or or just a linear projection to turn that into the probability vector. No. So it's it's it's not uh it's not just then again remember that you have so many uh self attention networks here, right? layers here and those self attention layers are basically fusing the information creating attention between all of the tokens and this class token. So when you supervise it from here the loss function comes in this will represent as the will represent the class uh class token uh the class probabilities vector. So the question is if there are nice intuitions uh what different experts are doing. Uh that's a great question because they are trained in parallel and they are initialized differently. They often try to learn one aspect or uh or a related maybe also sometimes very much related aspect but um it's just adding more more u compute and more parameters giving the network to learn different things um if if it does have to uh learn multiple concepts for example if you have to cover multiple probability distributions then then with these ops you often have the power to like separate those modes of um data. So the question is if the the number of experts is a hyperparameter or or not. Yes, definitely it's a hyperparameter. From what I know, it's often predefined. Uh don't necessarily like uh over fine-tune them, but yes, they are all hyperparameters. Yes. And they are. So why moving the the layer norm helps us learn identifi identity transformation. So look at this architecture will you be able to create any form of identity because right after that uh residual connection the feature values are changed because you have a normalization. you will never have the identity in the features, right? Because right after that you see the layer norm normalization, right? And that's why what we do is we bring it in. We have a few quite a few different tasks in computer vision and these these were the core and more important the most important tasks over the years for computer vision applications. Although these days we we're solving much much harder tasks and nobody cares about object detection anymore because now we can just do it with one line of code but over the past 10 15 years there has been a lot of advances and we want to cover I want to really cover some of those today just so if you have to design something new yourself you know uh where to look and how to design your models and then ultimately there is the topic Think of visualization and understanding which is very important in in many applications. For example, if you're working with medical data, often the visualization understanding is more important than the classification itself or detection of tumor for example. You want to know where, why and so on. And um the way we started the class and this this slide is probably uh very much familiar to everybody. We talked about different tasks and for object classification for for the task of classification. We talked about this. We spent quite a lot of time over the first few lectures saying how we can create a classifier that classifies images from pixels into labels. But then one of the other tasks important um similarly is is semantic segmentation. And within semantic segmentation what we care about is to assign a label for to every single pixel inside the image. turn each each of the pixels into a label that is is the label for that object or or um anything in the scene. So basically at the when we train a model that does this at the test time we want to take an image and generate the same map as the output. How do we do that? There are many different options. So let's say um I can what I can do is just look at each pixel every single pixel and say what the value or what the label for that pixel should be. Some it in in the very basic form as you can see here it's it's actually very much impossible. It's hard to say what pixel that um represent that that specific what what object that specific pixel represents. because there's no context if you only look at the the pixel itself. So that's why context is important. We look at the surrounding areas and um and then if I take these patches the pixel in the center and the surrounding areas. Now I can train a convolutional neural network or any network that generates the output label for us. Right? It's the same architecture that we've talked about over the quarter and you can select any of those that we used for for image classification because now you're classifying the image the entire image. It could be a CNN, could be a ResNet, could be a VIT or whatever. This is really time consuming because if you want to run one full network for every single p pixel in an image, it will take forever to to turn this into a segmentation map. The other option that we often uh we can use is to instead of running one network for every single pixel, what if we train a neural network that takes the image as the input and outputs the entire pixel map, the segmentation map, not just one single label, a matrix of labels, right? And in that case um we will have our segmentation task solved. And in order to do that we need to have a layer in the in the input that is the same size of as the image. And also in the in the output you also need some sort of an inflated layer. You can't go to fully connected layers and so on because now we are generating an image and because of that we need to keep the the network inflated and and then um that's what we call often fully convolutional neural networks or or FCN. So with fully connection uh convolutional neural networks this is this is definitely a great idea but there is a caveat there is a problem these images are are large and and these networks these uh layers will become very large and there will be so many parameters to optimize especially in the early years that we didn't have powerful GPUs this was a bottleneck a problem a challenge for training algorithms And that's why the algorithms evolved into starting from full size images going down in terms of the resolution making the convolutions the the special resolutions smaller and smaller through down sampling operations. And then somewhere in the in the middle we'll have a low resolution but but somehow thick in terms of the number of channels. And and then from there what we do is we go back up to the same size of the image to create the output pixel. And in order to do that we know how to do the down sampling. Right? Downsampling was was easy. We've talked about it. We talked about um pulling operation, strided convolution and um several other um steps or operations that could be used here. But on the upsampling side, we don't we don't really how know how to to do the upsampling, right? because we don't have pulling or reverse of uppooling or reverse uh reverse right convolutions right and and because of that we had to invent some new operations that reverses down sampling uh by itself but before I go to the upsampling uh defining what upsampling is I just briefly want to tell you that um how this uh maybe I can ask you a question. How do you think this network is trained? Because now we have a network that starts from an image and ends with an image and then the tools that we have for training this network was a loss function, right? How do you think is the best to train or or define a loss function for this network? We talked about softmax loss, right? We talked also about a little bit about some regression losses and SVM loss. But assuming that we want to use softmax loss function, how could we define this uh or train this network? What would the objective be? So mean you said mean classification loss for each of the pixels and that's uh that's correct. You can add the loss function for every single pixel because every single pixel is like is doing a classification right. So you will have a sigma over all pixels of the image and the loss function is just a simple soft max and then you can back prop that's that's that's the entire loss function that you need. The question is um do we need what we call ground truth for training? So that's that's actually the ground truth of segmentation and that yes for these types of algorithms because they are fully supervised. We do need the ground truth label maps and early years there has been a lot of work doing and sitting down and and and u manually labeling the the pixels to be able to train these algorithms. Yes, these days we don't need that because we have tools. But early on in order to train these algorithms, we needed the ground truth. Okay, very briefly let me tell you what we do with upsampling. Upsampling is actually not that hard. We can use an unpulling operation. There are different ways of doing it. One is nearest neighbor. If I want to go from a two 2x two As an example here, a matrix 2 to 4x4. I just need to copy the data kind of for each of these. Take the nearest neighbor in the in the lower resolution one or bed of nails is you just select one of those in in the upsampled version. You only select one of those or the one on the corner to copy the data. replace everything else with zero and through multiple layers of convolution these values will will start um appearing. If we used max pooling in our um in our network in the in the encoding side, what we can do is we can save the locations of the max um the the ones that were selected and then copy the data in the unpulling max unpulling stage right over there that the the uh max was defined. So basically we we save the locations in the encoding part and in decoding part in the upsampling step we reuse those saved coordinates. The other option is to to do a learned upsampling. All of these that I I showed there is no parameter to be learned. It's just an operation. But learned upsamplings are also possible. Very simply, let's let's revisit the convolution. In the convolution layer, what we we did was applying a convolution filter for a pixel and generating the output and do this repeatedly for all of the pixels, right? And when we wanted to do um to to down sample what we did was strided convolution where instead of taking steps of one we take steps of two and generate the outputs step by step. If you don't remember this part go back to the lecture we talked about it third lecture I think and um and then we can replicate the same for the offsampling process. So this one will represent this area in the upsampled image. And then we define some weights here to map that to the output um map. And then for the next one, same story, but there will be overlaps. And for the overlaps, you often sum over the the values. Let me give you an example. Sum over the outputs. Let me give you an example. And that's with a simple 1D uh function. If the input is um just two values of A and B, we learn a filter that that filter maps it to the higher resolution output, right? And for doing that, we just apply the filter to each of the values and write the outputs here. for the parts that there's an overlap, it's a summation addition of what is um coming from each of the two uh locations. So we did talk about this um fully convolutional neural networks and and how they are being used. This these are actually some of the most basic and mostly widely used algor algorithms for segmentation. Um and I want to also very quickly highlight one of the widely used networks unit as you can see the the the shape U. It's actually the same architecture as I showed here just let's let's uh draw it as a similar to a U shape. And uh the reason that I'm highlighting this is that still today some of the medical applications uh that that work on segmentation use segmentation algorithms still this is uh the this this unit or its variance generate the state-of-the-art um results if you don't want to use any foundation model. So what it does is exactly what we explained a down sampling phase that increases the field of view and loses some special uh information and then upsampling phase that um goes back to the image resolution. The only difference that unit has is because it's the it's used for segmentation. There is this um understanding that we need to keep the the the special uh information in the decoder side because when we down sample we somehow lose resolution. And then upsampling if you don't have the information it's going to be a little bit um hard and we often get into sometimes boundaries are are faded and in order not to um get there the feature maps in the encoder side are actually copied as inputs to the decoder layers. In that way you are act you're you're keeping this structural information within the image and generate the outputs that are much sharper. So this was the idea about um behind unit and as I said it's actually being used quite often. Summary of semantic segmentation. Um what we talked about today the fully convolutional neural networks you have same filter as um as before that we had for down sampling here. So you have a um a actually so for to save time I actually removed some of the slides from this part and I have it in the backup slides you should check it out. This is a reverse um this is a transform transpose convolution. So we do have a 3x3 matrix here and then instead of convolving the input image input data we convolve we we do the convolution on the applied convolution on the transposed version of the input and it actually generates an a larger output. So it's the transposed convolution it's it's uh it's the reverse of the regular convolution. But why transposed? I would refer you to take a look at the the additional slides. So the question is is that filter trained? Yes, it's very much similar to other convolution layers. All of the filters are trained. Yes. Yeah. Okay. Um great. This was uh the topic of semantic segmentation. And as we talked about this, we only get labels for the pixels. But if there are two instances of the same object, we have no idea which one is which, right? Because this is just generating or outputting the the labels, pixel labels. And this brings us to that brings us to the topic of instance segmentation where now we not only care about the pixel uh classes but also I want to know that um these pixels belong to one instance of the dog and this uh next one is actually a different dog right and and for doing that what we need is understanding objects multiple objects in the image and brings us to the topic of object detection. Object detection has been one of the uh besides image classification or after image classification has been one of the core computer vision problems and and tasks. And for many years many many different algorithms were proposed for just doing the task of object detection. We are going to fly over some of them and highlight a couple of important ones but again there are so many works in the literature that even in literature of deep learning that I'm not covering uh here. So in the past 101 15 years how we can do that um and and solve the problem of object detection if it's just a single object it means that we need to generate we need to do the classification generate a label class scores as well as getting a bounding box coordinates of a box right so you need the coordinates of the box XY and H and and W as the output as well as what class it is. Right? So this is exactly the task of object detection. How can we solve this? It's very simple. Right? We can define a softmax loss soft max loss function for the class scores. And we can define an L2 loss function which is a simple distance metric a regression loss for the box coordinates. And having these two defined we have a multitask loss. We are solving two tasks at the same time. And for doing that we again uh add the loss values and generate a compound loss function as you can see here. So this is simple. It's doable. If we have one single object, we can for sure um solve this problem using this architecture that I talked about. But this is not that easy if we have multiple objects in the in the scene. So for three objects, we have to generate 12 output numbers and and if there are more then it it's going to be too many uh numbers to generate. So this is this algorithm is not really scalable. it's it's it's just extending the classification into some sort of object detection which is fine but it's not really scalable. So um if the when there are multiple objects, one solution is um instead of going or or getting the entire image as the input, why not to look at bounding boxes for each bounding box, we can say we only have one label and um whether it's a cat or a dog or or the background, right? And if I have this this um way of classifying each of the bounding boxes, I can do a sliding window. I can create just bounding boxes slided over the image from coordinate 0 0 to all combination of XY and H and W and see if we can detect the object. So step by step I can I can create uh I can find the bounding boxes that have the the maximum probability of each of the objects. But there is a huge problem here right again there are so many different combinations of bounding boxes that we can use and again this algorithm is not scalable. what we've been doing in the literature again early early years if you look at the uh the years these articles were published 2014 and before there has been a lot of research around finding regions that are um they have high probability of having the object in them. So region proposals and if I have a way to find region proposals that's actually going to be a easyish problem. I can do the same thing as as I explained earlier, right? For a for an image, what we can do is if if I have region proposals, I can just take that part that patch out and run a CNN on that on that patch, right? And convolutional neuronet network on the patch and then classify it. And in order to even I can I can refine the bounding boxes. So classify oops classify and then refine the bounding boxes to have um the object detected. So we can we can classify the boxes and also refine the bounding boxes if if I have to change the coordinates a little bit. And this is what is called um RCNN um algorithm. And uh although it's it's it's it works and again this is one of the early algorithms CPR 2014 there are these are very slow because for each of these box boxes again we are running a full convolutional neural network. But there is one catch that um um what what we can do is um is instead of running convolution neural network on each of these boxes because convo operations convolution operations they preserve the special information right they either down sample or or upsample we always have a a way to to track them where in the pixel space they are Right. So in that uh case what we do is instead of running the convolutional neural network on the patches let's say we run one big convolution on the entire image and then now we have the regions in that feature map which is corresponding to the entire image. Let's look at those regions and now run a smaller CNN for on top of those and generate the outputs. for each of these two um outputs that I want. First the box offset like should I move the bounding box a little bit or what the object category is. So this is the the fast version of RCNN. These are some uh some basic algorithms that you can use convolutional neural networks for detecting objects their bounding boxes and so on. The question is if the number of pre proposed uh regions are predefined. Uh that is the the short answer to that is yes. I will talk very briefly about the region proposal networks too. So easy algorithms right? One puts the bounding boxes of the regions the proposed regions um on the images one puts it on the feature maps of the connet and both of those generate the output class label as well as offset of improving the the location of the detected object. But this will mean this requires us to do um that bounding box uh region proposal um first and and have a region proposal network that tells us where to in image we should look at uh look for and uh there has been research on on building region proposal networks RPNs and here what we do is we just randomly start with with a CNN we try to randomly start in in different locations in the image and through layers of convolution we refine those regions where they have the higher probability of having an object in them because we have the object labels and and and locations. So we can optim we can uh supervise this and then each of those also refine the box coordinates. So basically a neural u a region proposal network what it does is it refineses the boxes each of those boxes that have a probability a high probability of an object in them and the box uh the output boxes the box corrections again I'm I'm leaving all of the details about the coordinate uh coordinates and all of these u dimensionalities for you to pick uh afterwards because you it will take too much time and we don't want to spend too much time on this uh algorithm. But what's important here is back to your question, we often take the top k the ones that have the highest probability of having an object in them as the proposals for this image. This is an simple image. So then then most of the and only has one object. So most of the regions are centered around that single object but in general that's not the case. So in in many setups we can have region proposals used in in different setups. um we can get different objects and with higher probabilities with that and after talking a little bit about RCNN and mask RCNN which again for you it's it's important to to go through the details and if you can spend some time um doing the calculations yourself that would be very good but those types of algorithms RCNN mask RCNN they're not being used anymore more these days because because they are very heavy computationally. Although it's important to understand how we got to this point but but those are for uh for many reasons. One of those reasons is that we we need two separate networks. One region proposal network and then one um classification and box refundment network. So it's like at least two passes for detecting objects on the on the uh for each image right and that's why there has been advances after these um using single single stage object detectors SSDs and one of the most popular ones is called YOLO. YOLO is um probably if if you work with any computer vision problem you've you've heard about YOLO even to date although it's a it's a convolution um heavy network today uh at least its earlier versions uh in many even industrial applications YOLO is being used as as the base for object detection because it's a fast uh object detector and It's um very good in terms of detecting uh the objects. What YOLO does I want to very briefly uh tell you a little bit about. It's basically you look only once with one single pass on the image. You generate all of the bounding boxes. How it does it is it maps it. It divides the image into S bys S grid of S bys and in this example it's 7 by 7. What happens is that for each single box in that grid it tries to output it. It it creates a fully con convolutional network that outputs the probability of an object being in that location. refinements of the bounding boxes. So it generates Bbounding boxes and new hyperparameters B bounding boxes that is the refinement of the object that is present in that box and also it generates a probability the class uh class probability object class probabilities and um in this case for example if it's B equal to two it generates just two B bounding boxes with different probabilities. It it does this for all of the boxes at the same time. So basically it's the it's the same network that is being uh generating something as the output for each of these bounding boxes and it it it does generate a number of different options for the for the for the object and as I said each of those boxes are associated with a probability and pro in this example the probability is shown with the weight of the edges uh in each of those boxes. And um for these many different bounding boxes and object probabilities now we can do thresholding and also um there is an algorithm that they used in the paper again I don't want to go into the details uh non uh maximal suppression and um some some algorithms uh with thresholding involved that that identifies the ones that have the highest probabilities. So this is this is a simple implementation or or use of the uh object detection. Again this is this is something very useful if you have time spend time with the repositories of YOLO. There are so many different uh newer versions of YOLO that is being used for many applications in in medicine robotics and also in many industrial applications. So the question is how do we get this uh second image and what's the intuition behind it right as I said for each of the grids we generate B bounding boxes like for for this one we generated two for all of all others we also generate two this B is again a a probability vector and each of these boxes are associated with probability of uh existing an object in them and then if I put all of them together for all of the patches I have so many boxes And now each of those are associated with a probability right. Um let's move on. Um and one of the more recent approaches for object detection is deter a detection transformer. This is purely based on transformers and the topic that we did discuss last uh week and also I started today same type of self attention and cross attention modules could also generate some uh some object detections and bounding boxes for us and how this works. This is actually not a very old paper 2020 uh almost 5 years ago although it's it's now kind of deprecated. Nobody uses this for real applications but but it's a it's a very good example of how to use transformers for object detection. And what we do here is basically similar to what we've explained earlier. We make turn the image into patches and then those patches are passed through CNN's creating a token. Then we add positional encoding the same way that I explained to the patches and those define our tokens for the input uh which are inputs to the transformer encoder. a transformer encoder again a bunch of self attention layer normalization or any normalization as well as MLP layers that generates the output tokens after multiple layers of uh transformer encoder. Then in order to generate the bounding boxes, this is this is the smart part for this algorithm that it does take the encoder output tokens as input in the transformer decoder. But we also define some queries which are trainable parameters themselves that each of those for example if I add five five queries as input four queries as input or 10 or 20 inquiries as input I'm seeking up to 20 objects to be detected in that image and then again through a combination of self attention layers in the beginning of this transformer decoder as well as cross attention with the encoder output. So cross attention and self attention networks layers. It generates it it it generates the output values for each of these queries which are passed through an FFN uh freeforward network to generate either class um labels and and the bounding boxes very similar to what we discussed earlier or even uh in some cases it it just says there's no object to be detected and and at the end we have the bounding boxes and the classes associated with bounding boxes as the output. So the question is are we inputting every possible box to the transformer? No, the input here are some general parameters that are representing they are queries uh representing asking the question that I actually want an object to be outputed in in in place of this input query. Right? So there is no box or anything as the input. It's part of the output that it generates the class label and the box coordinates. So the question is um if the queries are formed in a way that's it it it actually represents what we want to look for and where in the image right so in this case um what we are looking for is defined by class labels which are defined uh predefined and they are as part of the output. So our supervision is based on the class labels. We have a class probability vector same same way that we defined it for the other algorithms. Right? So that's how um the algorithm knows what type of classes to look for. And then in terms of the outputs uh again these outputs are supervised if you remember based on the L2 norm L2 loss of the ground truth boxes right. So we're not telling anything in the query part um what to or where to look uh for any of the objects. The training the the process itself is back propagating um if there are any losses any any errors it back propagates the the outputs. So basically it's um we are not determining anything in the beginning or in in this part. So the question was the query is give me up to nine objects and uh and yes the that's the that's that that's basically what this means and through the self attention and cost attention it will turn try to generate output tokens that are turned into class and box coordinates through that FFN operation. Your question is if the queries over there if they are image patches or not. No, they are they are not image patches. They are just uh queries for uh trainable parameters to to you you put them in to generate the outputs for each of the inputs. You you get the value as the output and that value is turned into class and box coordinates. Again the question is what is object queries? They are trainable learnable parameters. So you initialize them, the network finds the best values for them and that's what you get as the output. The question is if uh if there's any intuition uh which FFN gets which box, right? The short answer to that is no. uh there's there's nothing that stops the network from I mean we are not including anything that stops the network from uh generating multiple but remember that are so many self attention and cross attention layers over there that they are actually interacting with each other and and makes each of those queries match with one of the uh output layer. So it's it's not generating the exact same thing at the as the output. And we also have control over how to supervise supervising those uh FFNS as well. So your question is if there are image segmentations pixel level segmentations as part of the training. This algorithm does not require the segment level um pixel level segmentations. It it's only supervised based on class labels and bounding boxes. But if you have the b the pixel level segmentations, you can always turn the pixel level segmentations into a bounding box to train this algorithm, right? But it doesn't necessarily require that. So the question is if it's possible to generalize unseen objects. Um and by unseen you mean a new class label. uh for these types of algorithms that they are fully supervised often there is no way because you are creating class probability vector there's no way of like adding something at the end for a new class um without previously knowing there's there's some some other classes right so fully supervised networks they are often there's no new object we can have a background object or no object as you can see we have the the label of no object But there are many algorithms and extensions of these types of algorithms that are used for zeroot learning. Zero shot means understanding something new without having an example of those in the training data. But uh it's beyond this this topic. What happens if you have more objects in the scene than um what you uh put in as the query? Right. So that's a that's a great question. it often generates the ones that have the has has the highest confidence on the objects. So bounding boxes with the highest confidence and in in those cases you often want to add more queries just so you can get more objects, right? Okay. Um I'll be here to answer questions if you have any uh after after the class, but we have a bunch of other topics to cover and I want to make sure we we go over them. at least you get uh familiar with the topics. So with the object detections now back to the question that was asked earlier, how can we use those types of algorithms for instance segmentation and that's actually not too hard. We talked about this um when when we were talking about our CNN algorithms where we run a CNN on the image then we have a region proposal network that gives us the bounding boxes and those bounding boxes are turned into either class labels and bounding box refinements. Right? So this is what we've talked so far with RCNN and so on. Now we can turn this into a mask RCNN that also generates the mask. So, so basically same architecture that we talked about earlier. Now we can we can uh take one more output make it more multitask and generate the mask predictions. So what we used to be doing before was again image region proposals CNN gives us class label and the box coordinates. Now we add another layer of convolution that generates the mask the the mask for for that object on in the pixel level. And that mask again could be the same size as the input and the and the image and basically on the on the layer itself. If we use fully convolutional neural network that's that's what we often get as the output for each of the objects. When we have that box tiny box we can always get the mask for that. the chair in different settings of the box itself if you have different boxes, the bed uh and the human the baby in the image and this is what we an extension of the RCNN algorithm which we call mascarn and um with mascarn it's actually the the results have been very very good in detecting different objects different um known objects that we could train the algorithms for. And then um there are so many APIs and and open-source versions of object detectors that you can um explore. There are some some links and resources here. But this all basically rounds up and summarizes some of the tasks that we wanted to cover and and they are actually very important for you to understand these tasks. They have been core computer revision tasks. Although these days computer vision is is way more advanced, they're not bound to these tasks. But if if you have industrial applications of for example uh quality control of separating rotten tomatoes and and good tomatoes in an industrial pipeline then with computer vision you need to be able to detect objects and then classify them into good or bad. Right? So so that that's why it's important to still understand and know these um steps and pipelines and how to do them in real time. But now there are larger scale models that you're all familiar with. This um summarizes the first part the computer vision tasks that I wanted to talk about. And the last piece that I want to spend 10 minutes on is around visualization and understanding. Again this has been a big lecture by itself and and in 2050s60s until 2020s that the topic of computer u and and and even before that 2014 13 the topic of visualizing neural networks has been very hot and very uh much it gained it it helped us gain understanding into what the networks are learning and I'm going to summarize some of those the most important ones here that you may need to un to use in your applications. But before that, let me go back to the linear classifier that we talked about. Um, we spent quite a lot of time on linear classifiers. And with the linear classifiers what we we did was at the end we said if if I look at the linear function the the what what the network is learning I can have a template for each of those classes like for example for this car you can always see a front-facing car um as a template right we can do the same with neural networks if I visualize one of the filters so here we visualize the weights of the linear function it was the visual viewpoint point I can do the same with linear uh sorry with visualizing the filters in the neural networks. So for each of the filters the the network is for example is learning something that is basically some of the basic shapes um orientations or or simple shapes as you can see here. Although this visualization is it's we can only do it for the layers that we have few channels like for example if we have three three channels I can put them in an RGB image and just just visualize it but as you remember in in CNN's that was not the case in CNN's we had different um sometimes quite a few uh channels in the middle layer so it's not easy to visualize those in something that we can see but but that's that's what you basically in early layers that we have fewer channels we can visualize them and see the network is actually learning some patterns. So we start it starts learning patterns and then later stages it um gets uh more holistic and and bigger patterns as um if if we train um sorry if we run some something that we call guided back propagation we can also visualize those but not as simple as this. I want to highlight an a couple of ways of evaluating uh understanding and visualizing the neural networks which are actually kind of important. One is uh the concept of salencies. So in many applications it's very important for you to know which pixel matters. For example, in a medical application when you do a classification of tumor versus none, you want to see which parts of that image is actually the the tumor because if you want to automate this, nobody cares about knowing if there is tumor or not. Everybody cares about where in the image the tumor is, right? So in order to do that simplest application is we train a network a feed forward um neural network that generates the value or the class label doc. But what we can do is we can um actually before I showed you that in in this case in order to train this network what we've done was we always took the derivative derivative of the neural network the weights sorry of the loss or of the class score with respect to the weights in order to update the weights. Right now what I need is for each pixel I want to see how much changing the pixel value how much changing the pixel value would affect the dog score right what does this mean what I explained is is the the meaning of derivation right so if I u this means that the meaning of basically gradient so if I take the gradient of the core with respect to now the pixel values not the network weights anymore with pixel values I can visualize those gradients and visualizing those means that these are the pixels that are that that matter in order to classify dog on this image those are the pixels that matter. So if I change the values of those pixels the score the dog score will change will be changed right again this is the basic meaning and definition of gradients that uh we've talked about. So this is one way if you run this on on different objects uh that you've trained in in the network then this is what you get. So that's one way of understanding salency. Uh and it's very effective in many cases. But sometimes it's not just about the the pixel values all the way to the to the back. You want to see um for each of the classes how the activations uh work. And this brings us to class activation maps or CAM algorithm. uh class activation mapping cam or grat cam that I will talk about in a in two minutes are uh one of the most and widely used algorithms for understanding CNN's and also could be used for other architectures too but for transformers we we have a much better way of um making sense of those which actually we talked last uh in the last lecture. So what happens is that for each of the convolution layers um we often do pooling and the pooling generates feature maps. The feature maps are then turned into scores and those scores with with those uh values of the uh weights. If I if we extend the math, basically we simply can highlight the class scores in a weighted sum form. And this means that um you can trace back class predictions all the way back to the feature maps and a specific locations of the space because convolution layers are always mapped to a space in the image space too, right? we do convolution that's the the special uh consistency across all of the operations can can help us trace back all the way to the image space. So anyways we can we can look at the feature maps and see how the class activations each of these classes are actually impacting those locations in the in the image. And with that now if I if I do this um multiplication of weights versus uh the the the weights that we've learned on top of the feature values we create some class activations. And this means that I have a way now to go back to the image space because as long as I'm in the convolutional space, I can go all the way back to the image and create these maps of like for example for each of the classes, palace, dome, church, u altar and and uh monastery. We can have different class activation maps. These are the weights. These are the pixels or areas of the convolution layer that have been driving the scores for these specific uh classes. It's the same for for others um like class activation maps for one single object in different images. But there's a problem with this and that problem is that we can only apply this to the last convolution layer because this is the only way we can do it. Like we can only go to the last convolution layer the way that we did the calculations here. And in order to solve that problem there is one variant of the algorithm called grat cam. So gradient weighted class activation maps. It's basically the same algorithm just we need to weight calculate the weights with respect to the we basically take one of the uh layers that created some sort of activation in the class class uh level. We compute the gradients instead of just calculating the multiplication between W and feature. we all we go all the way back with the gradients and create a weight based on the gradients um and then that is used instead of the weights it's aggregate of all of the weights and gradients up to that specific layer and then we weigh that with with that and then we also use ReLU to only map uh to pass the uh be the positive ones and And that could also be all the way shown in the class in the image space. So I talked about cam which was only applied to the last convolution layer. If you wants to but but this is not possible because in most of the CNN algorithms we don't have just u like one convolution layer at the end right we always have some operations fully connected and so on. So in order to be able to carry this class activation to the convolution layer if there is something else in the middle we often use the gradients and weight basically weigh the maps with with the gradients uh aggregates and then we can actually do the visualization they create these heat maps for each of the objects. So this was about CNN's but we talked about u transformers last week right u last in the last lecture that they actually they inherently come with the activation maps. Did you do you remember that language um matrix that that Justin showed that for each of the output words there is a tension weight for the input. We can do that the the same thing for the pixels for each of the outputs we can create these maps in the pixel space and and visualize the features of the VITs in the pixel space. So basically with vit and and transformers this is much easier. you already have a way to to visualize the ch the attentions the weights but with CNN's we often use grat cam or these types of algorithms that said I did this u task that I thought I wouldn't be able to completing the the the topics I wanted to talk about today and next session we'll have the lecture around video understanding. Thank you.