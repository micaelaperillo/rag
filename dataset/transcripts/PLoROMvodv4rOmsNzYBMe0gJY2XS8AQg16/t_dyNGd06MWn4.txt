Today's lecture topic will be about uh regularization and optimization which are two very important concepts more broadly in deep learning and machine learning uh but especially important for computer vision. And we're going to start with a recap from last week and discuss some of the topics that we discussed last time. So um we really honed in on this idea of image classification as a core task in computer vision. And what this task is is given an image as input you try to map this image to um a a label inside of a set of labels. So here we have five different labels uh cat, dog, bird, deer, and truck. And the goal is to assign the correct label to the input image. and you're creating some model or some function that takes an image as input and outputs the specific label here. And we also talked about you know a lot of the challenges for classification. So one of the main challenges is uh shown in the top left here but it's this idea of the uh semantic gap between what we as humans perceive in the image which is the cat and what it's actually represented uh as in the computer which is this grid of uh pixel values where uh you know you have this multi-dimensional array or tensor um and you have discrete values for each of the pixels. This is very different from how we're perceiving the image and deciding that this is a cat. So being able to map from this uh complex numeric representation into one that we humans understand is the core challenge here. But also there's challenges uh surrounding the images themselves. So if you look at something like the uh illumination of the scene. So here you'll have different pixel intensities based on where the lighting is in the scene. Uh also you know you could have certain parts of your object are in the shade and harder to see. Um cats by nature are very deformable. So you talk about deformable objects. They can move around and twist and bend in different ways, so they won't always have the same shape. And this can prove challenging if you're trying to design an algorithm to detect objects. Uh there's also the challenge of occlusion. So you could have a cat that's like hiding underneath the couch uh cushions here, but we as a human can clearly tell this a cat because of the tail uh how it's sort of sticking out at the end here. And then you know the way that cats behave, we can infer that this is a cat. Uh you'll also have things like background clutter where the object could blend into the background. So we need to account for this somehow as well. And finally, there's this idea of interclass variation where different objects in the same category can look very different from each other, but we still need to group them all into the same category. So here are a lot of just the challenges of recognition and why it isn't such a simple problem where you can just sort of write if else rules to account for everything and just simple logic to classify. So if logic's sort of thrown out the window, you can't just create these logic rules. um how do you actually create a classifier? Here's where we talked about datadriven approaches. And we talked about basically the simplest uh machine learning model which is this K nearest neighbors model. And the idea is that you look at for a given data point, what are the existing data points in your training set that uh are very close in distance to your new data point coming in. And for the one nearest neighbor case, uh this just results in, you know, you find the closest data point, you assign it that class label. And you can also look at multiple nearest neighbors where you're assigning the most common class label among those nearest neighbors. So we talked about these two different approaches. We talked about how um you ideally don't want to split your data set into train and test but you can do train validation and test so that you can use this validation set here to actually uh help you choose your hyperparameters. So the main hyperparameter for k nearest neighbors is this k uh one or five in these examples. And what we showed is an example where you're plotting what is your accuracy on this validation set here um over the different k values here. and you would, you know, you would choose the one that has the highest accuracy. Um, so this is how you'd use the validation set and then you would reserve this test set for okay, how does your model do on completely new data it's never seen before. That would be the purpose of the test set. This is all just recap. Um, there was a bit of confusion about uh distance metrics. We put a post on ED that explains this in more detail. Um but um we've talked about two different distance me metrics. The most two commonly used ones in machine learning which are the sort of Manhattan distance and or L1 distance and L2 distance or uklidian distance. Um L2 distance is like um if you imagine just the straight line distance sort of how we think of distance in everyday usage of the word uh geometrically. And then Manhattan distance is this idea where you can only sort of traverse left and right and up and down in this diagram. and you can't move diagonally. So specifically looking at just one quick example here um the reason why all these points on the line are the same distance from the origin here um is because you can't move diagonally. So you have to move in this case up 0.5 and to the right 0.5. So the total distance is one whereas you know here you're just going a straight line but it's uh one also the same distance here whereas in the L2 distance um all the points equidistant from the origin here form a circle because you can you can just go in the direct line here. So this is maybe a brief explanation. The final thing we sort of honed in on last time was this idea of a linear classifier. So um the basic idea in the in the basic setting that we did is we have an image which is you know say width 32 and height 32 and there are three uh pixel values for each of the spatial locations in our image representing the red, green and blue uh intensities forming the color. And the idea is we take this array of numbers for our image and we flatten it out into an array of just 3,000 different uh numbers 372. And then we're multiplying this vector by our weight matrix W. And um the basic idea is if we have a weight matrix W that has uh you know uh height here of uh 10 and then the width is 3 372. We're multiplying each of these rows by uh our input sample X and this will give us 10 resulting uh class scores. So um often times we'll add a bias term as well which would just be one bias term for each class. So this would be you know a size 10 vector here. And we also talked about three different ways you can view or think about this these linear models. One is the algebraic viewpoint which I described here where each row is um represented sort of independently representing the class and you multiply it by the input vector x. You get your uh score and you add the bias to get your your final score. um you do each row sort of independently in this sense. You can also view um these uh learned class uh weights here as templates where if we then uh sort of reravel the uh vector into the original shape of the image uh we could plot the intensities here and understand what um what is sort of the uh template per class which is what this visualization represents. And then the final way you can think about it is a sort of geometric viewpoint where uh each of these um rows in our weight matrix are represented by these lines here in our um input space and specifically the line is where we set this equation to zero um which is the decision boundary. So this forms the point where uh you know above the line you could have a positive score and below the line you would have a negative score for the for the for the class. So these are sort of the different viewpoints for how you can view these linear models. They're all doing the same thing. And one nice thing about the geometric viewpoint is that uh if you visualize your data like say you want to classify uh blue versus red here it's very easy to tell that you can't draw a line that perfectly separates um the data here. So it's kind of a nice uh way you can gain intuition about what is possible for a linear model to do. Okay. Um I think that's sort of the highle recap of what we discussed last time. Um I'll actually be going into a little bit more detail on sort of the new content for uh this lecture now but I just wanted to pause briefly if anyone had any questions about what we discussed last time or at the beginning of this lecture feel free to ask. Yeah. So the question for those online is for this uh visual viewpoint is this the same as um sort of running K nearest neighbors and this would be maybe one of the neighbors that you're comparing against uh are they mathematically equivalent? Um no they're not the same because um these templates are formed from this line. And so it's not like one specific uh data point but you can still calculate the templates based on the uh this sort of uh they would represent more like if we see in this diagram here there's the line pointing in the direction of the class. So it would be sort of representing this point here. Yeah. So the question is how did we get this like 372 number? So the idea here is that um if the height of our image is 32 pixels and the width is 32 pixels and then each location in the image is represented by three values the red, green and blue pixel intensities um we would then get 32 * 32 * 3 total values to represent the entire image and that's how we get this 3,72 number. So um here's a I guess very specific example of a uh linear model here. And we when we multiply our input X by our weight matrix W we get the resulting scores for these different classes. And you can see that uh for cat it's not doing so well because car has a higher score and we want the highest score for the uh correct class. Also here on the second example does pretty well because it's doing it correctly. But then in the frog example, it sort of gets it completely wrong where it's by far the lowest score of the three. So um intuitively we can tell that these scores are not very good. But how do we sort of mathematically formalize this intuition and how do we determine how good a given model is? And this is the idea of a loss function which tells you how good or specifically tells you how bad uh a classifier is. So given a data set of examples uh where we're indexing by with this uh letter I we have X I is each of the training examples Yi is each of the training labels. Um we can compute the loss over our entire data set where we calculate this uh loss for each training example by sending it through our model here which is this f ofxi w. We get our label and then we compute it compared to the ground truth label yi and we just take the average over our whole data set. So this is how we do this. We talked about um in last lecture the softmax loss or the cross entropy loss which is the most commonly used loss for classification. And um so I won't discuss that again in so much detail here, but um basically it's very high uh it's very high loss when you predict low probability of the correct class. It's very low loss when you're predicting the correct class at very high probability. Um so these this what I just uh explained is all uh contained within what we call the data loss. So this is a loss that tells you how well do the model predictions match our training data. And obviously we want this to be very low and if it's very low it means our model's fitting our training data well. Um but there's a second component which I'll discuss today which is this regularization term of the loss function. So what this does is it's intended to it's intended to prevent the model from doing too well on the training data. Uh so it actually does worse on the training data, but the goal is to make it do better on new test data or unseen data. So worse on training, but better on on a test set. That's the point of regularization. Um and we'll go over a lot of the intuition for how to think about it in the in the next slides here. But the highle goal is to do worse on the training data, but then better on the test data or just unseen data. That's the point of regularization. Yeah. So we're computing the loss on each of the eye training examples. Yeah. So the loss of the i example uses the x i and the y i. Um does that make sense? I mean you you could not have an i here, but this is just saying the i the loss term. Yeah. Yeah. You you normally don't have a different loss for each I if that's what you're asking. Yeah. Yeah. Yeah. So this is just you could we describe li as the loss for the training example. So we're just using here. But yeah, it could be. Um so for regularization um people usually have this intuition when thinking about it where this is sort of like a toy example and the the idea is we want to fit some function to these points where our input is x and our output is y and uh you say have two different types of models f_sub_1 and f_sub_2 um and you're trying to decide which of these is better. So F1 goes through all of our data points. So the training or the data loss will be very low because it's basically doing it perfectly. Whereas F2 um doesn't go through every point perfectly, but intuitively it feels like probably F2 is a better model when we're now testing on new data we've never seen before. So um regularization sort of captures this intuition of you don't want to overfit your data so hard and you might actually be better off with a model that fits the data less but is either simpler or has some other properties that uh make it a better choice. And so if we you know ask okay how is this how are these models going to do on new data that's within our same distribution you'll find that you know F2 does a much better job at modeling. So here's it's doing better on the unseen data. Um so um I think there's also an intuition in this previous example that's demonstrated very well where we're preferring simpler models uh where it's sort of like AAM's razor which is this idea in uh philosophy and also scientific discovery where if you have multiple competing hypotheses you should go with the simplest one first and then if that if you know for sure that's wrong then you can start trying out more complicated ones as you go. This is maybe also some intuition you can have for why regularization can be useful. Okay. And then one final thing about this equation that I didn't touch on yet is this lambda parameter here. So this is the regularization strength which is another hyperparameter. So we might use uh training validation sets to set what is the optimal lambda here as well. But the basic idea is we can set this to a floating point between I guess zero and infinity where zero would be basically there is no regularization infinity you know up to infinity you have really strong progressively stronger uh regularization. So um it's very much a tunable knob you have for determining how much you want to prevent the model from fitting to your training data. And I'll go through some simple examples now of regularization. So here we have L2 regularization which uh basically what you do is you have your weight matrix you square each of the terms in your weight matrix and then you sum them all together. That gives you your your score here that you then multiply by lambda and you add to your total loss. That's L2 regularization. L1 regularization is very similar but instead of squaring you're taking the absolute value. So in practice there are some differences between uh how these two regularizations uh perform when you're training models. So one of the things that happens with L2 regularization because you're squaring each of the values when you have a really small value it gets squared like say you have you know 0.001 you square it becomes even smaller. Um so L2 regularization allows for sort of these really small values close to zero because you then square them so they become even smaller and so your your your penalty here is very very low if you have these very small values with L2 whereas L1 you're not squaring it. So it's sort of just whatever the the baseline value was. It's not like it's getting smaller before you're uh computing this regularization term. So in practice what this leads to is L1 regularization you get a lot more values that are zero actually in your weight matrix um or very close to zero whereas L2 you can have generally it's more spread out where you have values that are small but uh but but non zero because the penalty becomes so small seems pretty clear to you why L2 prefers sort of spread out weights that are all small but why does L1 prefer sparse uh vectors so I think the way to think of it is that if a value can be zero uh and your performance is roughly the same uh then this would push you towards zeroing that value whereas for L2 what you might have is the value just becomes very small but non zero because of the squaring so uh the question is can we talk about what does pushing towards a zero value mean so um we're going to talk about more how we use this loss term but the basic idea is we're trying to minimize it so um we're trying to minimize the loss or minimize the error of our model and um if we have a term here which is giving us positive values that don't it's sort of not affecting the model performance and the data loss we will uh sort of remove those through the optimization procedure. It's sort of a trade-off. You're trying to optimize the joint uh sum of the regularization term and the data loss term. So if your data loss isn't changing much but you're able to go lower on the regularization term, you'll get a more optimized model. So it will it will uh it will be preferred based on trying to minimize the overall term. Um so uh I think we'll also touch later in the course about much more complex forms of regularization where they're all doing this basic idea of worse on the training data to do better on the test data. Um but some of them you will even like change the layers of your model. So um they actually get pretty complicated. This is like an ongoing research area of how to regularize models. Uh there's new papers each year. So lots of stuff here and we'll only cover a small subset in this course. So to summarize why do we regularize models? Um the first is you know it allows us to express some sort of preference over weights. So if for some reason in our problem we think the solution should be spread out or should contain a lot of sparity where a lot of the values in the weight matrix are zero, we might prefer one set of regularization L2 versus L1 over another. Um it also can depending on how we're regularizing make the model simpler um so that it works better on test data. So it could simplify the model if we're say uh heavily regularizing really high polomial terms in our model. Uh for example in what I showed earlier or and something we won't touch on in too much detail is uh especially L2 regularization can actually improve the optimization process because um if you imagine like the squared is like a parabola. Uh so if you're plotting y equals x^2 it's a parabola and uh these are like convex so you get a lot of nice optimization properties where there's a global minimum. Uh we won't touch on that in this course that's like beyond the scope but know that for certain types of optimization the regularization actually helps train the model faster too. Okay. Um I guess I have a question for you all and what we'll do is you'll do uh um uh one if it's W1 and two with your hand if it's W2. So uh which of these two um weights w1 w2 would l the l2 regularizer prefer? So we have our input x. It's when you multiply it, you do the dotproduct with the weights, you get the same score. So you get a score of one either way. And here's where the data loss would be the same. And we're trying to determine which of the uh weights would our regularizer prefer. So go one if you think it's W1 and go two if you think it's W2. All right, lots of twos. Yeah, it's W2 because as you said, it's more spread out. You're going to be squaring each of these turns. So, it's 1/4. You square it, becomes 1/16th. You sum it all together, it's 1/4 is the total regularization term here. And then here, it's, you know, you square it, so it's one. So, it's four times lower in terms of the regularization loss. Um, and as we said, the intuition is you like more spread out weights. Um, and then here's another question. Which one would L1 prefer? Now, so you do one if it's uh weight one and two if it's weight two. Okay, we got a lot of ones. Uh so this one's actually a bit of a trick question. So um what L1 regularization is, you sum each of the terms so they'll both be sum to one. Uh in practice, you probably would see this one because as we said, sparity, but in terms of a loss standpoint, these two um weights would actually be equivalent uh in terms of L1 because one is just the sum of uh the 0.25 25 uh four times and then the other one's just one. So they're both summed to one and so the the actual um regularization term is the same for these. Yeah. Okay. So what's an example where L1 would be preferred if this is like 0.9 for example? Okay. So just to recap um we have a data set of XY pairs and we have some way to calculate scores for each of the classes uh with which in our case is just a linear model. You're doing a matrix multiply. Um the loss for each of the I training examples in the softmax loss which we discussed last time is you exponentiate each of your scores um and then you divide by the total sum of the scores. So you exponentiate to make them all positive and then you sum to get a probability distribution. So they all the final values in this all sum to one and you have a score for each class and you take the minus log of your of the correct label. So this is uh of the the probability of the correct label which is given here. Um and the full loss is you just run this over each of your training examples, calculate li for each of those and then you um add your regularization term here depending on what is the weights of your model. Why do we use softmax in general? So soft is great because um what it does as a function is it converts any set of floatingoint numbers into a probability distribution where uh they will sum to one and uh depending on the value of the score that will translate to the relative probability of that value. So if you have a really high positive number and everything else is very low negative you'll have merely one for softmax and zeros almost for the the other values. So it's nice because it converts any list of uh floatingoint numbers into a list of probabilities based on the uh the the values of the of the list. That's the main utility of softmax. So the question is that you can view the regularization we talked about which is L1 L2 as um a way of regularizing based on the magnitude of the weights which is true and how does that translate to simpler models? So I think in L1's explanation is actually pretty simple because uh if we prefer say um terms that have a lot of zeros in it, it's basically a linear model with fewer coefficients. So that one is actually I think relatively straightforward. But I think in general regularization is not always going to give you a simpler model. It depends on how it's used. So for example in the diagram we showed at the very beginning here um here you could imagine that um you have L2 regularization or L1 regularization where you're penalizing more the higher degree polinomial terms of your function. So in that sense it's pretty clear how you could design regularization to prefer a simpler model. But it doesn't always need to be that way. Really what it is is this idea of uh doing worse on the training data to do better on the test data. And uh that's not always going to give you a simpler model. And in fact, there are many types of uh uh um regularization like dropout that actually make your model more complex uh but give you better performance on the on the test data. Cool. Um so now that we've talked about how we can calculate how good a given W is based on the training data and uh this regularization term, the question now is like how do we actually find the best W? uh and this is what optimization is which is the second half of today's lecture. So I think when people describe uh optimization they will usually use this idea of a loss landscape which you can think of as like a normal landscape like on planet earth where the up and down vertical or z-axis direction is the loss. So this is the value you're trying to minimize and then say in this example you have two uh parameters in your model which is sort of the x and y direction of where you are in this landscape. And the idea is you're just like uh basically a person. You're walking around this landscape and you're trying to find what is the smallest or lowest point in the entire landscape. I think one of the reasons this analogy and this is a very commonly used analogy. Um a little bit falls apart is because you know as humans we can just see like visually we can look into the distance and see what is the lowest point of the valley. But I think this analogy is actually pretty accurate if you think of the person as being blindfolded. They don't have access to any visual information. they can only feel sort of the earth where they are right now and understand what is the slope of the ground on the current point in which they're standing. I think if you view it in mat lens this analogies actually becomes extremely accurate um for how uh we're trying to find the best model and we have this complex landscape of uh different loss values depending on the parameters of our model which translate to the location of the person in this uh landscape. So how can you find the best point? uh we could go with like a really simple idea which is maybe a really bad uh you know bad idea but it could work. So here it's just basically a for loop where we're trying a thousand different values of W randomly and we're just choosing the best one. So obviously not very mathematically rigorous but you know you will do better than uh a random baseline and if you had nothing else to go for maybe this isn't so bad. um you would get like 15.5% accuracy on the uh sciphar 10 data set which is the one I showed earlier with the frog and the car and things like that uh with the 10 different categories. Uh but it doesn't perform very good. I mean the state-of-the-art on this data set it's basically solved through modern uh deep learning. You get 99.7% accuracy. So uh clearly it's not bad but it's also I wouldn't say particularly good. Uh strategy number two, which is sort of what I maybe uh explained a bit earlier, is this idea of following the slope. So for this um you can imagine you're like blindfolded on the lost landscape and you're feeling the ground underneath you and you're thinking okay which you know which way is the slope of the earth pointing me and I should walk in that direction at all times. This basic idea is the fundamental way in which we train all the models in this course and in which basically all deep learning models are trained where you're feeling the location of the current place in the lost landscape and you're going down the hill. Um so this is very intuitive way to explain it. We'll now go over more of the math uh behind it but this is what you should be maybe visualizing in your head. So uh how do you actually follow the slope? Um so in one dimension I'm sure you all are familiar with the idea of a derivative which in calculus we can think of as uh this limit h definition where we add a very small number to uh our current location. We calculate the value of the function at that new location. We subtract the current location and then we divide by the step size. And as we take the limit uh for h approaching zero this gives us the uh derivative of the uh of the function at that point. Now uh this is for 1D but in multiple dimensions you use the gradient which is where you're calculating essentially this limit definition for each of the values uh uh separately. So you'll have a different derivative for each of your values and the and you get a vector instead. Um and this gives you the direction along each dimension. So um you can actually calculate the slope in the dimension by taking the dotproduct of the gradient with the direction and specifically the direction of the steepest descent or down the hill is the negative gradient. So the gradient points up the hill, the negative gradient points uh down the hill. So this is the you know the direction we should be traveling if we're trying to get to the bottom of this lost landscape. So you know maybe what are some ways you can calculate the uh derivative? A really simple one is you could just actually try to use the limit h definition with a very small h. Uh so you add you know 0.00001 you uh actually can compute you know the last two digits of the loss change slightly. So you can compute the difference divide by the step size and you can get like an approximation of your derivative here. And you could actually do this for each of your uh values in your w. You just do this procedure over and over again. Um but it has a few problems. is very slow because you just need to loop through each of the values. It's also approximate. So you're not even calculating the actual derivative and especially with floating point arithmetic you can get pretty significant errors here. So uh this is not really preferred but this basic idea or intuition of what we could be doing is to calculate the derivative this way. But really we have the loss as a function of w. So um we know how to calculate the scores to calculate uh to get our loss which is given by our function for our model and we can then compute the total loss with the regularization terms as well. And this entire uh loss is a function of uh basically w's the w's the x i's and the y i's. So you have your w matrix, you have your x i's and y i's and then you have this formula with you know maybe some logs and exponents but fundamentally um this is a function of w x and y and we specifically want to calculate the gradient which is given by this Greek letter naba of our loss with respect to the weights. So we can imagine our x and i uh x i and y i's are held constant and we're trying to calculate the uh the derivative just with respect to the weights. So to do this we can just uh use calculus use the chain rule use the different methods we've learned for calculating derivatives based on uh sort of complex equations uh or not so complex but you know you need to have some logs and exponents and chain rules here to solve it. Um so this will be an exercise in the homework so I won't go through step by step how to do it now but it is relatively straightforward I think conceptually it should make sense to you all how you do this. You assume the x and the y's are constant and you solve for what is the derivative as you change w. So now we actually have a way where we can calculate W based on our uh or sorry DW the gradient of W with respect to our data and the current W and whatever our loss function is which is how to compute the error. So this is I guess summary. So um you could do a numerical gradient but it's approximate slow and uh the nice thing is that it's very easy to write. you just add a really small h, take the difference, divide by h. Um, the analytic gradient is nice because it's exact, it's fast, but you could like potentially if you're creating a new gradient from scratch, like new code to calculate from scratch, you could have an error in it. So, if you are doing this, people normally will have a gradient check, which is where they try the h uh version where they have a really small h value and then they uh make sure that it's around the same neighborhood and that's a good way to make sure you don't have any bugs in your code. Um so um you'll be doing in there'll be gradient checks in your homework assignments to make sure your implementations are correct also. Yeah. So the question is um we often say we want a loss function that's differentiable uh because then we can calculate the gradients but if we have a better loss function somehow and uh we can't analytically calculate the gradient but we could use uh this h kind of numerical method. Could we do that? I think um in general it's hard to construct a better loss function that would be um non uh like non non-ifferiable. You could possibly though and if you there is just a true loss function that is the best for your case but it is non-ifferiable you could go with uh this approach and it it may work. I think it would struggle if like for example your loss uh is just truly non-ifferiable across all points and it's basically like a cluster of non-connected uh points then you know moving in the step of deepest steepest descent wouldn't really get you necessarily at your best solution uh if they're not well connected and forming sort of this geography so uh it could work but I would think that if your loss is non- differentiable across most of the domain then probably you wouldn't be able to use these approaches to find the uh the bottom point. Yeah. So the I guess TLDDR of the explanation is that if your function's convex then uh it works very well with this sort of gradient uh descent or steepest descent type of approach. But if you have a non- differentiable non-convex function probably this approach won't work uh as well because you're not going to be stepping in the right direction. It's not necessarily errorprone if your code is perfectly good but maybe you have a mistake in your code and it's hard to tell right away. uh and but the h uh the limit h definition is very easy to code up right you just set h to be a very small value you run it through your function and you add a very small amount so that's less errorrone for implementation okay for implementation okay not more errorrone if it's working correctly okay so um now I'll talk about this fundamental algorithm for optimization called gradient descent and the basic intuition is what we already explained before we calculate the slope at each point when we're on our loss landscape and we take a step in the direction downwards towards the bottom of the loss landscape. So what we do is we calculate the um gradients of our weights given the loss function, the data and our current weight values. This tells us how much we should change each of the weights to go down the slope. And then we have to have a step size. So how far down the hill are we taking a step in the direction. Um and so you go down the hill, so it's the minus sign here and the step size times uh the gradient. So this is basically what gradient descent is. Uh you're calculating the gradient at each step and you're moving a fixed direction uh in the direction of the negative gradient down the hill. Um so given a concrete example here. So instead of this being like a 3D loss landscape, often people will visualize it like this where we're sort of looking down at the landscape and uh purple would represent the highest points and red would represent the bottom or the valley here. Um and we could imagine we have our original W. We can calculate the loss. We know the direction of the slope of the negative gradient direction. And this arrow might represent the uh fixed step size that we talked about before. We're taking a fixed uh step size in that direction. Yes. So you can see it's fixed step size, but as the uh gradient becomes smaller, we're still multiplying it by this fixed step size. So the step the effective step size actually does become smaller because the gradient is smaller near the end where it's flat or near the end where it's more flat. So this is what it looks like when we're always heading in the direction of the steepest descent. So the question is when we step down, how do we know when we're going to stop? Well, I guess in uh in this formula, right, like you just keep looping forever, so you never stop. Uh so this was probably not the best. Normally you have a predetermined number of iterations that you run it for. Or you can look at um if the loss is not significantly changing by a fixed amount. Also you could have like a tolerance for how much you're expecting the loss to keep decreasing by and if it's no longer decreasing. You know it's only decreasing by one e minus 5 or 1 e - 9 you know maybe you maybe you stop there because it's good enough. Um so those are the two ways you can determine when to stop is fixed number of iterations or a a stopping criteria of you know how much we're not really improving that much anymore. Okay. Um so now I'll talk about the sort of most popular variant of gradient descent which is called stochastic gradient descent. And when we talked about gradient descent before we talked about calculating the loss of our weights by summing over our entire training set the loss of li for each i in our entire n training sets. Um but this is like potentially a lot of computation if we have a very large data set. So what uh stochastic gradient descent is is it basically now instead of looking at the entire data set we're looking at a subset each time which we call a mini batch or a batch of data. And um so here if we look at the code it's like you know we're sampling 256 data points from our data set. So the batch size is 256. We evaluate the gradients of this 256 subset of our data set and then we do the same thing as before. So the reason why it's called stochcastic gradient descent is because we're sampling a random subset of our data set each time we're running the algorithm uh each step of the algorithm. So um this is stochastic gradient descent. You're basically on a running it on a random subset each time. And in practice people won't just sample it completely random. They'll make sure to um get through all the examples in their data set and then sort of loop around again. And that's called one epoch of training where you loop through all your data samples once in a random order. Okay. Um there are some problems with uh gradient descent or stocastic gradient descent. So um this visualization is sort of the same type as the colored one I showed before where we're looking down the loss landscape. But these curves are called the level set where it's a set of points where the loss is the same on all of them. So this is another way of visualizing sort of very popular way to visualize top down looking at the loss but it's without the colors. Um and so you could imagine that you have uh this phenomenon where it's like a really narrow valley where it's really steep along the sides and you're trying to traverse the center of the valley and um you know gradient descent actually does run into issues here. Does anyone um have any ideas for what could go wrong? Yeah. So, one of the things you could do is overshoot. Um, where you're sort of moving up and down along this direction. Um, and if the if it's steep enough and your step size is large enough, you you might actually oscillate out of the valley. So, uh you can imagine if your step size is very large and this is really steep, you're actually going to be gaining like you're moving out and out each time because you're you always have this fixed step size. So, um if it's steep enough, you could just like bounce out of the valley. Uh that that that actually does happen if your learning rate is too large. So that's one thing that can happen. And then also um even if your learning rate's not too large or your step size is not too large um you can have this phenomenon where you're sort of jittering because the gradient is much larger in the steep direction. So uh you're sort of jittering but you're not making very much meaningful progress towards the actual center because you're spending all this time oscillating back and forth up and down. So this is a pretty big issue with just default SGD. Um and then mathematically just an aside um the loss function we consider here to have a high condition number which is the ratio of the largest to smallest singular value of the hessen matrix which is the second derivative. So you can imagine like the second derivative along this up and down direction is very high but then side to side it's very low because it's very flat. So that's what causes this phenomenon. All right. Um so one of the things we also might have an issue with SGD is um what happens if the loss function has a local minima or local minimum or a saddle point. So um for example here it's it for like just the very end of this curve it's completely flat. So if we were to imagine um there's like we're moving down the hill here um we would just get stuck here because it's flat and we wouldn't be able to progress any further because when we take the gradient here is zero. Um so this is actually a pretty big uh issue where it'll get stuck either in a local minimum because we you know once we reach here we don't really have any direction to go the gradient is zero or it's very small and we'll just sort of oscillate back and forth here. And then here it could actually get stuck on this uh bottom example because uh the gradient zero here even though if it went a little bit further it could go down significantly more. Yeah. So the question is um you know maybe we can change the way we're doing the steps. Maybe we could use the hessen to determine the direction we go. Um we actually do have a brief slide talking about the sort of hessen style approach at the very end. That's not very commonly used in deep learning. But the short answer is yes. There are going to be actually several ways in which you can account for this that we're going to go into in like 5 minutes. So it's a good question. Yeah, we'll get to that. Okay. Um so um I think one of the other things that you might not know is that empirically saddle points are actually much more common as you move to higher dimensional models. So as your weight uh matrix gets larger and larger um you're more likely to find these saddle points. And there's this paper describing the frequency of them. Uh if you don't know a saddle point uh it's called a saddle point because it's shaped like a like a saddle like on a horse. And at the center of this saddle uh the uh gradient is actually zero in all directions. So it's like the bottom of this and at the top of this sort of uh curvature and so in both the x and the y directions the gradient zero. So you could get stuck here despite being very close to like going significantly down the lost landscape on either side. So this is also a pretty common issue with SGD where these saddle points and as we move to higher dimensional spaces or this is equivalent to models with more parameters uh this is more and more common. This is a big issue. Um and then a final issue with SGD is that um we are sampling a subset of our data each time. Right? So we're not looking at the whole this represents the entire loss across all the data but we're looking at just a subset each time. So we'll actually have somewhat noisy update steps because we're not looking at the entire data uh set. So we'll sort of be stepping towards the center uh towards this sort of uh local minimum that we're trying to reach here. But uh each step doesn't go directly in that direction. So there's some noise in how we're progressing because we're subsampling the data set. Okay, cool. Um I think uh to summarize these are the main issues and there's a pretty neat trick you can do uh where you just basically add momentum and you can really think of this as the same way as like if you have a ball that's rolling down the hill where it gains momentum. It's actually very similar to how it's modeled in terms of the physical properties. So u it's a good way to gain intuition about it at the very least. So um you can imagine that um it could help with the you have these local minimum because if you're rolling down with enough velocity you'll be able to come out of it. Um if you have the saddle points or the like the just uh flat point here, the model has been rolling down the entire hill. So it won't get stuck here anymore. It will continue. Um also um if you have this poor conditioning value, you will still have maybe some uh oscillation, but the nice thing is that um it will sort of accumulate speed in this direction to the right because it will have multiple steps that keep going that way. So it'll gain faster and faster uh towards the center here. So it also helps with this problem. Finally, it can also help sort of average out some of the noise with the gradients because they all sort of have a direction in common uh which is towards this uh minimum here. And so um as you're computing the momentum it sort of builds on itself and it will converge faster because um it sort of the noise uh is uh accounted for by looking at the direction they all share in common which is uh uh which is included in the momentum. So let me show you how to actually do it. But this is sort of the general intuition for how momentum works. So we have SGD here. We have our uh mini batch x. We're computing the gradient which is dx. We have the learning rate or the step size. If you multiply and then we do the negative because we need to go down the hill. This gives us our new x. So this is sgd for with momentum. We're now updating by this velocity term. So instead of updating by the gradient at the specific point, we're updating by the velocity. And the velocity at a given time step is given by the previous velocity uh plus the current uh slope. So this is sort of how you calculate it. And you have this row value which is the momentum the actual like how much momentum you want to have. And if you have it very high then your new velocity is more dependent on the previous uh time steps velocity. And this sort of is a running average uh therefore of the uh last gradients uh depend and the momentum term here gives you how much to weight the past versus the present. So now we're updating by this and we still have this alpha which is the step size. So um it's actually a very simple change, right? You just are now computing the velocity which is a function of the current velocity plus our gradient. So I think I'll pause for questions here. Um this is the explanation of momentum and um maybe I could also recap briefly how it resolves all these issues we saw. So um you know if now that you're adding momentum in the past uh over the past gradient steps you could see how it would keep continuing along this direction and depending on your row if your momentum is very high it would keep going and be able to account for a very large sort of hump here with the local minimum. Also it's very good at sort of these saddle points because it will just continue along the direction in which it was going previously for a significant amount of time. and uh poor conditioning. You know, if we're having cumulatively going to the right upon each step, the momentum will also um be consistent there and build up. And then if uh we're oscillating significantly here, it will move less um it will move less in the direction because it's sort of the values will cancel out um in terms of the current direction and the velocity. Um they'll be pointing the opposite direction so it will get minimized. The question is what happens if you're rolling like right along the saddle? I mean I think in practice it's very unlikely but in that case yeah you would be you would just get stuck uh in the saddle. Yeah I think that's like you know your initial conditions like wherever you start is very unfortunate. So uh yeah sometimes I guess that could happen but it's very unlikely. Yeah. And it's also why in practice people won't run like a single model um training run. Often they'll run multiple ones with different random seeds just in case something like that could happen. Another thing is if you're doing stochastic uh gradient descent, you're much more likely to have at least a little bit of noise to get you out of like directly in that saddle uh back and forth. So I think it's basically it never would happen because of the randomness, but I hypothetically I think it could that could occur. Yeah. So the question is why is the saddle just an issue with SGD and not optimization in general? Um it would also be an issue with the entire data set. It might even be more common with the entire data set. So uh it's an issue that SGD faces but it's also an issue that other optimization algorithms that just rely on gradient descent with no uh sort of bells and whistles attached would also um they would face the same thing. Yeah. Yeah. So the question is does uh adding the momentum make it more difficult to converge because we'll overshoot and then you know have to come back. Uh I think the short answer is yeah it might not help with converging but it will help you find uh on average it will help you find a better uh minimum point to converge to. So it will converge maybe more slowly because you won't get stuck on a in a local minimum uh like you would just converge here if there was no momentum right versus overshooting. So I think a lot of this stuff is empirically uh shown where it's like it happens to be with the specific class of neural networks the momentum does help uh training but um this is the intuition for why we prefer it. Uh I think to be honest um people will use whatever works best and there are cases where people have found that stocastic gradient descent without momentum would outperform for a particular model. So, uh, here's the intuition about why it could perform better, but in practice, people will just try a bunch of different ones and see what works best. And I'm going over the most common ones that people try now. Yeah. But yeah, you're right. It could hurt convergence potentially. Okay. Um, all right. I'll continue then. So, um, uh, yeah, I think we went through this. Um, and, uh, one other thing I wanted to point out is that there are different ways you can formulate this. So these equations are identical but you'll sometimes depending on the implementation see it written in different ways but they you know they're doing the same thing. Uh maybe in interest of time I'll skip over why they're identical but you could go over in the slide and prove to yourself that these are essentially the same formulations. Okay. Um I think the next thing I'll talk about is a different optimizer. So we talked about momentum and now we'll talk about something called RMS prop. So um RMS prop is a bit you know bit of an older method now 2012 but uh came out by Jeffrey Hinton's group and the basic idea is to instead of just having this running um velocity which the momentum captures it's to actually add u elementwise scaling of the gradient. So um when we're when we're how do we do this is we have this gradient squared term and the decay rate here is very much like the momentum that we the momentum term we explained before but now it's on the squared gradient. So uh we have this sort of running average where we take the previous term here the gradient squared and then we do 1 minus times and then here it is the literally the gradient squared and so this is a running average of our squared gradients. So uh you know bigger values will get much bigger, smaller values will get much smaller and if there are consistently large gradients in certain values those will get very large as we continue our uh running average here and we're actually going to divide here in the update step we divide by the square root of it. So the basic idea here is we're actually now stepping someone asked earlier I think there was a question what if we change the direction in which we're stepping. uh this is exactly the type of thing you can do and this is what this is doing where we're dividing by this squared gradient term. So for values in which uh we have very large squared gradients for the values of w in which the derivative is very large um we'll we'll uh divide by a larger value. So we'll step not as far in that direction. In the more flat regions we'll step farther because we're dividing by a smaller term here. So this is the basic intuition behind it and it very much addresses one of the questions someone had earlier about can we change the way we're stepping in the direction and that's exactly what this is doing here. So you still have a learning rate but you're dividing it by this uh square root of the cumulative uh squared gradients which gives you larger steps in the flatter areas of your loss landscape and shorter steps in the very steep areas. Can anyone explain? I sort of just gave a brief summary, but what happens in this specific line here of the code? Why does uh what happens with our gradient step direction? How does it change? We're dividing by this value which is dependent on the current gradient and also the past gradients. So the values one of these values are very large. So these are you know you know these are vector operations. So we have a set of um derivatives here and we're dividing elementwise by another set of uh squared gradient values. So when it's very large well the denominator is very large then the step becomes effectively less in that direction because we're dividing by a large value. And when it's a very small value the step becomes much larger because uh the gradient squared term is small. So it's in the denominator and we're uh increasing the effective step size. Oh yeah. So it's uh specifically for this type of example here where you have maybe a very narrow valley where you want to be moving more uh in the flatter direction. Yeah. The question is what does a small gradient mean in this context and how does this help us move uh along the steep directions less and along the flat directions more? Yeah. Yeah. So I think actually this is maybe a great visual because it compares the three different approaches here. So we have uh with momentum which you can see sort of it overshoots as there was a question about earlier but then it kind of comes back. Um you have SGD which is slower because it's not it's just sort of always moving in the fixed direction and then you have RMS prop which we just mentioned. So uh the way that RMS prop works is because the gradient and the direction that I'm moving my mouse here is uh is higher uh the gradient square term is larger. So we move less in that sorry we move less in that direction. So it's you can see it actually quickly starts turning here towards the center where it's a flatter landscape at this point but it's traversing more in that direction. So we're actually sort of changing the direction we're going based on uh going less in the steep direction and more in the flat direction. So these are the uh sort of three and then there's one more we'll discuss which is by far the most popular optimizer used uh in modern deep learning. Um so it's sort of just a combination of the SGD momentum and RMS prop. So here is almost what the atom optimizer is which is the most popular optimizer in deep learning and you also have all the prerequisite knowledge now to understand it. Um so you look at it and this first term here in the red is basically the momentum we described before where we have uh the current uh so the beta 1 is like the momentum term and then we have the velocity here and we're taking a running average. Um the second moment here is like the gradient squared term for uh RMS prop and we're doing the same thing here where we're multiplying the learning rate instead of by the step size by the velocity but now we're still doing the thing where we take the square root. Um and it's the second moment here. And the reason they use first moment second moment is like a relation to physics and mechanics. Um, but it's basically just a combination of the two things we explained earlier where you're accelerating movement along the flat directions, dampening it among the steep ones, and then you're also adding this notion of momentum and velocity. So, you gradually build up speed if you're continuously moving in the same direction. Um, so, uh, as it's written right now, this will actually run into issues at the very first time step. And it might be a little bit uh unclear to you why, but I'll actually wait for someone to have a guess. So, one thing to note is that these betas beta 1 beta 2 are usually initialized very close to one. So, like 0.9 0.999 uh and that these two values are also initialized to zero. So, uh during your first time step, if you just use this formulation of atom, you would run into potentially unwanted behavior. So um one of the other things is it has to do with the second moment calculation. So this is the main issue here. When you calculate the second moment and then use it on the next line, you sort of run into an issue. Yeah. So the denominator is zero basically. Yeah, that's the exact issue. So it starts at zero. So this term is zero. Um you have a very large beta. Uh so this value is very small. And if your gradient is not very large in your first step, you can have this whole term basically be very close to zero. Now we're dividing by something very close to zero and it just creates a very large initial step even though our gradient was small. So that's probably not something we really want. And so the final thing that Adam has is it adds these bias terms here which is specifically to account for this issue where it's dependent now on the time step of training. So uh you know I think this is also something you'll go into in the homework. I just want to give the u basically intuition behind atom why the naive implementation wouldn't work which is this really large initial step and you'll go over in the homework implementing this and you'll see how the time step is used but the basic idea is this is to account for that very large initial step and uh as your time step increases these uh bias terms are not needed as much okay cool um these are some like good defaults that people normally use um if you're training a model with atom you could go with these and you know maybe it'll work maybe it won't but it's a good starting point and you can then tell from the remaining slides we'll talk about how do you know if your learning rate's right how do you know if uh these other values are right so I'll I'll speed up a little bit um just to in the interest of time but the basic idea is that you can see these all all these different optimizers converging um they all have different properties you can sort of see how atom is this combination of RMS prop and SGD with momentum where it has characteristics of both which is very neat to see visually it aligns with our intuition. Um one final topic related to atom is that um we could look at how regularization interacts with the optimizer. So um for example if we have L2 regularization how does this affect how the optimizer uh works and I think the answer is it's actually not immediately obvious and you can do it in different ways. So uh in default atom they compute L2 when they're computing their gradient. So you know we looked at the gradient and there was the loss portion of our so the data loss portion and then the regularization loss for atom it's using both of those when it computes the gradient um but atom w basically does only looks at the data loss for doing all of these moment calculations and all these steps and it just adds the regularization term at the end here. Um so basically all I'm trying to describe to you all is there is flexibility for how you incorporate regularization into your optimizers. Um weight decay is generally when you just add it at the end L2 regularization at the end and you don't includes it include it in the uh actual optimizer for how you're calculating the velocities and momentum etc. Um so this is the main difference and sometimes uh under a lot of settings atom w works slightly better like I think the llama series from meta they all use atom w uh assuming I assume because it does slightly better for them. So we have one function optimize why are you splitting it into two? Yeah. So if you mix it into one function, that's what atom does. And atom w is specifically the separating it into two. So why you might want to do that is because if you don't want your velocities, your momentums to actually be a function of the weights, you want it to be a function of the loss. So if you're trying to traverse your loss landscape sort of more independent of your actual weight values, that's why you might want to separate it. But you still might want a regularization term, but you don't want it to interfere with the moment calculation. So this is the specific reason why they do it. Ultimately it's empirical. You try both and you see which one works better. But this is why you would do it that way. Okay cool. Um so we'll talk about learning rates. So um there are different uh ways in which uh learning rates can be chosen and sometimes you'll get a very high learning rate where what will happen is basically your loss will uh get very large as you sort of oscillate out of the loss landscape as we described earlier. Um if you have a very low learning rate your issue is you just converge very slowly. If you have a high learning rate, but you're not oscillating out, but you might not be able to converge because you're sort of bumping around the uh local minimum, but you're not actually able to get uh any any lower in because your learning rate's too high. And ideally, a good learning rate would have this property where it decreere it causes your loss to decrease quickly over time, but then you see continued improvements as you continue to train the model. Um in reality actually depending on the situation a lot of these could be good learning rates and also depending on the step in training which is the final uh thing we'll discuss in lecture today. So you can actually change your learning rate as you train your model. You don't need to always have a fixed learning rate or step size and pretty much all modern uh deep learning like all the best models coming out have different ways they vary the learning rate during training. So one really simple way you could do it is after a fixed number of iterations you just uh take onetenth of the learning rate and you continue training. So this can resolve the issue of where uh your learning rate is too high for you to be able to converge any further. So then you reduce it and you're able to get lower into the loss landscape. Uh and this is really commonly used when training ResNets. So that's a very popular type of convolutional neural network which we'll discuss later in the course. Um another thing you could do is sort of cosine learning rate decay. So this is one is also extremely popular. So um here you have uh basically this is like half of a cosine wave where you're uh starting at your maximum learning rate here and then you go down to zero to the end and it follows this sort of half cosine uh shape. And here's the formula for calculating it. I won't go into too many any details but the basic idea is there's a ton of different ways to do it. when your loss uses a cosign uh learning rateuler, you'll often see a shape like this where um it sort of you get pretty good continued gains in the middle part of training. But the basic idea is that the actual shape of your loss during training will highly depend on whatuler you use. So this is the basic idea I'm trying to convey. It looks very different for example than this one where you can literally see where we're uh taking onetenth of the learning rate during training. Um another thing you do is just a linear learning rate decay. So um it just follows a straight line. uh you could do inverse square root etc etc. There's basically an unlimited number of ways you could uh mess with your learning rate during training and depending on the type of model you're training and depending on what works best you you just choose the one that works best but here are some ones you could try that uh could perform well in your setting. Uh also a really really popular strategy is to have a linear warm-up. So instead of just starting at your maximum learning rate, you spend a fixed number of iterations to sort of linearly warm up to whatever your maximum value is and then you go about doing whateveruler you had afterwards. So for example, linear warm-up and then this would be like the inverse square root or linear warm-up and then um like cosine is a very popular setup for training models. Uh one final thing is that there is this empirical rule of thumb uh or called the linear uh scaling um linear scaling hypothesis or linear scaling law or something like that. I forget the name. I think it's linear scaling law where it shows the uh that if you increase your batch size or the number of training examples per update by n uh you could you should also scale your learning rate by n. So as you increase your batch size, you should increase your learning rate uh directly proportionally. So uh I think the math behind this is a bit involved and also it's more of an empirical rule of thumb. So people have like tried to show uh mathematical proofs for why this could be useful but and based on you know the variation of gradients and your batch and the number of uh gradients you calculate per batch etc. But really this is just shown empirically to be true for a large number of problems. So uh this is a good rule of thumb. If you have a winning recipe but you want to increase the batch size then also increase your learning rate by the same amount. Cool. Um and then the final thing I'll talk I'll touch upon very briefly is this idea of uh second order optimization which is uses the hessen that someone asked a question about earlier too. So we won't talk about this very much in depth but just to let you know this exists. It's not something we cover in the course very much, but um the basic idea is right now we're using the gradient to form a linear approximation of basically where is the downward direction where we're trying to traverse this lost landscape. Um and we just sort of look at the direction and we take a general step in that direction. And we added fancy things like momentum and uh you know the RMS prop where we're de accelerating along the steep directions. But this is the basic idea. We're using this uh gradient uh at each time step. The idea of the hessen is instead of using the gradient you basically try to fit a uh pol a quadratic or um like a two second degree polinomial to your uh function based on the derivatives at that point or the hessions at that point and uh you you then try to find the minimum this way and in certain uh optimization problems this actually works extremely well but generally um we don't use it in deep learning because it requires two things. So one you have to do this like tailaylor series expansion whereas right now we're just sort of doing the first part where we're taking the derivative but you would need to be able to calculate the second mixed derivatives which is already uh maybe difficult and then on top of that this mixed uh derivative of all of your uh parameters in your model by all the other parameters in your model can get very large as you have like these many million or billion parameter neural networks. So in practice uh we don't use it because um these values have the the matrices become way too large and so you run out of memory on your computer if you try to run it specifically your GPU memory. But if you're training a smaller model or if you're okay with spending much more time uh to get better steps towards your uh minimum then maybe you want to look into this. Depends on the problem set but for smaller models this actually works quite well. But for these large neural networks we're training, we basically never do this due to the memory restrictions and all the time you spent uh computationally trying to calculate the hashing etc. You would rather just see more data during training. All right. So um some I guess uh concluding uh thoughts for you all that can be useful. So um Adam or Adamw is a really good default choice for training your first model. If you're working on a new problem in a domain, I would recommend it. Um, and it could even work okay even if you do constant learning rate. So, usually people will try Adam W's constant learning rate or with like a linear warm-up and then a cosine decay. Those are like a really uh popular combination. Uh, also um I think SGD and momentum can sometimes outperform atom. But the tricky thing is because you uh you you generally have to like tune the values more. So you have to try many more learning rates because uh you don't have this like RMS prop term to account for the steep directions and also you might have to try different scheduling values whereas in practice atoms sort of like best by test like people have tried in a bunch of different domains and it works very well. It's very uh adaptive to the loss landscape. If you're doing like a full batch update where you're already at each step, you can fit basically your entire training set into your batch size, uh you might want to look beyond first order optimization into second order and beyond because it seems like your data set's not very large or maybe your model is not very large and you could potentially benefit from having these uh non nonlinear sort of update steps and uh computing more sophisticated strategies for going down the uh trying to find the the minimum here. Um yeah so I think uh we're essentially done with the lecture. I'll give some slides about uh looking forward. So how do we optimize more complex functions than linear models which is what we covered in this lecture and uh next lecture specifically we'll be looking at uh neural networks which uh you know is very exciting topic. um a two-layer neural network the one we'll discuss in class is basically you have two of these weight matrices now one for each layer and you have something called a nonlinearity sort of stuck between so um in this case the most common sorry not the most common but the most simple one is just this uh ru function which you'll learn about more but basic idea is now we have two weight matrices and we have this additional function that's done in between the weight matrix calculations this is nice because as I said it's nonlinear So, you know, if we're trying to build a linear classifier to classify data like this, you'll run into an issue where the blue points and the red points are not linearly separable. But maybe there's some transformations we can do or through many layers of a model, we can eventually transform the data into a way in which it is separable by a line, which would be our then sort of final layer of the model.