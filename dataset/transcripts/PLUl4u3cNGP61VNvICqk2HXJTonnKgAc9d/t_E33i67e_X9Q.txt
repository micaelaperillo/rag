All right, welcome back to graphs. This will be our last lecture that's fully about graphs. Um, and we're going to move away from what we've been talking about for the last three lectures, simple undirected graphs, and move to the other kind of graph we talk about in this class, which is directed graphs, which we sometimes shorten to diagraph. So, from now on, we're going to have to be explicit. Every time we talk about a graph, we should say directed or undirected. Simple is implicit when we say undirected. But uh we should distinguish. Uh so the motivation is uh what if your connections only go in one direction, right? Maybe uh you have one-way roads like all of Boston or you have uh one-way follow relationships instead of friendships uh or you have network connections you can only send in one direction. those do exist though they're less common and so directed graphs are going to model that for us. Uh so things are going to look very similar to the old definition uh just the edges are different. So you may recall we had this cross productduct notation. I'm going to remind you so we can compare. uh this is a set of ordered pairs of vertices in this case. Uh so very similar uh property here edges were unordered sets of two vertices here they are ordered pairs of two vertices. Uh the other difference is here we forbade having a loop. Uh here we allow it. So this was not allowed over here but allowed over here. Um still uh we over here we forbade uh two edges connecting the same two vertices. Uh here we allow uh two edges going in opposite directions. Uh so this was a no. uh we still forbid in this picture having two edges going in the same direction between the same two vertices. Okay, that's a consequence because uh E is a set. You're only allowed to have one of each. So that's the definition. Very simple. Uh here's a little example on six vertices. It's a floating vertex out here f. Uh so you see uh you can have sometimes you can have an edge that goes in both directions. Sometimes you have an edge that only goes in one direction. Uh maybe sometimes you add a loop. I think I wanted one here. And I should have written it over here that just looks like da d. And there you go. Okay. Uh now the first part of today is just going to be porting over all the definitions from last class into this model. Most of the things we talked about behave similarly. Some of them behave different in interesting ways. So bear with me while we define a few things. Um the very first thing is what about degrees? So that's uh way back in the beginning of graphs we talked about the degree of a vertex was the number of incident edges. But now a vertex can have two types of incident edges. It can have some incoming edges and it can have some outgoing edges. So in general of a vertex V, it's going to have some incoming guys and some outgoing. We call this the in degree and we call this the outderee. We don't define degree because it's confusing and ambiguous. So in degree for example of a vertex is uh the sorry not the sum the cardality of the set of uh edges uv that are in e where u is some vertex. So this is just counting the number of edges going into V and the outderee is symmetric. This would be like V comma U. Before we only had one type of edge. So uh there was only one notion. Sometimes these are called degree plus and degree minus or degree sub in sub out. I can never remember which is plus and which is minus. So I always write in degree and outderee. Uh you can choose your own adventure. Um a related thing to degree is the handshaking lema. Who remembers what the handshaking lema was for undirected graphs? That was ages ago before spring break. Yeah. Right. The sum of the degrees of the vertices uh is uh twice the number of edges. I think you said over two but close. Um so in this world this was in some sense because every edge uh was getting counted twice right you count if you sum the degrees of of this vertex and of this vertex you count this edge once from this side and once from this side. Uh now we have distinguished in some sense each end of each edge. There's the in edge and the out edge or I guess call this the in edge and this is the it comes out of B and goes into D. So if we write the sum of say in degrees. So this was just remembering undirected in the directed case we have uh sum over all vertices of inderee equals exactly the number of edges and also equals the sum of the outdrees uh because now our edges are directed. I have an arrow. And so if we sum the in degrees, we will only count this edge from this side when we're looking at this vertex. If we sum the outderees, we'll only count count this edge once from this vertex. So these two sums are the same and they both give you the number of edges. So in some sense, a little cleaner than what we had from undirected graphs because we split each edge and uh or we've distinguished the two sides of the edge, I should say. All right, let's catch up to last lecture which is was all about locks. So this definition is actually it's pretty much exactly the same. Just repeat it so we remember. We had a sequence of vertices where each uh vi to vi + one is an edge. But before we had curly braces here saying the order didn't matter because edges were undirected. Now we have parenthesis saying this is an ordered pair. You have to go forward along each edge. So that's a crucial distinction. must uh go forward along the edge. In other words, you have to respect the arrows. So, for example, this graph uh we could start at A and go to C and then go to E. That would be an example of not just a walk but a path which has no repeated vertices or edges. Or we could go from A to B to A to B to A to B to D to D to D to D to D to E. That would be an example of a walk exciting. I should say these structures should look familiar. If you can remember even farther back to lecture four which was about state machines, these are exactly state machines. Like the only difference is in directed graphs we usually think about finite graphs and with state machines we're often thinking about infinite machines but uh this is the same setup. This definition mirrors exactly um state machines and walks. The only difference another difference is state machine had an initial state. There's no notion of initial state here. But a walk is just like an execution in a state machine. So unifying all that terminology. We didn't have a notion of path or anything. We just had walks. Okay. Uh as before, a walk like this is from v 0 to vk and k is the length. Length always counts the number of edges not vertices. Uh then we had the closed notion. there was a closed walk uh and in particular a cycle. Closed meant the v 0 equals vk and cycle was essentially the closed version of a path. So this was closed walk of length greater than zero with no other repeated vertices or edges other than the first equaling the last. Okay. The difference is um in an undirected graph, the shortest cycle had length three. We could make a cycle like this, a triangle. Uh we weren't allowed to make a cycle with a single edge because then you'd necessarily repeat that edge. Where's my red? In an undirected graph, this was forbidden. But in a directed graph it's possible. Uh if we have the edge uv and we have the edge vu then there is a two cycle like that goes u vu um there's even a one cycle right because we can have loops. So here for example D to D is a length one cycle in this notation. Okay so a little different than what we had before a little more general but turns out this notion of cycle is still interesting for directed graphs. All right let's move on to connectivity. Before we had a nice notion of U and V being connected to each other and it was symmetric. U is connected to V if and only if V is connected to U. Now it's no longer symmetric and we're going to use a different term for it to make that clear. So we'll say v is reachable from u or u can reach v if uh there exists a walk from u to v. Okay. So this looks like the same definition of uv connected in the undirected case. But now because walks can't be reversed because if you reverse a walk uh if I go from VK back to V0 then I'm using all the edges in the opposite direction and that's not in general allowed unless you happen to have a graph where every edge is doubled. You won't be able to reverse your walk. Uh so this is no longer equivalent to there existing a walk from V to U. But it is still equivalent to there existing a path from V to U from U to V sorry that part is still true. The proof is actually exactly the same. You take a shortest walk from U to V and uh if it's not a path then it has some loop in it some cycle essentially and then you omit that cycle and you get a shorter walk contradiction. Okay. So same exactly the same proof as before still respects the edge directions which is nice. So uh for the purpose of reachability now uh walks and paths are the same. So we can assume whichever one is more convenient. Um this relation uh is still this you know reachability property is still reflexive and transitive. Remind you what those mean. uh reflexive means V can reach V for any vertex V you can always get to yourself you just don't do anything u and it's still transitive meaning if you can get from A to B and u B can reach if A can reach B and B can reach C then A can reach C you can still concatenate paths together and that's fine okay but uh no longer symmetric already said this a couple times but if you If a can a can reach b, we don't necessarily have b can reach a. So that makes things more interesting. In particular, what does it mean for the whole graph to be connected? Uh for that it's useful to talk about um there's a very strong notion of connectivity. First uh say two vertices u and v are strongly connected in a directed graph. if uh they're mutually reachable. In other words, uh U can reach V and V can reach U. Okay. Uh this behaves symmetrically. If U and V are strongly connected, then V and U are also strongly connected sort of by definition. It's like U can reach V and V can reach U. uh and in particular we call the entire graph strongly connected if everything can reach everything. If uh for all u all vertices u and v um u can reach v or in other words uh for all u and v uh u and v are strongly connected. So again the pairwise relation these graphs are nice because you can start anywhere and get anywhere else. you never get stuck. But in general in a directed graph, you could walk and not be able to return. So for example, in this graph, uh maybe you walk around like you do take one of the walks that I drew, but once you end up in E, you're stuck. Or if you start in E, you can't get anywhere except E itself. Start in F, you also can't go anywhere. But that's a little less exciting. So that graph obviously is not strongly connected. Um, maybe I'll draw an example of one that is This graph is strongly connected. It's also oileran. So remember uh an oiler tour was a closed walk. It visits every edge exactly once uh and visits every vertex at least once. So this graph is orarian. I drew it in an orarian way, but I will also redraw a path here. Uh, you can go around the outside. Notice now I'm respecting the edge directions. That's a little different from the example we saw last class. I think I got all the edges exactly once. So, uh, again, you might ask when which graphs are. We did the really long proof last class and it turns out um there's a very similar characterization. So, oiler tour exists if and only if uh your graph is strongly connected and uh for every vertex the in degree equals the outderee. And there's another there's a similar characterization for oiler walks instead of oiler tours. It's just a little uglier. So I won't go into it. It's just you allow these to differ by one for a couple vertices in a particular way. Um so in particular if you look at this example it should be every vertex either has two in two out. This one has two coming in two going out or one in one out. And in general, if you have k incoming edges, you should have also k outgoing edges in order to have an oiler tour. Why? Uh because what goes in must come out. Uh similar all the proofs here are similar to what we did for undirected graphs. Um before we are saying the number of edges at each vertex had to be even because every time we enter the vertex, we also had to leave. But this is the directed version of that where every time you come in, you must also exit. if you have a closed loop, if you have a a closed walk that visits all these edges. Uh so certainly this is necessary. Strong connectivity is also necessary because the oiler tour gives you a way to get from anywhere to anywhere. It's like a giant subway line. You can get on whenever you want and follow it until you get to whatever vertex you want. Because we said the oiler tour visits every vertex, that's a way to get from anywhere to anywhere. Might take a while, but you'll get there. Uh, and what else? Yeah, the proof again is similar. If you want to show that these properties are sufficient, you take the longest possible trail that visits every edge at most once and argue that it's closed for the same reasons as before and that it actually has to visit every edge because otherwise you could make it longer. Okay, I think we're pretty much through the the main repetition from before. There's one more related notion to connectivity uh which is connected components. We talked about connected components a little bit for undirected graphs. They're a little more interesting for directed graphs. Uh in particular they mainly make sense for strong connectivity. We don't define any other types. uh it's not surprising. So strong connected component or sec of a vertex V is the uh graph induced. This is similar to how we set it up before by uh all the vertices that are strongly connected to V. Okay, let's look at an example. This example it's a little a bit interesting. So for example, what is the strongly connected component of A? I haven't made this too ugly. Redraw this a little bit. What vertices are strongly connected to A? Shout them out. B can I can definitely get to B and B can get back to A. C. Uh, I can get from A Oh, no. I can get from A to C this way. And C can get to A via B. Anything else? No. Turns out if you if you follow any of these edges any of these two edges, you're never getting back. And also f is neither reachable is not reachable from a in either direction. So uh this chunk here is a strongly connected component. And again when I say the graph induced by these three vertices, so this is the graph induced by ABC, I mean that's my vertex set. I take those three vertices, none of the others, and I take all the edges that were in my original graph among those three vertices and throw away everything outside. So that boxed region is the strongly connected component of A. It's also the strongly connected component of B and the strongly connected component of C. What's the strongly connected component of D? Just D. So this is in a strongly connected component by itself. and E similarly and F similarly. Okay. Um in general we get a bunch of strongly connected components and before it was very clean, right? Everything like anything you could possibly touch was in one strongly connect was when one connected component and every vertex and every edge fell into one connected component. That was the situation for undirected graphs. We said that connected components partitioned the graph. Everything appeared exactly once in one of the pieces. Now it's not so clean. The vertices get partitioned. There's some vertices in this component, some in this, some in this, some in this. Every vertex is in exactly one strongly connected component. But for directed graphs, there are also some edges between components. That's a little more interesting. Uh so let's say every vertex is in exactly one strongly connected component. But uh we have some edges between and there's a nice way to kind of organize this which we call condensation graph. Okay. Uh condensation graph of a graph G is another graph H vertex set C and edge set F. These are just some madeup letters where C is I forgot some notation. So the strongly connected component of vertex V I'm going to call square bracket V probably just for this lecture but it's convenient to have some name for the strongly connected component that contains V. And C is going to be the set of all strongly connected components. So now I can write it as square bracket V for all vertices V and V. Okay. So in this example I draw a graph over here. So this was G. H is going to have one vertex for this strongly connected component. One for this one, one for this one, one for this one. So four vertices. I'm going to label them with the vertices that in the original graph that they came from. Got D up here, E roughly over here, and F over here. Okay. Now, what are the edges? Well, the edges are just going to correspond to uh whenever uh I want to read this from right to left. So, let me write it first. So, whenever I have an edge UV in the original graph, I want to draw an edge uh between the corresponding strongly connected components. the strongly connected component containing U to the strongly connected component containing V. Uh except I don't want edges from a strongly connected component to itself. Uh just because it's cleaner that way. So uh this will represent exactly the edges between components, right? There are some edges within components which I kind of want to ignore because I can get anywhere within this component. So it's for from a connectivity standpoint, from a reachability standpoint, I don't really care about the distinction between A, B, and C. They're all essentially the same. If I'm at any of them, I can go to any other. But then there's these edges between components that are interesting. Highlight them here in bold. So we've got ABC to D. We've got ABC to E. We've got D to E. And that's it. So this is the condensation graph. And so if you want to know uh reachability between two vertices in the original graph, you can figure it out by looking at reachability within H. So if everything's strongly connected, you're in one big component. That's easy. Answer is always yes. Otherwise, you have to look at walks in the cond condensation graph. All right, next topic. And actually the last topic which we'll spend a bunch of time on is much more interesting than last time is asyclic graphs. So what would you call an undirected asyclic graph? Anyone remember if we mentioned it tree what you're gonna say uh actually trees are connected undirected as cyclic graphs. So almost so we we may not have even mentioned it here but we use the word forest for the general case when you're not necessarily connected. So in the undirected case a forest is just any asyclic graph. Each of its connected components is a tree but it might have more than one. Okay. Uh today we're going to talk about directed as cyclic graphs. I feel like there should be some nature-based pun for directed as cyclic graphs, but instead they're just commonly called DAGs uh for directed as cyclic graph. So it's kind of boring. Sometimes I've seen dog like D- AWG. That's another type of graph which we won't get into. So that's nature based I guess. Um all right. So what's an example of a directed as cyclic graph? Have we seen any in this class? Yeah. A terminating state machine. Yeah. uh maybe not all terminating state machines but the ones that we the method we use to prove them. So like a state machine. Uh oh maybe yeah maybe all terminating state machines actually but in particular those with uh strictly decreasing uh derived variable it's strictly decreasing then you can't make any cycles. I think even conversely if you made a cycle then that would be repeating that cycle over and over would be a non-terminating execution. So indeed or terminating state machines good. Um another one is this graph the condensation graph we just defined maybe not obvious. These are just examples. Uh of course there are many directed asyclic graphs but there's no definition here. I mean the definition is directed as cyclic and graph or I guess as cyclic you might say as cyclic diagramraph given the terms we've used but it's always written d a g so there you go uh condensation graph is also always as cyclic because if there was a cycle like if I could get from ABC to D to E back to ABC well then these three should actually all be in one uh strongly connected component right so there can't be any cycles in this graph otherwise I would collapsed further. So there we go. Uh the what makes the condensation graph interesting in some sense is it becomes a cyclic. You can track all the cycles into one vertex or each cycle into one vertex. All right. Uh I have a quote practical example here. You'll see you see why I'm laughing in a second. So directed as cyclic graphs come up naturally a lot. I mean you can make any graph a cyclic by constructing the condensation graph. But uh where they're quite common is in a set of constraints. Suppose you have a bunch of tasks you need to do and there's some precedence constraints. This has to happen before that. This has to happen before that. You could imagine project management or whatever. I'm going to give a very real world example you do every single day I hope. uh which is getting dressed. So suppose you have some socks you want to put on and you may know that socks come before shoes. I've tried it the other way. It doesn't work so well. And uh usually I put one sock on at a time. So there's your left sock and your right sock. Uh apologies for people who have different number of feet. uh like dogs, it's even more annoying. Um but uh these are examples of precedence relations. You want to put the left sock on before you put your left shoe on. You want to put your right sock on before you put your right shoe on. But you don't really care about the ordering between these two sides. I could do both of these and then do both of these. Or I could do both socks and then both shoes. I could do left, right, right, left. There are lots of possible orderings that are valid here. You can add more fun things like uh pants. Those have to happen before your shoes. And I don't know, you probably want to put a shirt on. And maybe you have uh a jacket on top of that. And you have a belt, which you should put on before after your pants, let's say. And maybe like a winter coat. Still kind of winter, maybe. We'll see. Uh, and if you're really cold, maybe you want a scarf. And I've heard debates about which of these goes first, but let's say scarf before coat. Uh, certainly after your jacket and maybe you want a hat on, but that could happen pretty much anytime after your jacket. So something like this. I think those are all the all the annotations I had here. And now you'd like to find a valid ordering. If you want to find a valid ordering, there better not be any cycles here because then you could never resolve that cycle. If you have a cycle of precedence constraints and A has to happen before B, B has to happen before C and C has to happen before A, then nothing, none of A, B and C could happen first. So that would be bad. So in any kind of uh precedence constraint graph like this, you want to have no cycle. So this is practical motivation for directed asyclic graphs. And now we'd like to let's define a notion of um topological ordering of a DAG is an ordering of all the vertices. such that each vertex uh appears before all vertices it can reach. give it a name. So I'll call it V here. So for example, uh pants has to happen before belt, but also pants has to happen before your winter coat because of these two connections. And pants has to happen before shoe. So pants should be listed earlier in your sequence before your right shoe, before your left shoe, before your winter coat, before your belt, before uh that's that's it for pants. Okay. Okay. And if we can find an ordering uh that satisfies that property, then that would be a legitimate way to execute this these set of tasks. And the next thing I'd like to do is prove that topological orderings always exist. Seems reasonable in a DAG. If, as I said before, if you have a cyclic graph, these don't exist. That's a little less exciting. As long as your graph is a cyclic, you can always find a topological order. And there's a nice way to do it. Uh let me first define a couple of useful terms. One is source. This is for any directed graph. A source is a vertex of in degree zero. This is going to act kind of like a leaf for us and that we had for trees. Remember uh directly cyclic graphs here are sort of our analogy of trees or forests more generally and degree one vertices. Well, there's no notion of degree here anymore. We just have inderee and outderee. So instead of degree one vertices we're going to look at inderee zero. So for example, left sock is a source, right sock is a source, pants is a source and shirt is a source in this graph. Uh and the complimentary notion now is sync and this is a vertex of outderee zero. So in this graph we have few of them also left shoe is a sink right shoe is a sink I have no outgoing edges winter coat is a sink and hat is a sink these were sinks. Okay. And then there are some vertices that are neither. Probably in general most of them are neither. But uh what we do know like the fact that every tree with at least two vertices has at least two leaves. We know that every DAG has at least one source and at least one sync. So if you're wondering why two leaves for a tree, this is kind of why. Maybe this is uh distinguishing the two two ends. And the proof is actually very similar. So let's do lema. Every DAG has a source and a sync proof. I don't know if you remember the proof that every tree has at least two leaves. Uh it was take a longest path or walk. Uh maybe it's worth mentioning for DAGs, walks and paths are the same thing because the thing we're worried about with paths is do you ever cycle? Do you ever repeat a vertex? But if there's no cycles, you can't. So for DAGs, uh walks equal paths, which is nice. So for the rest of today, I don't have to reme remember which is which, but I'll try to be a little bit careful if I need something. So uh let's just take the longest or a longest path in the DAG. So let's say it starts at v 0 to v1 and goes up to v k as usual. Then the claim is Uh, V 0 is a sync and VK sorry backwards. E0 is a source and VK is a sync and that so then we found it. Okay. Why? Uh, let me do a proof by picture. So here is our hypothetical longest path and I want to claim for example that v 0 is a source meaning there is no incoming edge into v 0 in degree is zero. Well let's prove it by contradiction. Suppose there was some vertex here u and there was an edge from u to v 0. This is for contradiction. Imagine that this happens. Well, then that looks like a longer path in the graph. How could it not be a longer path? Well, maybe it cycles. Maybe this vertex u was actually one of these other visi. Uh but it can't be because our graph is as cyclic. So either uh we get a cycle which contradicts that we're a DAG or we get a longer path in the graph which contradicts that we were longest. So either way we get a contradiction and that proves uh that v 0 is a source and exactly symmetric argument proves that vk is a sync. Cool. So these things exist. Now let's use them to construct a an order to getting dressed. I know you you've been wondering this the answer to this gripping question. How should I get dressed every morning? Finally in 61200 I learned how to do it for any graph. Uh the idea algorithmically is very simple. If you want a topological order for your graph, what should I start with? A source seems good. Source has nothing that has to come before it. So any one of these would be a valid first step. So the algorithm is just take any source, do that first, then conceptually remove that vertex to represent that it's been finished and then find another source. And because from this lema there's always a source, you never get stuck. So you can just keep going and eventually you'll have done executed every task. So for let's do it um briefly on this example. Say we uh we do right sock first and we up we see if there are any new sources. Uh I think there are not. This is still not a source because it has an incoming edge one left. Uh then maybe we do our pants. Put those on. Seems like a good idea. Now we can put on our right shoe. Finally. Been waiting for that right shoe because there's no more incoming edges because we've eliminated we've finished the tasks from before. So, if I write down the ordering, it would be right sock, then pants. Uh, now I'm going to do right shoe. Uh, then I don't know, I'm lazy. Let's do left sock and left shoe next. Then, uh, let's see the at this point I've eliminated the left half of the graph. only source is shirt. So I have to do shirt next. I had a lot of choices up till now but that that choice is forced. Uh after I've eliminated these two this becomes a source and this becomes a source. So I can choose belt or jacket first. Let's do belt. Still can't do uh winter coat because I need to do scarf before it. Uh so jacket is the only source at this point. So do that. I think you get the idea. Now I could do hat for example both the source and a sync. Um I can do scarf and then finally I can do winter coat. So that was a valid execution order or valid topological ordering. Cool. Let's uh prove that this always works a little more carefully uh using induction instead of an algorithm because this is more of a math class than a algorithms class though of course we talk about both but I think it's good to get more uh induction practice so in particular induction with graphs is still a pretty new thing. But again, induction about graphs is always induction or some regular induction over natural numbers. So we're going to do induction on the number of vertices in the graph, which is how we normally do it. So our uh p of n is that every dag with n vertices has a topological order. We're going to assume by induction that all smaller n have this property and prove it for n or prove it for v size of v. So what is the smallest that size of V can be? Uh well I didn't put it in the definition but I meant to say V has to be at least one as before. We don't allow the empty graph with no vertices. You have to have at least one vertex. So smallest graph has one vertex. So here it is. Uh I guess yeah uh I mean in a directed graph you could have a loop here but that's that's a cycle. So not allowed. In a DAG, there's only one graph of one vertex. It's that one. Let's call this A. The ordering is A. It satisfies all the properties. Okay, that's their topological order. So, this is easy. More interesting is the induction step. So, uh now we can assume that D is at least two bigger than the base case. Uh so what should we do? Well, we have this nice lema that tells us there's a source. So whereas the algorithm took a source and then had to do more work to continue, we can just take one source and then use induction. So uh by the lema there exists a source let's call it s and the intuition is I want to do s first. Okay. But to figure out how to do the rest of the graph, I can use induction. So let's uh let G prime be G minus S. Remember this was the graph. G minus S was the graph induced by the subset of vertices. That's everything except S. So in other words, erase the vertex S and all of its incident edges, which in this case will only be outgoing edges. So here we have s and there's some outgoing edges into g prime and g is this whole graph. Okay so the point is um number of vertices in g prime is one less than the number of vertices in g. We just removed s everyone else is there. Uh but in particular g prime is smaller than g was. So we can use induction by the induction hypothesis. Uh G prime satisfies this. I guess we also need to check G prime is a DAG. But if you remove a vertex, you're not going to add any cycles. So that's cool. Uh so G prime is a smaller DAG according to the measure of number of vertices. So by induction hypothesis, it has a topological order. G prime has topological order. Let's call it uh vub1 v2 up to v sub I guess number of vertices minus one. That's how many vertices there are. So I can you by induction I get an explicit ordering of all those vertices. And now I just put s first before all of those. So we get s comma v1 v2 and so on v sub v minus one. Uh this is a topological order of g and this is where I guess you have to actually check something. So I claim it's okay to put s before all these other vertices and that will be a valid ordering. uh intuitively because S has nothing that has to happen before it. So if you check the constraint of a topological ordering, every vertex that's listed must appear before all vertices that it can reach. Well, S definitely appears before all vertices it can reach because it appears before all vertices. And the other vertices are also okay like V2 or V1 let's say appears before all the vertices it can reach because all the vertices it can reach it's inside G prime. Once you're inside G prime, you can't escape. you can't get back to S, right? It's like in a different strongly connected component. Uh any vertex down here, which is all of the V1 through VV minus one, cannot reach S because it only has outgoing edges. And so they also appear before all the vertices they can reach because all the vertices they can reach are within G prime. So great. Now given a set of tasks with precedence constraints, you can find a valid ordering so you can get dressed in the morning. But uh you know we're most of us are computer scientists and so often we're not just one computer or one uh agent executing a bunch of tasks but maybe we have many computers to execute our tasks. What if we allow parallelism? Can we do this any faster? You imagine you want to get dressed, but you have a crazy uh Rube Goldberg style machine that can put on many garments all at once. So maybe then you can do left and right sock in parallel. If I had four arms, I would do that. Maybe even three arms. I don't know. That's a good question. Okay. So let's think about that version where uh we can do parallel task execution. Then I'd like to do it as fast as possible. If I'm just a measly single processor and I want to execute the tasks in this graph, well, the number of steps it takes me is exactly the number of vertices. No matter how I do it, it doesn't really matter which ordering I choose in terms of speed, assuming, I don't know, assuming instantaneous switching time between different tasks. Okay. But with parallel task scheduling, you can do more. So let me first define the problem a little bit um and then we'll prove a nice theorem. So I want to define a schedule to say basically what order you should do the tasks in. But now I'm allowing multiple tasks to be scheduled at the same time. So a schedule is going to assign what time uh t of v it's going to be a natural number uh to each vertex v and it needs to satisfy the property that uh if u can reach V and U does not equal V, then uh the time that we schedule U has to be strictly less than the time we schedule V. This is the analog of topological ordering except now I can assign the same time to multiple tasks. So maybe I do a bunch of tasks at test at time zero as long as none of them has to happen before another. Then I could do a bunch of tasks at time one, a bunch of tasks at time two, and so on. And the um I'll call it the span of the schedule is the number of distinct times t of v. So I can look at all the different times assigned to the vertices and if I use 17 different times, my span is 17. span is a measure of how long it takes to execute schedule. Uh here I'm assuming an unlimited number of processors. I can do all the tasks at once if I need if I'm allowed to. If I have no edges in my graph, I'll just do everything in time zero and it'll be done. And that will be a span of one. There's only one time that value that I needed. But in general, we're going to need more, right? I can't do left sock and left shoe at the same time because left sock has to happen strictly before left shoe. That's what this says. If one vertex can reach another vertex that's different, then the time assigned to the the earlier precedent has to be strictly less than the later one. Okay, so that's the definition of the problem. And now there's a nice theorem says uh the minimum possible span of any schedule. So certainly a schedule like this exists, right? We could do the one given by the topological order. We could do right sock in time zero, pants in time one, right shoe in time two. Like we could do this sequentially, one task at a time. But if I want to minimize the span, I want to do lots of things in parallel. Uh this is equal to the number of vertices in a longest path in the graph. So very simple characterization of how long this takes. Uh in this example I think the longest path is shirt to jacket to scarf to coat. So that was uh that's a path of length one, two, three. But what I wrote there is not the length of the path. I wrote the number of vertices in the path and that's four. So this is the one time when we want to count vertices, not edges. So I write number of vertices in the longest path. It was shirt, jacket, scarf, winter coat. So that's four. And so I claim this uh this set of tasks can be executed in just four steps, four rounds of parallel execution. Namely, I think I need another color. Let me see what I brought. Oh, purple. Okay, so how I do it is actually pretty intuitive. I just want to take all the sources at each step. So in the beginning I had four sources. I'm going to do all of those at time zero. Then what's the source? Uh this becomes a source because both left sock and pants were done in the previous step. This becomes a source because right sock and pants were done in the previous step. Uh this becomes a source and this becomes a source. I think that's it. So then in the second step I'm going to do all these everything above that purple line. And then uh hat becomes a source, scarf becomes a source, but winter coat is still being annoying. So we do everything above that line. And then in the fourth round, we just do winter coat. Okay, so that's the algorithm. But let's uh let's prove it's not obvious that this algorithm takes exactly the longest path time. And there's a nice elegant proof of it. Let's go back here. So um in general when we want to prove that two numbers are equal usually we do it in two parts. We prove that one number is greater than equal to the other and the one number is less than or equal to the other. seen that a couple of times I think and this is definitely one of those situations. So first let's think about why is the minimum span of a schedule at least the number of vertices in the longest path. Well that makes sense because if I just think about uh this longest path you know it's some sequence of vertices and let's say it has length L. So we start at number one or it has L different vertices in it. Well, in the first round, uh, just looking at this path, the first round, the only thing I can do is the first vertex in the path. In the second round, the only thing I can do is the second one. Like to execute this path clearly requires at least L time steps, right? So, this is just to schedule the longest path, you certainly need the number of vertices of that path. That's a bit handwavy, but pretty intuitive. The interesting part is the other direction. So let's suppose I found the longest path and it has L vertices. I claim that I can achieve I can find a schedule that does that that is that fast. And here it is. Given a vertex, we're going to define its depth to be uh the length of longest path ending at V or say I guess 2V. So it can start anywhere, but I want the a longest path that ends at V. Okay. Okay. And I'm going to here I'll use length measuring the number of edges as usual because it doesn't really matter. So I claim uh that uh depth is a schedule and that it has the right span uh because depth assigns a natural number 012 whatever uh to every vertex and I claim it is a valid schedule. Let's prove that. Okay. So what does it mean to be a valid schedule? It says that uh if I have two vertices u and v where u can reach v and u is different from v then I have to prove that depth of u is less than depth of v. So uh suppose uh u can reach v and suppose v does not equal u. We want to show uh depth of u is strictly less than depth of v. This is what it means to be a valid schedule. Okay. So, uh what does it mean that U can reach V? We still have the definition over here. V is reachable from U or U can reach V if there exists a walk from U to V or for DAGs it's the same as or in general it's the same as there exists a path from U to V. So uh we know that there exists a path let's call it uh p uv from u to v that's the meaning of ukv uh and we also know that it has length greater than zero. Why? Because these vertices are different and there are no cycles. uh I guess the only way to have a length zero path is if you start and end at the same vertex but if v is different from you that path has to have uh nonzero length this will be important in a moment okay let's see what else do we know we also know uh or we know that there's a path uh there's a longest path pu to uh to u my my words are starting to sound like numbers. Uh this to o um right that's the definition of depth. Depth is the length of the longest path that ends at that vertex. So let's just take that longest path. Uh so this one has length exactly depth of u. That's the definition of depth. It might be zero. Maybe the longest path that ends at U. If U is a source, it's just the path that starts at U and doesn't do anything. But it's there is this path. All right. Now, my big idea is concatenate these two paths because here we have a path that ends at U and here we have a path that starts at U and goes to V. So, obvious thing to do is glue them together. Uh, so let's concatenate uh and I'll call this p subv is going to be uh p sub u comma p subuv. So do follow the path pu follow the path puv and the length of this path. So first of all it's a path because this one ended at u and this one starts at u. So you can actually concatenate them. The length is just going to be the sum of these two lengths. So it's it starts with a depth of view number of edges and then it has at least one more. So the length is strictly greater than depth of U because it's just the sum of these two numbers. Okay. Now what do I know about this path? It is a path that ends at V. PV ends at V, right? Because if you look at the last vertex it visits, it's V. So this is a path that ends at V and it's its length is this greater than depth of U. Now depth of V is the length of the longest path that ends at V. And the longest path is it greater than or equal to any particular path. So I found a path here. That means that uh the longest path is greater than or equal to this one. And that's strictly greater than depth of U. And the length of the longest path here is equal to depth of V. So I've proved that depth of V is strictly greater than depth of U because I found a path that was longer than depth of U. Therefore, depth of V is greater than depth of U. Okay, so that's cool. Uh there's one last thing to check here. So I I proved that this is a schedule that it it processes things in a valid order. But we also need to check uh how many time steps does it take? Why is it the number of vertices in the longest path and not the number of edges for example? Okay, the span of depth I claim is one plus uh the number of one plus the length of longest path. This is what I wanted to prove because length counts edges. So if I add one more, that's the number of vertices in the path. And that's exactly what this theorem says. Uh why? Uh because if you look at all the possible depths, they start at zero, right? All the sources are going to get depth zero. Uh and they go one, two, and so on. And I claim they end at the length of the longest path. Why? Because what are the depths? The depths are the length of the longest path ending at a particular vertex. And we do this for all the vertices. And so the maximum value this is going to take on is just whatever the globally longest path is. It ends anywhere. Okay? So that's why we max out here. And it's plus one because we started at zero and we ended here. So we have one through length of longest path. That's length of longest path. But then we have a plus one for zero. Okay? So that proves this theorem and tells us how to optimally schedule if we have an infinite number of processors or not infinite but enough that we could do all the tasks at once if we wanted to. Cool. Let me recast these results in a slightly different form that's interesting and we'll connect to the next lecture. So, let me give a couple of terms that are uh concept called partial order which we'll talk about next time and they let us think about these kinds of precedence relations in a different way. So in particular there's this idea that okay uh left sock and left shoe they're related. Left sock has to happen before left shoe. Uh right shoe and pants are also related. Right shoe has to happen after pants. Also winter coat has to happen after pants. But for example pants and shirt are unrelated either. They could happen in either order. There's no path from pants to shirt. There's no path from shirt to pants. So let's define uh a notion of comparable meaning they are there is a precedence relation between them in one direction or the other. So either U is reachable from V or V is reachable from U. In either case, we call U and V comparable and otherwise they're not comparable. These are vertices. All right. So there's two complimentary notions I want to define. Chain is a set of pair-wise or mutually comparable vertices. And an anti-chain is a set of pair-wise incomparable vertices. Okay, so we're thinking about sets of vertices, subsets of the whole graph that are either all mutually related like they they have a defined sequence for them or not. So for example, uh where's my purple? Need even more colors. Hopefully this will still be clear. Uh so for example if I take shirt and jacket and winter coat these are three vertices and I claim they form a chain because these two are related. These two are related. These two are related. All pairs of them are related. And in fact they they have a fixed order right? Shirt has to happen before jacket. Jacket has to happen before winter coat. I happen to skip scarf but I could also put scarf in. that would also that would be a longer chain. Okay, chains basically correspond to paths except you can throw some of the vertices away. So that's why I left a blank line here. This is basically a subset of a path. Every chain is going to be a subset of a path. An anti-chain though is is sort of the interesting new notion here. That's going to be a set of vertices that are mutually incomparable. For example, all the sources by definition, you can't reach any of them for any other because you if you start at a source, you can't get to any sorry if you start outside you can never get to a source. That's what I want to say unless you started at that exact source. So all the circled red vertices for the sources are an anti-chain. So are all the squared ones at the bottom. Those are the syncs. There are four of them. Those are form an anti-chain. Okay. Now I claim anti-chains are really what's going on in the scheduling problem in the sense that all the tasks that are done at the same time must be an anti-chain. So if you look at all the things that happen at time zero those are the sources those have to be an anti-chain. uh all the things done at time 53 also have to be an anti-chain because if there's any relation between them if one of them had to precede another if there was a path between two of the vertices done at time I then that would be invalid you're not allowed to do that if two if things if u and v have a reachability relationship they must be done at different times okay so the things you do at a particular time form an anti-chain and so by that exact theorem We get this correlary. Uh Another way to look at a schedule is it's partitioning the vertices. The vertices are all the tasks I have to do and into the fewest number of anti-chains. Anti-chain is a set of vertices I can do at the same time. So the number of steps I need the span of that schedule is exactly the number of anti-chains I need to cover all the vertices to execute all the tasks. Okay, this the theorem is this has the same size as the size of the partition. And the number of anti-chains you need is uh the longest chain. Okay, the proof is exactly you you apply the schedule and then your anti-chains are all the things I do at time zero, that's one anti-chain. All the things I do at time one, that's another anti-chain. And so this is kind of a neat connection between these are kind of dual notions. one you want lots of comparisons possible and the anti-chain you want no comparisons possible and this is saying that they're related in that if I try to split into anti-chains the number of anti-chains I need to cover everything is equal to the longest chain turns out the reverse is also true the if I take if I want to partition into the fewest chains uh the number of chains I need is the same as the longest anti-chain kind of cool. I guess this should technically be the largest because it's a set. Um, what else do I want to say? You can use this to prove some fun facts like uh there's always a large chain or a large anti-chain. So for example for any t either there exists a chain of length or of size uh greater than t or there exists an anti-chain of size at least n / t or size of vertices over t. Okay, this just follows from that thinking about the partitions. If you if you know that all the chains are small, suppose all the chains have size at most t then you know that the partition into anti-chains or sorry uh yeah one one way or the other uh if I know all the chains are small then I know I can partition into anti-chains uh with that number. So I can partition into t anti-chains if all the chains have length at most t. And that means that uh the longest anti-chain has to be at least v over t because all the vertices get covered by the partition and so the biggest one is going to be uh bigger than the average. So you can use it to do that. You can use it to do other fun things like have you ever wondered I don't know what your calendar looks like but for me I have lots of conflicting events. I have to be in many places at the same time. It's pretty annoying and partly because I have many calendars all displayed in the same place. And so, have you ever wondered how do you display a calendar like this? Well, for for one event that appear that occurs strictly before another, uh, I'll draw an edge. Okay. So, uh, here's an edge there. There's an edge there. Uh, some of these have edges, some don't. Then in particular if I look at two conflicting events they will be incomparable according to this definition. There'll be no path from this vertex to this vertex or this vertex to that vertex. And so what I'm looking at here is a partition into chains. Each of these chains has to be nicely ordered. And I want to know how many different chains do I need? Well, it turns out the answer is exactly the size of the largest anti-chain. In other words, the size of the largest set of events that are pair-wise conflicting, which is sort of the best you could hope for, right? I need exact I need at least one column for each of these guys if they're all conflicting with each other. And it turns out you can always achieve that. And that's how calendars draw their events. All right, that's it.