all right welcome back to 61200 we continue probability and not only that we continue on expectation today that we also get to a second measure of distri of random variables which is variance but first most of today will be uh expectation remember what we did last class so uh for random variable X over a sample space s this is just a function from s to real numbers uh we had three different formulas at least for expectation in different scenarios first one was the definition sum overall outcomes Omega probability of the outcome times x evaluated at that outcome sorry this is a Omega uh or we could rewrite that sum according to the distinct values of capital x and so sum over the range of X and take the probability uh that big x equals Little X multip by Little X that's equivalent um and in the special case where X is a function to natural numbers then we could write this as an infinite series sum of I equal 1 to Infinity probability of X greater than or equal to I and that turned out to be equivalent to this thing by expanding it out so if the CDF is nice that's a nice formula if it's in the special case z over natural numbers for independent for uh indicator random variables where X is zero or one another special case then the expectation is just equal to the probability uh equaling one which is like probability of the event that this is an indicator for and the big thing was linearity of expectation which told us if we summed and also multiplied by a constant uh inside the expectation we could push the expectation inside and or pull the sum and the multiplication out so this we're going to use all of these things today uh and see how they contrast with other other fun properties um but I'm going to start out with some motivating examples to give us some interesting uh problems to think about uh so let's talk about a bunch of events uh E1 up to e n so these are subsets of our sample space and we can think of those events being uh a good things or bad things uh maybe uh these are different ways in which your computer could fail or your algorithm could fail or maybe they're good things maybe these are uh you'd like at least one of these to happen or you'd like at least half of them to happen happen or whatever uh so we're going to think about um in particular the number of events that happen so let's Define a rener variable n to be uh the number of EI events that happen and uh first problem is what is the expected value of n so what is the expected number of events in general there's a nice formula for this which is sum I = 1 to n of the probability of event I okay this is pretty much a warmup it's very similar to things that we did last time in fact we did a special case of this last time uh but let me remind you those techniques and let's actually do it for this example so um remember the technique was if I have some random variable that is tricky to analyze and I want to compute this expectation uh write split it up into a sum of different random variables so here there's a pretty obvious way to split it up n is supposed to count the number of Eis and so each of these uh suggests we should count them individually let's define an indicator random variable I subi uh for each event uh and that that is counting among all the events that are known as EI there's only one of them how many of them occurred zero or what did it occur or not so then n is going to be just the sum uh IAL 1 to n of I I I I captain then uh we can compute the expectation using all these fun things so we've got the expected value of n is uh by linearity the sum of the expected values of the IIs uh and the if because the IIs are indicator random variables their expectation is exactly the probability that they're equal to one so this is sum IAL 1 to n uh probability sorry I keep forgetting my x's in next to my e uh I I equals 1 uh and that's just by definition of the IIs that is the probability of EI okay so very easy proof just but refreshing how we use all these fun facts we did a special case of this last time which was if all uh we were counting if I flip a coin n times and a prob probability of heads is p uh so this is biased coins uh heads probability P then the expected number of heads uh was equal to P * n that's something we computed last time in exactly the same way so this isn't terribly new but this is more general form each the events here all the events were the same probability now they could have different probabilities still you sum them uh cool we'll use this again as well all right so that's simple warm-up problem just counting the number of events now we'd like to understand a little deeper how many events are going to occur in particular a really interesting uh question is when does at least one event occur so if you think of these for example as being different failure modes for your system you want to know what's the chance of anything failing out of all the different ways that things could fail and those events may not be independent so uh all of this works even if the Eis are dependent on each other maybe they're all the same who knows but uh this summation is exactly the right answer for expectation by linearity uh so let's start with with a very simple upper bound called the union bound on the chance of something happening so this is probability of the Union hence the name Union bound is at most the sum of the probabilities of the events okay in other words using the terminology from over here the probability that at least one event occurs that's the probability that n is at least one is at most the expectation of the number of events because these two things are equal by this previous theorem here so uh these two statements are equivalent because the number being at least one that's the same as well that's at least one event occurring that's the same as the union of these events Okay cool so this is like uh like the some rule but some rule had an equality and was only um when the events were disjoint this works all the time but it can give a very weak upper bound depending on your events this sum may be larger than one so that doesn't tell you very very much to say probability is less than equal to one it's like yeah I knew that uh you didn't have to do any work let me um in parallel write some examples here uh so let's do our same example of n independent coin flips uh probability of heads is p then the union bound says that uh the probability of at least one head is at most P * n right that's this expectation that I just wrote down over here P * n was the expected value for the number of heads so in particular the probability is at least one head is at most P * n so as soon as p is bigger than one over n this is not a useful upper bound right the the true answer actual uh probability there's at least one head this is uh 1 - 1 - P to the N easier to compute the chance the probability that there are zero heads that's the same as saying that there are N Tails so uh this is the probability of a tals one minus P raise it to the nth power we get the probability of Nils we take one minus it we get the probability that there're not Nils and other words there at least one head so these are close for very small P but not so great for uh uh even for small P maybe not so great I don't know all right um so this is an example where Union bounds not so helpful um maybe I'll do I'll come to other examples in a moment let me tell you uh a nice property on the other side uh which I'll call Murphy's Law it's not the official name but probably heard of some form of Murphy's Law in the past uh whatever can go wrong will go wrong this is a formalization of that so if uh the N events we have are mutually independent this is the first time I'm making this assumption other than that example uh then probability of the Union is at least uh 1 minus 1 over e to the sum of the probabilities of the events okay this is without any uh n terminology but maybe a nicer way way is to write it uh in terms of n probability that at least one event occurs is at least 1 - 1 over e to the expectation of n Again by this theorem relating the expectation of n to the sum of the it is just a sum of those probabilities so um the union bound gives an upper bound we want to the chance of some event occurring like some failure happening uh this tells us it's at most something and this tells us it's at least something but this theorem assumes Mutual Independence so doesn't always apply but in this example it does apply so we can apply Murphy's Law here and it gives us the probability of at least one head is wrong page at least one minus one/ E to the PN right again the expectation is PN so 1 minus 1 over e to the PN so uh the bounds tell us it's somewhere between this uh thing that's kind of close to one especially if this exponent is large so this is the point is here this uh denominator increases exponentially with expectation so uh that means this is going to be very small if if expectation of n is large um and so that means this is very close to one right so when the expectation is large the union bound doesn't tell us anything useful says it's at most one or whatever but also Murphy's law says it's actually pretty close to one so maybe the union bound wasn't so bad after all that's the idea um I mean in particular we also know that it's less than equal to one so those two facts combined are maybe okay when the expectation is large so uh what can go wrong will go wrong with high probability assuming Mutual Independence and assuming expectation is large I guess that's the formal version of Murphy's Law okay uh cool let's do to a particular example of this that I think is instructive which is suppose that um each event occurs with probability 1 n so this is same thing but I'm just plugging in P 1 n then with the Union bound I get uh the probability of the Union I'll write it one more time is at most one exactly so it's useless um Murphy's Law tells me the probability is at least uh 1 minus 1/ E to the one which is about 63 so they're they're pretty close they're within a factor of two of each other uh one and 63 and the right answer uh well I guess is this thing 1 minus one/ n to the N I'm not sure it has a nice nice formula it depends on N which one is more correct but they're both they're pretty close so they're both pretty correct uh here is maybe another example so we did two examples at the end of last class with randomized uh phone returning at some terrible school where you get where you get a randomly selected phone when you go home um and then we talked about a version with a turntable or Lazy Susan uh where either everyone gets their phone back or no one does and it was kind of weird because in expectation these were the same the expected number of phones that went back to the correct person was one independent of how many people in phones there were as long as number of phones equals number of people which is kind of funny expectation was always one um so uh yeah this is an example if you recall we wrote down uh there was an indicator random variable I forget what we called it let's say I sub I uh which was does the I person get back their phone and we computed the probability of that equaling one here and here and in both cases it was 1 n so these are actually both examples of this picture okay I should say uh if mutually independent I wanted to in this example I didn't want to assume Mutual Independence this holds in all cases this holds if the events are mutually independent uh because here the events are not necessarily mutually independent right in fact they're not in both of these examples they're different levels of Independence I would say but it's they both satis they're both special cases of this property the probability of each event is exactly one over n the probability each person gets their phone back is one over n both with random permutation of the phones and with random rotation of the phones um and here I will give you the actual answers the probability uh that any phone is returned correctly that's this probability of the Union of the different uh events uh in this case uh well it's easier to compute the probability that no one gets it correctly this is like the uh no repeated serial numbers in the dollar bill kind of um there so for the first person they n minus one choices out of n where they get the wrong phone back and then the second person there's nus 2 choices out of nus1 for them to not get their correct phone back and so on and get my cancellation shock uh n minus one's cancel nus 2's cancel nus 3's cancel and so on we're left with 1 - 1 n okay which is in between these two values so that's good uh and it's actually very close to the union bound uh closer to the union bound than the other one so I would say yeah um I don't know these are uh pretty independent is my fancy analysis uh yeah maybe they're independent if you look at it the right way it depends how you define the problem exactly um over here what's the probability that any phone gets returned correctly this is actually a really easy one I rotate the phones on a table what's the chance that anyone gets their phone back correctly yeah one over n there just n possible outcomes here and one of them everyone gets their phone back so capital N equals little n and in the other situation zero people get their fund back so this seems to violate uh this lower bound right we said it should be at least 63 so uh in order to do that it means that they're very dependent I mean it's it's very hard for events to be more dependent than them being equal in fact the first person gets their fund back if and only if the second person gets their fund back if and only if the third person gets their Fone back uh so indeed the events are equal to each other here the same single outcome and so that's why this is breaking okay uh I wanted to prove Murphy's Law and then we'll come back to one more example uh where we at over here like where does this exponential come from it's an approximation so uh it comes from one of these fun uh facts that we've used before so just like in this last example um instead of computing the probability that at least one things happens let's compute the probability that zero of the events happen and then do one minus it okay so I'm going to compute uh the probability complement of this event is that n equals zero so no events occur okay in other words the probability uh that E1 does not occur and E2 does not occur and uh e n does not occur okay uh well um the E are independent are mutually independent and one of the things we we mentioned uh and may be proved in recitation I forget back when we were doing the independence lecture is that if uh events are independent then also their complements are independent if E1 and E2 are independent from each other then E1 complement is independent from E2 complement uh and so um we from the product rule this probability of intersections for inde and mutually independent events events is just the product of the probabilities okay uh what else so this probability is of course one minus the probability of EI and now we're going to use this fun fact uh which is this is at most e to the minus probability of EI by this uh fact that 1 - x is less than or equal to eus x uh for x non- negative we've used this few lectures ago I think uh this was the Taylor series expansion of e to Theus X you take the first two terms it's a very good approximation when X is small which is kind of an interesting case we're thinking of these probabilities individually as being small but there might be lots of events then this will be a good approximation uh but it's always true that it's at less than or equal to in all cases so this is fine um and so now we have a product of exponentials and by properties of exponentials we can pull the exponential out and sorry this is less than or equal to e uh to this sum minus uh sum I = 1 to n of probability of EI okay which if you stare at it long enough that's exactly what we got here e to the minus is the same as one over e to the thing this sum and because we are Computing the complement event we have to do a one minus at front and instead of an upper bound this was an upper bound on the failure uh this is a lower bound on success if success is one of the events occurring Okay cool so that's where the exponentials come from it's a way to product of one minus something is ugly there's no nice way to simplify that but if we use this approximation then we get a product of exponentials and life is good so um for fun you know as we're winding down the class uh I think it's time to enjoy music video so uh the application is we're going to think about uh we have some probabilities of failures and here this is a really cool rubbe Goldberg machine made by the rock band okay go uh and there are about over a hundred different components in this rub Goldberg machine so the the Challen what you want to think about is what's the probability of at least one of those components failing in a single uncut video [Music] [Applause] [Music] [Music] can't you let me get you and they can't keep ding [Music] around ground [Music] can't why would you want toally [Music] [Music] [Music] the the m [Music] let go [Music] to let [Music] go let me get you down let me get you down to [Music] [Applause] [Music] so good [Applause] [Music] so uh if you haven't watched all of ok Go's music videos I highly recommend instead of studying for the final you should watch them uh amazing stuff I actually I've watched this video like 10 times in the last day and I only just now realized how many references there are to other okaygo music videos so if you watch them all you'll get extra appreciation then watch this one again and you'll see them all um cool so I I happen to be friends uh in particular with Damien who's the the lead singer and I was chatting with him like how how how did you make this work because uh he's in many talks that he gives like he has a TED talk it's linked in the lecture notes he said okay uh there about 130 components in the setup uh so if you imagine each uh fails uh with probability say 10% reasonable if you engineer something well should work like 90% of the time I don't know if you ever built a rub Goldberg machine even that's difficult to engineer it's very easy to get more like 50 70% success rate uh but let's say you can get 90% success rate then uh what's the probability that uh there is a failure number of fails is at least one well you know it's the same thing we had before uh one minus 1 - 1 minus P to the N this is the actual we won't use any approximation here uh it turns out to be uh 99.999 89% which is exactly one in a million chance that this video will come out good so uh that's terrible uh and then you know and his talks he goes on it's like yeah so you know we've really go to um we got to take chances and you know uh just explore and not be um not worry about failing it's the main point of his talk but then he never explains like well how did you make the video if there's a one in a million chance that it comes out right did you do a million takes uh and I don't know how many takes uh they did he didn't just didn't remember uh but he said the the secret for it working was to do the math after you've made the video instead of before uh maybe a little unsatisfying uh but on reflection um I mean you could work out some other probabilities right if you say okay there's only uh 1% of uh chance of failure then the probability the whole thing succeeds is uh or the probability of failure overall is 73% still not very good but doable like you could do four takes and then one of them will be good in expectation uh but 99% reliability is challenging it's only when you get to like if if you get to 99.9% success rate then this starts to get impressive you only a 12% chance that something fails um but apparently the in hindsight what worked well for this video is because the the effects the interactions get big and bigger towards the end of the video because they just wanted to make them more and more impressive with bowling balls and giant things and you know huge interactions huge interactions actually fail less often because something like a bowling ball is less affected by air currents in the room and stuff like that so the bowling balls were super reliable they were probably down here in the very very like maybe several nines uh Effectiveness whereas some of the very early effects in particular The Dominoes which open the video I'll just play that again because I love watching this video um they failed almost every single time so um this this very first move apparently dominoes are extremely unpredictable so doing this in a way they said they set up the dominoes roughly six billion times I think that's an exaggeration but uh that's I guess the other answer is to do uh six billion uh takes and then you'll be set and they got one take with everything in it which is pretty pretty amazing despite the odds all right so that was just a fun diversion but I love an opportunity to talk about rub Goldberg machines and stare at them all right uh let's do some more fun things about uh expectation products so it turns out there's a product rule just like for probability but for expectation it relies on the random variables being independent and then it's exactly what you expect the expectation of the product is equal to product of the expectations so similar to probability in fact this is really just a generalized version of the product rule for probability maybe I'll give like a quick sketch of the proof how's my time yeah let's do it uh so this expectation of X product y if I just write out the definition uh we just take all the possible outcomes we don't know anything about them but x * Y is a function we evaluated it at that outcome this is the same thing as taking X of the outcome and multiplying by y of the outcome that's the definition of this product function uh and then we multiply yeah we multiply by the probability outcome sum them up that's the deal uh just like with this formula we can rearrange the terms in that sum to where here it was by x value but we could do it by X and y value so we can rewrite this as a sum over uh X and a sum over y other words a sum over pairs X comma y uh of this thing uh but now so there are a bunch of probabilities of outcomes that have the same X and Y so we're just going to write the probability that x equals Little X and Big Y equals little Y and multiply that by x * y okay that's rearranging uh terms with the same X time X and Y value like we did uh last class and then this probability is a product right by the product rule for for probabilities X and Y are independent means that these two events are independent for all X and Y and so uh we can rewrite this as probability of X equaling little x times probability of Y equaling little Y and now we have a s a double sum over some things involving X and some things involving y sorry for the pun I didn't mean to say some all the time there uh things involving X and things involving Y and we know from way back in our summation formulas we can separate out the summation over X from the summation over y take their product and if you do that you get exactly uh this formula with X and with y and so I didn't write it here but this is x in the range of X this is y in the range of Y and so that's uh that's why this works so it's really just using product rule for probabilities cool uh let's do an example with that suppose we've invented a new game which is We Roll two independent uh six-sided Dice and then the score is the product of those two dice not a typical game usually some but uh let's say we take the product what is the expectation of that product well if they're independent we know it's the product of the expectations and expectation of a die roll we covered last time it's a 3.5 so this is 3.5 squared which is uh 12.25 okay weird number there it is whereas if I take the expectation of D1 * D1 also known as D1 squared D1 is not independent from D1 in fact is the most dependent it could be they're the same same random variable uh I claim I do not get 12.25 even though each of these individually has an expectation of 3.5 uh let's just compute that or get an idea how we would compute it think could just use the the definition we have or really the summing uh summing over X in range of X so we have six possible outcomes for D1 and we want the probability that D1 equals Little D times Little D right that's the the second version of expect second formula for expectation uh this is just 1 six for fair dice I I didn't mention fair but I mentioned I meant to say fair dice so this is the Aver uh sorry didn't get this right I'm missing a a product right this is this thing so sum over the different die rolls but now we have D squ here okay and I can never remember the formula for sum of d uh sum of squares but we get 1 16 times that sum of squares you know 1 2 + 2 S Plus 3^2 up to 6^ S which uh turns out to be 15.1 1666 so bigger kind of weird but there you go all right so this was product rule for for two random variables of course if you take the product of n random variables you can do the same thing uh if there's product of nxis here you just get a product of n expectations of XIs so when things are independent life is good what about division we spend all this time adding and multiplying probabilities and expectations can we divide them uh the short answer is no never divide random variables ever I'll show you why uh don't divide okay one reason is um expectation of 1 /x this is sort of if if we could do this we could multi we could multiply it with something and end up dividing uh does not equal one over expectation of x uh in general or probably usually and here's one example uh where this is uh particularly striking they're very different um uh suppose you have X this is almost a uh an indicator random variable but it's going to be instead of one or zero it's going to be one or minus one so let's say it's one with probability a half and it's minus one with probability a half okay what's the expectation zero it's just the average because they're equally likely uh so one over the expectation is not a very good number it's like infinity or something this is bad one over zero uh whereas if I take uh expectation of 1 /x well one over one over one is just one one over minus one is also minus one like one over doesn't do anything to X this expectation of 1/x is expectation of X which is zero so this thing is like infinity or just undefined and this thing is zero so they're very different from each other and in some sense uh you should just never do this this is a reasonable quantity to think about don't don't think about this one although that seems backwards given here given what we showed here um here's here's a more here's a real world example this is based on a a paper in computer architecture where they're trying to argue that one architecture I don't know if it was P1 or P2 I think this a little divorced from the original example um so there were two different processor architectures for solving three different possible workloads these were like benchmarks that they could evaluate them on and uh processor one on workload one took 10 seconds to complete processor 2 took 16 seconds uh and so on there's a table here you run each of these processors for each of these workloads and you see what you get let's say these are each uh equally likely 13 probability of occurring and you want to know which one is better and this is one of those things where you think well either I should take the expectation of the ratios or I should take the ratio of the expectations um if you let's compute the the expectations individually first so if I add these up I get 30 there's three of them so the uh expectation is 10 and here it's also 10 okay so in fact the the ratio of the expectations which I think is a reasonable thing to do so let's just call this P1 performance over expectation of P2 performance uh is one so in fact these are equally good in expectation okay this is because if they're equally like the intuition why you should do this is in each if you assume that these workloads are equally likely then uh yeah you you should be Computing how long will your process take given a randomly chosen workload according to that distribution okay so you should be summing all these up before you do any kind of ratio but what the paper did was take the expectation of the ratio and with these particular carefully chosen numbers but they roughly match those at the paper uh you get a funny fact which is uh if you take the expectation of P1 over P2 this is a ton of arithmetic so I will not do it uh we get 10 over9 so it looks like um these are running times so smaller is better that looks like P2 is better than P1 but if you take the ra the expected ratio of P2 over P1 it's also 10 over 9 so clearly P1 is better than P2 uh why is this happening because expectation of ratios is just something you should never do and is meaningless okay or takeen a different way if you want to lie with Statistics here is a great way to do it take expectation of ratios you can make you could support any story you like uh at least with these if you're lucky the numbers are close then uh you get this funny funny Behavior this is what you should do if you don't want to lie and you want to compare performance okay don't divide random variables I think this is all you'll have to do with dividing random variables uh cool but multiplying is good as long as they're mutually independent any questions cool so the last topic for today is variance so expectation was a way to take a whole function a random variable and reduce it down to one number now we're going to talk about how to take a whole function and reduce it down to two num expectation and variance and I'll say a little bit why these are maybe the first two numbers you might care about there are more of course you might care about the whole distribution and in some sense what we've been doing what we did with the these guys the union bound of the Murphy's Law was trying to capture a little more than just the expectation but just trying to understand how much is at the zero spot versus everything else and next class on Tuesday we'll we'll talk about a generalized form of that where there's some cut off and we want to know how much how much probability is out beyond the cut off those are called Tales of a distribution but today we're going to just in the rest of today we're just going to look at one more number that measures some kind of like well variance uh it it tries to measure how good of an approximation is the expectation that's I think a good way to think about it and I'm sure you've seen variance in some form before but uh maybe motivating example is investment AKA gambling sorry for the Dig uh but uh let's consider an investment process XI which is um plus I with probability a half going to be a generalized version of the one I just did uh and minus I with probability sorry not I2 probability one2 okay uh so like X1 is like I invest $1 and 50% chance I double my money 50% chance I lose my money uh x 100 is I put in $100 and 50% chance I double my money 50% chance I lose all my money or I could put in you know a billion dollars do are these exactly the same investment strategies I mean you would probably say with when I is larger this is a higher risk investment if I put in a billion dollars that's very scary I could potentially lose a billion dollars of course I could also win a billion dollars so that's like high risk High reward if I invest $1 like yeah I'd play that game why not it's for just for fun okay uh so if we look at the expectation we learn nothing expectation of all of these is zero just like it was in the eal one case uh variance is going to try to capture uh something involving I okay let me Define variance it's kind of a big formula but not too big once you stare at it long enough so many brackets though alternating a square and round all right so uh what we're going to do is take the deviation from the mean x minus the expectation of X so this is a random variable it's it's just like X but we subtract the expectation of x and by linearity of expectation that means this may be worth writing down the expectation of x minus the expectation of this braced quantity here is is zero because uh linearity says well this is expectation of x uh minus the expectation of the expectation of X but uh the expectation of the expectation of X is just the expectation of X because um while X is a random variable I mean expectation of X is also a random variable but it's doesn't change right expectation of X is a single number uh you reroll your dice the expectation doesn't change only X the the X the value of x can change because it's a function but expectation of X is a constant thing so its expectation is itself okay so these cancel okay so in other words I'm taking X which has some expectation but then I'm shifting it over so it's uh now its expectation is zero I'd like to Center things on zero additively then I'm going to square it which is maybe a little weird we'll talk about why in a second uh and then I take the expectation of that because while this thing is constant this thing is not so if your expectation happens to be zero this is just the expectation of X squ whereas before we're taking the expectation of X now we're taking expectation of x s and for X squ to be nice we're going to first shift it so that its mean is zero okay those a lot of words uh in this example the expectation was already zero and so the variance of XI is just the expectation of x i s which is uh get this right oh yeah fun thing about square is it makes positive and negative numbers treats positive and negative numbers the same so the what is x i squ it's always i^ squ no matter how the coin flip occurred if it was plus I the square of that is I squ if it was minus I the square of that is I squ okay so U this is part of the magic of variance that we treat negative and positive numbers the same and so uh the expectation of I squ I I squ doesn't depend on anything so it's just I squ and for this reason we also often Define uh the square root of variance to be the standard deviation uh which we usually write Sigma subx here is going to be square root of the variance of X so you can use whichever of these you want I mean they're the same up to a square root or a square uh but it's a second number Sigma subx I here is equal to I so while expectation told us nothing about the risk of our investment Sigma told us exactly what the risk is and this very set simple setup okay let's go on to the next topic which is y squared try to give you some intuition okay uh y^ 2 there are many possible answers to this uh but probably the simplest thing is to try what if I didn't put this Square here and I left the formula otherwise the same what if I did expectation of xus expectation of X what would happen then yeah zero I wrote it right here expectation I I saying oh this shifted things over so the expectation was zero so if you remove the square you just are going to get zero this would be a useless number right so we want to put something here one one possible answer now uh there different things you might try to do um one would be uh expectation of x minus expectation of x to the P for some uh number P what we did just did was two another thing you might try to do to to fix it is absolute value and just for fun we could also raise that to the PE power and uh so these are natural things to try I would argue especially if you know anything about Norms or you know geometry and stuff um and it turns out all of these are interesting question yeah i s or are you setting the expectation of ah I uh I wasn't setting anything I was claiming that x i squ as a random variable is always I squ because x i is either plus I or minus I and in both cases the square of that is i s so this is inside the expectation yeah and therefore the expectation doesn't have anything to do it's a constant not a not much of a random variable okay back to these guys uh these have names this is uh for p greater than one this is uh the PE uh Central moment uh and this one for p greater than equal to one is the pee uh probably Central this one's not talked about nearly as much uh but I would call it the absolute moment and they're all interesting uh for this one p equals 2 is called variance that's the one we're talking about uh P equals 3 with some normalization which I'm not going to do here it's called the skew of the distribution P equal 4 is something it's got a really fancy name just showing off my my English knowledge is called curtosis uh I assume people looked at other uh moments too although usually they just called you know first moment second moment third moment whatever these are all very interesting quantities they capture different things about the distribution variance is how far from the mean are you on average in some sense skew is sort of how to the left or to the right is your distribution cryosis I have no idea I'm not a uh St statistician but they all have some meaning and you can do the same thing with absolute moment you can also do it without this uh normalization term then you remove the word central uh these are interesting cool quantities so there's no particular reason two is special and in fact if you do absolute values you can even go down to one and you get a reasonably interesting thing um but often we really like uh the version without absolute value because it's smooth because it's infinitely differentiable whereas absolute value is not differentiable so it's just uglier to work with analytically so historically this is why we go here I think this is also a very interesting measure and you could use it just we're not going to use it in this class okay they give you different they give you different things because this square is exaggerating larger differences right if you square something that's big it has a bigger impact than if you square something that's small so if you care about deviation from the mean and you really you care a little more about big deviations than small deviations then raising this to some power is a meaningful thing and that's that's why we do it uh there are some other reasons um if in in the stat statistics world if you know that X is normally distributed which actually is kind of common Even in our world if you take a sum of a bunch of uh bruli trials this is um uh has some name right not the bruli distribution so like brui trials are coin flips you take a sum of them you get uh a binomial distribution I should know that uh then uh there's something called a central limit theorem that says that's very close to a normal distribution normal distribution is you know gaussian bell curve uh and it turns out if you know the expectation and the variance of your distribution and it's normal then you know everything about that distribution so because Normal distributions are frequently occurring in nature both in real life and in mathematical life uh like when we sum a bunch of uh coin flips uh these this expectation variant are often the first two things you should care about maybe they're the only things you need to know if you want to understand your distribution and then because I'm a geometer I have to say because we live in a because we're living in a ukian world you know in ukian world uh you you square things and then you take the square root at the end right this is this is the norm this is actually the distance between two points in uh size of s Dimensions okay I'm a I'm I'm usually a two-dimensional geometer maybe three dimensions but if you're crazy you can live all the way up in size of s Dimensions where you can represent you can represent a function as just like a big Vector very popular these days um uh just you write down all the possible for every outcome you write what x is that's a vector in s dimensions and if you also write down for every possible outcome what expected value of x is uh it's the same every time it's mean mean mean mean mean mean mean very mean then and you take the distance between those two vectors that's exactly variance or sorry standard deviation so if you live in ukian world that's why two is natural of course you can live in any P norm and then you get all these other things all right let me tell you some cool properties of invariance uh here some of these I'll prove some of these I'll just State and you'll prove them in recitation tomorrow but we'll see some we'll use them to analyze some examples okay so our first one is translation and variance so what I'd like to do is um take some random variable X and add a constant to it now when I add a constant to X I change the expectation right it also shifts over by linearity with variance it actually doesn't change the variance of X plus C equals the variance of X that's a nice property probably true of all of all of these definitions all of the moments but we'll just do it for variance uh so let's prove it so as I said the expectation we need that in order to compute variance expectation is always what you should do first by linearity this is expectation of X Plus expectation of C but expectation of C is just C because it doesn't change depending on the random event so variance of X plus C we just plug in the definition here uh so we want expectation of x minus the expectation of X but now X is X plus C so we want expectation of first term X is so I guess this is going to be for the square then uh the first X is actually X+ C and then we have minus the expectation of X plus C uh and then that's squared and then we close the expectation and this thing is what we just computed here so we have in inside the Parn here we have x + C minus uh expectation of X plus C in parenthesis and get my red chalk the C's cancel right so we end up with x minus expectation of x squared and expect expected um and that's exactly the definition of variation of X variance of X okay so that's kind of nice okay couple more one is here is another way to compute variance uh I can't say I have a lot of intuition for this one but it works out nicely mathematically variance of X is the difference between the expect the squared value expected and the expected squared the square of the expectation versus the expectation of the square with this sign turns out to be right uh I guess this is sort of related to whether you do things inside or outside the expectation this is going to exaggerate larger values of X more and this uh adds them all up first and then squares so it's less dramatic and the difference in those dramas are exactly the variance turns out so we're not going to prove that you'll prove that in in recitation tomorrow but let's do a simple example of it so suppose we have a bias coin flip uh where the probability of heads is p and uh take an indicator random variable H or equivalently this holds for every indicator random variable uh H we know the expectation of H is just probability of one which is p the variance is quite nice to compute with this formula because again with with um indicator random variables they're always zero or one and so when you square them nothing happens right so this is uh if according to this formula it's expectation of x^2 of h^2 minus uh expectation of H outside squared and H squ is equal to H because if you take zero you square it you get zero if you take one you square it you get one so this thing is just uh expectation of H which is p and then we have minus expectation of H squ well expectation of H again this is p so this is p^2 so P minus p^ 2 also known as P * 1 minus P looks a little bit more natural um it's probability of heads times the probability of Tails turns out to be the variance in this case if you look at standard deviation it's the geometric mean between p and one minus P which is kind of a nice number okay next one uh do I want this example I might let's go over here so expectation had this nice linearity property how about variance well it has this nice nonlinearity property it's not usually called nonlinearity of variance but it's kind of linear is uh so the the nice part um yeah all right let's start with this part the less Nice Part the less linear part so while variance was scale inar it was um translation invariant is that what I called it yeah if I add something to X it doesn't change the variance If I multiply by something it it better change the variance right I mean that's the whole point of the investment say If I multiply XI by a million that has higher risk so variance should go up uh and it's a little funny because everything's squared so it doesn't go up linearly it goes up by c^ squ instead of C of course if you take standard deviation it is linear in this special case like if you take standard deviation of C * X it's going to be C * standard deviation of X okay cool but what if we add to to random variables um here unfortunately we need Independence so if X and Y are independent random variables then uh the variation the variance of the sum is equal to the sum of the variations okay so it's not as nice as we had with expectation where it just always holds but still pretty good so if you sum things and multiply them by things it gets a little weird you've got to square all of these coefficients but these are sort of the analoges of the things we have in expectation this is again something we will uh you will do in lecture uh so I'm going to just do an example two examples related examples okay so uh let's say we have n mutually independent coin flips and I want H to be the number of heads this is what we were calling n before and maybe on this board H we've often called this H we've counted this before we've done the expectation uh twice now we did it last lecture and we did it at the beginning of this lecture the expectation of H is p * n if the probability of heads is p okay what about the variance uh and the idea just like when we computed expectation we split up H as a sum of nhh where hi was uh an indicator ROM variable for whether the I flip was a heads hi is indicator ROM variable for I flip being heads uh and so then we just took the sum here and so each of them had a probability P of occurring we so we got n of them got P * n let's do the same thing with variance because we're assuming these coin Clips are independent with the expectation we didn't have to but with variance we have to so we can use this property this is going to be equal to by this kind of linearity of variance uh sum of variance of hi for IAL 1 to n cool so then we just need the variance of each individual coin flip which we just did right bias coin flip uh probability P of heads variance was P * 1 minus P so we get n * P * 1 - P okay uh I find it a little hard to have intuition for this but uh it's a number here's here's another example last example will give you some intuition for why this is interesting back to gambling I mean investment uh let's say you have n doll you want to invest back to my great investment strategy here you can you can let's this xn now I could either invest I I could invest n in this thing and I get a Varan of n s i could win n lose n okay but now suppose there's not just one investment like this Suppose there a whole bunch of Investments like XI let's call them stocks right you have uh K different stocks and I can choose I'll invest so many dollars in this stock so many dollars in this stock so many dollars in this stock and let's just assume like this in great investment uh setup that each of them will either double your money or you lose all your money does this make a difference is it any different to invest in K different stocks equally versus investing in one stock everything okay I claim they're different so let's do it in general and then we can plug in different values of K so let's say we have K different stocks and we're going to equally uh split our investment of$ across those stocks and let's say that each stock SI is plus let's just do plus one and minus one simple of course you could generalize each of these occurs with probability a half just like our previous Investments but now each stock is uh let's say they're independent mutually independent this is probably not a reasonable assumption do not invest in the stock market with this advice but if we assume that they're mutually independent then uh what we get if we want the variance of our overall outcome which is uh sum IAL 1 to n of n / K time SI right this is how many dollars I assume you know the basics of investment if you invest more dollars then you're G to either get more back or lose more just like this uh XI but I pulled the pulled it outside so if we invest n overk dollars in this stock then we get we end up with this many dollars in the end we add it up over all stocks okay now we want to know the variance of this investment strategy which I'm going to call risk so probably risk is standard deviation but if variance is lower so we standard deviation uh so the variation of sum if they're mutually independent uh is exactly the sum of the variations of the uh insides which is n over K * SI that was uh First Property um and then the second one if we want to pull this outside it gets squared so we get sum I = 1 to n of n / k s time variance of Si variance of Si is one I think this was the early example we did in general the the variance of this x i we're we're interested in the case I equals 1 over here uh the variance was i^ s so that's just one okay so we get the sum of something that doesn't depend on on I so we just get n of these things we get n cubed over k^2 this is our variance and if you take the square root you get the standard deviation okay and if you hold n fixed of course the more dollars you put in here the more risk you expect uh so it turns out to grow uh if you're doing standard deviation um you get n to the 1.5 risk so don't invest a lot of money maybe um we get divided by K the cool thing is the bigger K is the smaller you risk so this is why I conjecture this is why Financial people say diversify your portfolio because the more things you invest in if they're mutually independent it reduces your risk uh it's kind of a fun conclusion to variance of course if they're mutually dependent which stock market tends to be somewhat then you don't get this as much of a benefit but you probably get some benefit by diversifying so it's a good thing to do when you grow up and uh invest your money all right that's it for today one more lecture on probability Tuesday