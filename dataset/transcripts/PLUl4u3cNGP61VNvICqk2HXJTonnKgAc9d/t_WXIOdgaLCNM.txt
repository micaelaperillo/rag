Hello out there. Can people hear me? Oh, nice one. People at least can uh intuitive what it means when I wave at them. Uh okay. Uh so today we are going to uh you know continue our discussion of probability and in particular we're going to talk about um a very naughty word uh what we call random variables. Um unfortunately a random variable is neither random nor a variable. Uh so like it it's just called this for historical reasons. Um it's very silly. I'm just going to be referring to them as RVs through the uh through the lecture. Um this is what it stands for, but uh you know it's it's it's a very silly name. It's it's a bit of a misnomer. Okay. So what is an RV? Okay. So basically what an RV is like it's neither random nor a variable. It is a function. uh from our sample space to say the real numbers. Okay, that's all it is. It's just it's just a function. Um you can have you know like the the co-omain r is not particularly important. You could have uh you know you could just easily have co- doain n. Uh you could also have complex valued rs but like you know we won't we won't be looking at those in in this class um for now. You can just think of it as you know real valued function. Okay, whose uh domain is the sample space. Okay, so for instance suppose our sample space or you know suppose uh you know our our experiment is we're we're flipping three coins. Okay. Uh so we're going to assume for now that they are fair. They're you know they're they're you know normal physical coins. They're independent. If you flip one, it's not going to affect the uh the outcome of any of the others. Uh so what is our sample space in this case? What does our sample space look like? Am I giving the right lecture? This this is 61200, right? Okay. Uh if we flip three coins, what are the possible outcomes? Yeah. Yeah. each of them being heads or tails. Okay, so if we flip three coins, our sample space is going to be so each coin can take either, you know, it can be either heads or tails. There are three of them. So we're going to be considering, you know, ordered triples of, you know, either heads or tails. Okay. And maybe we've got this function f that maps uh an outcome omega to the number of heads in omega. Okay, so that's one possible uh one possible RB, right? We're taking an outcome and we're mapping it to some number, right? The the number of heads. Okay, another one could be uh maybe instead we just look at you know is the first uh coin flip heads or tails. Oh, maybe I should not write it as uh that just yet. So one if and only if uh first flip is heads and it's zero otherwise. Okay, so these are two examples of of RBs. There are functions that map outcomes to real numbers. Okay, does this make sense to everybody? We could have one more that's like h of omega is one if uh all three coins match zero otherwise. Okay. So these are just you know examples of RVs that we could have. Uh so G and H in particular have a special property right they are either one or zero. So this is what we call an indicator. So all that an indicator is is just an RV whose co- doain is 0 and one. Okay, that's that's all it is. It's just a binary RV. Okay. uh and uh RVs and in particular indicator RVs naturally give rise to events. Does anybody see how Yeah. Yeah, exactly. So the answer was you know in particular if you have an indicator RB right you can look at the set of outcomes where it's one right that's an event or its complement is the set of outcomes where it's zero okay so uh more generally if we have an RBF uh and a value x uh we can define the event. So a slightly weird notation here fals x okay uh by the set of outcomes omega where uh f of omega equals x. Okay. uh or you know you may also have seen the notation uh f inverse of x. Okay. Um inverse here meaning inverse as a relation not as a birectionraction. Uh but if you haven't seen this notation don't worry about it. Um this is what we uh this is what we call it in terms of probability. Okay. Uh and uh you know vice versa. If you have uh an event, you can define the uh indicator of that event. So if we have uh an event A corresponds to the event here, this is the notation I was trying to use before. Uh blackboard bold one of a Uh this notation here just means the indicator right that is one precisely for the outcomes in a. So if omega is in a and zero otherwise. Okay. Does that make sense? Does this uh correspondence make sense? Yeah, question explain the notation again. Okay. Yeah. So, um basically we are taking an event and using it to define an indicator RV. Okay. Um this notation is just how we write it. Uh this is called the indicator of A. Okay. And all it means is just the the indicator RV which takes the value one on all of the events in A and it takes the value zero on all of the events that or sorry all of the outcomes that aren't in A. Okay. Does that make sense? Cool. Uh any other questions? Okay. Or actually maybe maybe we should keep that visible for now. We can also define uh a different kind of event f greater than or equal to x right. Uh so that one you know obviously doesn't make sense uh for for complex numbers but you know for real numbers or any any subset of the reals this this is perfectly fine. Okay as you might expect uh this is going to be the event uh that contains all of the outcomes omega such that f of omega is greater than or equal to x. Okay. So analogous to what we just defined. Okay. Um and more generally we could say you know f in some set s or maybe t right because s is our sample space. This is going to be omega such that f of omega is in t. Okay, does that make sense to everybody? Okay, so as you might expect, we can, you know, condition on these events uh just as we can, you know, condition on any other events. uh we can also have you know independence of random variables just as we have you know independence of events. Okay sorry for instance now we could talk about the probability that uh uh what did we call them uh f = 2 conditioned on uh h = uh 1. Okay. So what is this probability? What is it saying? Yeah. Oh, is that a hand or just Okay, sorry. Yeah. Yeah. Exactly. So we're looking at the probability that we have exactly two heads conditioned on all three coins being the same. Okay. So you know we could think of these as two events A and B. uh you know if we actually you know write out what this means this is the probability that uh f= 2 uh and h = 1 uh divided by probability that h = 1 right uh and this probability in particular is zero okay so all three coins are the same. It's impossible for exactly two of them to be heads. Okay, does that make sense to everybody? Like we're basically doing exactly the same thing with these weirdly written events as we did with, you know, any other events. Okay. Uh and we can also extend this to independence. But I should point out here that things are going to diverge a little bit. Okay. Uh so we're going to say that two uh RBs are independent. If and only if for all possible uh x and y. Uh probability that f = x uh and v = y equals the probability that f= x times the probability that t equals y. Okay, so it's similar to what we had for events, right? How is it different? Well, for events, right, we didn't have this quantifier, right? We just said that two events are independent if the probability of their intersection is the same as the product of their probabilities. Right? Now we have to quantify everything. Right? It's not just enough to say you know the the event that each of them you know takes value zero is independent. We kind of need that for all possible pairs of values that they could take. Okay. Does that at least intuitively make sense? Yes. No. Some nods, tentative nods at least. Okay. Uh or you know just as with uh you know normal events we could equivalent equivalently say uh for all xy either uh probability uh that t equals y equals zero. Whoops, not of or uh probability that f= x conditioned on p= y is the same as the unconditional uh probability. Okay, so if you prefer you could think of it in terms of uh conditional probability. So we're basically saying that two random two two RBs are independent if and only if like all of the events that they generate you know the events f= x and gals y uh those are all independent events and you know intuitively it's saying that uh if we know something about the the value that that g takes for instance it doesn't tell us anything about what value f takes. Okay. So, example uh where were they? FG and H. Are those independent? So, are F and G independent? Who thinks F and G as I've defined them over there are independent? Show hands. Yeah. Who thinks they're not independent? Few hands. Who's still fast asleep? Oh. Oh, that's unfortunate. Um, I'm sorry. Me, too. Uh, okay. So, does anybody want to give me an explanation one way or the other? Presumably, somebody is not asleep. I don't need an explanation for why you're asleep. Anybody else? No. Yeah. Yeah. Exactly. So if g is zero right that tells us something about the value that f can take right it cannot be three. Okay. So the events g is zero and f and three or f is three are not independent events. So the rvs are not independent. Okay. Uh or perhaps even more obvious if f is zero right that tells us exactly what g is right g must also be zero. Okay. Uh what about uh f and h. So f is the number of heads. H is one if and only if all three coins match. Who thinks those are independent? No. Who thinks they're not? Few hands. Okay. Why? Yeah. Yeah. Yeah. So f basically tells us everything we need to know about about h right like h is just a function of f and well neither of them is is trivial. Okay so if we know if we know f no matter what it is we know what what h is okay so these are also not independent. What about G&H? Who thinks G and H are independent? Hans. Who thinks they're not? Oh, nobody. Okay. Why? Yeah, sorry. Yeah, intuitively that's what's going on. The the first flip doesn't tell us anything about the other two. And in particular, it doesn't tell us anything about, you know, whether the other two match each other and the first flip, right? Uh so we could, you know, put it all into here and, you know, figure it out. But uh you know I will leave that as an exercise to the reader. Okay. Uh basically you know no matter what happens you've got uh uh one in4 chance that the other two coins are going to match the the first one. Uh and that matches the probability that uh that they all match to begin with. Okay. Okay. Does anybody have any questions about uh independence before we move on? people reasonably happy. Okay, so next topic is distributions. Okay, so for those of you who have seen RVs before, uh this is probably the context or this is probably, you know, the context in which you've seen them. Uh historically, this is where they started and more or less where they got their name. Uh but basically um the distribution uh is is it's basically a fancy way of you know talking about the RV without the underlying sample space. Okay, it's talking about the probabilities that it takes each uh each individual value uh in its range. Okay, so there are kind of two equivalent ways to uh to talk about it uh at least for real valued uh RVs. So for an RV uh f uh we define the probability mass function often abbreviated as PMF uh of omega oh sorry x and this is just the probability that f equals = x. Okay. So, does that definition make sense to everybody? Right. the the PMF of an RV is just talking about the probability that that RV takes each individual value in its range. Okay, so you could think of this as uh you know a probability space uh over R. Okay, so that's that's how RBS originated like you know talking about probability spaces over R. Equivalently, you could also define the cumulative distribution function. CDF as CDF subfx. This is just the probability that uh f is less than or equal to uh x which is the sum y less than x of the probability that uh f equals x. Okay. Uh so I should warn you that um you know if you see like in many other probability classes uh a PMF would or something analogous to a PMF would often be called a PDF uh for uh probability density function instead of probability mass function. Uh and I believe in particular the uh the text does this. Okay. So for the purposes of our class, those are the same, right? Uh if you see PDF, uh you know, think to yourselves PMF. Uh we prefer this notation because this is what's used for uh discrete distributions. But you know, if you have a continuous distribution like uh well, it's kind of hard to talk about the probability that a uniform random variable over, you know, the interval from 0 to one takes exactly a particular value, right? like that's, you know, if you throw throw a dart at a dart board, you're you're never going to hit one particular point essentially, like you you have null events. So it it doesn't really make sense to talk about the probability that you hit a particular point because that's always zero. So instead you look at the the density function instead. Okay. But for discrete distributions, uh you know, it does make sense to talk about the the probability that you hit exactly one particular one particular value. Okay, does that make sense to everybody? So for this class you know PMF and PDF are the same. If you see continuous probability they won't be. So often it is useful to talk about uh RBs in terms of their distributions like in terms of their uh PMFs or CDFs uh and just kind of forget about the uh the underlying sample space. Okay. So, a couple of examples um indicator uh indicators like what does the PMF of an indicator look like? Well, it can only take two possible values, right? So, it's going to be zero everywhere except at zero and one. Okay. And you know at one of those it'll take the value P. At the other one it'll take the value 1 minus P. Okay. Uh and we call these uh we say that these have a Bernoli distribution. Okay. So there uh PMFs like basically look like this. you'll have you know two values everywhere else it's zero. Okay. And the uh CDF is going to look like this. Okay. Oh, wait. No, should be at one. Okay. Uh, another useful one is a uniform random variable. Okay. So a uniform RV right it can take uh you know any value uh with with equal probability. So in particular let's look at uh uh an RV on the set up to n. Okay. So here the PDF of X is going to be 1 / n if and only if uh X is in this set and zero otherwise and CDF of X what is that going to be? Well, you're basically going to have a peacewise function, right? So, it's going to be zero up until one. Then at one, it'll go up to uh 1 / n. At two, it'll go up to 2 overn, etc. Okay. So, you got something that looks like that. So, uh yeah, question Uh oh. Explain again how I got to this graph. Okay. Um so uh well the CDF right is you could think of it as like a a sum of the PDF or uh or sorry sum of the the PMF right. So you know the CDF at uh at one right it's the probability that this uniform RV is no greater than one like how can that happen well only if you you take the value one that happens with probability 1 overn right so at one you know we should get probability 1 over Okay, anything between one and two has that same probability, right? The the CDF of one and a half, right? The the probability that we're no greater than one and a half is the same as the probability that we're we're no greater than one, right? Because we can't take any value between one and one uh one and a half. Okay, so like all the way up to two, it's going to be constant. Okay. And then at two, it's going to jump up by another one overn, right? Because at that point, right, the probability that we're no greater than two is the probability that we take either of these two values, which happens with probability 2 overn. Okay? So we kind of get this like pie-wise step function that uh you know at every integer it increases by 1 over n. Does that make sense? Is there any way that you can, you know, beat random chance? Is there any way that you can do better like win with probability greater than 1/2? Right? It's not really obvious. Okay, so let's simplify the game a bit. So suppose I guarantee that one of the boxes has more than five candies and the other one doesn't. Can you then beat random chance? I'm seeing some nods. Yeah. How how do you beat random chance in that case? Yeah. less than five. Yeah, exactly. You you you pick either box, you open it, right? If it has more than five, that's that's the the box with more, right? You stick with that. If it doesn't have more than five, well, the other one does, so the other one has more. Okay? So, if you know that one of the boxes is is, you know, has more than five and the other one doesn't, then you can win with certainty, right? Like, it's still not probabilistic. Well, I I suppose you could think of it as probabilistic, but like you can definitely do better than a half. Okay. Does this give you any ideas as to how you might win with probability greater than a half, even if you don't know that one of the boxes has more than five candies and the other one doesn't? Any ideas? Yeah. Okay. So, uh your colleague's proposal was do it anyway, right? Uh if you, you know, open up a box with less than five candies, you switch and hope that the other one has has at least more than than uh what you found. If you open up a box that has more than five candies, you stick with it and, you know, hope that the other one has fewer. Um, does anybody see a problem with this? What is the probability of winning? Yeah. Yeah. So, let let me try and rephrase that. Um I haven't given you any probabilities of like you know uh what the probability is that that I've put a certain number of candies in in either box right so it kind of depends on what I've done right there are kind of two cases right if I have split them like you know greater than five and and and less then you win with certainty if I haven't then you win with probability exactly 1/2 right if they're both greater than five right you you win if you picked the uh the bigger one to begin with. And if they're both less than five, then you win if you picked the smaller one to begin with. Okay? So, you're still not doing worse than a half, but you're not doing better, right? Or you're not guaranteeing that you're doing better because, you know, I made no guarantees as to how I was splitting up the candies. I may put fewer than five in both of them. Or, as I did this time, I may put more than five in both. But can you use a similar idea to get greater than 1/2 probability no matter what I've done? What if instead you know that I've put more than two candies in one box and not the other? Can you win in that case? Anyone? If if you know that I've put more than two candies in one box and not the other one, how do you win? It's It's not a trick question. I promise. Like just as what your colleague said with five, you can open up one of the boxes and see whether or not it has more than two candies, right? If you know that one of the boxes has more than two candies and the other one doesn't, right? You can identify which is which by, you know, looking at one of them. Okay, so why don't I start writing some of this down? So if one box so exactly one box has uh you know at least one candy right then you know we open up our box we see whether or not it has any candies and if it does we take it. Okay so we can win. So if we know that two boxes, sorry, if one box has greater than or equal to two candies, right? If we know this, we can win, right? If we know that one of them has greater than or equal to three candies, we can still win. Okay, the problem is, you know, we don't actually know any of these. So, what can we do about that lack of knowledge? Yeah. Yeah. Exactly. So, we don't know, but we can guess, right? like one of these uh 10 things has to be true, right? I I've told you that I've put different numbers of candies in each box, right? So there is some divider. One of these at least one is true. Like maybe I've put zero and one and 10 in the other, in which case they're all true, right? But at least one of them has to be true. So if you randomly guess one of them and just say like like pretend that you know it, okay? and you know adopt the strategy we were just talking about, right? If you guessed correctly, then you win. And if you didn't guess correctly, as uh your colleague in the back pointed out, you still don't lose at least like you still win with probability 1/2. Okay, does that make sense to everybody? So our strat is guess the threshold Okay. And then assume it's true. Okay. So what exactly is the probability of winning if we do this? Well, it's going to be we can use uh the law of total probability. It's going to be the probability that we win conditioned on being correct, right? times the probability that we are actually correct plus the probability that we win conditioned on being wrong times the probability that we're wrong. Okay, so what are each of these probabilities? What's the probability that we win if we made the correct guess or a correct guess rather? One. Yeah, exactly. Okay. What's the probability that we made a correct guess? Yeah, one10enth. Uh, well, it might be greater, but it's at least one/10enth, right? So, it's at least 1 * 110th plus what's the probability that we win? Condition on being wrong. Yeah. 12. Yeah. Right? So if we guess wrong, you know, say without loss of generality, our threshold is uh is too low, we're going to open up a box and we're going to stick with it regardless of what it is. Okay? So we win if and only if we happen to pick the uh the bigger one. Okay? So we win with probability 1/2. And what's the probability that we made the wrong guess? Yeah. Nine times. Yeah. So, what is this going to be? This is going to be what? 55%. Okay. So, it's not that much better than a half, right? But it is better. So, does that make sense to everybody? So we we Oh, yep. Yeah. So we we we don't actually know that, right? Um so it's it's possible, right, that I could have, you know, a large difference in in the number of candies in each box, right? Uh we do know that there's at least a onetenth probability that that uh you know, we picked the right threshold. Like it it could be more than that, right? Um but like you know if there are say two correct thresholds then instead of this you have uh 1x 210 plus 1/2 by 8/10. Oh uh because you know these are the the thresholds that we're picking from right. So like we we do know that there are uh between zero and 10 candies in each box, right? So we're going to pick a threshold that's you know in that range. Okay. But yeah, if if I have uh you know put you know if I've allowed you to pick you know more than one correct threshold then that just you know increases by 5% per you know candy difference between the boxes. Okay are people reasonably happy with this? Okay. So this is an example uh of a problem where we've taken something that isn't exactly a probability problem to begin with but we've turned it into a probability problem and we've given what we call a probabilistic algorithm for solving it. Okay. So, uh yeah, if you do it deterministically, right, you know, there's not much you can do, right? You you pick a box and you know, if I happen to feed you the correct one, then uh you know, you you win. Uh but you can do much better if you introduce randomness into your algorithm and make random choices. Okay? So, you will see a lot more of this in uh in doub uh 61210 and even more of that in uh 1220. And uh Eric, do you remember 5210, 5220? There are other randomized algorithms classes there. There are some uh you know higher level ones whose numbers I forget that are you know entirely about randomized algorithms. So it's it's a a very powerful tool that that is frequently used in computer science to uh you know make your algorithms uh you know more efficient or make them better in in various ways. Okay. Uh, so now that we've finished with our with our candy boxes, unless anybody has any questions questions about candy boxes before we move on? No. Okay. So let's give another example of a very common distribution called the binomial distribution. Okay. Uh so this is a distribution that arises very frequently in computer science. Okay. And in fact we've already been kind of dealing with it implicitly in this class. We just haven't you know named it. Uh it's parameterized by two values. So bin of n comma p. Okay. And uh basically this models uh the following experiment. So we have n coins. Uh these are all, you know, they're coins. They're independent. Uh you know, flipping one doesn't affect, you know, any of the others. They're all mutually independent. Okay. And they're biased, right? They're not necessarily fair coins. Each comes up heads with probability P. Okay. And then the RV X which is the number of coins that come up heads is or has the uh binomial distribution. Oops. Run out of frame. Sorry. Okay, so we've already seen this uh you know in particular with with P equals 1/2, right? Flip several fair coins and uh you know count the number of heads. uh we did that at the very beginning with n equals 3. Okay. But more generally, right, you can flip n coins and they can be biased towards heads with probability p or away from heads as the case may be. You could also model different things like you know maybe you've got a very complicated device. Each component you know may or may not fail. They're all independent. fail fail with probability P. Uh then you could also count the number of failures. This would also follow the uh binomial distribution. Okay. So can anybody tell me the uh PMF of a binomial distribution? Okay. So, sorry. Could you repeat that? Nice. Okay. Why? Yeah. So if if you look at you know sequences of uh of these coin flips right um there are n choose x sequences right that have exactly x heads right and if you have a sequence with exactly x heads right uh you know each index which is a heads that's heads with probability p and each index that's tails of which there are n minus x of them is tails with probability n minus uh sorry 1 minus p. Okay, so product rule says that we can multiply all of those together. We get p to the x 1 - p to the n minus x, right? And now each of those uh each of those sequences, you know, they're distinct outcomes. Uh each occurs with the same probability. So we just count them and then multiply by that probability. Okay, does that make sense to everybody? Okay. Uh, what about the CDF? Can anybody tell me what the CDF of the binomial distribution is? Yeah. Mhm. Yeah. Exactly. So let's say from i = 0 to floor of x of pmf of i. Okay, so that is frequently how you'll want to compute a CDF in practice, right? Like it it it is often, you know, easy to figure out the uh the uh PMF uh and then you can just, you know, compute sums to give you uh to give you the CDF. Um there are other cases where it's easier to go in reverse, right? Like maybe the CDF is easier to uh to compute and then you can compute finite differences to get back to the PMF. Okay, but uh you know the fact that that both of these are equivalent ways of talking about the distribution uh means that you know you can use either one of them and the other one can be computed fairly easily from from the first. Okay. Uh okay. So unfortunately right there are summations involved though. So that can be a little bit unwieldy. uh so it is often easier rather than dealing with the CDF you know exactly to approximate it and this might be good enough for you know whatever purpose you you have in mind okay so let's see how we can do that or actually let's um For the sake of notation, let's abbreviate uh the the PMF. Let's just call it F. Okay. So, and big F can be the CDF. Okay. So, how could we approximate f of say alpha * n. Any ideas? Well, first why don't we write out you know what exactly this says in terms of factorials. So this is going to be n factorial / alphan n factorial uh time 1 - alpha * n factorial times uh p to the alpha n 1 - p uh to the 1 - alpha n. Okay. Now, who remembers sterling? Oh, hand. Yeah. Okay. How can we use sterling to uh to simplify this? Oh, was that not a hand? Oh, sorry. Well, we we've got a bunch of factorials here, right? If we approximate each of these factorials using sterling, then we're going to end up with the following. So n factorial is going to be roo<unk> 2 pi n uh actually let's um let's kind of split up the terms nicely over<unk> 2 pi alpha n uh<unk> 2 pi 1 - alpha n. Okay. Uh then multiply this by okay. So for the top we have uh n minus uh sorry n / e to the n. Uh on the bottom we've got uh alpha n / e to the alpha n uh 1 - alpha uh n / e to the 1 - alpha n. Uh and then these p and 1 minus p terms we can just uh keep as they are. to the 1 - alpha. Okay, so a little bit unwieldy as it is, but we can simplify it. Okay, uh how can we simplify this? Well, those n's and e's over on the right, those aren't really doing anything, right? We've got n / e to the alpha n e 1 - alpha * n. And here we've got n / e to the n. Okay, so we can get rid of those. This is just going to end up being 1 / uh is it uh alpha to the alpha n 1 - alpha to the 1 - alpha * n. Okay. And over here we have 1 /<unk> 2<unk> alpha * 1 - alpha * n And then our P terms remain. Okay. What next? Well, let's try shoving these P's and one minus P's in here. Okay, so this carries down. We've got uh p / alpha to the alpha n time uh 1 - p over 1 - alpha to the 1 - alpha* n What can we do next? Well, what happens if we take the log of this and then raise two to that power? It's going to be the same thing, right? But as it turns out, that's going to make our life a bit easier. So, this continues to carry down. So, two to the uh this is now going to be n times alpha log of P / alpha plus 1 - alpha * log of 1 - P / 1 - alpha and parentheses here. Are any of you here physicists, proficient in physics? Does that uh exponent look familiar to anyone? Well, as it turns out, this exponent, even though it looks kind of ugly, uh it's always negative except for when uh p equals alpha. Okay, so what happens when p equals alpha? Well, we've got a log of p over alpha here. That's zero. Log of 1 - p over 1 - alpha, also zero, right? So the exponent is zero there. And if p is not equal to alpha, uh then you know because log is concave, this is just going to be uh it's going to be negative. So we've kind of got this this bump right at P= alpha and it's going to get exponentially smaller everywhere else. Okay, does does that make sense to everybody? So if we actually put in some concrete values, so for the sake of uh simplicity, let's say P equals.5 uh N equals say 100, right? Evaluating, you know, 100 choose whatever is going to be a bit of a pain, right? But if we just use this approximation, then uh you know, we can get uh we can get actually some pretty good approximations. So if alpha also equals 0.5, right? What happens? Well, as we've just established, this exponent is zero, right? So, this this term is is one here. We've got 2 pi alpha * 1 - alpha * n, right? So this is going to be 1 over<unk> pi uh sorry 1 over<unk> 2<unk>i * 12 * 12 * n. Okay. So that's that's going to simplify to pi n /2. It's going to be uh f of 50 is approximately equal to uh 1 over square<unk> of pi * 50. Okay. And as it turns out, this is about uh 0.08. So pretty small. What about alpha equals 0.25? As it turns out when alpha is 0.25 right like this you know it doesn't change too much but this term starts decreasing exponentially so we actually end up with uh f of 25 is about uh 10us 7. Okay. So if you flip a 100 coins, right, the probability that you get exactly 25 heads is minuscule. Okay. Uh and I think the last lecture, right, Eric, is about tailbounds. Do you remember? Okay. So, uh yeah, last lecture we'll we'll discuss how to get bounds on the uh uh on the CDF uh instead of just the uh the PMF. Um, and yeah, as it turns out, it's kind of funny, right? The probability that you get at most 24 heads, right? How do you think this compares with the with the probability that you get exactly 25? Turns out that this is actually smaller than uh f of 25. So because it's decreasing exponentially, right? The CDF is also decreasing exponentially. So it turns out that it's it's more likely to get exactly 25 heads than to get fewer than 25 heads, which might be a little bit counterintuitive, right? like for larger numbers that's that's definitely not true, right? Like it's much more likely that I'll get uh fewer than 50 heads than exactly 50 heads, right? Because the uh the probability of exactly 50 heads is 8%. Right? So the probability of fewer than 50 heads is going to be, you know, half of the complement of that, right? So what's that? 47% 46% I can't I can't math. 46%. Right? So, it's much more likely that you get fewer than uh 50 heads than exactly 50 heads. But then because it decreases exponentially as you get close to the tail, uh turns out it's it's more likely to get exactly 25 than fewer. Okay. Uh so that's all we have time for today or sorry that's all we have planned for today. We have a little bit of time left over for questions if you want to uh come up and ask and otherwise we will see you next week.