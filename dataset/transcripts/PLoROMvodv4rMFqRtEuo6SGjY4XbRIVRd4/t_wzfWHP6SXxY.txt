hello everyone and welcome back into week four um so for week four uh it's gonna come in two halves so today i'm going to talk about machine translation related topics and then in the second half of the week we take a little bit of a break from learning more and more neural networks topics and talk about final projects but also some practical tips for building your network systems so for today's lecture this is an important content for lecture so first of all i'm going to introduce a new task machine translation and it turns out that task is a major use case of a new architectural technique to teach you about deep learning which is sequence to sequence models and so we'll spend a lot of time on those and then there's a crucial way that's been developed to improve sequence to sequence models which is the idea of attention and so that's what i'll talk about in the final part of the class um just checking everyone's keeping up with what's happening so first of all um assignment three is due today so hopefully you've all gotten your new dependency passes parsing text well um at the same time assignment 4 is out today and really today's lecture is the primary content for what you'll be using for building your assignment four systems switching it up for a little for assignment four we give you a mighty two extra days um so you get nine days for it and it's due on thursday on the other hand do please be aware that assignment 4 is bigger and harder than the previous assignments so do make sure you get started on it early and then as i mentioned thursday i'll turn to final projects okay so let's get straight into this um with machine translation so very quickly i wanted to tell you a little bit about you know where we were and what we did before we get to neural machine translation and so let's do the pre-history of machine translation so machine translation is the task of translating a sentence x from one language which is called the source language to another language the target language forming a sentence y so we start off with a source language sentence x lo mein um and then we translate it and we get out the translation man is born free but everywhere he is in chains okay so there's our machine translation okay so in the early 1950s they started to be work on machine translation um and so it's actually thing about computer science if you find things um that have machine in the name most of them are old things um so um and this really kind of came about in the u.s context in the context of the cold war um so there was this desire to keep tabs on what the russians were doing and people had the idea that because some of the earliest computers had been so successful at um doing code breaking during the second world war then maybe we could set um early computers to work during the cold war to do translation um and hopefully this will play and you'll be able to hear it here's a little video clip um showing some of the earliest work um in machine translation from 1954. [Music] they hadn't reckoned with ambiguity when they set out to use computers to translate languages a 500 000 super calculator most versatile electronic brain known translates russian into english instead of mathematical wizardry a sentence in russian one of the first non-numerical applications of computers it was hyped as the solution to the cold war obsession of keeping tabs on what the russians were doing claims were made that the computer would replace most human translators of course you're just in the experimental stage when you go in for full scale production what will the capacity be we should be able to do about with a modern commercial computer uh about one to two million worth an hour and this will be quite an adequate speed to cope with the whole output of the soviet union in just a few hours computer time a week when do you have to be able to achieve the speed if our experiments go well then perhaps within uh five years or so and finally mr mcdaniel does this mean the end of human translators it's a yes for uh translators of scientific and technical material but as regards poetry and novels no i don't think we'll ever replace the translators of that type of material mr mcdaniel thank you very much but despite the hype it ran into deep trouble um yeah so the experiments did not go well um and so you know in retrospect it's not very surprising that the early work did not um work out very well i mean this was in the sort of really beginning of the computer age in the 1950s but it was also the beginning of you know people starting to understand the science of human languages the field of linguistics so really people had not much understanding of either side of what was happening um so what you had was people are trying to write systems on really incredibly primitive computers right you know it's probably the case that now if you if you have a usbc power brick that it has more computational capacity inside it than the computers that they were using to translate um and so effectively what you were getting were very simple rule-based systems and word lookup so there was so like dictionary look up a word and get its translation but that just didn't work well because human languages are much more complex than that often words have many meanings and different senses as we've sort of discussed about a bit often there are idioms you need to understand the grammar to rewrite the sentences so for all sorts of reasons it didn't work well and this idea was largely canned in particular there was a famous us government report in the mid 1960s the alpaca report which basically concluded this wasn't working oops okay work then did revive in ai at doing rule-based methods of machine translation in the um 90s but when things really became alive um was once you got into the mid 90s and when they were in the period of statistical nlp that we've seen in other places in the course and then the idea began can we start with just data about translation ie sentences and their translations and learn a probabilistic model that can predict the translations of fresh sentences so suppose we're translating french into english so what we want to do is build a probabilistic model that given a french sentence we can say what's the probability of different english translations and then we'll choose the most likely translation um we can then found it was found to felicitus to break this down into two components by just reversing this with bayes rule so if instead we had a probability over english sentences p of y um and then a probability of a french sentence given an english sentence that people were able to make more progress and it's not immediately obvious as to why this should be because this is just sort of a trivial rewrite with bayes rule but it allowed the problem to be separated into two parts which proved to be more tractable so on the left hand side you effectively had a translation model where you could just give a probability of words or phrases being translated between the two languages without having to bother about the structural word order of the languages and then on the right hand you saw precisely what we spent a long time with last week which is this is just a probabilistic language model so if we have a very good model of what good fluent english sentences sound like which we can build just from monolingual data we can then get it to make sure we're producing sentences that sound good while the translation model hopefully puts the right words into them so how do we learn the translation model since we haven't covered that so the starting point was to get a large amount of parallel data which is human translated sentences and this point is mandatory that i show a particular a picture of the rosetta stone which is the famous original piece of parallel data that allowed the decoding of egyptian hieroglyphs because it had the same piece of text in different languages in the modern world there are fortunately for people who build natural language processing systems quite a few places where parallel data is produced in large quantities so the european union produces a huge amount of parallel text across european languages the french sorry not the french the canadian parliament conveniently produces parallel text between french and english and even a limited amount in an institute canadian eskimo and then the hong kong parliament produces um english and chinese so there's a fair availability from different sources and we can use that to build models so how do we do it though all we have is these sentences and it's not quite obvious how to build a probabilistic model out of those well as before what we want to do is break this problem down so in this case what we're going to do is introduce an extra variable which is an alignment variable so a is the alignment variable which is going to give a word level or sometimes phrase level correspondence between parts of the source sentence and the target sentence so this is an example of an alignment and so if we could induce this alignment between the two sentences then we have can have probabilities of pieces of how likely a word or a short phrase is translated in a particular way and in general you know alignment is working out the correspondence between words that is capturing the grammatical differences between languages so words will occur in different orders in different languages depending on whether it's a language that puts on the subject before the verb or the subject after the verb or the verb before both the subject and the object and the alignments will also capture something about differences about the ways the word languages do things so what we find is that we get every possibility of how words can align between languages so you can have words that don't get translated at all in the other language so in french you put a definite article the before country names like japan so when that gets translated to english you just get japan so there's no translation of the so it just goes away um on the other hand you can get many-to-one translations where one french word gets translated as several english words so for the last french word it's being translated as aboriginal people as multiple words um you can get the reverse where you can have several french words um that get translated as one english word so nissan application is getting translators implemented and you can get but even more complicated um one so here we sort of have four english words being translated as two french words but they don't really break down and translate each other well i mean these things don't only happen across languages they also happen within the language when you have different ways of saying the same thing so another way you might have um expressed the poor don't have any money is to say the poor are moneyless and that's much more similar um to how the french is being rendered here and so even english to english you have the same kind of alignment problem so in probabilistic or statistical machine translation is more commonly known what we wanted to do is learn these alignments and there's a bunch of sources of information you could use if you start with parallel sentences you can see how often words and phrases co-occur in parallel sentences you can look at their positions in the sentence and figure out what a good alignments but alignments are a categorical thing they're not probabilistic and so they are latent variables and so you need to use special learning algorithms like the expectation maximization algorithm for learning about latent variables in the olden days of cs224n before we started doing it all with deep learning we spent tons of cs224 in dealing with latent variable algorithms but these days we don't cover that at all and you're going to have to go off and see cs228 if you want to know more about that and you know we're not really expecting you to understand the details here but i did then want to say a bit more about how decoding was done um in a statistical machine translation system um so what we wanted to do is to say we had a translation model and a language model and we want to pick out the most likely why there's the translation of the sentence and what kind of process could we use to do that um well you know the naive thing is to say well let's just enumerate every possible y and calculate its probability but we can't possibly do that because there's a number of translation sentences in the target language that's exponential on the length of the sentence so that's way too expensive so we need to have some way to break it down more and well we had a simple way um for language models we just generated words one at a time and laid out the sentence and so that seems a reasonable thing to do but here we need to deal with the fact that things occur in different orders in source languages and in translations and so we do want to break it into pieces with an independence assumption like the language model but then we want a way of breaking things apart and exploring it in what's called a decoding process so this is the way it was done so we'd start with a source sentence so this is a german sentence and as is um standard in german you're getting this second position verb so that's probably not in the right position for where the english translation is going to be so we might need to rearrange the words so what we have is based on the translation model we have words or phrases that are reasonably likely um translations of each german word or sometimes a german phrase so these are effectively the lego pieces out of which we're going to want to create the translation and so then inside that what we're going making use of this data we're going to generate the translation piece by piece kind of like we did with our newer language models so we're going to start with an empty translation and then we're going to say well we want to use one of these lego pieces and so we could explore different possible ones so there's a search process but one of the possible pieces is we could translate er with he or we could start the sentence with r translating the second word so we could explore various likely possibilities and if we're guided by our language model it's probably much more likely to start the sentence with he than it is to start the sentence with r though r is not impossible okay and then the other thing we're doing with these little blotches of black at the top we're sort of recording which german words we've translated and so we explore forward in a translation process and we could decide that we could translate next the second word goes or we could translate the negation here and translate that as does not when we explore various continuations and in the process i'll go through in more detail later when we do the neural equivalent we sort of do this search where we explore likely translations and prune and eventually we've translated the whole of the input sentence and worked out a fairly likely translation he does not go home and that's what we'll use as the translation okay so in the period from about 1997 to around um 2013 um statistical machine translation was a huge research field the best systems were extremely complex and they had hundreds of details that i certainly haven't mentioned here the systems had lots of separately designed and built components so i mentioned a language model and a translation model but they had lots of other components for reordering models and inflection models and other things there was lots of feature engineering typically the models also made use of lots of extra resources um and there were lots of human effort to maintain um but nevertheless they were already fairly successful so um google translate launched in the mid-2000s and people thought wow this is amazing you could start to get sort of semi-decent automatic translations um for different web pages um but that was chugging along well enough and then we got to 2014 and really with enormous suddenness um people then worked out ways of doing um machine translation using a large neural network and these large neural networks proved to be just extremely successful and largely blew away everything that preceded it so for the next big part of the lecture what i'd like to do is tell you something about newer machine translation neural machine translation well it means you're using a new network to do machine translation but in practice it's meant slightly more than that it has meant that we're going to build one very large neural network which completely does translation end to end so we're going to have a large neural network we're going to feed in the source sentence into the input and what's going to come out as the output of the neural network is the translation of the sentence we're going to train that model end-to-end on parallel sentences and it's the entire system rather than being lots of separate components as in an old-fashioned machine translation system and we'll see that in a bit um so these neural network architectures are called sequence to sequence models or commonly abbreviated seek to seek um and um they involve two neural networks here it says two rnns the version i'm um presenting now has two rnns but more generally they involve two neural networks there's one neural network that is going to encode the source sentence so if we have a source sentence here we're going to encode that sentence and well we know about a way that we can do that so using the kind of lstms that we saw last class we can start at the beginning and go through a sentence and update the hidden state each time and that will give us a representation of the content of the source sentence so that's the first sequence model um which encodes the source sentence and we'll use the idea that the final hidden state of the encoder rnn is going to in sense represent um the source sentence and we're going to feed it indirectly as the initial hidden state for the decoder rnn so then on the other side of the picture we have our decoder rnn and it's a language model that's going to generate a target sentence conditioned on the final hidden state of the encoder rnn so we're going to start with the input of start symbol we're going to feed in the hidden state from the encoder rnn and now this second green rnn has completely separate parameters i might just emphasize but we do the same kind of lstm computations and generate a first word of the sentence he and so then doing lstm generation just like last class we copy that down as the next input we run the next step of the lstm generate another word hit copy it down and chug along and we've translated the sentence right so this is showing um the test time behavior um when we're generating the next sentence for the training time behavior when we have parallel sentences we're still using the same kind of sequence to sequence model but we're doing it with the decoder part just like um training a language model where we're wanting to do teacher forcing and predict each word that's actually found in the source language sentence um sequences sequence models have been an incredibly powerful widely used workhorse in new neural networks for nlp so although you know historically machine translation was the first big use of them and is sort of the canonical use they're used everywhere else as well so you can do many other nlp tasks for them so you can do summarization you can think of text summarization as translating a long text into a short text but you can use them for other things that are in no way a translation whatsoever so they're commonly used for neural dialogue systems so the encoder will encode um the previous two utterances say and then you will use the decoder to generate a neck star trends um some other uses are even freakier but have proven to be quite successful um so if you have any way of representing the paths of a sentence as a string and if you sort of think a little it's fairly obvious how you can turn the paths of a sentence into a string by just making use of extra syntax like parentheses or putting in explicit words that are saying left arc right arc um shifts like the transition systems that you used for assignment three well then we could say let's use the encoder feed the input sentence to the encoder and let it output the transition sequence of our dependency parser and somewhat surprisingly that actually works well as another way to build a dependency parser or other kinds of parser these models have also been applied not just to natural languages but to other kinds of languages including music and also programming language code so you can train a seek to seek system where it reads in pseudocode in natural language and it generates out python code and if you have a good enough one it can do the assignment for you um so the essential new idea here with our sequence to sequence models is we have an example of conditional language models so previously the main thing we were doing was just so start at the beginning of the sentence and generate a sentence based on nothing but here we have something that is going to determine or partially determined that is going to condition what we should produce so we have a source sentence and that's going to strongly determine what is a good translation and so to achieve that what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do and the two standard ways of doing that are you either feed in a hidden state as the initial hidden state to the decoder or sometimes you will feed something in as the initial input to the decoder and so the in your mainstream translation our we're directly calculating this conditional model probability of target language sentence given source language sentence and so at each step as we break down the word by word generation that we're conditioning not only on previous words of the target language but also each time on our source language sentence x because of this we actually know a ton more about what our sentence that we generate should be so if you look at the perplexities of these kind of conditional language models you will find and like the numbers i showed last time they usually have almost frequently low perplexities that you will have models with perplexities that are something like four or even less sometimes you know 2.5 because you get a lot of information about what words you should be generating okay so then we have the same questions as we had for language models in general how to train a neural machine translation system and then how to use it at runtime so let's go through both of those in a bit more detail so the first step is we get a large parallel corpus so we run off to the european union for example and we grab a lot of parallel english french data from the european parliament proceedings so then once we have our parallel sentences um what we're going to do is take um batches of source sentences and target sentences will encode the source sentence with our encoder lstm will feed its final hidden state into a target lstm and this one we are now then going to train word by word by comparing what it predicts is the most likely word to be produced versus what the actual first word and then the actual second word is and to the extent that we get it wrong we're going to suffer some loss so this is going to be the negative log probability of generating the correct next word he and so on along the sentence and so in the same way that we saw last time for language models we can work out our overall loss for the sentence doing this teacher forcing style generate one word at a time calculate a loss relative to the word that you should have produced and so that loss then gives us information that we can back propagate through the entire network and the crucial thing about these sequence the sequence models that has made them extremely successful in practice is that the entire thing is optimized as a single system end to end so starting with our final loss we back propagate it right through the system so we not only update all the parameters of the decoder model but we also update all of the parameters of the encoder model which in turn will influence what conditioning gets passed over from the encoder to the decoder um so this moment is a good moment for me to return um to the three slides that i um skipped i'm running out of time at the end of last time which is to mention multi-layer rnns um so the rnn's that we've looked at so far uh already deep on one dimension then unroll horizontally over many time steps but they've been shallow in that there's just been a single layer of recurrent structure above our sentences we can also make them deepen the other dimension by applying multiple rnn's on top of each other and this gives us some multi-layer rnn uh often also called a stacked rnn and having a multi-layer rnn allows us the network to compute more complex representations so simply put the lower rnns tend to compute lower level features and the higher rnns should compute higher level features and just like in other neural networks whether it's feed forward networks or the kind of networks you see in vision systems you get much greater power and success by having a stack of multi multiple layers of recurrent neural networks right that you might think that all there are two things i could do i could have a single lstm with a hidden state of dimension 2000 or i could have four layers of lstms with a hidden state of 500 each and it shouldn't make any difference because i've got the same number of parameters roughly but that's not true in practice it does make a big difference and multi-layer or stacked rnns are more powerful so uh could i ask you um there's a good student question here about what lower level versus higher level features mean in this context sure um yeah so i mean in some sense these are kind of um somewhat flimsy ways um you know um terms this meaning isn't precise um but typically what that's meaning is that lower level features and knowing sort of more basic things about words and phrases so that commonly might be things like what part of speech is this word or are these words the name of a person or the name of a company whereas higher level features refer to things that are at a higher semantic level so knowing more about the overall structure of a sentence knowing something about what it means whether a phrase has positive or negative connotations um what its um semantics are when you put together several words into an idiomatic phrase um roughly the higher level kinds of things okay jump ahead okay so when we build um one of these end-to-end neural machine translation systems if we want them to work well single layer lstm encoder decode in your machine translation systems just don't work well but you can build something that is no more complex than the model that i've just explained now that does work pretty well by making it a multi-layer stacked lstm neural machine translation system so therefore the picture looks like this so we've got this multi-layer lstm that's going through the source sentence and so now at each point in time we calculate a new hidden representation that rather than stopping there we sort of feed it as into the input into another layer of lstm and we calculate in the standard way it's new hidden representation and the output of it we feed into a third layer of lstm and so we run that right along and so our representation of the source sentence from our encoder is then this stack of three hidden layers and then that we use um to then feed in as the initial um as the initial hidden layer into then sort of generating translations or for training the model of comparing to losses so this is kind of what the picture of a lstm encoder decoder your machine translation system really looks like so in particular um you know to give you some idea of that um so a 2017 paper um by denny brits and others that what they found was that for the encoder rnn it worked best if it had two to four layers and four layers was best for the decoder rnn and the details here like for a lot of neural nets depends so much on what you're doing and how much data you have and things like that but you know as rules of thumb to have in your head it's almost invariably the case that having a two layer lstm works a lot better than having a one layer lstm after that things become much less clear you know it's not so infrequent that if you try three layers it's a fraction better than two but not really and if you try four layers it's actually getting worse again you know it depends on how much data etc you have at any rate um it's normally very hard with the kind of model architecture that i just showed back here to get better results with more than four layers of lstm normally to do deeper lstm models and get even better results you have to be adding extra skip connections of the kind that i talked about at the very end of the last class next week john is going to talk about transformer-based networks in contrast for fairly fundamental reasons they're typically much deeper but we'll leave discussing them until we get on further um so that was how we trained the model um so let's just go a bit more through what the possibilities are for decoding and explore a more complex form of decoding than we've looked at the simplest way to decode is the one that we've presented so far so that we have our lstm we start generate a hidden state it has a probability distribution over words and you choose the most probable one the arg max and you say he and you copy it down and you repeat over so doing this is referred to as greedy decoding taking the most probable word on each step and it's sort of the obvious thing to do and doesn't seem like it could be a bad thing to do but it turns out that it actually can be a fairly problematic thing to do and the idea of that is that you know with greedy decoding you're sort of taking locally what seems the best choice and then you're stuck with it and you have no way to undo decisions um so if um these examples have been using this sentence about he hit me with a pie going from translating from french to english so you know if you start off and you say okay ill the first word in the translation should be he um that looks good but then you um and then you say well hit i'll generate hit then somehow the model thinks that the most likely next word is hit after hit is r and there are lots of reasons it could think so um because after hit most commonly there's a direct object now and then you know he hit a car he hit a roadblock right so that's prett sounds pretty likely but you know once you've generated it there's no way to go backwards and so you just have to keep on going from there and you may not be able to generate the translation you want um at best you can generate um he hit a pie oops um something so we'd like to be able to explore a bit more in generating our translations and well you know what could we do well you know i sort of mentioned this before looking at the statistical mt models overall what we'd like to do is find translations that maximize the probability of y given x and at least if we know what the length of that translation is we can do that as a product of generating a word at a time and so to have a full model we also have to have a probability distribution over how long the translation length would be so we could say this is the model and let's you know generate and score all possible sequences y using this model and that's where that then requires generating an exponential number of translations and is far far too expensive so beyond greedy decoding the the most important method that is used and you'll see lots of places is something called beam search decoding and so this isn't what neural well any kind of machine translation is one place where it's commonly used but this isn't a method that's specific to me machine translation you find lots of other places including all other kinds of sequences sequence models it's not the only other decoding method once when we got on to the language generation class we'll see a couple more but this is sort of the next one that you should know about so beam searches idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation while keeping the search tractable so what we do is ch choose a beam size and for neural mt the beam size is normally fairly small something like five to ten and at each step of the decoder we're going to keep track of the k most probable partial translations so initial subsequences of what we're generating which we call hypotheses um so a hypothesis which is then sort of the prefix of a translation has a score which is this log probability up to what's been generated so far so we can generate that in the typical way using our conditional language model so as written all of the scores are negative and so the least negative one i the highest probability one is the best one so what we want to do is search for high probability hypotheses so this is a heuristic method it's not guaranteed to find the highest probability decoding but at least it gives you more of a shot than simply doing greedy decoding so let's go through an example to see how it works so in this case so i can fit it on a slide the size of our beam is just two um though normally um it would actually be a bit bigger than that and the blue numbers are the scores of the prefixes so these are these log probabilities of a prefix so we start off with our start symbol and we're going to say okay what are the two most likely words to generate first according to our language model and so maybe the first two most likely words are he and i and there are their log probabilities then what we do next is for each of these k hypotheses we find what are likely words to follow them in particular we find one of the k most likely words to follow each of those so we might generate he hit he struck i was i got okay so at this point it sort of looks like we're heading down what will turn into an exponential um true size tree structure again but what we do now is we work out the scores of each of these partial hypotheses so we have four partial hypotheses he hit he struck i was i got and we can do that by taking the previous score that we have the partial hypothesis and adding on the log probability of generating the next word here here hit so this gives us scores for each hypothesis and then we can say which of those two partial hypotheses because our beam size k equals two have the highest score and so they are i was and he hit so we keep those two and ignore the rest and so then for those two we're going to generate um k hypotheses for the most likely following word he hit uh he hit me i was hit i was struck um and again now we want to find the k most likely hypotheses out of this full set and so that's going to be he struck me and i was oh no he struck me and he hit r um so we keep just those ones and then for each of those we generate the k most likely next words tart pie with on and then again we filter back down to size k by saying okay the two most likely things here are pie or width so we continue working on those generate things find the two most likely generate things find the two most likely and at this point um we would generate end of string and say okay we've got a complete hypothesis he struck me with a pie um and we could then trace back um through the tree um to obtain the full hypothesis for this sentence so that's most of the algorithm there's one more detail which is the stopping criterion so in greedy decoding we usually um decode until the model produces an n token and when it produces the end token we say we are done in beam search decoding different hypotheses may produce n tokens on different time steps and so we don't want to stop as soon as one path through the search tree has generated end because it could turn out there's a different path through the search tree which will still prove to be better so what we do is sort of put us put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search and so usually we will then either stop when we've hit a cut-off length or when we've completed n complete hypotheses and then we'll look through the hypotheses that we've completed and say which is the best one of those and that's the one we'll use okay so at that point we have our list of completed hypotheses and we want to select the top one with the highest score well that's exactly what we've been computing each one has a probability that we've worked out but it turns out that we might not want to use that just so naively because there turns out to be a kind of a systematic problem which is you know not as a theorem but in general longer hypotheses have lower scores so if you think about this as probabilities of successively generating each word that basically at each step you're multiplying by another chance of generating the next word probability and commonly those might be you know 10 to the minus three ten to the minus two so just from the length of the sentence um your probabilities are getting much lower the longer that they go on in a way that appears to be unfair since although in some sense extremely long sentences aren't as likely as short ones they're not less likely by that much a lot of the time we produce long sentences so for example you know a newspaper um the median length of sentences is over 20 so you wouldn't want to be having a decoding model when translating news articles that sort of says oh just generate two-word sentences they're just way higher probability according to my language model um so the commonest way of dealing with that is that we normalize by length um so if we're working in log probabilities that means taking dividing through by the length of the sentence and then you have a per word log probability score and you know you can argue that this isn't quite right in some theoretical sense but in practice it works pretty well and it's very commonly used um neural translation has proven to be much much better i'll show you a couple of statistics and about that in a moment it has many advantages um it gives better performance the translations are better in particular they're more fluent because newer language models produce much more fluent sentences but also they much better use context because neural language models including conditional neural language models give us a very good way of conditioning on a lot of context in particular we can just run a long encoder and condition on the previous sentence or we can translate words well in context by making use of neural context neural models better understand phrase similarities and phrases that mean approximately the same thing um and then the technique of optimizing all parameters of the model end to end in a single large neural network is just proved to be a really powerful idea so previously a lot of the time people were building separate components and tuning them individually which just meant that they weren't actually optimal when put into a much bigger system so really a a hugely powerful guiding idea in neural network land is if you can sort of build one huge network and just optimize the entire thing end to end that will give you much better performance and component-wise systems we'll come back to the costs of that later in the course the the models are also actually great in other ways they actually require much less human effort to build there's no feature engineering there's in general no language specific components you're using the same method for all language pairs of course it's rare for things to be perfect in every way so neural machine translation systems also have some disadvantages compared to the older statistical machine translation systems they're less interpretable it's harder to see why they're doing what they're doing where you before you could actually look at phrase tables and they were useful so they're hard to debug they also tend to be sort of difficult to control um so compared to anything like writing rules um you can't really give much specification as if you'd like to say oh i'd like my translations to be more casual or something like that it's hard to know what they'll generate so there are various safety concerns um i'll show a few examples of that in just a minute but first before doing that quickly how do we evaluate machine translation um the best way to evaluate machine translation is to show a human being who's fluent in the source and target languages the sentences and get them to give judgment on how good a translation it is but that's expensive to do and might not even be possible if you don't have the right human beings around so a lot of work was put into finding automatic methods of scoring translations that were good enough and the most famous method of doing that is what's called blue and the way you do blue is you have a human translation or several human translations of the source sentence and you're comparing a machine-generated translation to those pre-given human written translations and you score them for similarity by calculating engram precisions i.e words that overlap between the computer and human written translation diagrams trigrams and four grams and then working out a geometric average between overlaps of n engrams plus there's a penalty for two short system translations so blue has proven to be a really useful measure but it's an imperfect measure that commonly there are many valid ways to translate a sentence and so there's some luck as to whether the human written translations you have happen to correspond to which to what might be a good translation from the system there's more to say about the details of blue and how it's implemented but you're going to see all of that doing assignment 4 because you will be building your machine translation systems and evaluating with them with the blue algorithm and their full details about blue in the assignment handout but at the end of the day blue gives the score between zero and a hundred where your score is a hundred if you're exactly producing one of the human written translations and zero if you there's not even a single unigram that overlaps between the two with that rather brief intro i wanted to show you sort of what happened in machine translation so machine translation with statistical models phrase-based statistical machine translation that i showed at the beginning of the class had been going on since the mid 2000s decade and it had produced sort of semi-good results of the kind that are in google translate in those days but by the time you'd entered um the 2010s basically progress and statistical machine translation had stalled and you are getting barely any increase over time and most of the increase in time you were getting over time was simply because you're training your models on more data um in those years around the early 2010s the big hope that most people had someone asked what is the y-axis here this y-axis is this blue score that i told you about on the previous slide in the early 2010s the big hope that most people in the machine translation field had was well if we built a more complex kind of machine translation model that knows about the syntactic structure of languages that makes use of tools like dependency parsers will be able to build much better translations and so those are the purple systems here which i haven't described at all but it's sort of as the years went by it was pretty obvious that that barely seemed to help and so then in the mid 2000 2010s so in 2014 was the first modern attempt to build a neural network for machine translations and encoder decoder model um and by the time it was sort of evaluated in bake offs in 2015 it wasn't as good as what had been built up over the preceding decade but it's already getting pretty good but what was found was that these newer models just really opened up a whole new pathway to start building much much better machine translation systems and since then things have just sort of taken off and year by year newer machine translation systems are getting much better and far better than anything we had preceding that so for you know at least the early part of application of deep learning and natural language processing neural machine translation was the huge big success story um in the last few years when we've had um models like gpt2 and gpt3 and other huge neural models like bird improving web search you know it's a bit more complex but this was the first area where there was a neural network which was hugely better than what it proceeded and was actually solving a practical problem that lots of people in the world need and it was stunning with the speed at which success was achieved so were the first um what i call here fringe research attempts um to build in your machine translation system meaning that um three or four people who were working on neural network models thought oh why don't we see if we can use one of these to translate learn to translate sentences where they weren't really people with a background in machine translation at all but a success was achieved so quickly that within two years time google had switched to using neural machine translation for most languages and by a couple of years later after that essentially anybody who does machine translation is now deploying um live neural machine translation systems and getting um much much better results so that was sort of just an amazing technological transition that for the preceding decade the big statistical machine translation systems like the previous generation of google translate had literally been built up by hundreds of engineers over years but a comparatively small group of deep learning people in a few months with a small amount of code and hopefully you'll even get a sense of this doing assignment 4 we're able to build newer machine translation systems that prove to work much better does that mean that machine translation is solved uh no um there are still lots of different ease which people continue to work on very actively and you can see more about it in the sky yet today article was linked at the bottom but you know there are lots of problems without a vocabulary words they're domain mismatches between the training and test data so it might be trained mainly on newswire data but you want to translate people's facebook messages there are still problems with maintaining context over longer text we'd like to translate um languages for which we don't have much data and so these methods work by far the best when we have huge amounts of parallel data even our best multi-layer lstms aren't that great at capturing sentence meaning there are particular problems such as interpreting what pronouns refer to or in languages like um chinese or japanese where there's often no pronoun present but there is an implied reference to some person working out how to translate that for languages the laws have lots of inflectional forms of nouns verbs and adjectives these systems often get them wrong so there's still tons of stuff to do so here's just sort of quick funny examples of the kind of things that go wrong right so if you've asked to translate paper jam google translate is deciding that this is a kind of jam just like there's some raspberry jam and strawberry jam and so this becomes a jam of paper um there are problems of agreement and choice um so if you have many languages don't distinguish gender and so the sentences uh neutral between things are masculine or feminine so malay or turkish are two well-known languages of that sort but what happens when that gets translated into english by google translate is that the english language model just kicks in and applies stereotypical biases and so these gender neutral sentences get translated into she works as a nurse he works as a programmer so if you want to help solve this problem you all of you can help by using singular they in all contexts when you're putting material online and that could then change the distribution of what's generated but people also work on modeling improvements to try and avoid this here's one more example that's kind of funny um people noticed um a couple of years ago that if you choose one of the rarer languages that google will translate um such as somali um that and you just write in some rubbish like a gay gag um freakily it would produce out of nowhere prophetic and biblical texts as the name of the lord was written in the hebrew language it was written in the language of the hebrew nation which makes no sense at all we're about to see a bit more about why this happens um but but ah that was sort of a bit worrying um this as far as i can see this problem is now fixed in 2021 i couldn't actually get google translate um to generate examples like this anymore um but you know so there's lots of ways to keep on doing research um nmt is certainly is you know a flagship task for nlp and deep learning and it was a place where many of the innovations of deep learning nlp were pioneered and people continue to work hard on it people found many many improvements um and actually for the last bit of the class in the minute i'm going to present one huge improvement which is so important that it's really come to dominate the whole of the recent field of newell neural networks for nlp and that's the idea of attention but before i get on to attention i want to spend three minutes on our assignment for so for assignment four this year um we've got a new version of the assignment um which we hope will be interesting but it's also a real challenge so for assignment four this year we've decided to do cherokee english machine translation so cherokee is an endangered native american language it has about 2 000 fluent speakers it's an extremely low resource language so it's just there isn't much written cherokee data available period and particularly there's not a lot of parallel sentences between cherokee and english and here's the answer um to the google's freaky prophetic translations for languages for which there isn't um much parallel data available commonly the biggest place where you can get parallel data is from bible translations so you can have your own personal choice wherever it is over the map as to where you stand with respect to religion but the fact of the matter is if you work on indigenous languages what you very um very quickly find is that a lot of the work that's done on collecting data on indigenous languages and a lot of the material that is available in written form for many indigenous languages is bible translations um yeah okay so this is um what cherokee looks like and so you can see that the writing system has a mixture of things that look like english letters and then all sorts of letters that don't and so here's the initial bit of a story long ago was seven boys who used to spend all their time down by the townhouse so this is a piece of parallel data that we can learn from so um the cherokee writing system has 85 letters and the reason why it has so many letters is that each of these letters actually represents a syllable so many languages of the world have strict consonant vowel syllable structure so you have words like rata per or something like that or cherokee right and another language like that's hawaiian and so each of the letters represents a combination of a consonant and a vowel and um that's um the set of those um you then get 17 by five gives you 85 letters um yeah so being able to do this assignment big thanks to um people from university of north carolina chapel hill um who've provided the resources um we're using for this assignment although you can do quite a lot of languages on google translate um cherokee is not a language that google offers on google translate so we can see how far um we can get um but we have to be modest in our expectations because it's hard to build a very good mt system with only a fairly limited amount of data so we'll see how far we can get there is a flip side which is for you students doing the assignment the advantage of having not too much data is that your models will train relatively quickly so we'll actually have less troubles than we did last year with people's models taking hours to train as the assignment deadline closed in there's a couple more words about cherokee so we have some idea what we're talking about so the cherokee originally lived in western north carolina and tennis eastern tennessee they then sort of got shunted um south west from that and then in particular for those of you who went to american high schools and paid attention you might remember um discussion of the trail of tears when a lot of the native americans from the southeast of the us got forcibly shoved a long way further west and so most cherokee now live in oklahoma though there's there are some that are in north carolina the writing system that i showed on this previous slide it was invented by a cherokee man sequoia that's a drawing of him there and that was actually a kind of incredible thing so um he started off a literate and worked out how to write or produce a writing system that would be goodbye good for um cherokee and given that it has this consummate vowel structure um he chose a celebrity which um turned out to be a good choice um so here's here's a neat historical fact so in the 1830s and 1840s um the percentage of cherokee that were literate in cherokee written like this was actually higher than the percentage of white people in the southeastern united states at that point in time okay before time disappears um oops time has almost disappeared i'll just start to say um and then i'll have to do a bit more of this uh i'll have to do a bit more of this next time that'll be okay right so the final idea that's really important for sequence to sequence models is the idea of attention um and so we had this model of doing sequence to sequence models such as the neural machine translation and the problem with this architecture is that we have this one hidden state which has to encode all the information about the source sentence so it acts as a kind of an information bottleneck and that's all the information that the generation is conditioned on well i didn't already mention one idea last time of how to get more information where i said look maybe you could kind of average all of the vectors of the source to get a sentence representation but you know that method turns out to be better for things like sentiment analysis and not so good for machine translation where the order of words is very important to preserve so it's it seems like we would do better if somehow we could get more information from the source sentence while we're generating the translation and in some sense this just corresponds to what a human translator does right if you're a human translator you read the sentence that you're meant to translate and you maybe start translating a few words but then you look back at the source sentence to see what else was in it and translate some more words so um very quickly after the first neural machine translation systems people came up with the idea of maybe we could build a better neural mt model that did that and that's the idea of a tension so the core idea is on each step of the decoder um we're going to use a direct link between the encoder and the decoder that will allow us to focus on particular a particular word or words in the source sequence and use it to help us generate what words come next i'll just go through now showing you the pictures of what attention does and then at the start of next time we'll go through the equations in more detail so we generate we use our encoder just as before and generate our representations feed in our conditioning as before and say we're starting our translation but at this point we take this hidden representation and say i'm going to use this hidden representation to look back at the source to get information directly from it so what i will do is i will compare the hidden state of the decoder with the hidden state of the encoder at each position and generate an attention score which is a kind of similarity score like a dot product and then based on those attention scores i'm going to calculate a probability distribution um as to by using a softmax as usual to say which of these encoder states is most like my decoder state and so we'll be training the model here to be saying well probably you should translate the first word of the sentence first so that's where the attention should be placed so then based on this attention distribution which is a probability distribution coming out of the softmax we're going to generate um a new um attention outport and so this attention output is going to be an average of the hidden states of the encoder model but it's going to be a weighted average based on our attention distribution and so we're then going to take that attention output combine it with the hidden state of the decoder rnn um and together the two of them are then going to be used to predict virus soft max what word to generate first and we hope to generate he and then at that point we sort of chug along and keep doing these same um kind of computations at each position um there's a little side note here that says um sometimes we take the attention output from the previous step and also feed into the decoder along with the usual decoder input so we're taking this attention out from actually feeding it back in to the hidden state calculation and that can sometimes improve performance and we actually have that trick in the assignment four system and you can try it out okay so we generate along and generate our whole sentence in this manner and that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation i'll stop here for now and at the start of next time i'll finish this off by going through the actual equations for how attention works you