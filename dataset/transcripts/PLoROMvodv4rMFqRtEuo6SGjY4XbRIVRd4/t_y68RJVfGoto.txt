welcome to cs224n lecture 15. so i'm megan and i'm one of the cities in this course and i'm also a phd student working with chris ray and today i'll be talking about integrating knowledge and language models so some quick reminders your project milestones were due today so hopefully you turn those in already or we'll be turning them in the next couple of days and we'll try to get feedback on those as fast as possible so something to be aware of is a change of grading basis and course withdrawal deadline is this friday um so if you want to make any change your grade make sure to do that by then and we'll be getting you the grades back on assignment five by then as well in case that's helpful in making your decision and finally your final projects are due in two weeks so hopefully those are going smoothly so the topic of the day is integrating knowledge and language models you've seen a bit about this idea in assignment five and also in colin raffle's lecture last class so in assignment 5 the task was to train a model to predict the birthplace of a person given their name and you saw it by pre-training on a larger data set you're actually able to do better on this task since you could encode some world knowledge into the language model and then last lecture colin raffle presented how t5 could actually be fine-tuned for a closed domain question answering task such that you can give t5 a natural language question and it'll return an answer so they will be building on these threads and looking at techniques that researchers have recently been developing to increase the amount of knowledge in language models so we're going to start with a quick recap of language models just to make sure we're all on the same page then we're going to talk about what types of knowledge language models can already encode and what they might struggle on we'll also motivate why researchers are interested in increasing the amount of knowledge in language models and what this could enable for future ai systems if we have language models that can actually reliably recall knowledge we'll talk about three broad classes of techniques that researchers have been using to add knowledge to language models these include adding pre-trained entity embeddings using external memory or key value store or even just modifying the training data and for each of these techniques we'll talk about at least one recent work that used the technique so hopefully it's clear to see how to actually employ it in practice and then finally we'll wrap up by talking about how to evaluate the knowledge in language models and the challenges that come up in trying to do this so let's dive right in we're going to start by talking about standard language models you learned about these at the beginning of the course and the task is to predict the next word and sequence of text and to compute the probability of a sequence so you may remember the example that students opened their blank and we talked about could be minds exams bring both books here and the task of the standard language model is to predict the most likely next word in the sequence a couple lectures ago john also introduced the notion of mass language models instead of predicting the next word in a sequence of text the tasks predict the mass token and this is done using bi-directional context so you may remember the example i masked the mask and the goal the mass language model is to make the most likely token for each of the masked out words so maybe i went to the store so while there's some differences in these two types of language models whether you're predicting the next word or whether you're predicting the massdot token they're similar and that they can both be trained over large amounts of unlabeled text and this is one of the reasons why they've been so widely adopted they don't require any human annotated data so you've seen that language models can be used for a variety of tasks from summarization to dialogue to fluency evaluation tasks that involve either generating text or evaluating the probability of text and more recently we've seen that language models can also be used to generate pre-changed representations of text that encodes some notion of language understanding and has been shown to be widely useful for different downstream nlp tasks and then finally today we're going to touch on this idea that if language models are trained over massive amounts of text can they even be used as a knowledge base so we're going to start by looking at what types of factual knowledge a language model might already know and these examples are taken from a paper by petroni at all in emlp a couple years ago and the goal is to test the factual or common sense knowledge in existing language models such as bert large so let's check out what bert large predicts ipod touch is produced by apple london jazz festival is located in london uh danny alves plays with santos carl iii used to communicate in german and ravens can fly so here we have the correct predictions in green and the incorrect predictions in red and if you know anything about sports you may know that danny alves is a soccer player santos is a soccer team here they were hoping that it would predict barcelona because at least at the time of this data set apparently he played for barcelona and carl iii actually used to communicate in swedish not german so what's good about these examples is the predictions are generally reasonable if you didn't know the ground truth they all make sense when you want to produce a when you want to predict a language you do in fact predict a language but of course they're not all factually correct so why might this happen well for one the fact might not been seen in training and you can't expect the language model to do more than recall facts that it has seen in training it can't make up facts about the world for instance it's also possible the fact is just really rare so maybe the language model has seen the factoring training but it hasn't seen it enough times actually memorize the fact and the last issue is a little more subtle which a model might just be very sensitive to the phrasing of the fill in the blank statement and so for example you might have statements like x was created in blank that the model can't predict correctly but if you change it to x was made in blank suddenly it can predict it correctly and we'll come back to this in how to actually evaluate the knowledge in these language models so this inability to reliably recall knowledge is a key challenge facing language models today that'll be the focus of this talk recent works have found that language models can recover some knowledge including the work that colin presented last class they've had very encouraging results but there's still a way to go as we saw with the fill in the blank statements and with these challenges that we just discussed above so as a result the past couple years have had ton of rapid progress in this area of research in terms of trying to figure out how do you actually encode more knowledge in language models so i also want to motivate why researchers are interested in building language models that can more reliably or call knowledge and one of these reasons is that the pre-trained representations are used in a variety of downstream tasks and some of these downstream tests are knowledge intensive so for instance you might have a downstream task to extract the relations between two entities in a sentence and this is commonly known as relation extraction and this is much easier if you have some knowledge of the entities which could be potentially provided by this pre-trained language model representation and we talk about evaluation we'll talk about what types of tasks are most likely to benefit from these knowledge rich pre-trained representations and then as a stretch goal some researchers are starting to propose the idea that can language models actually ultimately be used to replace traditional knowledge bases so instead of creating a knowledge base for a fact like you might right now with sql you would create a language model with a natural language prompt and of course this does require the language model to have high quality under calling facts so we might not be there yet but it's an interesting direction for us to be moving towards so i want to make it super clear what i mean by a knowledge base here we're just talking about a knowledge graph where the nodes in the graph would be entities and the edges are going to be relations between the entities so for example here we have a subset of a knowledge graph for franklin d roosevelt and you see the information about his spouse his place of birth his date of birth and so on an important thing to note is it's a structured way of storing the knowledge since it's just in a graph form and you can actually describe these graphs with knowledge graph triples which will be an important vocabulary word throughout this talk so knowledge graph triple would be um consisting of a subject entity a relation and then an object entity so for instance here we might have franklin d roosevelt date of birth january 30th 1882 and that would form a knowledge graph triple we'll also refer to this as a parent entity a relation and a tail entity so wikidata is one very popular knowledge base you might come across if you're working this area it's a free knowledge base that's actually populated by humans so they're filling in these relations and entities and it's also multilingual so if you want information from this knowledge base what you do is write as you would write a sql query this is a simplified one but the idea is you'd want to figure out the date of birth of franklin roosevelt so you would write a query like follows now if instead you want to query a language model as a knowledge base you'll have something like this diagram that you've actually probably seen in several lectures now and the idea is you'll train a language model over this unstructured text and then you'll use a language model to just answer these natural language query statements so here this is the work on t5 where they're training t5 over natural language or just unstructured text with a span corruption task and then they're asking t5 when was franklin d roosevelt born and the idea is t5 will produce a textual answer so you can see this contrast very much with the old approach of using a traditional knowledge base where the knowledge base is structured and you have the sql statements to query it so what are the advantages of using language models over traditional knowledge bases and why might people think this this could be a good idea well for one the language models are pre-trained over large amounts of unstructured and unlabeled text whereas traditional knowledge bases require manual annotation like with wikidata people actually populating it or complex nlp pipelines to extract from unstructured text into a structured form that forms a knowledge base language models can also support more flexible natural language queries so if we take the example what does the final f in the song ufo f stand for a knowledge base probably won't have a field for final f so it won't be able to answer your query but there's a chance that a language model could actually learn and have a response for this natural language query they also had a less extreme example in this paper by petroni and others where maybe your relation would be is works for in your knowledge base and then you ask for is working for and the knowledge base doesn't have an exact match on the field and so it returns an empty response and it's much it's reasonable to believe that your language model could figure out that these relations are similar so if i know the answer to one of them i probably know the answer to the other of course it's not all advantages there's also many open challenges to using language models as knowledge bases so for one it's harder to interpret when a traditional knowledge base produces an answer there's actually provenance information associated with why did it return that particular query but with a language model it's really not clear why it might produce a prediction the knowledge is just encoded in the parameters of the model it's also harder to trust so you saw this in assignment 5 where the language model could produce realistic predictions but they are incorrect so it's not easy to know when the language model actually knows the fact versus it's using some like biases to make its prediction and in the case of the traditional knowledge base if it doesn't know a fact it's just going to have an empty response and then finally knowledge bases or language models are harder to modify so in a knowledge base if you want to update a fact you just change the fact directly in the structured data but in a language model it's not quite clear how you would do this you could fine-tune the model longer on the updated data but how do you know if it still has some memorization of the old fact so there are a lot of open challenges to this goal of actually using language models as traditional knowledge bases but hopefully you see why some people think this could actually be a good idea and why researchers are interested in training language models that can actually integrate more knowledge so that brings us to section two of the talk so i want to pause here just in case there's any questions okay i think that's okay yeah okay awesome um so now we're going to be talking about what techniques researchers are using to actually add more knowledge to language models so we're going to talk about three broad classes of techniques this is by no means exhaustive but hopefully it gives you a good overview so that if you want to dive deeper you can so we'll start by talking about adding pre-trained entity embeddings and for each section we'll kind of focus on the first work that you see in the bullets but we'll also talk about briefly some of the variants so you see how the works within each class can differ and what knobs you can turn so for adding pre-trained embeddings we first need to figure out what pre-trained embeddings would actually be the most useful to add knowledge to language models and this can start with an observation but facts about the world are usually in terms of entities so if we have a fact like washington was the first president of the united states we have the entities washington united states but pre-trained word embeddings don't have this notion of entities so we'd have different word embeddings for usa united states america and america even though these all refer to the same entity and this makes it challenging for the language model to actually learn any representations over these entities since they may be referred to many ways in the text so what if instead we have a single embedding per entity and we'll refer to these as entity embeddings so now you'd have a single entity embedding for usa united states of america and america and whenever you see a phrase in text referring to this entity you would use the same entity embedding and these entity embeddings can actually be pre-trained to encode this factual knowledge about the world and this first class techniques we'll be looking at would be how do you actually best use these pre-trained entity embeddings in a language model so i need to make a quick note that these entity embeddings are only useful to language models though if you can do another nlp task called entity linking well so i'm going to take a quick aside and explain what is entity linking so the definition of entity linking is to link mentions in text to entities in a knowledge base i like to think about this in terms of how you use word embeddings so if you want to use word embeddings and you have a sentence you're going to first tokenize that sentence into words and then for each word you'll look up their corresponding id and some word embedding matrix and now you have your word embedding well for entity and bettings the dictionary lookup isn't so easy you might have sentences like washington is the first president united states well washington has two different candidates are we talking about george washington or are we talking about washington state and these are different entities that have different entity embeddings and the queue ids here would just be their identifiers and wiki data and then united states just has a single entity so task of entity linking is to figure out correctly these ambiguous mentions what entity do they actually link to in a knowledge base and there's many different ways you can do this entity linking so one way you might be able to do this is to figure out that oh i see the context word of president so washington probably links to george washington just some more definitions we're going to refer to washington as i mentioned the united states as i mentioned and then the things that the mention could link to so the two options for washington are going to be candidates so this is a whole research area of its own and i encourage you to check out the resources at the bottom if you're interested in learning more but right now the most important thing to understand is that entity linking is what is going to tell us which entity embeddings are actually relevant to the text and which ones you want to use as you iterate through a sequence megan there are a few a few questions around here one of them is so that's entity linking but what about the relations um yeah so some of the works we'll talk about will only use the entity embeddings um so some of these have been pre-trained with relation information but in the end you only have an entity embedding so relation extraction is yet another nlp test you could also do but yeah here we're just talking about nc linking but if you have the knowledge graph you showed earlier it had relations in it right do you get any connection between that and the text um i mean that's the goal of relation extraction right is to figure out like given the entities what is relation between them which would then form the full triple of head entity tail entity and relation um okay then i think people want to know more about house it's going to be used but maybe you should go on and show some examples yeah i will for sure okay um right so entity embeddings just to summarize they're like word embeddings um but they're for entities in a knowledge base so you'll have some vector associated george washington and it should be meaningful in embedding space such that maybe the george washington vector is close to the vectors for other founding fathers so we're going to briefly talk about some methods for a training entity in betting's there's knowledge graph embedding methods you might have heard of the transient betting method so this starts from the idea of having these knowledge graph triples and you want to learn pre-trained entity and pre-trained relation embeddings and you want to be the case that the subject embedding and the relation embedding the sum of those two is close to the object embedding in vector space so it's an algorithm to learn that constraint there's also word entity coccurrence methods so these build off of word to back one of them is even called wikipedia to back and the idea is given an entity you want to figure out what words are most likely to co-occur around it and then the last method or one of the other methods that is common now is actually just using the transformer to learn representations of an entity by encoding the entity description and so blink from facebook is a an approach that does this so the methods we'll talk about today are actually agnostic to how you train your pre-trained entity embedding but i think it's important to know that there's actually a wide variety of methods to train these preaching entity embeddings and it's actually not clear which method is best for using them downstream and language models so one of the key challenges of using pre-trained entity embeddings in language models is figuring out how to incorporate them when they're from a different embedding space than the language model and so we'll do or the approaches we'll look at today we'll learn a fusion layer to combine this context and entity information so we have entity embeddings and we have the contextualized word embeddings from our language model so if we take a sequence of text and we imagine that j indicates the j element in a sequence then the challenge here is we want to figure out how do we combine some word embedding wj with some aligned entity embedding ek so here an alignment could be like in the example where we had washington was the first president washington would be your word embedding and george washington would be the aligned entity embedding there so you could imagine in this case let's say your wj is washington and your ek is your entity embedding for george washington and you want to align them together so what you can do is learn a weight matrix wt for the text and w e for the entity to project these embeddings to the same dimension before you sum them and finally take an activation function over them so the idea is that by having some fusion layer mechanism like this you can actually use these entity embeddings and these contextual word embeddings that are in different embedding spaces and fuse them together to have this single hidden representation for the element in the sequence so the approaches we'll talk about today all have some mechanism either very similar to this or some variation of this to do this combination of the context and entity information so the first approach we're going to talk about is called ernie enhanced language representation with informative entities and so this just builds on what we've already talked about it uses pre-trained entity embeddings and it also uses this notion of a fusion layer so the first block in ernie is a text encoder which is a multi-layer bi-directional transformer encoder for their experiments they use bert but it doesn't have to be burnt and this is followed by a knowledge encoder which has stacked blocks composed of two multi-headed attentions one is over the entity embeddings and one is over your token or subword embeddings and then the output of these contextualized entity and token embeddings from the multi-headed attentions are passed to a fusion layer which looks very similar to what we just looked at but now you also have new word and entity embeddings that you're producing uh as output of your fusion layer so you see this w j um and this e k which are produced as the next layer of word and entity embeddings so the i here indicates that it's the ith block in the knowledge encoder so you'll actually have multiple stacks of these knowledge encoders and you'll be doing a fusion of the word entity embedding producing new word and entity embeddings and then passing this to the next block of the knowledge encoder so this is what the architecture diagram looks like on the left side we have the t encoder or the text encoder followed by the k encoder or the knowledge encoder and then on the right side we have a zoomed in version of your knowledge encoder so you see the multi-headed attentions over the tokens in orange and then over the entities in yellow and then you have this alignment between the word and entities with the dashed lines so they have this example as bob dylan wrote blowing in the wind in 1962 the entities here are bob dylan and blowing in the wind and they have a simple alignment rule where you want to align the entity to the first word in the entity phrase so you want to align bob dylan to bob that's what the dash line's trying to indicate and you want a line blowing the wind to blow so here this already assumes entity linking has been done and you know your entities in advance so you can see that the entities are actually input into the model so after you have your word nt alignment this goes to the information fusion layer and this light purple gray color and then finally it produces these new word entity embed things as output and then remember that you have multiple blocks of these so those will be passed into the next block of your knowledge encoder so how do you actually train this it's pretty similar to bert you have a mass language model loss and you have a next sentence prediction loss and they also introduce a knowledge pre-training task which they refer to as the dea task it's named after a denoising entity autoencoder from an icml paper in 2008 and the idea is they're going to randomly mask these token entity alignments so the idea that bob goes to bob dylan they're going to mask that out with some random percentage and then they're going to predict the corresponding entity for a token out of the entities in this in the sequence so this looks like as follows the summation is over m entities in the sequence so this would be over bob dylan and blowing in the wind in the previous example and given a particular word they want to figure out um what entity is it most likely to align to in that sequence so does bob align to bob dylan or does bob align to blowing in the wind and their motivation for doing this is that if you don't have this task all you're ever going to be predicting is the token with the mass language model loss and you really 10 code knowledge should also probably be predicting over entities so by adding this task they have some kind of task that is actually predicting the entity and they also suggest that this might better fuse the knowledge or the entity and the word representations than just using the fusion layer their final loss is then the summation of the mass language model loss the next sentence prediction loss and this dea knowledge pre-training task loss so they showed an ablation experiment that it's actually very important to have this knowledge pre-training task so this has um bert on the left-most bar ernie as the second bar from the left and so that's with all the features of ernie and then they try removing the pre-trained entity embeddings and removing this knowledge pre-training task so you see that bert performs the worst this isn't very surprising and that ernie performs the best but what's interesting is that if you remove the entity embeddings or you remove the pre-training task they only do a little better than bert and so it's really necessary to actually use this pre-training task to get the most use of your pre-trained entity and bettings so some strengths of this work were that they introduced some way to combine entity and context information through this fusion layer and this knowledge pre-training task and then they also show improved performance on downstream tasks which we'll come back to when we talk about evaluation but of course there's also some limitations so it needs text data with the entities annotated as input and this is even true for downstream tasks so if you remember on the architecture diagram we had the entity entity information actually input into the architecture but it's not very realistic that you're necessarily going to have a good entity linker for any downstream tasks that you want to use ernie on and the next challenge is this requires more pre-training of your language model so now you don't just need to pretend burt but you also need to pre-train your knowledge encoder on top for the first challenge we're going to actually talk about a work that presents a solution to address this for the second challenge i encourage you to check out the footnote on the bottom this introduces a work that actually uses pre-trained entity embeddings uses them in a language model and doesn't require any more pre-training so it's pretty cool uh i guess that's all i have for ernie so i want to pause here for questions well here's one that's up here so on the fusion layer it has it observed that passing the entity embedding into a fusion layer to combine with word embedding is more powerful than just concatenating the entity embedding onto the end of the word embedding question mark yeah so i guess people are still a little bit confused as to the motivation for that fusion layer and so i guess here is this the simplest strategy would be since you've got the entity linking you could just concatenate entity embeddings onto the end of word embeddings and do regular would that work just as well um i think the idea is it wouldn't because if you imagine that let's say your magnitudes are very different um you need some way to i guess align the spaces so that anything meaningful in the entity embedding space is still meaningful in the word embedding space so if you're close in the word embedding space you also would be you'd want to be close in entity embedding space so i guess that's one argument yeah i mean i mean i think the question isn't you know it's a good question as people say i mean it's not completely obvious that it wouldn't work to do that it seems like one of the potential problems is some words have entity links to them and some words don't and so you then you'd sort of have zero vectors for the ones that don't have anything that's a good point linked and that might act a bit weirdly but yeah in this case when they don't have entities linked which is a great point um yeah the first equation just simplifies to the first term plus the bias so like there's an obvious solution in that case when you're not concatenating that you just don't add on the term um yeah that could be one reason too okay are there any other questions [Music] i think you can go on okay cool um right so now we're talking about novert and this is from the same folks that introduced the elmo work and the idea here is that they're going to pre-train an integrated entity linker as an extension to bird and so their loss function will now be the summation of the next sentence prediction the mass language model loss and this entity linking loss so instead of the knowledge pre-training dea task from ernie we'll have an entity linking loss and the idea of the entity linker is you'll now have just as normal sequence as input and the integrated empty linker will figure out what are the entities in the sentence and um or what are the mentions in the sentence what are the candidates of those mentions and then what should be the scores those entities or the candidates given the context of the sentence and so this is all done now as part of the model rather than requiring it as some external pipeline stage before you could even use ernie for instance so now for downstream tasks you no longer need these entity annotations your integrated anti-linker will figure out what the correct entity is and be able to use the correct entity embedding so there's also this idea that learning is entity linking may actually better encode knowledge than this dea pre-training task because they show that nobody actually outperforms ernie on downstream tasks so one reason this may occur is that if you think about the the dea task it's actually a bit simpler than just entity linking so you're trying to predict for instance um what bob linked to out of bob dylan and blowing in the wind and it's much easier even as a human to see that bob dylan will more likely link to or bob will more likely link to bob dylan than that bob will link to blowing in the wind and anti-linking task you actually have a much harder set of candidates to predict over you're not just looking at the ones in a sentence so does washington link to george washington or washington state actually requires you using more information about the entity so given it's a harder task it's not too surprising that it might perform better than just this easier knowledge pre-training task that ernie introduced so otherwise nobert has a lot of similarities to ernie it uses a fusion layer that combines this context and entity information and it introduces some knowledge pre-training task so i'd say a high-level takeaway is if you want to use pre-trained entity embeddings in a language model you'll probably at least want to consider both of these components in terms of actually going to integrate the preaching entity embeddings and take the most advantage of the knowledge in them as possible so that brings us to the next class of techniques which is using external memory and here we'll mainly focus on this work called kglm and then we'll also briefly talk about k n lm so the previous methods that we've talked about have relied on pre-change entity embeddings to encode the factual knowledge from knowledge bases and the one problem with this or one of the problems with this is if you want to let's say modify your knowledge base you now need to retrain your entity embeddings and then retrain your language model on top of those entity embeddings so this begs the question are there more direct ways in pre-trained entity embeddings to provide the model factual knowledge and so we're going to talk about is how you can actually use an external memory or a key value store to give the model access to either knowledge graph triples or context information and a key thing about this external memory is that it's independent of the learned model parameters so this means you can actually support injecting and updating factual knowledge you can do this directly to the symbolic external memory by let's say changing the value for a particular key or maybe adding another key and you don't have to pre-train or retrain your entity embeddings when you make this change and the approaches we'll talk about today can actually even have these updates to the external memory without more pre-training of the language model so that's pretty neat and then another benefit of using external memory over these pre-trained nc embedding approaches is they can also be more interpretable so if you have an a bug or not a bug an air in your model where it's not predicting a correct fact it's very challenging to figure out with pre-trained anti-embeddings um what the problem might be was it the original knowledge base was it the encoding in the entity embeddings is it how the language model is using the entity embeddings and here you have a little more information with an external memory and that you can look in the external memory and see was the fact in the external memory was was it not in external memory and so on so it adds a little bit more interpretability than just using these pre-trained ncm bettings as an indirect way to encode the knowledge base so the first work we're going to talk about is called kglm and unlike the other approaches we've talked about so far this actually uses lstms and not transformers so the key idea here is to condition the language model on a knowledge graph so recall with a standard language model we want to predict the next word given the previous words in the sequence well now we also want to predict the next entity given the previous words in the sequence and given the previous entities in the sentence or the entities that are relevant to the sentence i should say so kglm will be building a local knowledge graph as it iterates over the sequence and a local knowledge graph is just a subset of a full knowledge graph that only has the entities that are actually relevant to the sequence oops so if we have this example here um a simplified example from the paper that super mario land is a game developed by blank and super mario land here is an entity you'd want a local knowledge graph as follows where you see that super mario land is in the local knowledge graph but we also have the relations to super mario land to other entities that are copied from the full knowledge graph into this local knowledge graph and you would build up this local knowledge graph as you iterate over the sentence so whenever you see an entity you would add it to the local knowledge graph as well as its relations to other entities so obviously this is a much smaller example than what would really have all the relations to super mario land just for the purpose of the example but hopefully it's clear that all of these are relevant to the sequence something important to note here is that this does assume that the entities are known during training so that you do have this entity annotated data for training and therefore your local knowledge graph is always the ground truth local knowledge graph as you iterate over the sequence so why might this be a good idea to do this well here the next word you want to predict is nintendo and you may notice that nintendo is in your local knowledge graph so sometimes this local knowledge graph can actually serve as a very strong signal for what you want to predict for your next word now you may be thinking well this wouldn't always be helpful and that's true is all as well so if you look at just like the third word in the sequence and you want to predict that word um so is a game for instance well if this isn't in the local knowledge graph this wouldn't be necessarily that helpful you would just do a standard language model prediction or if you're at the beginning of the sequence your local knowledge graph is empty so of course you're not going to get any signal from it so the first question they ask in kglm is how can a language model know when to use a local knowledge graph and when it might actually be useful for predicting the next word so we're going to keep the same example as a running example and we have our local knowledge graph here we now have an lstm that looks similar to the representations you've seen throughout this class and normally you've seen the lstm predicts the next word well now we're also going to use the lstm to predict the next type of the word so is the next word going to be a related entity meaning it's in the local knowledge graph already is it going to be a new entity meaning it's not in a local knowledge graph or is it going to be not an entity in which case you just revert to a normal lstm prediction and they're going to use the lstm hidden state to do this prediction of the type of the next word over this three-way three different classes that they might want to consider so in the case of super mario land as a game developed by nintendo we saw that this would be a related entity case because you saw that nintendo was in the local knowledge graph for the other cases super mario land would be a new entity case since it's the local knowledge graph is empty at that point and then any of the words between super mario land and nintendo would be non-entity as they're just a standard lstm language model prediction that doesn't involve any entities so now we need to talk about what the language model actually does in these three different scenarios to predict the next entity and the next word so we're going to keep the example up at the top in case you want to refer back to three different cases and we're going to start with the related entity case so here we assume that the next word or entity is actually in your local knowledge graph and remember that we can describe a knowledge graph in terms of triples so in terms of uh pairs of parent entities relations and tail entities and in the case of predicting the next word as nintendo there's only one possible parent entity in the local knowledge graph which is super mario land and the goal is you want to figure out what is the most relevant triple that will be useful in helping to predict the next word so in this case you could have the triple super mario land publisher nintendo you might have the triple super mario land genre platform game which of these is actually helpful in predicting that nintendo should be the next word so here what you would want kglm to do is predict that the top scoring parent entity is super mario land and the top scoring relation is publisher and you can see there are actually contextual cues in a sentence that could help you figure out uh which triple you're talking about and then given that your top scoring parent entity is super mario land and your top scoring relation is publisher you can figure out that using knowledge graph triples the tail entity has to be nintendo and therefore this gives you a strong signal that the next word will be nintendo so the goal is you're going to find the top scoring parent entity and the top scoring relation using the nodes in your local knowledge graph and you can do this by using the lstm hidden state combined with pre-trained entity and relation embeddings so i do admit i cheated here a little bit and that this does use pre-trained embeddings but hopefully you'll see by the end of this discussion why i think it fits a bit better in this external memory use case as well so what they're going to do is they're going to take a softmax using the lstm hidden state and the entity embeddings for each of the potential parent entities and then we'll take this top scoring one as a parent entity and they'll do the same thing for the relation embeddings the next entity is then just this tail entity from the knowledge graph triple so it's relatively trivial to figure out what the next entity should be once you've figured out the top scoring parent entity and your top scoring relation and then finally to predict the next word they take the vocabulary and they expand it to include different aliases that could refer to that entity so what we mean by aliases here are phrases that could refer to the entity and text so you might not just call it nintendo you might also say nintendo company or copai and you want any of these to be possible words that you could predict as a next word so the goal of this vocabulary expansion is to increase the probability that the next word you predict will actually be related to this next entity so new entity case is a bit simpler this means that the entity that you're predicting is not in the local knowledge graph so you're not getting any signal from this local knowledge graph that you've been building up and all you want to do is find the top scoring entity in the full knowledge graph and you can do this using the lstm hidden state and pretend ntm bettings similar to how we found the score for the top parent entity your next entity will just be the top scoring entity out of the full knowledge graph and then your next word is once again this vocabulary expanded to include aliases of that entity the not in empty case is the simplest you just revert to normal lstm you don't have an x entity to predict and your next word is just the most likely next token over your normal vocabulary so here's a diagram from their paper that hopefully summarizes and makes even clearer what i just went over so they have a longer example than the one we are looking at but it's the same prediction as nintendo is the next word and they have their predictions in red so this is what they want kglm to predict the three different cases are in the horizontals and we see that here you're in the related entity case since nintendo is in your local knowledge graph so they want kglm to predict that nintendo should be a related entity type of word that super mario land should be its parent entity that publisher should be the relevant relation and as a result the next entity is nintendo and then they expand their vocabulary you see the aliases aliases of nintendo at the bottom and then finally they actually predict nintendo as the next word and the other cases just summarize what we also already went over so i find that kglm actually outperforms gpt2 and awd lstm which is a strong lstm language model on a fat completion task similar to the fill in the blank examples that we looked at at the beginning of the talk they also find qualitatively that compared to gpt 2 kglm tends to predict more specific tokens since it can predict these tokens from just copying from the local knowledge graph whereas gpt2 will tend to predict more generic tokens so if you want to predict the birthplace of someone gpt2 is more likely to predict new york for example and kglm might predict some obscure place and then they had these really cool set of experiments where they showed that kglm actually supports modifying or updating facts so they made a direct change in the knowledge graph and then they saw what is the change in kglm's predictions so they have this example where um the sequence was barack obama was born on blank they had their knowledge graph triple as barack obama's original birth date and then their most likely next tokens were as expected august 4th 1961 and then they just changed their knowledge graph so they changed the birth date of obama they said okay he's now born 2013. and they look to see what the next uh predictions were for kglm and it changed its predictions to match what was in the local knowledge graph so this is something that's pretty cool and that really only external memory approaches um can do compared to these to the original pre-trained anti-embedding approaches we talked about and i think it's one of the one of the reasons that kglm at least in my opinion fits better in these external memory use cases right so the next slide is a different paper so i guess i'll take questions on kglm if there are any it's a pretty complex method so feel free to have questions yeah um could you one more time explain what the definition of the local knowledge graph is in relationship to the global knowledge graph yep um so local knowledge graph is supposed to be a subset of the full knowledge graph and it's only supposed to consist of entities that are actually have actually been seen in the sequence um as well as uh their relevant entities okay oops all right so here you see that super mario land is in the local knowledge graph because super mario land is an entity that is seen in the sequence and then you also want to copy over all the edges from super mario land that would be in the full knowledge graph so this is just a subset of them for the purpose of the example but you see that super mario land has an edge nintendo to game boy to platform game and so you would copy all edges that super mario land has to another node in the full knowledge graph and they know in advance like they have the labels here for what the entities are during training so that's how they can actually create this ground truth knowledge graph and then briefly a student asked why we can't just use the whole knowledge graph and i gave an answer but maybe you know better um yeah i think the idea is the signal will be much stronger if you just use a local knowledge graph so in the softmax for the related entity case you would just be predicting over the potential parent entities in your local knowledge graph which is a much smaller set than what's in your full knowledge graph so i guess it's more likely that you're going to predict something that is correct in that case then when you have like five million or so entities in your full knowledge graph it's also much cheaper to compute um in this case there's only a single parent entity but you could have multiple parent entities that you're trying to compute which one's most likely over is that what you were also thinking john um yeah i mainly just said uh uh efficiency so the the signal thing is cool too um here's an exciting question what about queries that require more than one uh step in the knowledge graph such as the location of the publisher of super mario land um yeah that's a good question so the idea is like can it support those types like does it support multi-hop kind of building of the knowledge graph yeah yeah how does kglm perform in those cases yeah i don't know that's a very good question they build up the knowledge graph so that it's just single hop as far as i know but like if you saw the other entities if you were to see the entities along the hops it would have them in the local knowledge graph um yeah that's a good question i don't know if they explored that great okay um let's move along then okay so the next piece of work we're going to talk about you guys have actually briefly seen in the natural language generation lecture but i'm going to go over it again quickly here um so unlike the other works that we talked about that used knowledge graph triples this is actually going to take kind of a looser notion of knowledge in that the knowledge will just be encoded in the text in the training data set so this is called k n lm and the idea is that we're building the idea that language models not only learn to predict the next word and text but they also learn these representations of text and the authors suggest that it might actually be easier to learn similarities between text sequences than it is to predict the next word in the text so you have this example that dickens is the author of blank and dickens wrote blank and they argue that it's easier to tell for a human but also for a model that these sequences are similar and they should probably have the same next word even if you don't know what the next word is um so that's suggesting that you know it's easier to learn these similarities than it is to actually predict the next word and they argue that this is even more true for long tail patterns where it's very challenging for the model to predict that the next word is some rarely seen token or rare entity than it is to find another similar sequence that it's already seen and just copy the next word from that sequence so what they propose to do is store all representations of text sequences in a nearest neighbor data store and then at inference what you'll want to do is you find the k most similar sequence as a text you then retrieve their corresponding values so you just peek at those sequences and see what were their next words and then you combine the probability from this nearest neighbor data store with just a typical language model prediction and so they call this an interpolation step in that they're waiting how much to pay attention to the probability from this k n approach and how much to pay attention to this language model approach and the lambda here is just a hyperparameter they tune so they have this diagram from their paper where they want to predict the next word in the sequence shakespeare's play blank so what they do is they have all the training contacts already encoded in their data store so they have representations of all the training contacts and then they compute a representation of their text context and they want to figure out which representations in the training context are most similar to this text test context representation and so here in the external memory view of things the keys would be the representations of the training context and the values would be the next words so to get the k nearest training representations um they then copy over their values so that's what you see with this macbeth hamlet macbeth example they have a normalization step where they convert this to probability space um and then finally they have an aggregation step so if a word is seen as the next word and several of these uh k nearest neighbors then they want to count more for that so that's why they aggregate say c macbeth twice it means macbeth is more likely um and then finally they have this interpolation step where they try to balance between the classification probabilities from the language model and from the k n approach so some immediate observation you might have is this seems really expensive they do propose ways to kind of try to minimize the expense of actually having to store all the training contacts in this data store because they actually store it for every single window of next word in the training context and you can do quantization on some nearest neighbor approaches to try to make this less expensive but i imagine this would still be pretty expensive for really large training data sets they also have some cool experiments that show that this is very good for domain adaptation so if you take your language model and you have a new domain that you want to apply your language model to you could just create a nearest neighbor data store of your new domain so you encode all the representations of that new domain you stick it in a data store and then you can just use your language model with these k n probabilities as well just immediately on this new domain without actually having to further train your language model so i thought that was a pretty cool use case of this external memory approach so while it doesn't leverage knowledge bases directly it does have this loose knowledge of or loose idea of encoding knowledge that is in a textual representation form into some external memory that the model can then take advantage of um that's all i have for this approach are there any questions on this approach well so only one person is asking how does the k n make predictions for the next word the k neighbors are for the context instead of the next word oh okay that wasn't clear um so the keys are the representations of the context the values in your external memory are the next words so when you figure out you figure out your nearest neighbors using your keys and then you copy over their values so it does actually know what the next words are for each of those representations okay um so finally we're going to talk about how you can just modify the training data to better encode knowledge and language models so approaches we've talked about so far are actually incorporating knowledge explicitly by using either pre-trained embeddings or an external memory we also want to talk about how can you just incorporate knowledge implicitly through the unstructured text so what we're going to do is either mask or corrupt the data to introduce additional training tasks that require factual knowledge to figure out what data was masked for instance so this has some clear advantages it doesn't have any additional memory or computation requirements you don't have a data store to deal with you don't have extra knowledge encoder layers to train all you do is modify the training data and you don't have to modify your architecture either so you can continue using your favorite bert model and just make these changes to the training data so the first work we're going to look at is called wklm weekly supervised knowledge pre-training language model or pre-trained language model and the key idea here is to train the model to distinguish between true and false knowledge so they're going to corrupt the data by replacing mentions in the text with mentions that refer to different entities of the same type to create what they refer to as negative knowledge statements and then the model will just predict has the entity been replaced or corrupted this type constraint is necessary to make sure that or to encourage the model to actually use factual knowledge to figure out if this corruption is taking place so you could imagine if you replace it with something that's not realistic at all the model could just be basing its prediction based on is this sentence linguistically correct so as an example we have a true knowledge statement as jk rowling is the author of harry potter and then we want to modify this to replace it with another author so let's say we change this to j r tolkien as the author of harry potter so you can see that this requires some amount of knowledge background knowledge to actually be able to figure out which statement's true and which statement is false the idea is that the model will be able to predict for each of these mentions whether it's a true or false mention so this diagram here is from the paper and hopefully explains this a bit better they have the original article on the left and then they have the replaced article with the corruptions on the right and the entities are are in blue so what they do is for a given entity they first look up its type they find other entities of that type and then they randomly sample the entity and get an alias of it to replace in the text so they're going to play stan lee for instance with brian johnson and marvel comics with dc comics and their placements are in red on the right and then the idea is that the model be able to predict for each of these mentions um was it replaced or not so in the case of brian johnson they have the red x for this is a false mention and in the case of the true mentions they have the check mark so it's a pretty simple approach but they actually show that it can help the model increase the amount of knowledge that's encoded in parameters okay so wklm uses an entity replacement loss to train the model to distinguish between these true and false mentions and this just looks like a binary classification loss where your true mentions are on the left and your false mentions are on the right and you want to increase the probability that this p of e given c so the probability entity given the context you want to increase that for the true mentions and decrease it for the false mentions the total loss is then just a combination of the mass language model loss and this entity replacement loss the mass language mods the mass language model loss is defined at the token level and the entity replacement loss is defined at the entity level meaning it's not just over sub words it's even potentially over words if you have multi-word entities phrases for instance and this is an important point or an important theme that we really see occurring throughout these works that we'll look at in that modifying the data at the entity level seems to be an important component of actually increasing the amount of knowledge that a language model can encode so they find that wklm improves over bert and gpt2 in fact completion tasks like the fill in the blank statements that we looked at at the beginning they also find that it improves over the ernie paper that we talked about on a downstream task and they had a set of ablation experiments where they looked at can you just remove this mass language model loss now and if you just train bert for longer do you really need this entity replacement loss so that's what the table here is looking at um the second row is looking at if we remove the mass language model lost what happens we see that it performs much worse without the mass language model loss so you really need both losses their intuition there was the mass language model loss helps to encode just general language understanding and then training bert for longer um performs much worse than using his entity replacement loss so this motivates even farther that you really do need um or the entity replacement loss is actually really helping encode more knowledge in these language models so in addition to corrupting the data we're also going to look at can we just mask the data differently can we be more clever about how we do the masking and this is a thread in several recent works so there's actually another paper called ernie so this is different than the one we talked about before and this is enhanced representation through knowledge integration and what they do is show improvements on downstream chinese nlp tasks by doing phrase level and entity level masking so instead of just masking out sub words they're going to mask out phrases of multiple words and entities the full phrase of an entity which corresponds to some entity and text that they might find with like nar techniques for example and then the second work is actually something you heard about in the last lecture which is the idea of using salient span masking to mask out salient spans and a saline span is just a named entity or a date so you can see this is pretty similar to what ernie is doing and they found that using salient span masking actually significantly helped t5 performance on these closed domain question answering tasks so just to make sure we're all on the same page um with the different masking techniques this diagram from the ernie paper is comparing to what bert does versus what ernie does the top shows that ernie masked out the subword tokens or that bert massed out the subway tokens whereas ernie massed out phrases like a series of as well as entities like jk rowling there's some interesting results on showing that salient span masking is helping encode more knowledge in these representations so on the left we're looking at the results of the original paper that proposed salient sand salient span masking so this is the realm work and the idea here was that they were training a knowledge retriever so it's actually more of an external memory class of techniques but they find that by using the salient span masking technique they could actually train a much better knowledge retriever so it's a good example of how these techniques are really complementary so while i presented three classes of techniques you can definitely get benefits by doing multiple techniques together and they found that doing silence fan masking compared to using masking from bert which would be the random uniform masks or doing random masking of spans from a paper called spanbert um it performs much better to do salience band masking so you see like a 38 exact match score versus like a 32 exact match score for instance and on the right we have results um from fine tuning t5 with either ceiling span masking or the span corruption task that you saw in assignment five and you can see that on these different qa data sets science band masking does significantly better than just using the span corruption technique so this really suggests that doing the salient span masking and masking out these salient spans of these entities is in fact helping to encode more knowledge in these language models so to recap we talked about three different classes of techniques to add knowledge to language models we talked about using pre-trained entity embeddings these weren't too difficult to apply to existing architectures and as a way to leverage this knowledge graph pre-training but it's a rather indirect way of incorporating knowledge and it could be hard to interpret we also talked about approaches to add an external memory this could support modifying the knowledge base it was also easier to interpret but they tended to be more complex in implementation like we saw at kglm and they also required more memory like we saw with the k nlm approach and then finally we talk about modifying the training data so this requires no model changes or additional computation it also might be the easiest to theoretically analyze so it's actually an active area research right now but still an open question if modifying the training data is always as effective as model changes and what the trade-offs are in terms of amount of data required versus doing one of these other knowledge enhancement approaches so that leads us to section three um so i guess i'll pause again for questions i think we may be good awesome okay um so section three is about how researchers are actually going about evaluating the knowledge and language models and um i guess how some of the techniques we actually just talked about stand up in this evaluation so first we're going to talk about probes which don't require any fine tuning of the language model and then we're going to talk about downstream tasks which look at how well do these pre-trained representations actually transfer their knowledge to other tasks so one of the initial works in this area was called llama and this really started a series of works to look into how much knowledge is already encoded in these language models so their question was how much relational common sense and factual knowledge is in off-the-shelf language models such as taking pre-trained language models and evaluating the knowledge in them and this is without any additional training or fine-tuning so they mainly constructed a set of what they refer to as closed statements and these are just the fill in the blank statements that we actually drew from at the beginning of the talk we have some more examples here and they manually created these templates of closed statements using knowledge graph triples and question answering pairs from existing data sets they wanted to compare pre-trained language models to supervise relation extraction and question answering systems to see how do these language models that were trained in an unsupervised fashion compared to these baseline systems that are not only supervised but really targeted for this task of knowledge extraction and their goal was to evaluate the knowledge in existing pre-trained language models and a key point about this is like they're just using the language models as they are available to researchers so this means there could be differences in the pre-training corpora for example so when you look at the following table and you're comparing language models also keep in mind that these don't like account for the differences in the pre-trained corpora so a lot of these language models probably look familiar to you either from previous lectures or maybe your final projects and what we see is that overall bert base and bert large pre-trained models are performing much better than the previous language or the other language models here i guess i forgot to mention what mean precision at one is um this is a pretty simple metric the idea is if you look at the blank and you look at the top predictions for the top prediction for the blank is it correct or not so that's what precision at 1 means precision at 10 would be let's look at the top 10 predictions is the correct prediction in the top 10. um so in addition to bert large and base performing well overall we do see that in the t-rex data set the relation extraction baseline is performing a bit better than bert one thing they notice here that's pretty interesting is that this data set has a lot of different types of relations and relations can be classified in terms of are they a one-to-one relation are they an end-to-one relation are they an end-to-end relation an example of a one-to-one relation would be your student id relation so you have a unique student id an example of an end-to-end relation would be they enrolled in relation so there's lots of students enrolled in lots of classes so this would be an end-to-end relation and they find that bert really struggles on these end-to-end relations so while it performs better than relation extraction baseline on some types of relations overall it does pretty terribly on these end-to-end relations so overall it does a bit worse than the baseline on this trx data set they also compare to squad on docker qa and they find that it does a fair amount worse they note that the language model is not fine-tuned here and also has no access to an information retrieval system and then they look at the precision at 10 they find that this gap between docker qa's performance and bert actually closes quite a bit which suggests that these language models do have some amount of knowledge encoded in them and that they're even competitive with these knowledge extraction supervised baselines so you can also try out examples on their github repo for the llama probe we have an example that was from their repo that was the cat is on the mask you can see what the top 10 predictions are to fill in the closed statement here they have the cat is on the phone so this can be a fun way just to figure out what factual common sense knowledge is in existing language models and it's pretty easy to use with this interactive prompt so some limitations of the llama probe are that it can be hard to understand why the models perform well when they do so for instance bert might just be predicting the most popular token and this happens to be right maybe it's just memorizing co-occurrence patterns and doesn't really understand the knowledge statement and doesn't understand what the fact is it might also just be identifying similarities between surface forms of the subject and object so for instance example pope clement vii has a position of blank even if you don't know anything about pope clement vii you might be able to figure out that pope is a likely next word for this uh triple or for this template so the problem with this is if the model is just making these predictions based on these surface forms or co-occurrence patterns it's difficult to know for actually evaluating the knowledge in the model maybe it's just making correct predictions for other reasons and the more subtle issue that we've brought up is that language models might be just sensitive to the phrasing of the statement so for each uh triple in their data set or for each relation their data set they just had one manually defined template and qualitatively they found that if they just make small changes as a template it could actually change whether or not the model could recall the correct prediction or not and so this means that the probe results are really a lower bound on the knowledge that's encoded in the language model so if you change the phrasing it's possible that the model might show that actually does have the knowledge encoded in it so the next lines of work we'll talk about um are really building on these two limitations of this original llama probe so the first one is called lama oon or llama unhelpful names and the key idea is to remove these examples from lama that can be answered without the relational knowledge so this is kind of addressing the first limitation on the last slide so they observed that bert relies under surface forms entities might not be using knowledge to make these predictions this includes the string match situation that we talked about with the pope this also is dealing with the revealing person name issue that you saw in assignment five so this is where the name could be an incorrect prior for the native language of someone their place of birth their nationality they have this example from the table or from the paper where they look at different people names or person's names and then they look at births prediction for their native language and these are all french speaking actors and burt just predicts very biased and stereotypical languages for these particular names so this can really work both ways it can lead bert to make incorrect predictions and sometimes or in some cases but it could also work to make or to let bert make correct predictions even if it has no factual knowledge of those people so that's the issue they're trying to get at here is do we know that bert actually knows this fact or is it just using some bias to make its prediction so what they do is they introduce a couple heuristics to basically just filter out these examples from the llama probe um that can either be solved by the string match setting or the surveilling person name setting so they make a harder subset of the llama data set essentially they find that when they test bert on this harder subset that its performance drops about eight percent but when they test their knowledge enhanced model which they call ebert the score only drops about one percent so it's possible that as we make harder knowledge probes we'll actually see even bigger differences in the performance of knowledge enhanced models to models without these knowledge enhancements the next piece of work we'll talk about is actually getting at this issue of um the phrasing of the the prompt might actually trigger different responses from the language model so the language model might know the fact but it might fail on the task due to the phrasing one reason this might happen is the pre-training is on different contexts and sentence structures in the query so for example you might have in your pre-training corpus the birthplace of barack obama is honolulu hawaii and this might be something you see in wikipedia for instance that's a common training data set and then as a researcher you write barack obama is born in blank and you can see that these sentence structures are pretty different so the model might have seen the first fact but the sentence structure difference is actually enough to confuse it so it can't answer this query so what they do is they generate a lot more of these prompts by mining templates from wikipedia one of their techniques actually uses dependency parsing and also generating paraphrase prompts by taking inspiration from the machine translation literature and using back translation so generate a lot more prompts to try to query the language models and figure out do small variations in the prompt trigger the correct prediction from the language model they also experiment then sombling prompts so if we give the model multiple prompts and then take some probability averaged over these different prompts can we improve the performance on the model returning the correct prediction so we give it a higher chance of seeing a context that it might have actually seen during pre-training they find that the performance on llama increases when they either use a top performing prompt or when they use this ensembling approach so this suggests that the original llama really was a lower bound on the amount of knowledge encoded in these language models and changing the phrasing can actually help the model recall the correct answer this table table's a bit frightening but they find that small changes in the query can lead to really large gains on performance so if you just have a query like x plays in y position and then you change that to x plays that y position this can actually lead to like a 23 accuracy gain on this particular relation in terms of the model actually being able to recall the correct answer or even just x was created in a y to x is created in y 10 accuracy gain so i think this motivates the need to not only develop better ways to query these models but probably also build language models that are actually more robust to the query itself so in addition to probes another way to evaluate these language models is by looking at how well they transfer from the pre-trained representation to downstream tasks and so the idea here is you're actually going to fine-tune the pre-trained representation on different downstream tasks similar to how you would evaluate bert on glue tasks some common tasks that are used for this are relation extraction entity typing and question answering relation extraction is where you want to predict the relation between two entities so this is getting back at one of the questions earlier in the talk in terms of well how do you get the relation that's the edges in these knowledge bases so given two entities you learn a model to predict what is a relation between them entity typing is a task of given an entity what is the type of the entity so here alice rob the bank you want to predict her as a criminal and then you guys are very familiar with question answering so the idea of these comment of these tasks is that they're knowledge intensive so they're good candidates to see how well do these pre-trained representations actually transfer their knowledge to these downstream tasks here we're looking at the performance on a relation extraction benchmark called tacrid and all the models that we show here were at one point state of the art on tacrid so this cgcn is a graph convolutional neural network over dependency trees the burt lstm base is a it's one of the first works that showed that you could actually get state of the art performance with bert on relation extraction and this is just putting lstm layer over bert's output ernie is the work that we talked about with the pre-trained entity embeddings matching the blanks we didn't get to today but it's a really interesting work about learning meaningful relation representations and it falls more into the training data modification approaches and that they are actually masking out entities again and then no birt is what we talked about um the w and w here means they actually encode two knowledge bases in novert so they're encoding wordnet and they're also encoding wikipedia and the high level takeaway from this table is that you can see that the recent knowledge enhanced models have achieved state of the art over the original models that once performed very well on tack grid and you we have about five f1 gains here another interesting takeaway from this table is there seems to be a trade-off in the size of the language model that's necessary to get a certain performance so if you just um consider the size of the language model then no birth performs the best but if you don't consider that then it's highs with matching the blanks so overall this is pretty good evidence that these knowledge enhanced methods are in fact transferring to these knowledge intensive downstream tasks that can really take advantage of these pre-trained representations we also have results on entity typing so here we're comparing a slightly different set of models some of the baselines are lstm models that were designed for entity typing and we have ernie and nobert um leading the the i guess leaderboard here on the entity typing task of open entity and we see gains of about 15 f1 points with ernie and no bert so once again we really do see that um these knowledge rich pre-trained representations are transferring and helping on these knowledge intensive downstream tasks so just to recap we talked about probes which evaluate the knowledge already present in models these don't require any more training but it can be challenging to construct benchmarks to actually make sure you're testing the knowledge in these language models it can also be challenging to construct the queries used in the probe we then talked about downstream tasks these are a bit of an indirect way to evaluate knowledge and that they have this extra component of fine tuning but it's a good way to evaluate how useful is this knowledge-rich pre-trained representation in actual applications so i just uh touched on the exciting work in this area but there's many other directions if you want to dive more into this so there's retrieval augmented language models which learn knowledge retrievers to figure out what documents might be relevant for predicting the next word there's work in modifying the knowledge in language models so i talked about how this is one of the obstacles and challenges to using language models as knowledge bases so there's been recent work in this area we also saw how important the knowledge pre-training task was well there's many papers that are proposing different tasks to do the knowledge pre-training so it's still an open question in terms of what tasks are best to add to encode more knowledge there's also been work on more efficient knowledge systems so at nurse there's now an efficient qa challenge which aims at building the smallest qa system and then finally there's been work on building better knowledge benchmarks that build on the benchmarks that we saw today that's all i have for today and i hope your final projects are going well