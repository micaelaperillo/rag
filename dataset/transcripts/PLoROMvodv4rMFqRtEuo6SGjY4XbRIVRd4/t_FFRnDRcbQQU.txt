okay hi everyone so we'll get started again we're now into um week seven of cs224 n um if you're following along the syllabus really closely we actually did a little bit of a rearrangement in classes and so today it's me and i'm going to talk about co-reference resolution which is another chance we get to take a deeper dive into a more linguistic topic they will also show you a couple of new things for deep learning models at the same time and then the lecture that had previously been scheduled at this point which was going to be john on explanation in neural models is being shifted later down into week nine i think it is um but you'll still get him later um before getting underway just a couple of announcements on things well first of all congratulations on surviving assignment 5 i hope i know it was a bit of a challenge for some of you but i hope it was a rewarding state of the art learning experience on the latest in neural nets and at any rate you know this was a brand new assignment that we use for the first time this year so we'll really appreciate later on when we do the second survey getting your feedback on it we've been busy reading people's final project proposals thanks lots of interesting stuff there our goal is to get them back to you tomorrow um but you know as soon as you've had a good night's sleep after assignment five now is also a great time to get started working on your final projects because there's just not that much time till the end of quarter and i particularly want to encourage all of you to chat to your mentor regularly go and visit office hours and keep in touch get advice just talking through things is a good way to keep you on track we also plan to be getting back assignment for grades later this week there's sort of the work never stops at this point so the next thing for the final project is the final project milestone um so that we handed out the details of that last friday and it's due a week from today um so the idea of this final project milestone is really to help keep you on track and keep things moving towards having a successful final project so our hope is that sort of most of what you write for the final project milestone is material you can also include in your final project except for a few paragraphs of here's exactly where i'm up to now um so the overall hope is that doing this in two parts and having a milestone before the final thing it's just making you make progress and be on track for having a successful final project um finally um the next class um on thursday is going to be colin raphael this is going to be super exciting so he's going to be talking more about the very latest in large pre-trained language models both what some of their successes are and also what some of the disconcerting not quite so good aspects of those models are so that should be a really good um interesting lecture when we had him come and talk to our nlp seminar we had several hundred people come along for that and so for this talk again we're asking that you write a reaction paragraph following the same instructions as last time um about what's in this lecture and someone someone asked in the questions well what about last thursdays the answer that is no so the distinction here is we're only doing the reaction paragraphs for outside um guest speakers and although it was great to have untron boss lou for last thursday's lecture he's a postdoc at stanford um so we don't count him as an outside guest speaker and so nothing needs to be done for that one so there are um three classes for which you need to do it um so there was the one before from dungeon um colin raphael which is thursday and then towards the end of the course um there's euless vipkov okay so this is the plan today so in the first part of it i'm actually going to spend a bit of time talking about what co-reference is what different kinds of reference and language are and then i'm going to move on and talk about some of the kind of methods that people have used for solving co-reference resolution now there's um one bug in our course design which was a lot of years we've had a whole lecture on doing convolutional neural nets for language applications and that slight bug appeared um the other day when um danchi talked about the baidaf model because she sort of slipped in oh there's a character cnn um representation of words and we hadn't actually covered that and so that was a slight oopsie i mean actually for applications in co-reference as well people commonly make use of character level confnets so i wanted to sort of spend a few minutes sort of doing basics of confidence for language the sort of reality here is that um given that there's no exam week this year to give people more time for final projects we sort of shorten the content by a week this year and so you're getting a little bit less of that content then going on from there um say some stuff about a state of the art new york co-reference system and right at the end talk about um how co-ref is evaluated and what some of the results are yeah so first of all uh what is this co-reference resolution term that i've been talking about a lot so co-reference resolution is meaning to mention to find all the mentions in a piece of text that refer to the same entity and sorry that's a typo it should be in the world not in the word um so let's make this concrete so um here's part of a short story by shruti rao called the star now i have to make a confession here um because this um is an nlp class um not a literature class i crudely made some cuts to the story to be able to have relevant parts um appear on my slide in a decent sized font for illustrating co-reference so it's not quite the full original text but um it basically um is a piece of this story right so what we're doing in co-reference resolution is we're working out what people are mentioned okay so here's a mention of a person vanaga and here's a mention of another person akilah and well mentions don't have to be people right so the local park that's also a mention and then here's akila again and akilah's son and then there's prajwal um then there's another son here and then her son and akash and they both went to the same school um and then there's a pre-school play and there's prajwal again um and then there's um a naughty child lord krishna and there's some that are a bit complicated like the lead role is that a mention it's sort of more of a functional specification of something in the play um there's akash and it's a tree um i won't go through the whole thing yet but i mean in general there are noun phrases that are mentioning things in the world and so then what we want to do for co-reference resolution is work out which of these mentions are talking about the same real world entity so um if we start off um so there's banaja um and so vanadja is the same person as her there and then we can read through um she resigned herself so that's both bernardjah um she bought him a brown t-shirt and brown trousers then oops then she made a large cut out tree um she attached right so all of that's about the narja but then we can have another person so here's a killer and here's a killer um and maybe those are the only mentions of aquila so then we can go on from there um okay and so then there's prajwal but note that prajwal note that prajwal is also aquila's son so really aquila's son is also prajwal and so an interesting thing here is that you can get nested syntactic structure so that we have um these sort of noun phrases so that you know overall we have sort of this noun phrase akila's son prajwal which consists of two noun phrases in opposition he is prajwal and then for the noun phrase akilah's son it sort of breaks down to itself having an extra possessive noun phrase in it and then a noun so that you have achilles and then this is sun so that you have these multiple noun phrases and so that you can then be sort of having different parts of this um be one person in the co-reference but this noun phrase here referring to a different person in the co-reference okay so back to um prajwal um right so well there's there's some easy other projects right there's protrol here um um and then you've got some more complicated things so one of the complicated cases here is that we have they went to the same school so that they there is what gets referred to as split antecedents because the they refers to both prajwal and akash and that's an interesting phenomenon that and so i could try and show that somehow i could put um some splashes in or something and if i get a different color akash we have akash and her son and then this one sort of both of them at once um right so um human languages have this phenomenon of split antecedence but you know one of the things that you should notice um when we start talking about algorithms that people use for doing co-reference resolution is that they make some simplified assumptions as they how do they go about treating the problem and one of the simplifications that most algorithms make is for any noun phrase like this pronoun they that's trying to work out what is it co-reference with and the answer is one thing and so actually most nlp algorithms for co-reference resolution just cannot get split and sequenced might any time it occurs in the text they guess something and they always get it wrong so that's the sort of a bit of a sad state of affairs but that's the truth of how it is okay so then going ahead we have akash here and then we have another tricky one so moving on from there um we then have this a tree so well in this context of this story um akash is going to be the tree so you could feel that it was okay to say well this tree um is also akash um you could also feel that that's a little bit weird and not want to do that and i mean actually um different people's co-reference data sets um differ in this so really that you know that we're predicating an identity relationship here between akash and the property of being a tree so do we regard the tree as the same as akash or not and people make different decisions there okay but then going ahead we have here's akash and she bought him so that's akash and then we have akash here and so then we go on okay um so then if we don't regard the tree as the same as akash we have a tree here um but then note that the next place over here well we have a mention of a tree the best tree but um that's sort of really a functional description of you know of possible trees making someone the best tree it's not really referential to a tree um and so it seems like that's not really co-referent but if we go on there's definitely um more mention of a tree so when she um she has made the tree truly the nicest tree or well i'm not sure is that one co-referent it the it is definitely referring to our tree and maybe this one again is a sort of a functional description that isn't referring to the tree um okay um there's different um and so um maybe this one though where it's a tree is referring to the tree but what i hope to have illustrated from this is you know most of the time when we do co-reference in nlp we just make it look sort of um like the conceptual phenomenon is you know kind of obvious that there's a mention of sarah and then it says she and you say ah they're co-referent um this is easy but if you actually start looking at real text especially when you're looking at something like this that is a piece of literature the the kind of phenomena you get for co-reference and overlapping reference and various other phenomena that i'll um talk about you know they actually get pretty complex and it's not you know there are a lot of hard cases that you actually have to think about as to what things you think about as co-referent or not okay but um basically we do want to be able to do something with co-reference because it's useful for a lot of things that we'd like to do in natural language processing so for one task that we've already talked about question answering but equally for other tasks such as summarization information extraction if you're doing something like reading through a piece of text and you've got a sentence like he was born in 1961. um you really want to know who he refers to to know if this is a good answer to the question of you know when was barack obama born or something like that um it turns out also that it's useful in machine translation so in most languages pronouns have features for gender and number and in quite a lot of languages um nouns and adjectives also show features of gender number and case and so when you're translating a sentence you want to be aware of these features and with what is co-referent as what to be able to get the translations correct so you know if you want to be able to con work out a translation and know whether saying alicia likes one because he's smart or alicia likes one because she's smart then you have to be sensitive to co-reference relationships to be able to choose the right translation when people build dialogue systems dialogue systems also have issues of co-reference a lot of the time um so you know if it's sort of book tickets to see james bond and the system replies spectra is playing near you at two and three today well there's actually co-reference relation oh sorry there's a reference relation between specter and james bond because spectra is a james bond film i'll come back to that one in a minute um but then it's how many tickets would you like two tickets for the showing at three that three is not just the number three that three is then a co-reference relationship back to the 3 p.m showing that was mentioned by the agent in the dialogue system so again to understand these we need to be understanding the co-reference relationships so how now can you go about doing co-reference so the standard traditional answer which i'll present first is co-reference is done in two steps on the first step what we do is detect mentions in a piece of text and that's actually pretty easy problem and then in the second step we work out how to cluster the mentions so as in my um example from the shruti rao text basically what you're doing with co-reference is you're building up these clusters sets of mentions um that refer to the same entity in the world so if we explore a little how we could do that it's a two-step solution the first part was detecting the mentions and so pretty much there are three kinds of things different kinds of noun phrases that can be mentioned there are pronouns like i you're it she him and also some demonstrative pronouns like this and that and things like that there are explicitly name things so things like paris joe biden nike and then there are plain noun phrases that describe things so a dog the big fluffy cat stuck in the tree and so all of these are things that we'd like to identify as mentions and well the straightforward way to identify these mentions is to use natural language processing tools several of which we've talked about already so to work out pronouns we can use what's called um a part of speech taker i'll change this choice um we can use a part of speech tagger which we haven't really explicitly talked about but we used when you build dependency parsers so that first of all assigns parts of speech to each word and so that we can just find the words that are pronouns for named entities we did talk just a little bit about named entity recognizers as a use of sequence models for neural networks so we can pick out things like person names and company names and then for the ones like the big fluffy um a big fluffy dog we could then be sort of picking out from syntactic structure noun phrases and regarding them as descriptions of things um so that we could use all of these tools and those would give us basically our mentions it's a little bit more subtle than that because it turns out there are some noun phrases and things of all of those kinds which don't actually refer so that they're not referential in the world so when you say it is sunny it doesn't really refer when you make universal claims like every student well every student isn't referring to something you can point to in the world and more dramatically when you have no student and making a negative universal claim it's not referential to anything there are also things that you can describe functionally which don't have any clear reference so if i say the best doughnut in the world that that's a functional claim but it doesn't necessarily have reference like if i've established um that i think a particular kind of donut is the best doughnut in the world um i could then say to you um i ate the the best don't i ate the best doughnut in the world yesterday and you know what i mean it might have reference but if i say something like i'm going around to all the donut stores trying to find the best doughnut in the world then it doesn't have any reference yet it's just a sort of a functional description i'm trying to satisfy you also then have things like quantities a hundred miles um it's a quantity there's not really something that has any particular reference you can mark out a hundred all sorts of places um so how do we deal with those things that aren't really mentions well one way is we could train a machine learning classifier to get rid of those spurious mentions but actually mostly people don't do that most commonly um if you're using this kind of pipeline model where you use a parser and a named entity recognizer you regard everything as you've found as a candidate mention and then you try and run your co-ref system and some of them like those ones hopefully aren't made co-referent with anything else and so then you just discard them at the end of the process hey chris yeah um we've got an interesting question that linguist experience on this a student asks can we say that it is sunny has it referring to the weather um i think the yeah so that's so that's a fair question um yeah so people have actually tried to suggest that when you say it is sunny it means the weather is sunny but i guess the majority opinion at least is um that isn't plausible and i mean for i guess many of you aren't native speakers of english but similar phenomena occur in many other languages i mean it just intuitively doesn't seem plausible um when you say it's sunny or it's raining today that you're really saying that as a shortcut for the weather is raining today it just seems like really what the case is is english likes to have something filling the subject position and when there's nothing better to fill the subject position you stick it in there um and get it's reigning um and so in general it's believed that you get this phenomenon of having these empty dummy it's that appear in various places i mean another place in which it seems like you clearly get dummy it's is that when you have clauses that are subjects of a verb you can move them to the end of the sentence so if you have a sentence where you put a clause in the subject position they normally in english sound fairly awkward so it's you have a sentence something like um that cs24n is a lot of work is known by all students um people don't normally say that the normal thing to do is to shift the clause to the end of the sentence but when you do that you stick in a dummy it to fill the subject position um so you say then have it is known by all students that cs224n is a lot of work um so that's the general feeling that this is a dummy yet that doesn't have any reference um okay there's one more question so if someone says it is sunny among other things and we ask how is the weather okay good point you've got me on that one right so someone says how is the weather and you answer it is sunny it then does seem like the it is in reference to the weather i'll buy that well you know i guess um this is what our co-reference systems are built trying to do in situations like that they're making a decision of co-reference or not and i guess what you'd want to say in that case is it seems reasonable to regard this one as co-referenced that weather that did appear before it i mean but that also indicates another reason to think that in the normal case it's not co-referent right because normally pronouns are only used when their reference is established that you've referred to uh noun like um john is answering questions and then you can say he types really quickly and it seemed odd to just sort of start the conversation by he touched really quickly because it doesn't have any established reference whereas that doesn't seem to be the case it seems like you can just sort of um start a conversation um by saying it's raining really hard today and that doesn't sound odd at all okay um so i've sort of there presented the traditional picture but you know this traditional picture doesn't mean something that was done last millennium before you were born i mean essentially um that was the picture until about um that essentially every co-reference system that was built um use tools like part of speech taggers ner systems and parsers to analyze sentences to identify mentions and to give you features for co-reference resolution and i'll show a bit more about that later but more recently in our newel systems people have moved to avoiding traditional pipeline systems and of doing one-shot end-to-end um co-reference resolution systems so if i skip directly to the second bullet there's a new generation of neural systems where you just start with your sequence of words and you do the maximally dumb thing you just say let's take all spans commonly with some heuristics for efficiency but you know conceptually all subsequences of this sentence they might be mentions let's feed them in to a new network which will simultaneously do mention detection and co-reference resolution end-to-end in one model and i'll give an example of that kind of system later in the lecture okay is everything good to there and i should go on okay um so i'm going to get on to how to do um do co-reference resolution systems um but before i do that i do actually want to show a little bit more of the the linguistics of um co-reference because they're actually a few more interesting things to understand and know here i mean when we say co-reference resolution um we really confuse together two linguistic things which are overlapping but different and so it's really actually good to understand the difference between these things so there are two things that can happen one is that you can have mentions which are essentially alone but happen to refer to the same entity in the world so if i have a piece of text that said barack obama traveled yesterday to nebraska obama was there to open a new meat processing plant or something like that i've mentioned with barack obama and obama there are two mentions there they refer to the same person in the world they are co-referent so that is true co-reference but there's a different but related linguistic concept called anaphra and anaphora is when you have a textual dependence of an anaphor on another term which is the antecedent and in this case the meaning of the anaphor is determined by the antecedent in a textual context and the canonical case of this is pronouns so when it's barack obama said he would sign the bill um he is an anaphor it's not a word that independently we can work out what its meaning is in the world apart from knowing the vagus feature that it's referring to something probably male but in the context of this text we have that this anaphor is textually dependent on barack obama and so then we have an anaphoric relationship which sort of means they um refer to the same thing in the world and so therefore you can say i'm their co-referent so the picture we have is like this right so for co-reference we have these separate textual mentions which are basically standalone which refer to the same thing in the world whereas in anaphora we actually have a textual relationship and you know you essentially have to use pronouns like he and she in legitimate ways in which the hearer can reconstruct the relationship from the text because they can't work out what he refers to if that's not there um and so that's a fair bit of the distinction but it's actually a little bit more to realize because there are more complex forms of anaphra which aren't co-reference because you have a textual dependence but it's not actually one of reference and so this comes back to things like these quantifier noun phrases um that don't have reference so when you have sentences like these ones every dancer twisted her knee well this her here has an anaphoric dependency on every dancer or even more clearly with no dancer twisted her knee the her here has an anaphoric dependence on no dancer but um for no dancer twisted her knee um no dancer isn't referential it's not referring to anything in the world and so there's no co-referential relationship because there's no reference relationship but there's still an anaphoric relationship between these two noun phrases um and then you have this other complex case that turns up quite a bit where you can have where the things being talked about do have reference but an anaphoric relationship is more subtle than identity so you commonly get constructions like this one we went to a concert last night the tickets were really expensive well the concert and the tickets are two different things they're not co-reference co-referential but in interpreting this sentence what this really means is the tickets of tickets to the tickets to the concert right and so there's sort of this hidden not not said dependence where this is referring back to the concert and so what we say is that these the tickets um does have an anaphoric dependence on the concert but they're not co-referential and so that's referred to as bridging and aphra and so overall there's the simple case and the common case which is pronominal and aphra where it's both co-reference and anaphora you then have other cases of co-reference such as every time you see a mention of the every time the united states is said it's co-referential with every other mention of the united states but those don't have any textual dependence on each other and then you have textual dependencies like bridging and aphra which aren't co-reference phew that's probably about us now i was going to say that's probably as much linguistics as you wanted to hear but actually i have one more point of linguistics um one or two of you but probably not many might have been troubled um by the fact that the term anaphra as a classical term um means that you are looking backward for your antecedent um that the anna part of anaphra means that you're looking backward for your antecedent and in um sort of classical terminology you have both anaphora and cataphra and it's cataphra where you look forward for your antecedent cataphra isn't that common but it does occur here's a beautiful example of cataphra so this is from oscar wilde from the corner of the divan of persian saddlebags on which he was lying smoking as was his custom innumerable cigarettes lord henry watten could just catch the gleam of a honey sweet and honey cup of the honey sweet and honey-colored blossoms of a leburnum okay so in this example here right the he and then this his are actually referring to lord henry watan and so these are both examples of cataphra in in modern linguistics um even though most reference to pronouns is backwards um we don't distinguish on in terms of order and so the term anaphor and anaphora is used for textual dependence regardless of whether it's forward or backwards okay a lot of details there but taking stock of this um so the basic observation is um language is interpreted in context that in general you can't work out the meaning or reference of things without looking at the context of the linguistic utterance so we've seen some simple examples before um so for something like word sense disambiguation you've if you see just the words the bank you don't know what it means and you need to look at a context to get some sense as to whether it means a financial institution or the bank of a river or something like that and so um a nephron co-reference give us additional examples where you need to be doing contextual interpretation of language so when you see a pronoun you need to be looking at the context to see what it refers to and so if you think about um text understanding as a human being does it reading a story or an article that we progress through the article from beginning to end and as we do it we build up a pretty complex discourse model in which new entities are introduced by mentions and then they're referred back to and relationships between them are established and they take actions and things like that and it sort of seems like in our head that we sort of build up a kind of a complex graph like discourse representation of us of a piece of text with all these relationships and so part of that is these anaphoric relationships and co-reference that we're talking about here and indeed in terms of cs224n the only um kind of whole discourse meaning that we're going to look at is looking a bit at an afro and co-reference but if you want to see more about higher level natural language understanding you can get more of this next quarter in cs224u so i want to tell you um a bit about several different ways of doing co-reference so broadly there are four different kinds of co-reference models so the traditional old way of doing it was rule-based systems and um this isn't the topic of this class and this is pretty archaic at this point this is stuff from last millennium but i wanted to say a little bit about it because it's actually kind of interesting as sort of food for thought as to how far along we are aren't in solving you know artificial intelligence and really being able to understand texts um then there are sort of classic machine learning methods of doing it which you can sort of divide up as mentioned pair methods mention ranking methods and really clustering methods and i'm sort of going to skip the clustering methods today because most of the work especially most of the recent work implicitly makes clusters by using either mentioned pair or mentioned ranking methods and so i'm going to talk about a couple of neural methods for doing that okay but first of all let me just tell you a little bit about rule-based co-reference so there's a famous historical algorithm in nlp for doing pronoun and afro resolution which is referred to as the hobbs algorithm um so everyone just refers to it as the hobbes algorithm and if you sort of look up a textbook like giraffsky and martin's textbook it's referred to as the hobbs algorithm but you know actually if you go back to jerry hobbs that's his picture over there in the corner um if you actually go back to his original paper he refers to it as the naive algorithm and then his naive algorithm for pronoun co-reference was this sort of intricate handwritten set of rules to work out co-reference so this is the start of the set of the rules um but there are more rules or more clauses of these rules for working out co-reference um and you know this looks like a hot mess but the funny thing was that this set of rules for determining co-reference were actually pretty good and so in the sort of 1990s and 2000s decade even when people were using machine learning based systems for doing co-reference they'd hide into those machine learning based systems that one of their features was the hobbs algorithm and that the predictions it made with a certain weight was then a feature in making your final decisions and it's only really in the last five years that people have moved away from using the hobbs algorithm let me give you a little bit of a sense of how it works okay so on the hobbs algorithm here's our example this is an example from a guardian book review niall ferguson is prolific well-paid and a snappy dresser stephen moss hated him okay so what the hobbs algorithm does is we start with a pronoun oops we start with a pronoun um and then it says step one go to the np that's immediately dominating the pronoun and then it says go up to the first nprs call this x and the path p then it says traverse all branches below x to the left of p left to right breadth first so then it's saying to go left to right for other branches below breadth first um so that's sort of working down the tree so we're going down and left to right and look for um an np okay and here's an np but then we have to read more carefully and say propose as antecedent any np that has an np or s between it and x well this np here has no np or s between npn and x so this isn't a possible antecedent so this is all very you know complex and handwritten but basically he sort of fit into the clauses of this kind of a lot of facts about how the grammar of english works um and so what this is capturing is if you imagine a different sentence you know if you imagine the sentence stephen moss's um brother hated him well then stephen moss would naturally be co-referent with him and in that case well precisely what you'd have is the noun phrase um with um well the noun brother and you'd have another noun phrase inside it um for the stephen moss and then that would go up to the sentence so in the case of stephen moss's brother when you looked at this noun phrase there would be an intervening noun phrase before you got to the node x and therefore um stephen moss is a possible and in fact good antecedent of him and the algorithm would choose stephen moss but the algorithm correctly captures when you have the sentence stephen moss hated him that him cannot refer to steven moss okay so um having worked that out it then says if x is the highest s in the sentence okay so my x here is definitely the highest s in the sentence because i've got the whole sentence what you should do is then traverse the pause trees of previous sentences in the order of recency so what i shouldn't do now is sort of work backwards in the text one sentence at a time going backwards looking for an antecedent um and then for each tree traverse each tree left to right breadth first so then within each tree i'm doing the same of going breadth first so sort of working down and then going left to right with an equal breadth and so hidden inside these clauses it's capturing a lot of the facts of how co-reference typically works so what you find in english i'll say but in general this is true of lots of languages is that there are general preferences and tendencies for co-reference so a lot of the time a pronoun will be co-referent with something in the same sentence like steven's moss's brother hated him but it can't be if it's too close to it so you can't say stephen moss hated him and have the him be stephen moss and if you're then looking for co-reference that's further away um the thing it's co-referent with is normally close by and so that's why you work backwards through sentences one by one but then once you're looking within a particular sentence the most likely thing it's going to be co-referent to is a topical noun phrase and default topics in english are subjects so by doing things breadth first left to right a preferred antecedent is then a subject and so this algorithm i won't go through all the complex clauses five through nine ends up saying okay um what you should do is propose nile ferguson as what is co-referent to him which is the obvious correct reading in this example okay you probably didn't want to know that and in some sense the details of that um aren't interesting but what is i think actually still interesting in 2021 is what points jerry hobbs was actually um trying to make um last millennium um and the point he was trying to make was the following so jerry hobbs wrote this algorithm the naive algorithm because what he said was well look if you want to try and crudely determine co-reference well there are these various preferences right there's the preference for same sentence there's the preference for recency there's a preference for topical things like subject and there are things where you know if it has gender it has to agree in gender so there are sort of strong constraints of that sort so i can write an algorithm using my linguistic nouns which captures all the main preferences and actually it works pretty well um doing that is a pretty strong baseline system but what jerry hobbs wanted to argue is um that this algorithm just isn't something you should believe in this isn't a solution to the problem this is just sort of um you know making a best guess according to um the preferences of what's most likely without actually understanding what's going on in the text at all and so actually um what jerry hobbs wanted to argue was the so-called hobbs algorithm now he wasn't a fan of the hobbs algorithm he was wanting to argue that the hobbs algorithm is completely inadequate as the solution to the problem and the only way we'll actually make progress in natural language understanding is by building systems that actually really understand the text and this is actually something that has come to the fore again more recently so the suggestion is that in general you can't work out co-reference or pronominal and afro in particular unless you're really understanding the meaning of the text and people look at pairs of examples like these ones so she poured water from the pitcher into the cup until it was full um so think for just half a moment well what is it in that example um that is full um so um that what's full there is the cup um but then if i say she poured water from the pitcher into the cup until it was empty well what's empty well that's the picture and the point that um is being made with these examples is the only thing that's been changed in these examples um is um the adjective right here so these two examples have exactly the same grammatical structure so in terms of the hobbs naive algorithm the hobbes naive algorithm necessarily has to predict the same answer for both of these but that's wrong you just cannot determine the correct um pronoun antecedent based on grammatical preferences of the kind that are used in the naive algorithm you actually have to conceptually understand about pictures and cups and water and full and empty to be able to choose the right antecedent here's another famous example that goes along the same lines so terry winograd shown here as a young man so long long ago terry winograd came to stanford as the natural language processing faculty and terry winograd became disillusioned with the symbolic ai of those days and just gave it up all together and he reinvented himself as being an hci person and so terry was then essentially the person who established the hci program at stanford but before he lost faith in symbolic ai he talked about the co-reference problem and pointed out a similar pair of examples here so we have the city council refused the women a permit because they feared violence versus the city council refused the women a permit because they advocated violence so again you have this situation where these two sentences have identical syntactic structure and they differ only in the choice of verb here but once you add um knowledge common sense knowledge of how the um human world works well what how this should pretty obviously um be interpreted that in the first one that they is referring to the city council whereas in the second one that they is referring to the women and so coming off of that example of terry these have been referred to as winograd schemers so a winner grad schemer challenge is sort of choosing the right reference here and so it's basically just doing pronominal and aphra but you know the interesting thing is people have been interested in you know what are tests of general intelligence and one famous general test of intelligence that i won't talk about now is the turing test and there's been a lot of debate about problems with the turing test and is it good um and so in particular hector levesque who's a um a very well-known senior ai person um he actually proposed that a better alternative to the turing test um might be to do what he then dub winograd schema and winograd schema is just solving pronominal co-reference in cases like this where you have to have knowledge about the situation in the world to get the answer right and so he's basically arguing that you know you can review really solving co-reference as solving artificial intelligence and that's sort of um what the position that hobbes wanted to advocate so what he actually said about his algorithm was that the naive approach is quite good computationally speaking it will be a long time before a semantically based algorithm is sophisticated enough to perform as well and these results set a very high standard for any other approach to aim for and he was proven right about that because there was sort of really talked to around 2015 before people thought they could do without the hobbes algorithm but then he notes yet there is every reason to pursue a semantically based approach the naive algorithm does not work anyone can think of examples where it fails in these cases it not only fails it gives no indication that it has failed and offers no help in finding the real antecedent and so i think this is actually still interesting stuff to think about because you know really for the kind of machine learning based co-reference systems that we're building you know they're not a hot mess of rules like the hobbs algorithm but basically they're still sort of working out statistical preferences of what patterns are most likely and choosing the antecedent that way they're really um have exactly the same deficiencies still that hobbs was talking about right that they fail in various cases it's easy to find places where they fail the algorithms give you no idea when they fail they're not really understanding the text in a way that a human does to determine the antecedent so we still actually have a lot more work to do before we're really doing full artificial intelligence but i'd best get on now and actually um tell you a bit about some co-reference algorithms um right so the simple way of thinking about co-reference is to say that you're making just a binary decision about a reference pair so if you have your mentions you can then say well i've come to my next mention she i want to work out what it's co-referent with and i can just look at all of the mentions that came before it and say is it co-referent or not and do a binary decision so at training time i'll be able to say i have positive examples assuming i've got some data labeled for what's correct to what as to these ones are co-referent and i've got some negative examples of these ones are not co-referent and what i want to do is build a model that learns to predict co-referent things and i can do that fairly straightforwardly in the kind of ways that we have talked about so i train with the regular kind of cross-entropy loss where i'm now summing over every pairwise binary decision as to whether two mentions are co-referent to each other or not and so then when i'm at test time um what i want to do is cluster the mentions that correspond to the same entity and i do that by making use of my pairwise scorer so i can run my pairwise scorer and it will give a probability or a score that any two mentions a co-referent so by picking some threshold like 0.5 i can add co-reference links for when the classifier says it's above the threshold um and then i do one more step to give me a clustering i then say okay let's also make the transitive closure to give me clusters so it thought that i and she were co-referent and my and she were co-referent therefore i also have to regard i and my as co-referent and so that's sort of the completion by transitivity and so since we always complete by transitivity note that this algorithm is very sensitive to making any mistake in a positive sense because if you make one mistake for example you say that he and my a co-referent then by transitivity all of the mentions in the sentence become one big cluster and that they're all co-referent with each other that's a workable algorithm and people have often used it but often people go a little bit beyond that and prefer um a mention ranking model so let me just explain the advantages of that that normally if you have a long document where it's ralph nader and he did this and someone did something to him and we visited his house and blah blah blah blah and then somebody voted for nader because he in terms of building a co-reference classifier it seems like it's easy and reasonable oops um it's easy and reasonable to be able to recover this he refers to nader but in terms of building a classifier for it to recognize that this he should be referring to this nader which might be three paragraphs back seems kind of unreasonable how you're going to recover that so those far away ones might be almost impossible to get correct and so that suggests that maybe we should have a different way of configuring this task so instead of doing it that way what we should say is well this he here has various possible antecedents and our job is to just choose one of them and that's almost sufficient apart from we need to add one more choice which is well some mentions won't be co-referent with anything that proceeds because we're introducing a new entity into the discourse so we can add one more dummy mention the n a mentioned so it doesn't refer to anything previously in the discourse discourse and then our job at each point is to do mention ranking to choose which one of these she refers to and then at that point rather than doing binary yes no classifiers that what we can do is say aha this is choose one classification and then we can use the kind of softmax classifiers that we've seen at many points previously okay so that gets us in business for building systems and for either of these kind of models um there are several ways in which we can build the system we could use any kind of traditional machine learning classifier we could use a simple neural network we can use more advanced ones with all of the tools that we've been learning about more recently let me just quickly show you um a simple neural network way of doing that so this is a model um that um my phd student um kevin clark did in 2015 so not that long ago um but what he was doing was doing co-reference resolution based on the mentions with a simple feed forward neural network kind of in some sense like we did dependency parsing with a simple feedback neural network so for the mention it had word embeddings antecedent had word embeddings there were some additional features of each of the mentioner candidate antecedent and then there were some final additional features that captured things like distance away which you can't see from either the mention or the candidate and they were all of those features were just fed into several feed-forward layers of a neural network and it gave you a score of are these things um co-referent or not and that by itself um just worked pretty well um and i won't say more details about that um but what i do want to show is sort of a more advanced and modern neural co-reference system but before i do that i want to take a digression and sort of say a few words about convolutional neural networks um so um the idea of when you apply a convolutional neural network to language i.e to sequences is that what you're going to do is you're going to compute vectors features effectively for every possible word subsequence of a certain length so that if you have a piece of text like tentative deal reach to keep government open you might say i'm going to take every three words of that i tentative deal reached deal reached to reach to keep and i'm going to compute a vector based on that subsequence of words and use those computed vectors in my model by somehow grouping them together so the canonical case of convolutional neural networks is in vision and so if after this next quarter um you go along to cs231n um you'll be able to spend weeks doing convolutional neural networks for vision and so the idea there is that you've got these convolutional filters that you sort of slide over an image and you compute a function of each place so the sort of little red numbers are showing you what you're computing but then you'll slide it over to the next position and fill in this cell and then you'll slide over the next position and fill in this cell and then you'll slide it down and fill in this cell and so you've got this sort of little function of a patch which you're sliding over your image and computing a convolution which is just a dot product effectively that you're then using to get an extra layer of representation and so by sliding things over you can pick out features and you've got a sort of a feature identifier that runs across every piece of the image well for language we've just got a sequence but you can do basically the same thing and what you then have is a 1d convolution for text so if here's my sentence tentative deal reached to keep the government open that what i can do is have so these words have a word representation which so this is my vector for each word and then i can have a filter sometimes called a kernel which i use for my convolution and what i'm going to do is slide that down the text so i can start with it with the first three words and then i sort of treat them as sort of elements i can dot product and sum and then i can compute a value as to what they all add up to which is minus one it turns out and so then i might have a bias that i add on and get an updated value if my bias is plus one and then i'd run it through a non-linearity and that will give me a final value and then i'll slide my filter down and i'd work out a computation for this window of three words and take 0.5 times 3 plus 0.2 times 1 etc and that comes out as this value i add the bias i put it i'm going to put it through my non-linearity and then i keep on sliding down and i'll do the next three words and keep on going down and so that gives me a 1d convolution and computes a representation of the text you might have noticed in the previous example that i started here with seven words but because i wanted to have a window of three for my convolution the end result is that things shrunk so on the output i only had five things that's not necessarily desirable so commonly people will deal with that with padding so if i put padding on both sides i can then start my three by three convolution my three sorry not three by three my three convolution i'm here and compute this one and then slide it down one and compute this one and so now my output is the same size as my real input and so that's a convolution with padding okay so that was the start of things but you know how you get more power with the convolutional network is you don't only have one filter you have several filters so if i have three filters each of which will have their own bias non-linearity i can then get a three-dimensional representation coming out the end and sort of you can think of these as conceptually computing different features of your text okay so that gives us a kind of a sort of a new feature re-representation of our text but commonly we then want to somehow summarize what we have and a very common way of summarizing what we have is to then do pooling so um if we sort of think of these features as detecting different things in the text so you know they might even be high level features like you know does this um show signs of toxicity or hate speech um is there reference to something so if you want to be interesting does it occur anywhere in the text what people often then do as a max pooling operation where for each feature they simply sort of compute the maximum value it ever achieved in any position as you went through the text and say that this vector ends up as the sentence representation sometimes for other purposes rather than max pooling people use average pooling where you take the averages of the different vectors um to get the sentence representation then general max pooling has been found to be more successful and that's kind of because if you think of it as feature detectors that are wanting to detect was this present somewhere then you know something like positive sentiment isn't going to be present in every three-word um sub-sequence you choose because they're somewhere there and so often max pooling works better um and so that's a very quick look at convolutional neural networks except to say this example was doing 1d convolutions with words but a very commonplace that convolutional neural networks being used in natural language is actually using them with characters and so what you can do is you can do convolutions over subsequences of the characters in the same way and if you do that this allows you to computer representation to any sequence of characters so you don't have any problems with being out of vocabulary or anything like that because for any sequence of characters you just compute your convolutional representation and max pool work and so quite commonly people use a character convolution to give a representation of words perhaps as the only representation of words but otherwise is something that you use in addition um to a word vector and so on both by def and the model i'm about to show that at the base level it makes use of both a word vector representation that we saw right at the beginning of the text and a character level convolutional representation of the words okay with that said i now want to show you before time runs out i'm an end-to-end neural co-references model so the model i'm going to show you is kenton lee's one from um so done university of washington 2017. this is no longer the state of the art i'll mention the state of the art at the end but this was the first model that really said get rid of all of that old stuff of having pipelines and mentioned detection first just build one end-to-end big model that does everything and returns co-reference so it's a good one to show so compared to the earlier simple thing i saw we're now going to process the text with biostms we're going to make use of attention and we're going to do all of mention detection co-reference in one step end-to-end and the way it does that is by considering every span of the text up to a certain length as a candidate mentioned and just figures out a representation for it and whether it's co-referent to other things so what we do at the start is we start with the sequence of words and we calculate from those uh standard word embedding and a character level cnn's embedding we then feed those as inputs into a bi-directional lstm of the kind that we saw quite a lot of before but then after this what we do is we compute representations for spans so when we have a sequence of words um we're then going to work out a representation of a sequence of words which we can then put into our co-reference model um so that we i can't fully illustrate in this picture but so see subsequences of different lengths so like general general electric general electric said um will all have a span representation which i've only shown a subset of them in green so how are those computed well the way they're computed is that the span representation is a vector that concatenates several vectors and it consists of four parts it consists of the representation that was computed for the start of the span from the um by lstm the representation for the end from the bio stm that's over here and then it has a third part that's kind of interesting this is an intention-based representation that calculate is calculated from the whole span but particularly sort of looks for the head of a span and then there are still a few additional features um so it turns out that you know some of these additional things um like length and so on is still a bit useful um so to work out the the final part is not the beginning and the end what's done is to calculate an attention weighted average of the word embeddings so what you're doing is you're taking the x-star representation of the final word of the span and you're feeding that into a neural network to get attention scores for every word in the span which are these three and that's giving you an attention distribution as we've seen previously and then you're calculating the third component of this as an attention weighted sum of of the different words in the span and so therefore you've got the sort of a sort of a soft average of the representations of the words of the span okay um so then once you've got that um what you're doing is then feeding these representations into having scores for whether spans are co-referent mentions um so you have a representation of uh the two spans um you have a score that's calculated for whether two different spans look co-referent and that overall you're getting a score for r different spans looking co-referent or not um and so this model is just run end to end on all stands now it sort of would get intractable if you scored literally every standalone piece of text so they do some pruning they sort of only allow spans up to a certain maximum size they only consider pairs of spans that aren't too distant from each other etc etc but basically it's in sort of an approximation to just a complete comparison of spans and this turns into a very effective co-reference resolution algorithm today it's not the best co-reference resolution algorithm um because maybe not surprisingly like everything else that we've been dealing with there's now been these transformer models like burt have come along and that they produce even better results so the best co-reference systems now have you make use of bert in particular when dante spoke she briefly mentioned spanbert which was a variant of bird which constructs blanks out for reconstruction subsequences of words rather than just a single word and span bird has actually proven to be very effective for doing co-reference perhaps because you can blink out whole mentions people have also gotten gains actually funnily by treating co-ref as a question-answering task so effectively you can find a mention like he or the person and say what is its antecedent and get a question answering answer and that's a good way to do co-reference so if we put that together um as time is running out um let me just sort of give you some sense of how results come out for co-reference systems so i'm skipping a bit actually that you can find in the slides which is how co-references scored but essentially it's scored on a clustering metric so a perfect clustering will give you a hundred and something that makes no correct decisions would give you zero and so this is sort of how the co-reference numbers have been panning out so back in 2010 actually this was a stanford system this was a state-of-the-art system for co-reference it won a competition it was actually a non-machine learning model because again we wanted to prove how these rule-based methods and practice work kind of well um and so its accuracy was around 55 for english 50 for chinese then gradually machine learning this was sort of statistical machine learning models got a bit better wiseman was the very first neural co-reference system and that gave some gains um here's a system that kevin clark and i did which gave a little bit further gains um so lee is the model um that i've just shown you as the end-to-end model and it got um a bit of further gains but then again you know what gave the huge um breakthrough just like question answering was that the use of span bert so once we moved to here we're now using span bird that's giving you about an extra 10 or so um the co-ref qa technique proved to be useful um and then the very latest best results are effectively combining together span bert and a larger version of spanbert and co-fqa and getting up to 83. so you might think from that that co-ref is sort of doing really well and is getting close to solved like other nlp tasks um well it's certainly true that in neural times the results have been getting way way better than they had been before but i would caution you that these results that i just showed were on a corpus called onto nodes which is mainly newswire and it turns out that newswire co-reference um is pretty easy i mean in particular there's a lot of mention of the same entities right so the newspaper articles are full of mentions of the united states and china and leaders of the different countries and it's sort of very easy to work out what they're co-referent to and so the co-reference scores are fairly high whereas if what you do is take something like a page of dialogue from a novel and feed that into a system and say okay do the co-reference correctly you'll find pretty rapidly um that the performance of the models is much more modest um if you'd like to try out a co-reference system for yourself um there are pointers um to a couple of them um here um where the top one's ours from certain um kevin clark's new co reference um and this is one that goes with the hugging face repository that we've mentioned you