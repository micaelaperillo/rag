hello everyone um my name is Lisa I'm a third year PhD student in the NLP group I'm advised by Percy and Tatsu today I will give a lecture on natural language generation and this is also the research area that I work on so I'm super excited about it I'm happy to answer any questions both during the lecture and after class about natural language generation so nlg is a super exciting area and is also moving really really fast so today we will discuss all the excitement of nlg but before we get into the really exciting part I have to make some announcements so first it is very very important for you to remember to sign up for AWS by midnight today so this will concern this is related to your homework 5 whether you have GPU access and then also related to our final project so please please remember to sign up for it for AWS by tonight and second the project proposal is due on Tuesday next Tuesday and I think assignment 4 should just do it hopefully you had fun in this machine translation and stuff and also assignment 5 is out today I think just now and it is due on Friday uh like basically Friday midnight and uh last we will hold a Transformer I will hold a hugging face Transformer Library tutorial this Friday so if your final project is related to implementing Transformers or playing with large language models you should definitely go to this tutorial because it's going to be very very helpful um also yeah just one more time please remember to sign up for AWS because this is the final hard deadline okay cool now moving on to the main topic for today um the very exciting natural language Generation stuff so today we will discuss what is an LG review sound models discuss about how to decode from language models and how to train language models um and we will also talk about evaluations and finally we'll discuss ethical and risk considerations with the current analogy systems so this natural language generation techniques are going to be really exciting because this is kind of getting us closer to explain the magic of chat GPT which is a super popular model recently and practically speaking they could also help you with your final project if you decide to work on something related to text generation so um let's get started to begin with let's ask the question of what is natural language generation so natural language generation is actually a really broad category people have divided an LP into natural language understanding and natural language generation so the understanding part mostly means that the task input is in natural language such as semantic parsing natural language inference and so on whereas natural language generation means that the task output is in natural language so nlg focuses on systems that produce fluent coherent and useful language outputs for human to use historically there are many analogy systems that use rule-based systems such as templates or infilling but nowadays deep learning is powering almost every text generation systems so this lecture today will be mostly focused on deep learning steps so um first what are some examples of natural language generation it's actually everywhere including our homework machine translation is a form of nlg where the input is some address in the source language and the output is generated text in a targeted language digital assistant such as series or Alexa they are also an LG systems so it takes in dialogue history and generates continuations of the conversation um there is also summarization systems that takes in a long document such as a research article and then the idea is trying to summarize it into a few sentences that are easy to read so beyond these classic tasks there are some more interesting uses like creative story writing where you can prompt a language model with a story plot and then it will give you some creative stories that are aligned with the plot there is state of the text where you give the language model some database or some tables and then the idea is that it will output some textual description of the table content and finally there is also like visual description based nlg systems like image captioning or like image based storytelling so the really cool example um is the popular track GPT models so chat GPT is also an analogy system it is very general purpose so therefore you can use it to do many many different tasks with different prompts for example we can use chat GPT to simulate a chatbot it can ask it can answer questions about like creative gifts for 10 years old it can be used to do poetry generation like for example we can ask you to generate a poem about sorting algorithms and it's actually well I wouldn't say it's very poetic but at least it has the same format as a poem and the content is actually correct so um charging Beauty can also be used in some really useful settings like a web search so here Bing is augmented with chat GPT and there are some twitters that are saying that the magic of chat GPT is that it actually makes people be happy to use Bing um so there are so many tasks that actually belong to the nlg category so how do we categorize these tasks one common way is to think about the open-endedness of the task so here we draw a line for the spectrum of open-endedness on the one end we have tasks like machine translation and summarization so we consider them not very open-ended because for each Source sentence the output is almost determined by the input because basically we are trying to do machine translation the semantic should be exactly similar to the input sentence so there are only a few ways that you can refreeze the output like authorities have announced that today is a national holiday you can rephrase it a little bit to say today is a national holiday announced by the authorities but the actual Space is really small because you have to make sure the semantics doesn't change so we can say that the output space here is not very diverse um and moving to the middle of the spectrum there is dialogue tasks such as task driven dialogue or Chit Chat dialogue so we can see that for each dialog input there are multiple responses and the degree of Freedom has increased here we can say like we can respond by saying good and you or we can say about thanks for asking barely surviving on my homeworks so here we are observing that there are actually multiple ways to continue this conversation and then this is where we say the output space is getting more and more diverse and on the other end of the spectrum there is a very open-ended generation tasks like story generation so given the input like write me a story about three little pigs there are so many ways to continue the prompt right we can write about them going to schools building houses like they always do um so the valid output here is extremely large and we call this open-ended generation so it's hard to really draw a boundary between open-ended and non-open-ended tasks but we still try to give a rough categorization so over the Ender generation refers to tasks whose output distribution has a high degree of Freedom or an non-open under generation tasks refers to tasks where the input will almost certainly determine the output generation examples of non-open ended Generations are machine translation summarization and examples of open-ended Generations are story generation Chit Chat dialogue task oriented dialogue Etc so how do we formalize this categorization one way of formalizing is by Computing the entropy of the nlg system so high entropy means that we we are to the right of the spectrum so it is more open-ended and low entropy means that we are to the left of the spectrum and less open-ended so there's two classes of nlg tasks actually require different decoding and training approaches as we'll talk about later okay cool now let's recall some previous lectures and review the nlg models and trainings that we have studied before so I think we discussed the basics of natural language generation so here is how other aggressive language model works at each time step our model would take in a sequence of tokens as input and here it is y less than T and the output is basically the new token YT so to decide on YT we first use the model to assign a score for each token in the vocabulary denoted as s and then we apply softmax to get the next token distribution p and we choose a token according to this next token distribution and summary once we have predicted YT hat we then pass it back into the language model as the input predict y hat t plus 1 and then we do so recursively until we reach the end of the sequence so any questions so far okay good um so for the two types of energy tasks that we talked about like the open-ended non-open-ended tasks they tend to prefer different model architectures so for now open-ended tasks like machine translation we typically use an encoder decoder system where like the other regressive decoder that we just talked about function as the decoder and then we have another bi-directional encoder for encoding the inputs so this is kind of what you implemented for assignment four because the encoder is like the bi-directional lstm and the decoder is another lstm that is auto regressive so for more open-ended tasks typically other aggressive generation model is the only Oppo is the only component um of course like this architectures are not really hard constraints because a auto-agressive decoder alone can also be used to do machine translation and an encoder decoder model can also be used for story generation so this is kind of the convention for now but it's a reasonable convention because like using decoder only model for Mt tends to hurt performance compared to an encoder decoder model for Mt and using an encoder decoder model for open-ended generation seems to like achieve similar performance to a decoder only model and therefore if you have the compute budget to train an encoder decoder model you might just be better off by only trading a larger decoder model so it's kind of more of an allocation of resources problem than whether this to architecture will type check with your task so um okay so how do we train such a language model in previous lectures we talked about that the language models are trained by maximum likelihood so basically we were trying to maximize the probability of the next token uh YT given the preceding words and this is our optimization objective so at each time step this can be regarded as a classification task because we are trying to distinguish the actual word uh YT star from all the remaining words in the vocabulary and this is also called teacher forcing because at each time step uh we are using the gold standard wise uh y star less than t as input to the model whereas presumably at generation time you wouldn't have any access to Y star so you would have to use the model's own prediction to fit it back into the model to generate the next token and that is called student forcing which will talk in detail later we never used that word before what does it mean Ultra aggressive oh this means like uh so let's look at this animations again oops sorry oh it just looks like uh you are generating word from left to right one by one so here suppose that you are given a y less than T and then other aggressive for your first general YT and then once you have YT you'll fit it back in general YT plus one and then feed it back and generate another thing so this left to right nature because you are using chain rule to like condition on the the tokens that you just generated this chain rule thing is called Auto regressive and typically like I think conventionally we are doing left to right other aggressive by generating from left to right but there are also like other more interesting models that can do backward or influence and other things this idea of generating one token at once is auto regressive cool any other questions yep um so at inference time our decoding algorithm will Define a function to select a token from this distribution so we've discussed that we can use the language model to compute this P which is the next token distribution and then G here based on our notation is the decoded algorithm which helps us select what token we are actually going to use for YT so the obvious decoding algorithm is to greatly choose the highest probability token as YT hat for each time step so well this basic algorithm sort of works because they work for your homework for to do better there are two main avenues that we can take we can decide to improve decoding and we can also decide to improve the training of course there are other things that we can do we can improve training data and we can improve model architectures but for this lecture we will focus on decoding and training so uh now let's talk about how decoding algorithms work for natural language generation models before that I'm happy to take any questions about the previous slides uh I think I'll go into this in detail later but sure so basically for teacher of forcing the idea is like you do teacher forcing where you'll train the language model because you already observe like the gold text so you kind of use the gold text up until timestamp t uh put put it into the model and then the model would try to predict why uh t plus one whereas student forcing means that you don't have access to this gold reference data instead you are still but you are still trying to generate a sequence of data so you have to use uh the text that you generated yourself using the model and then feed it back into the model as input to predict t plus one that's the primary difference cool um so what is decoding all about at each time step uh our model computes a vector of score for each token so it takes in preceding context while less than T and produce a score s and then we try to compute the probability distribution P all of this scores by just applying softmax to normalize them and our decoding algorithm is defined as this function G which takes in the probability distribution and try to map it to some word basically try to select a token from this probability distribution so in the machine translation lecture uh we talked about graded decoding which selects the highest probability token of this P distribution and we also talk about beam search which has the same objective as grade decoding which is that we are both trying to find the most likely string defined based on the model but instead of doing so greedily for beam search we actually explore a wider range of candidates so we have a wider exploration of candidates by keeping always like k k candidates in the beam so overall this maximum probability decoding is good for low entropy tasks like machine translation and summarization but it actually encounters more problems for open-ended generation so the most likely string is actually very repetitive when we try to do open-ended text generation as we can see in this example the context is perfect in normal it's about I mean a unicorn trying to speak English and for the continuation the first part of it is it looks great it's like valid English it talks about science but suddenly it starts to repeat and it starts to repeat like I think uh a institution's name so why does this happen um if we look at for example uh this plot which shows uh the problem the language model's probability assigned to the sequence I don't know we can see like here's the pattern um it has regular probability but if we keep repeating this phrase I don't know I don't know I don't know for 10 times then we can see that there's a decrease in Trend in their negative log likelihood so the y-axis is the negative log probability we can see this decreasing Trend which means that the model actually has higher probability uh as the repeat goes on which is quite strange because it's suggesting that there is a self-amplification effect so the more repeat we have the more confidence the model becomes about this repeat and this keeps going on we can see that for I am tired I'm tired repeat 100 times because it continuously decreasing Trend until the model is almost 100 sure that it's gonna keep repeating the same thing and sadly um this art this problem is not really solved by architecture here the Red Cloud is a lstm model and the blue curve is a Transformer model we can see that both model kind of suffers from the same problem and scale also doesn't solve this problem so we kind of believe that like scale is the magical thing in NLP but even even models with 175 billion parameters will still suffer from repetition if we try to find the most likely string so how do we reduce repetition um one canonical approach is to do unground blocking so the principle is very simple basically you just don't want to see the same engram twice if we send n to be three then for any text that contains the phrase I am happy the next time you see the prefix I am ungram blocking would automatically set the probability of happy to be zero so that you will never see this unground this trigram again but clearly this this underground blocking heuristic has some problems because sometimes it is quite common for you to want to see a person's name appear twice or three times or even more in the text but this unground blocking will eliminate that possibility so what are better options that possibly are more complicated for example we can use a different training objective instead of training by mle we can train by unlikelihood objective so in this approach uh the model is actually penalized for generating already seen tokens so it's kind of like putting this unground blocking idea into training time um rather than a decoding Time Force this constraint at trading time we just decrease the probability of repetition another another training objective is coverage Wells which uses kind of the attention mechanism to prevent repetition so basically if you try to regularize and enforce your attention so that it's always attending to different words for each token then uh it is highly likely that you are not going to repeat because repetition tends to happen when you have similar attention patterns another different angle is that instead of searching for the most likely string we can use a different decoding objective so maybe we can search for Strings that maximizes uh the difference between log probabilities of two models say that we want to maximize log problem large model minus a lot of problem small model in this way because both models are repetitive so they kind of cancels out so like they would both assign High probabilities repetition and after applying this new objective the repetition stuff will actually be penalized because it cancels out so here comes the broader question um it's finally the most likely string even a reasonable thing to do for open-ended text generation the answer is probably no because this doesn't really match human pattern so we can see In This Cloud the orange curve is the human pattern and the blue curve is the machine generated text using beam search so you can see that will with human talks there are actually lots of uncertainty uh in as we can see by the fluctuation of the probabilities like for some words we can be very certain for some words we are a little bit unsure whereas here for the model distribution is always very sure it's always assigning probability one to the sequence so because we now are seeing a answer obviously there's a mismatch between the two distributions so it's kind of suggesting that maybe searching for the most likely string is not the right decoding objective at all any questions so far before we move up yeah the online magazine for like some detector of whether some characters generated by Chinese um not really because uh so this can only detect the really simple things that humans are also able to detect like repetition so uh in order to avoid like the previous problems that we've talked about I'll talk about some other decoding families that generates more robust attacks that actually look like this um whose probability distribution looks like the orange curve so I wouldn't say this is like the to go answer for watermarking or detection oh yeah Okay cool so she asked about whether um whether this mechanism of plotting the probabilities of human text and machine generated text is one way of detecting whether some text is generated by model or human and my answer is I don't think so but this could be an interesting research Direction because I feel like they are more robust decoding approaches that generate texts that are that actually fluctuates a lot um so yeah let's talk about the decoding algorithm that is able to generate text that fluctuates so given that searching for the most likely string is a bad idea what else should we do and how do we simulate that human pattern and the answer to this is to introduce Randomness and stochasticity to decoding so um suppose that we are sampling a token from this distribution of P basically like we are trying to sample YT hat from this distribution It Is Random so that you can essentially sample any token distribution previously you are kind of restricted to selecting rest for more grocery but now you can select bathroom instead so however uh sampling introduces a new set of problems since we never really zero out any token probabilities vanilla vanilla sampling would make every token in the vocabulary a viable option and in some unlucky cases we might end up with a bad word so assuming that uh we already have a very well trade model like even if most of the probability mass of the distribution is over the limited set of good options the tail of the distribution will still be very long because we have so many words in our vocabulary and therefore if we add all those round Tails it Aggregates they still have a considerable Mass so statistically speaking this is called heavy tail distribution and language is exactly a heavy tail distribution so for example like uh many tokens are probably really wrong in this context and then given that we have a good language model we assign them each very little probability thus this doesn't really solve the problem because there are so many of them so you aggregate them as a group will still have a high chance of being selected and the solution here that we have for this problem of long tail is that we should just cut off the tail we should just zero out the probabilities that we don't want and one idea is called top place that a top case sampling where the idea is that we would only sample from the top K tokens in the probability distribution any questions for now okay yeah well the model we were looking at a second ago had some really low probability samples as well on the graph right I would copy something with that uh you mean this one or even uh the orange blue graph of the human versus uh oh yeah yeah so uh top cable basically uh eliminate it will not it will make it impossible to generate the super low probability tokens so technically it's not it's not exactly simulating this pattern because now you don't have the super low probability tokens whereas human can generate super low probability television affluence way but yeah that's that could be um another like hint that people can use for detecting a machine generated text yeah depends on the type and text you want to generate for example poem or novels or more creative writing but then you decide to hyper correct yeah yeah for sure case I have a parameter that depending on the type of task you will choose K differently uh maybe mostly for close and the task K should be small and for open-ended case should be large yeah cluster in the back how come like I guess intuitively this builds up of one of the earlier questions why don't we consider the case like where we sample and then we just weight the probability of each word by it's like score or something rather than just looking at top trade how can we don't do like a weighted sampling type of situation so we still have that small but non-zero probability of selecting uh I think Top Care is also like rated so like top K just kind of zeros out all the Tails of the distribution but for the things that I didn't zero out uh it's not like a uniform Choice among the K it's still trying to choose proportional to the scores that you computed is that just like a computational like 17 000 words it could be like for like 10 or something um yeah sure that could be one gain of 12K decoding is that your self Max will take in fewer uh fewer candidates yeah but it's not the main reason I think you should show yeah yeah I'll keep talking about the many reasons um so we've discussed this part and then here uh this is the formal this is kind of the formerly what is happening for top case sampling uh now that we are only sampling from the top K tokens of the probability distribution and as we've said K is a hyper parameter so we can set K to be large or small uh if we increase K this means that we are making our output more diverse but at the risk of including some tokens that are bad if we decrease k then we are making more conservative and safe options but possibly the generation will be quite generic and boring um so uh is top K decoding good enough the answer is not really because we can still find some problems with top K decoding for example in the context she said I never blank there are many words that are still valid options uh such as won't 8 but those words got zeroed out because they are not within the top K candidates so this actually leads to bad recall for your generation system and similarly another failure of top K is that it can also cut off too quickly so in this example code is not really a valid answer according to common sense because you probably don't want to eat a piece of code um but the probability remains non-zero meaning that the model might still sample code as an output despite this low probability but it might still happen and this means bad Precision for the generation model so given this problems with top K decoding how can we address them how can we address this of this issue of like there is no single K that fits all circumstances um this is basically because the probability distribution that we sample from our Dynamic so when the probability distribution is relatively flat having a small cable remove many viable options so the having a limited cable removes many viable options and we want K to be larger for this case similarly when a distribution p is too picky then we want the like a high K would allow for too many options uh to be viable and instead we might want a smaller K so that we are being safer um so the solution here is that maybe K is just a bad Haver parameter and instead of doing K we should doing we should think about probability we should think about how to sample from tokens in the top P probability percentiles of the cumulative probability mass of the CDF for example so now um the the advantage of doing top P sampling where we sample from the top P percentile of the cumulative probability mass is that this is actually equivalent to we have now a adaptive k for each different distribution and let me explain what what I mean by having like an Adaptive k so in the first distribution this is like a regular power law of language that's kind of typical and then uh doing top uh doing top case sampling means we're selecting the top K but doing the top P sampling means that we are zooming into maybe like like something that's similar to top K and in fact but if I have a relatively flat distribution like the blue one we can see that's doing top p means that we are including more candidates and then if we have a more schools distribution like the green one doing top p means that we actually include fewer candidates so by actually selecting like the the top P percentile in the probability distribution we are we are actually having a more uh flexible okay and therefore have a better sense of what are the good options uh in the model any questions about top P top K decoding so everything's clear yeah sounds good um so to go back to that question uh doing top K is not necessarily saving compute or like this whole idea is not really compute saving intended because uh in the case of top p in order to select the top P percentile we still need to compute the soft Max over the entire vocabulary set in order for us to do top pay properly to compute the P properly so therefore it's not really saving compute but it's improving performance moving on um so there are much more to go with decoding algorithms with uh besides the topic and top P that we've discussed there are some more recent approaches like typical typical sampling where the idea is that we want to relate the score based on the entropy of the distribution and try to generate tags that are closer to the negative whose probability is closer to the negative entropy of the data distribution this kind of means that if you have a closed-ended task or non-open-ended task you want it has smaller entropy so you want a negative log probability to be smaller so you want probabilities to be larger so it kind of TAPS it tap checks very well and additionally there is also Epsilon sampling coming from John so this is an idea where we set the threshold for to lower bound probabilities so basically if you have a word whose probability is less than .03 for example then that word will never appear um in the output distribution now that that word will never be part of your output because it has still will probability yeah oh cool great question so the entropy distribution is defined as um like you can suppose that we have a discrete distribution we can go over it like we'll just enumerate X and then it's like negative log probability of X so like if we write it from a from an expectation perspective it's basically expected of well probability of x okay I'll I have to do a little bit here so so this is the entropy of a distribution and then so basically if you are distribution is very very concentrated to a few words then the entropy will be relatively small if your distribution is very flat then your entropy will be very large yeah the Epsilon sampling is set such that we have no valid oh yeah I mean I bump back off cases I think so in the case that there is no valid options um You probably still want to select one or two things just as a edge case I think okay cool um moving on so another hyper parameter that we can tune to affect decoding is the temperature parameter so recall that previously at each time step we asked the model to compute a score um and then we renormalize that score using solve Max to get a probability distribution so one thing that we can adjust here is that we can insert this temperature parameter Tau to relate the score so basically we just divide all the SW by Tau and after dividing this we apply solve Max and we get a new distribution and this temperature adjustment is not really going to affect the monotonicity of the distribution for example if word a has higher probability than word b previously then after the adjustment where a is still going to have a higher probability than word b but still relative difference will change so um for example if we raise the temperature Tau to be greater than one then the distribution PT will become more uniform it will be flatter and this kind of implies that there will be more diverse output because our distribution is flatter and it's more spread out across different words in the vocabulary on the other hand if we lower the temperature Tau less than one then PT becomes very spiky and then this means that we are if we sample from the PT we'll get less diverse output um so because here the probability is concentrated only on the top words so in the very extreme case if we set Tau to be very very close to zero then the probability will kind of be a one hot Vector where all the probability mass will be centered on one word and then this kind of reduces back to Arc Max sampling or greedy decoding so temperature is a hyper parameter as well as as for K and P in top K on top P it is a hyper parameter for decoding it can be tuned for beam search and sampling algorithms so it's kind of orthogonal to the approaches that we discussed before any questions so far okay cool uh temperature is so easy so um well because something still involves Randomness like even though we do we try very hard in terms of truncation truncating the tail something still has Randomness so what if we're just unlucky and decode a bad sequence from the model um one common solution is to do re-ranking so basically we would decode a bunch of sequences like for example we can decode 10 candidates um but like 10 or 30 is up to you the only choice is that you want to balance between your compute efficiency and performance so if you decode too many sequences then of course your performance is going to increase but it's also very costly to to just generate a lot of things for one example and then so once you have a bunch of uh sample sequences then we are trying to define a score to approximate the quality of the sequence and re-rank everything and re-rank all the candidates by this score so the simple thing to do is we can use a perplexity as a metric as a score as a scoring function but we need to be careful that because we have talked about this like the extreme of perplexity like if we try to Arc Max log probability we will try to aim for a super well perplexity the attacks are actually very repetitive so we shouldn't really aim for extremely low perplexity and perplexity to some extent it's not a perfect re-scoring function it's it's not a perfect scoring function because it's not really robust to maximize so alternatively the re-rankers can actually use a wide variety of other scoring functions that we can score text based on their style their discourse coherence uh their entailment factuality properties consistency and so on um and additionally we can compose multiple re-rankers together uh yeah questions 10 candidates or any number of candidates yeah what's the strategy usually use to generate these other candidates like what you're listening to use oh yeah so basically the idea is to sample from the model right so when you sample from the model each time you sample you're going to get a different output and then that's what I mean by different candidates so if you sample 10 times you will get 10 you will very likely get 10 different outputs and then you are just given these 10 different outputs that come from sampling you can just decide re-rank them and select the candidate that has the highest score oh because we are sampling here yeah yeah for example if you are doing like top three something then well suppose that A and B are equally probable then you might sample a your max sample B with the same probability okay cool and another cool thing that we can do is re-ranking is that we can compose multiple re-rankers together so basically you can suppose you have a scoring function for style and you have a scoring function for factual consistency you can just add those two scoring functions together to get a new scoring function and then uh re-rank everything based on your new scoring function to get tags that are both good at style and good at factual consistency do we just pick the decoding that has the high score or do we do some more sampling again based on the story uh the idea is you just take the decoding that has the highest score because you already have like say 10 candidates so out of this 10 you only need one and then you just choose one that has the highest score yeah cool any other questions yeah sorry what what is perplexity oh yeah perplexity is like you can kind of regard it as log probabilities uh it's it's proportion it's like e to the negative well probabilities kind of like uh if uh if a talker has high perplexity then it means it has a low probability because you are more perplexed okay so um taking a step back to summarize this decoding section we have discussed uh many decoding approaches from selecting the most probable string to selecting uh to sampling and then to various truncation approaches that we can do to improve sampling like top P top K Epsilon typical decoding and finally we discuss how we can do in terms of re-ranking the results so uh decoding is still a really essential problem in energy and there are lots of Works to be done here still especially as like chai GPD is so powerful we should all go study decoding um so it would be interesting if you want to do such final projects and also different decoding algorithms can allow us to inject different inductive biases uh to the to the text that we are trying to generate and some of the most impactful advances in nlg in the last couple years actually come from simple but effective decoding algorithms for example the nuclear sampling is the nuclear sampling paper is actually very very highly cited so moving on to talk about training analogy models well we have seen this example before in the decoding slides and I'm just trying to show them again uh because even though we can solve this repetition Problem by by instead of doing search doing sampling um the but it's still concerning from a language modeling perspective that's your model would put so much probability on such repetitive and degenerate text so we asked this question well is repetition due to how language models are trained you have also seen this Cloud before which shows this decaying pattern or like the self amplification effect so we can conclude from this observation that model trained via a mle objective wears a really bad like whereas really bad mode of the distribution by mode of the distribution I mean the arguments of the distribution so basically they would assign high probability to terrible strings and this is definitely problematic for a model perspective so why is this the case shouldn't mle be like a gold standard in machine translation uh in in machine learning in general not just machine translation should an ml be like a gold standard for machine learning um the answer here is not really especially for text because mle has some problem for sequential data and we call this problem exposure bias um so training with teacher forcing leads to exposure bias at generation time because during training our model's inputs are gold context tokens from real human generated text as denoted as I had less than T here but during generation time our model's input become previously decoded tokens from the model well I had to and suppose that our model has minor arrows than like what I had T why had less than T will be much worse in terms of quality than y star less than T and this discrepancy is terrible because it actually causes a discrepancy between trading and test time which actually hurts model performance and we call this problem exposure bias um so people have proposed many solutions to address this exposure bias problem uh one thing to do is to do um scheduled sampling which means that uh with probability P we try to decode a token uh and feed it back in as context to train the model and this probability one minus P we use the gold tag we use the gold token as context so throughout trading we try to increase P to gradually warm it up and then prepare it for test time generation so this leads to Improvement in practice because of using this T using this P probabilities we're actually graduating uh like trying to narrow the discrepancy between training and test time but the objective is actually quite strange and training can be very unstable another idea is to do data set aggregation and the method is called dagger essentially at various intervals during training we try to generate a sequence of text from the current model and then use this and then put the sequence of text into the training data so we're kind of continuously doing this uh training data augmentation scheme to make sure that the trading distribution and the generation distribution are closer together so both approaches both scheduled sampling and data set aggregation are ways to narrow the discrepancy between training and test yes question just means human text I mean it's like uh well so little language model you will see lots of Corpus that are human written gold is just human okay cool um so another approach is to do retrieval augmented generation so we first learned to retrieve a sequence from some existing Corpus of prototypes and then we train a model to actually edit the retrieved text by doing insertion deletion or swapping we can add or remove tokens from this prototype and then try to modify it into another into another sentence so this doesn't really suffer from exposure bias because we start from a high quality prototype so that's at trading time and at test time like you don't really have the discrepancy anymore because you are not generating from left to right um another approach is to do reinforcement learning so here the idea is to cast your generation problem as a Markov decision process so there is the state as uh which is the model's representation for all the preceding context there is action a uh which is basically like the next token that we are trying to pick and there's policy which is the language model or also called the decoder and there is the reward R which is provided by some external score and the idea here uh well like we won't go into details about reinforcement learning and how it works but we will recommend the class CS two three like 234. so um in the reinforcement learning context because reinforcement learning involves a reward function that's very important so how do we do reward estimation for tax Generation Well really natural idea is to just use the evaluation metrics so whatever because you are trying to do well in terms of evaluation so why not just improve for evaluation metrics directly at training time for example in the case of machine translation we can use blue score as the reward function in the case of summarization we can use root score as the reward function but we really need to be careful about optimizing for tasks as opposed to gaining the reward because evaluation metrics are merely proxies for the generation quality so sometimes suppose that you run RL and improve the blue score by a lot but will you will run human evaluations humans might still think that well this this generated tax is no better than the previous one or even worse even though it gives you a much better blue score so we want to like be careful about this case of not gaining the reward so what behaviors can we tied to a reward function this is about reward design and reward estimation there are so many things that we can do we can do cross modality consistency for image captioning we can do sentence similarity to a sentence Simplicity to make sure that we are generating simple English that are understandable we can do formality and politeness to make sure that I don't know like your chatbot doesn't suddenly yell at you um and the most important thing that's really really popular uh is recently is human preference so we should just build a remote a reward model that captures human preference and this is actually the technique behind the chat GPT model so the idea here is that we would ask human to rank a bunch of generated text based on their preference and then we will use this preference data to learn a reward function which will basically always assign high score to something that humans might prefer and assign a low score to something that humans wouldn't prefer Yeah question more expensive oh yeah sure I mean it is going to be very expensive but I feel like uh compared to all the cost of trading models trading like 170 billion parameter models um I feel like open Ai and Google are well they can't afford hiring lots of humans to do human annotations and ask their preference yeah yeah this is a great question so um I think it's kind of a mystery about how much data you exactly need to achieve the level of performance of chat GPT but roughly speaking I feel like I mean like whenever you try to fine-tune a model on some Downstream tasks similarly here you are trying to find through your model on on human preference it do need quite a lot of data like maybe on a scale of 50k to 100K that's roughly the scale that like anthropic actually released some data set about human preference that's roughly the skill that they released I think um if I remember correctly Yeah question we talked about earlier about how many of the state-of-the-art language models use Transformers as their architecture how do you apply reinforcement learning to this model uh to to what do you mean to Transformer model yeah yeah I feel like um reinforcement learning is kind of a modeling tool I mean it's kind of an objective that you are trying to optimize instead of an mlu objective now you are optimizing for an RL objective so uh it's not real it's kind of orthogonal to the architectural choice so uh Transformer is an architecture you just use Transformer to give you probability of the next token distribution or to to try to estimate probability of a sequence and then once you have the probability of a sequence you use that probability of the sequence pass it into the uh the RL objective that you have and then suppose that you are trying to do policy gradient or something then you need to estimate the probability of that sequence and then you just need to be able to back prop uh through Transformer which is doable yeah so I think like the question about architecture and objectives are orthogonal so even if you have an ostm you can do it you have a Transformer you can also do it yep cool hope I answered that question yeah can you just like with the model T4 to for this country well for example we can build another Transformer to like to calculate yeah I think that's exactly what they did so they uh so for example you would have gpt3 right um you use gpd3 as the generator that generate text and you kind of have another pre-trained model that it could probably also be gpd3 but I'm guessing here um that you fine-tune it to your human preference and then once you have a human preference model uh you use the human preference model to put it into RL as the reward model and then use the original gpd3 as the policy model and then you you apply our objectives and then update them so that you will get a new model that's better at everything okay cool um yeah actually if you are very curious about earlier chap I would encourage you to come to the next lecture which is uh and where Jesse will talk about rlhs which is uh RL HF is shorthand for RL using human pref uh using human feedback foreign teacher enforcing is still the main algorithm for training tax generation models and exposure bias causes problems in tax generation models for example it causes models to lose coherence cause this model to be repetitive and models must learn to recover from their own bad samples by using techniques like scheduled sampling or dagger and models shouldn't another approach to to reduce exposure bias is to start with good text like retrieval of class generation and we also discussed how to do training with RL and this can actually make model learn behaviors that are preferred by human perform that are preferred by human or preferred by some metrics so uh to be very up-to-date in the best language model nowadays check GPT the trading is actually pipelined for example we would first pre-train a large language models using internet Corpus by self-supervision and this kind of gets your chat GPT like the uh sorry gpt3 which is the original version and then you would do some sorts of instruction tuning to fine-tune the language model to fine-tune the pre-trained language model so that it learns roughly how to follow human instructions and finally we will do rlhs to make sure that these models are well aligned with human preference so if we start RL HF from scratch it's probably going to be very hard for the model to converge because RL is hard to train for text Data Etc so RL doesn't really work from scratch but with all these smart tricks about pre-training and instruction tuning suddenly now like they're they're off to a good start cool any questions so far okay oh yeah [Music] uh you mean the difference between dagger and schedule sampling is how long the the sequence are yeah I think roughly that is that is it because like for dagger you are kind of trying to you are trying to put in um full generated sequence but I feel like there can be variations of dagger dagger is just like a high level framework and idea there can be variations variations of dagger that are very similar to scheduled sampling I think I feel like for schedule sampling it's kind of a more smooth version of dagger because dagger for dagger you have to like uh for well basically for this Epoch I am generating something and then I after this Epoch finishes I put this into the data together and then train for another Epoch whereas dagger seems to be more flexible in terms of when you add data in yes look it's for daggers to the rest of the models coming out but like how does it helpful model um I think that's a that's a good question I feel like if you regress the model for example um if you regress the model on its own output uh I think well I think there are there should be smarter ways than to exactly regress on your own output for example you might still like consult some good reference data for example given that you ask the model to generate for something and then you can instead of using uh say you ask the model generate for five tokens and then instead of using like the models generation to be the sixth token you'll probably try to find some examples in the training data that would be a good continuations and then you try to plug that in by like connecting the generation the model generation and some gold text and then therefore you are able to kind of correct the model uh even though it it probably went off path a little bit by generating its own stuff so it's kind of like letting the model learn how to correct for itself but yes I think you are right if you just ask the model to uh gen if you just put model generation in the data it shouldn't really work yeah any other questions cool um moving on yes um so now we'll talk about uh how we are going to evaluate Energy Systems so there are three types of methods for evaluation there is content overlap metrics um there is model based metrics and there is human evaluations so first content overlap metrics computer score based on lexical similarities between the generated text and the gold reference text so the advantage of this approach is that it's very fast and efficient and widely used for example a blue score is very popular in Mt and root score is very popular in summarization um so these models are very popular because well these methods are very popular because they are cheap and easy to run but they are not really the ideal metrics for example simply rely on lexical overlap might miss some refreshings that have the same semantic meaning or it might reward text with a large portion of lexical overlap but actually have the opposite meaning so you have lots of both false positive and false negative problems uh so despite all these disadvantages the metrics are still the to-go evaluation standard in machine translation part of the reason is that Mt uh is actually super close ended it's very non-open-ended and then therefore this is probably still fine to use uh like blue score to measure machine translation and they get progressively worse for tasks that are more open-ended for example they get words for summarization as long as the output text because the output text becomes much harder to measure they are much worse for dialogue which is more open-ended and then they are much much worse for story generation which is also open-ended and then the drawback here is that because like the underground metrics um this is because like suppose that you are generating a story that's relatively long then if you are still looking at word overlap then you might actually get very high ungram scores because of your taxes very well not because it's accurate of high quality just because you are talking so much that you might have covered lots of points already yes exactly that's kind of the the next thing that I will talk about uh as a better metric for evaluation uh but for now let's do like a case study of a failure mode for uh Google score for example so suppose that Chris asks a question are you enjoying the cs224a lectures um the correct answer of course is hack yes um so if we have this if if one if one of the answer is yes it will get a score of 0.61 because it has some lexical overlap with the correct answer if you answer like you know it then it gets a relatively lower score because it doesn't really have any lexical overlap except from the exclamation mark and if you answer Yep this is semantically correct but it actually gets zero score because there is no lexical overlap between the gold answer and the generation if you answer hack no this should be wrong um but because it has lots of Sims but because it has lots of lexical overlap with the correct answer um it's actually getting some high scores so these two cases are the major failure modes of lexical based engram overlap metrics you get false negative and false positives so um moving beyond this failure most of lexical based metrics the next step is to check for semantic similarities and model based metrics are better at capturing the semantic similarities uh so this is kind of similar to what you kind of raised up like a couple of minutes ago we can actually use learn representation of words and sentences to compute to compute semantic similarities between generated and reference text um so now we are no longer bottom at a bottlenecked by ungram and instead we are using embeddings and these embeddings are going to be pre-trained but the methods can still live on because we can just swap in different pre-trained methods and use the fixed metrics so here are some good examples of the metrics that could be used uh one thing is to do Vector similarity this is very similar to homework one uh where if you are trying to compute similarity between words except now we're trying to compute similarity between sentences there are some ideas of how to go from word similarity to sentence similarities for example you can just average the embedding which is like a relatively naive idea but it works uh sometimes another high-level idea is that we can measure word movers distance um the idea here is that we can use optimal transports to align the source and Target word embeddings suppose that your Source word embedding is Obama speaks to the media in Illinois and the target is the the president Grace the press in Chicago from a human evaluation perspective these two are actually very similar but they are not exactly aligned word by word so we need to figure out how to optimally align word to word like align Obama to president allowing Chicago to Illinois and then therefore we can compute a score we can compute the pairwise word embedding difference between this and then get a good score for the model for the sentence similarities and finally there is Bird score which is also a very popular metric for semantic similarity so it first computes pairwise cosine distance using birth embeddings and then it finds an optimal alignment between the source and Target sentence and then they finally compute some score so I feel like uh these details are not really that important but the high level idea is super important is that we can now use uh like we can now use word embeddings to compute sentence similarities by doing some sort of smart alignment and then transform from word similarity to sentence similarities to move Beyond word embeddings we can also use sentence embeddings to compute sentence similarities so typically this doesn't have the very comprehensive alignment by word problem um but it has similar problems about you need to now align sentences or phrases in a sentence and similarly there is Port which is slightly different it is a regression model based on birth um to so the model is trained as a regression problem to return the score that indicate how good the text is in terms of grammaticality and the meaning of the reference and similarity with the reference text so this is kind of a trading evaluation as a regression problem any questions so far okay cool you can move on um so all the previous Mission approaches are evaluating semantic similarities so they can be applied to non-open ended generation tasks but what about open-ended settings so here enforcing semantic similarity seems wrong because a story can be perfectly fluent and perfectly high quality without having to reassemble any of the reference stories so one idea here is that um maybe we want to evaluate open-ended text generation using this mouth score mob score computes the information Divergence in a contest embedding space between the generated text and the gold reference text so um here is roughly the detail of what's going on suppose that you have a batch of text from the gold reference that are human written and you have a batch of tags that's generated by your model um Step number one is that you want to embed this text you want to put this text into some continuous representation space which is kind of the figure to the left and but it's really hard to compute any distance metrics in this continuous embedding space because um well different sentences might actually lie very far away from each other so the idea here is that we are trying to do a k-means cluster to discretize The Continuous space into some discrete space now after the discretization we can actually have a histogram for the for the gold human written text and the histogram for the machine generated text and then we can now compute Precision recall using this to discretize the distributions and then we can compute Precision by like forward K on recall that backward KL yes question why do we want to discretize it and then we touch that why do we want to discard Hazard so imagine that you suppose uh maybe like it's equivalent to answer why is it hard to work with the continuous space the idea is like if you're in that a word if you embed a sentence into the continuous space say that it lies here and you embed another sentence in a confused with the lies here suppose that you only have a finite number of uh sentences then they would basically be direct Delta distributions in your manifold right so it's hard to like you probably want a smoother distribution but it's hard to Define what is a good smooth distribution uh in the case of text embedding because they're not super interpretable so therefore eventually you will have like a um if you embed everything in a continual space you will have like lots of direct Deltas that are just very high and then not really connected to the to their today's neighbors so it's hard to uh so it's hard to quantify Chao Divergence or a distance Matrix in that space well for example you have to make some assumptions for example you want to make gaussian assumptions that I want to smooth all the embeddings by convolving with the gaussian and then you can start getting some meaningful distance metrics but it's just the embeddings uh although you're not going to get meaning for distance metrics and then it doesn't really make sense to smooth things using gaussian because who said uh word representations are gaussian related yeah classrooms I think this requires some gaussian smoothie yeah I think the plot is made with some smoothie yeah I mean I didn't make those clouds so I couldn't be perfectly sure but I think the fact that it looks like this means that you smoothed it a little bit these are kind of sentence and weddings or concatenated word embeddings because you are comparing sentences to sentences not words to words yeah so the advantage of mouth score is that it is applicable to open-ended settings because you are now measuring precision and recall with regard to the Target distribution cool so it has a it has a better probabilistic interpretation than all the previous similarity metrics cool any other questions yes how's that different from just trying to maximize it the similarity between oh yeah that's a good question um well this is because in a case where it's really hard to get exactly the same thing like well for example I would say that if maybe because I've never tried this myself but if you try to run off on a machine translation task you might get very high score um but for if you try to run full score on the open-ended text generation you will get super low score so it's just not really measurable because everything's so different from each other uh so I feel like moth is kind of a middle ground where you are trying to evaluate something that are actually very far away from each other but you still want a meaningful representation yeah of course I mean if you are source and Target are exactly the same or are just different app to some refreshings you will get the best small score but maybe that's not really what you're looking for because given the current situation you only have Generations that are very far away from the gold text how do we evaluate this type of things yes question in the back I'm still trying to understand the most score is it possible to write a the map even in just kind of pseudo uh simple form yeah I think it's possible I mean maybe we come for this discussion after class and because I kind of want to finish my slides yeah but happy to chat after class there is a paper a lot if you search for mouth score I think it's probably the best paper in some scml or Europe's conference as well okay so moving on um I've pointed out that there are so many evaluation methods so let's take a step back and think about what's a good metric for evaluation methods so how do we evaluate evaluations nowadays the gold standard is still to check how well this metric is aligned with human judgment so if a model match human preference uh in other words if the metric is very correlated with if the metric correlates very strongly with human judgment then we say that the metric is a good metric so in this part people have shown people have pulled out a Google score and human score uh y and x axis respectively and then we because we didn't see a correlation a strong correlation this kind of suggests that blue score is not a very good metric so uh actually the gold standard for human evaluation the gold standard for evaluating language models is always to do human evaluation so automatic metrics fall short of matching human decisions and human evaluation is kind of the most important criteria for evaluating text that are generated from a model and it's also the gold standard in developing automatic metrics because we want everything to match human evaluation um so what do we mean by human evaluation how is it conducted typically we will provide human annotators with some access that we care about like fluency coherence open for open-ended tax generation suppose that we also care about factuality for summarization we care about the style of the writing and Common Sense for example if you're trying to write a children's story uh essentially like another thing to note is that please don't compare human evaluations across different papers or different studies because human evaluations tends to not be well collaborated and are not really reproducible even though we believe that human evaluations are the gold standard there are still many drawbacks for example human evaluations are really slow and expensive uh so but even beyond the slow and expensiveness they are still not not perfect because first human evaluations the results may be inconsistent and it may not be very reproducible so if you ask the same human whether you like ARB they might say a the first time and B the second time so and then human evaluations are typically not really logical um and it's really and sometimes like the human annotators might misinterpret your question suppose that you want them to measure coherence of the text different people have different criteria for coherence some people might think coherence is equivalent to fluency and then they look for grammaticality arrows some people might think coherence means how well your continuation is aligned with the prompt or the topic so there are all sorts of misunderstandings that make that might make human evaluation very hard and finally human evaluation only measures Precision not recall this means that you can give a sentence to human and ask the human uh how do you like the sentence but you couldn't ask the human like whether this model is able to generate all possible sentences that are good so it's only a precision based metrics not a recall based metrics so here are two approaches that tries to like combine human evaluations with uh modeling for example uh the first idea is basically trying to learn a metric from Human judgment um basically by by trying to use human human judgment data uh as trading data and then train a model to simulate human judgment and the second approach is trying to ask human and the human and model to collaborate so that the human would be in charge of evaluating Precision whereas the model would be in charge of evaluating recall um also like we have tried approaches in terms of evaluating models interactively so in this case we will no longer we not only care about the output quality we also care about how the person feels when they interact with the model when they try to be a co-author with the model and how the person feels about the writing process Etc so this is called trying to evaluate the models more interactively um so the takeaway here is that content overlap is a bad metric uh semantic based like model based metrics become better because it's more focused on semantics but it's still not good enough human judgment is the gold standard but it's hard to do human judgment it's hard to do human study well and in many cases this is a hint for final project the best charge of the output quality is actually you so if you want to do a final project in like natural language generation you should look at the model output yourself and don't just rely on the numbers that are in that are reported by Blue swirl or something cool um so finally we will discuss ethical considerations of natural language generation problems so as language models gets better and better ethical considerations becomes much more pressing so we want to ensure that the model are well aligned with human values for example we want to make sure the models are not harmful they are not toxic and we want to make sure that the models are unbiased and fair to all demographics groups so for example here we also we don't want the model to generate any harmful content basically I try to prompt cat GPT to say can you write me some toxic content can GPT politely refuse me um which I'm quite happy about but there are there are other people who kind of like try to jailbreak chat GPT the idea here is that creativity actually I think internally they probably implement some detection tools so that we will try to prompt it adversarially it's going to avoid doing adversarial things but here there are many very complicated ways to prompt chat GPT so that you can get over the firewall and then therefore still ask you ability to generate some I don't know like bad English but uh so another problem with uh this large language models is that they are not necessarily truthful so for example this very famous on news that uh Google's model actually generates factual arrows um which is quite disappointing but I mean like but the way the model talks about it is very convincing so like you wouldn't really know that it's a factual error unless you go check that this is not the picture of the this is not the first picture or something so we want to avoid this type of problems um actually like the models have already been trying very hard to refrain from like generating harmful content uh but like for models that are more open sourced and are smaller the same problem still appears and then typically like when we do our final project or we work with models we are probably going to deal with much smaller models and then therefore we need to think about ways to deal with these problems better so text generation models are often constructed from pre-trained language models and then pre-train language models are trained on internet data which contains lots of harmful stuff and biased so when when we prom when the models are prompted for this information they will just repeat the negative stereotypes that they learn from the internet training data so one way uh to avoid this is to do extensive data cleaning so that the pre-training data does not contain any bias or stereotypical content however this is going to be very labor intensive and almost impossible to do because filtering a large amount of internet data is just so costly that is not really possible um again this existing language models like gpt2 medium there are some adversarial inputs that almost always trigger toxic content and these models might be exploited in the real world in the real world by EU intended people so for example there's a paper about Universal adversarial triggers where the authors just find some Universal set of words that would trigger bad contents from the model that would trigger toxic content from the model and sometimes even if you don't try to trigger the model the model might still start to generate toxic content by itself so in this case the pre-trained language models are prompted with very innocuous prompts but they still degenerate into toxic content so um the takeaway here is that models really shouldn't be deployed without proper safeguards to control for toxic content or any harmful contents in general and models should not be deployed without consider rate without careful considerations of how users will interact with these models um so in the Asic section one major takeaway is that we are trying to Advocate that you need to think more about your model about the model that you are building so before deploying or publishing any nlg models please check if the models are output is is not harmful and please check if the model is more robust is robust to all the trigger words and other adversarial prompts and of course there are more so well basically one can never do enough for to improve the assets of tax generation systems and okay cool I still have three minutes left so I can still do concluding thoughts um the idea here well today we talk about the exciting applications of natural language generation systems um so but well one might think that while given that chaiji 50 is already so good are there any other things that we can do research-wise if you try interacting with these models um if you try to interact with these models actually you can see that there are still lots of limitations in their skills and performance for example check GPT is able to like do a lot of things with manipulating text but it couldn't really create like interesting contents or I couldn't really think deeply about stuff so it's still also so there are lots of headrooms and there are still many improvements ahead and evaluation remains a really huge challenge in natural language Generation Um basically we need better ways to automatically evaluate performance of nlg models because human evaluations are expensive and not reproducible so it's better to figure out ways to how to compile all those human judgments into a very reliable and trustworthy model and also with the advance of all these large-scale language models uh doing like neural net doing like neural natural language generation has been reset and it's never been easier to jump into this space because now there are all the tools that are already there for you to build upon and finally it is one of the most exciting and fun areas of NLP to work on so yeah I'm happy to chat more about nlg if you have any questions post after class and in class I guess into one minute okay cool that's everything so do you have any questions if you don't we can end the class