okay hi everyone welcome back to cs224n so today is a pretty key lecture where we get through a number of important topics for neural networks especially as applied to natural language processing so right at the end of last time i started into current neural networks so we'll talk in detail more about current neural networks in the first part of the class and we'd emphasize language models um but then also getting a bit beyond that and then look at more advanced kinds of recurrent neural networks towards the end part of the class um i just wanted to sort of say a word before getting underway about the final project so hopefully by now you've started looking at assignment three which is the middle of the five assignments for the first half of the course and then in the second half of the course most of your effort goes into a final project so next week the thursday lecture is going to be about final projects and choosing the final project and tips for final projects etc so it's fine to delay thinking about final projects until next week if you want but you shouldn't delay it too so long because we do want you to get underway with what topic you're going to do for your final project if you are thinking about final projects you can find some info um on the website but note that the info that's there at the moment is still last year's information and it will be being updated over the coming week we'll also talk about project mentors if you've got ideas of people who on your own you can line up as a mentor that now be a good time to ask them about it and we'll sort of talk about what the alternatives are okay so um last lecture i introduced the idea of language models so probabilistic models that predict the probability of next words after a word sequence and then we looked at engram language models and started into a current neural network models so today we're going to talk more about the simple rnns we saw before talking about training rnn's and uses of rnns but then we'll also look into the problems that occur with rnns and how we might fix them these will motivate a more sophisticated rn architecture called lstms and we'll talk about other more complex rnn options bi-directional rnns and multi-layer rnns then next tuesday we're essentially going to ex further exploit and build on the rnn based architectures that we've been looking at to discuss how to build a newer machine translation system with the sequence to sequence and model with attention and effectively as that model um is what you'll use in assignment for but it also means that you'll be using all of the stuff that we're talking about today okay so if you remember from last time this was the idea of a simple recurrent neural network language model so we had a sequence of words as our context for which we've looked up word embeddings and then their current neural network model ran this recurrent layer where at each point we have a previous hidden state which can just be zero at the beginning of a sequence and you have feeding it in to the next hidden state the previous hidden state and encoding and transform the coding of a word using this recurrent neural network equation that i have on the left that's very central and based on that you compute a new hidden representation for the next time step and you can repeat that along for successive time steps now we also usually want our recurrent neural networks to produce outputs so i only show it at the end here but at each time step we're then also going to generate an output and so to do that we're feeding the hidden layer into a soft max layer so we're doing another matrix multiply add on a bias put it through the soft max equation and that will then gives the probability distribution over words and we can use that to predict how likely it is that different words are going to occur after the students open there okay so i didn't i'd introduced that model but i hadn't really gone through the specifics of how we um train this model how we use it and evaluate it so let me um go through this now so here's how we train an rnn language model we get a big corpus of text just a lot of text and so we can regard that as just a long sequence of words x1 to xt and what we're going to do is feed it into the rnnlm so for each posit we're going to take prefixes of that sequence and based on each prefix we're going to want to predict the probability distribution for the word that comes next and then we're going to train our model by assessing how good a job we do about that and so the loss function we use is the loss function normally referred to as cross-entropy loss in the literature which is this negative log likelihood lasts so we are going to predict some word to come next well we we have a probability distribution over predictions of what word comes next and actually there was an actual next word in the text and so we say well what probability did you give to that word and maybe we gave it a probability estimate of 0.01 well it would have been great if we'd given a probability estimate of almost one because that meant we've almost certain that what did come next in our model and so we'll take a loss to the extent that we're giving the actual next word a predicted probability of less than one to then get an idea of how well we're doing over the entire um corpus we work out that loss at each position and then we work out the average loss of the entire training set so let's just go through that again more graphically in the next couple of slides so down the bottom here's our corpus of text we're running it through our simple recurrent neural network and at each position we've predicting a probability distribution over words we then say well actually at each position we know what word is actually next so when we're at time step one the actual next word is students because we can see it just to the right of us here and we say what probability estimate did you give to students and to the extent that it's not high it's not one we take a loss and then we go on to the time step two and we say well at time step two you predicted probability distribution over words the actual next word is opened so to the extent that you haven't given high probability to open you take a loss and then that repeats for in time step three we're hoping the model predict there at time step four we're hoping the model will predict exams and then to work out our overall loss we're then um averaging our per time step loss so in a way this is a pretty obvious thing to do but note that there is a little subtlety here and in particular this algorithm is referred to in the literature as teacher forcing and so what does that mean well you know you can imagine what you can do with a recurrent neural network is say okay just start generating maybe i'll give you a hint as to where to start i'll say the sentence starts the students and then let it run and see what it generates coming next um it might start saying the students have been locked out of the classroom or whatever it is right um and that we could say is well that's not very close to what the actual text says and somehow we want to learn from that and if you go in that direction there's a space of things you can do that leads into more complex algorithms such as reinforcement learning um but from the perspective of training these neural models that's unduly complex and unnecessary so we have this very simple way of doing things which is what we do is just predict one time step forward so we say we know that the prefix is the predict a probability distribution over the next word it's good to the extent that you give probability master opened okay now the prefix is the student's opened predict a probability distribution over the next word it's good to the extent that you give probability master there and so effectively at each step we're resetting to what was actually in the corpus so you know it's possible after the students opened the model thinks that by far the most probable thing to come next is ah or thus say i mean we don't actually use what the model suggested we penalize the model for not having suggested there but then we just go with what's actually in the corpus and ask it to predict again this is just a little side thing but it's an important part to know if you're actually training your own neural language model i sort of presented as one huge corpus that we chug through but in practice we don't chug through a whole corpus one step at a time what we do is we cut the whole corpus into shorter pieces which might commonly be sentences or documents or sometimes they're literally just pieces that are chopped right so you'll recall that stochastic gradient scent allows us to compute a loss and gradients from a small chunk of data and update so what we do is we take these small pieces compute gradients from those and update weights and repeat and in particular we get a lot more speed and efficiency and training if we aren't actually doing an update for just one sentence at a time but actually a batch of sentences so typically what we'll actually do is we'll feed to the model 32 sentences say of a similar length at the same time compute gradients for them um update weights and then get another batch of sentences to train on um how do we train i haven't sort of gone through the details of this i mean in one sense the answer is just like we talked about in lecture three um we use back propagation to get gradients and update parameters um but let's take at least a minute to go through the differences and subtleties of the current neural network case um and the central thing that's a bit you know as before we're going to take our loss and we're going to back propagate it to all of the parameters of the network everything from word embeddings to biases etc but the central bit that's a little bit different and is more complicated is that we have this wh matrix that runs along the sequence that we keep on applying to update um our hidden state so what's the derivative of jt of theta with respect to the repeated weight matrix wh and well the the answer to that is that what we do is we look at it in each position and work out what the partials are of gt with respect to wh in position one or position two position three position four etcetera right along the sequence and we just sum up all of those partials and that gives us a partial for jt with respect to wh overall um so the answer for a current neural networks is the gradient with respect to a repeated weight in our current network is the sum of the gradient with respect to each time it appears um and let me just then go through a little why that is the case but before i do that let me just note one gotcha i mean it's just not the case that this means it equals t times the partial of jt with respect to wh because we're using wh here here here here here through the sequence and for each of the places we use it there's a different upstream gradient that's being fed into it so each of the values in this sum will be completely different from each other well why we get this answer is essentially a consequence of what we talked about in the third lecture so to take the simplest case of that right that if you have a multi-variable function f of x y and you have two single variable functions x of t and y of t which are fed one input t well then the um simple um version of working out the derivative derivative of this function is you take the derivative down one path and you take the derivative down the other path and so in the slides in lecture three that was what was summarized on a couple of slides by the slogan gradient sum at outward branches so t has outward branches and so you take gradient here on the left gradient on the right and you sum them together and so really what's happening with the recurrent neural network is just many pieces generalization of this so we have one wh matrix and we're using it to keep on updating the hidden state at time 1 time 2 time 3 right through time t and so what we're going to get is that this has a lot of um outward branches and we're going to sum the gradient path at each one of them but what is this gradient path here it's kind of goes down here and then goes down there but you know actually the bottom part is that we're just using wh at each position so we have the partial of wh used at position i with respect to the partial of wh which is just our weight matrix for our current neural network so that's just one because you know we're just using the same matrix everywhere and so we are just then summing um the partials in each position that we use it okay um practically what does that mean in terms of how you compute this um well if you're doing it by hand um what happens is you start at the end just like this general lecture three story you work out derivatives with respect to the hidden layer and then with respect to wh at the last time step and so that gives you one update for wh but then you continue passing the gradient back to the t minus one time step and after a couple more steps of the chain rule you get another update for wh and you simply sum that onto your previous update for wh and then you go to ht minus 2 you get another update for wh and you sum that onto your update for wh and you go back all the way and you sum up the gradients as you go um and that gives you a total update um for wh um and so there's so two tricks here and i'll just mention um the two tricks you have to kind of separately sum the updates for wh and then once you've finished apply them all at once you don't want to actually be changing the wh matrix as you go because that's then invalid because um the forward calculations were done with the constant wh that you had from the previous state all through the network the second trick is well if you're doing this for sentences you can normally just go back to the beginning of the sentence but if you've got very long sequences this can really slow you down if you're having to sort of run this algorithm back for a huge amount of time so something people commonly do is what's called truncated back propagation through time where you choose some constant say 20 and you say well i'm just going to run this back propagation for 20 time steps sum those 20 gradients and then i'm just done that's what i'll update the wh matrix with and that works just fine okay so now given a corpus we can train a simple rnn and so that's good progress but this is a model that can also generate text in general so how do we generate text well just like in our engram language model we're going to generate text by repeated sampling so we're going to start off with an initial state and um yeah this slide is imperfect um so the initial state for the hidden state um is is normally just taken as a zero vector and well then we need to have something for a first input and on this slide the first input is shown as the first word my and if you want to feed a starting point you could feed my but a lot of the time you'd like to generate a sentence from nothing and if you want to do that what's conventional is to additionally have a beginning of sequence token which is a special token so you'll feed in the beginning of sequence token in at the beginning as the first token it has an embedding and then you use the rnn update and then you generate using the softmax and next word and um well you generate a probability of probability distribution over next words and then at that point you sample from that and it chooses some word like favorite and so then the trick is for doing generation that you take this word that you sampled and you copy it back down to the input and then you feed it in as an import next step if you are an n sample from the softmax get another word and just keep repeating this over and over again and you start generating the text and how you end is as well as having a beginning of sequence um special symbol you usually have an end of sequence special symbol and at some point um the recurrent neural network will generate the end of um sequence symbol and then you say okay i'm done i'm finished generating text so before going on for the more of the difficult content of the lecture we can just have a little bit of fun with this and try training up and generating text with our current neural network model so you can generate you can train an rnn on any kind of text and so that means one of the fun things that you can do is generate text in different styles based on what you could train it from so here harry potter is a there is a fair amount of a corpus of text so you can train rnn lm on the harry potter books and then say dolph and generate some text and it'll generate text like this sorry how harry shouted panicking i'll leave those brooms in london are they no idea said nearly headless nick casting low close by cedric carrying the last bit of treacle charms from harry's shoulder and to answer him the common room pushed upon it four arms held a shining knob from when the spider hadn't felt it seemed he reached the teams too um well so on the one hand that's still kind of a bit incoherent as a story on the other hand it sort of sounds like harry potter and certainly the kind of you know vocabulary and constructions in users and i think you'd agree that you know even though it gets sort of incoherent it's sort of more coherent than what we got from an engram language model um when i showed a generation in the last lecture you can choose a very different style of text so you could instead train the model on a bunch of cookbooks and if you do that you can then say generate based on what you've learned about cookbooks and it'll just generate a recipe so here's a recipe chocolate ranch bbq categories yield six servings two tablespoons of parmesan cheese chopped and one cup of coconut milk three eggs beaten place each pasta over layers of lumps shaped mixture into the moderate oven and simmer until firm serve hot embodied fresh mustard orange and cheese combine the cheese and salt together the dough in a large skillet add the ingredients and stir in the chocolate and pepper so you know um this recipe makes um no sense and it's sufficiently um incoherent there's actually even no danger that you'll try cooking this at home um but you know something that's interesting is although you know there's really just isn't a recipe and the things that are done in the instructions have no relation um to the ingredients the the thing that's interesting that it has learned is this recurrent neural network model is that it's really mastered the overall structure of a recipe it knows the recipe has a title it often tells you about how many people it serves it lists the ingredients and then it has instructions um to make it so that's sort of fairly impressive in some sense for high level text structuring um so the one other thing i wanted to mention was when i say you can train an rnn language model in any kind of text the other difference from where we were in in gram language models was on engram language models that just meant counting engrams and meant it took um two minutes or even on a large corpus with any modern computer training your rnn lm actually can then be a time intensive activity and you can spend hours doing that as you might find next week when you're training machine translation models okay how do we decide if our models are good or not um so the standard evaluation metric for language models is what's called perplexity and what perplexity is is um kind of like when you were training your model you use teacher forcing over a piece of text that's a different piece of test text which isn't text that was in the training data and you say well given a sequence of t words what probability do you give to the actual t plus one word and you repeat that at each position and then you take the inverse of that probability and raise it to the one on t for the length of your test text sample and that number is the perplexity so it's a geometric mean of the inverse probabilities now um after that explanation perhaps an easier way to think of it is that the perplexity is simply the cross entropy loss that i introduced before exponentiated so but you know it's now the other way around so low perplexity um is better so there's actually an interesting story about these perplexities um so a famous figure in the development of probabilistic and machine learning approaches to natural language processing is fred gellanek who died a few years ago and he was trying to interest people in the idea of using probability models and machine learning um for natural language processing at a time i this is the 1970s and early 1980s when nearly everyone in the field of ai was still in the thrall of logic based models and blackboard architectures and things like that for artificial intelligence systems and so fred gellarnek was actually an information theorist by background um and who then got interested in working with speech and then language data so at that time the stuff that's this sort of um exponential or using cross-entropy losses was completely bread and butter to fred jelinek but he'd found that no one in ai could understand the bottom half of the slide and so he wanted to come up with something simple that ai people at that time could understand and perplexity has a kind of a simple interpretation you can tell people so if you get a perplexity of 53 that means how uncertain you are of the next word is equivalent to the uncertainty of that you're tossing a 53-sided dice and it's coming up as a one right so um that was kind of an easy simple metric and so he introduced um that idea um but you know i guess things stick and to this day um everyone evaluates their language models by providing perplexity numbers and so here are some perplexity numbers so traditional engram language models commonly had perplexities over a hundred but if you made them really big and really careful you carefully you could get them down into a number like 67 as people started to build more advanced recurrent neural networks especially as they moved beyond the kind of simple rnn's which is all i've shown you so far which one of is in the second site line of the slide into lstms which i talk about later in this course that people started producing much better perplexities and here we're getting perplexities down to 30 and this is results actually from a few years ago so nowadays people get perplexities of even lower than 30. um you have to be realistic in what you can expect right because if you're just generally generating a text some words are almost determined um so you know if it's something like um you know zoom gave the man a napkin he said thank you know basically a hundred percent you should be able to say the word that comes next is you um and so that you can predict really well but um you know if it's a lot of other sentences like um he looked out the window and saw uh something right no probability in the model model in the world can give a very good estimate of what's actually going to be coming next to that point and so that gives us the sort of residual uncertainty that leads to perplexities that are on average might be around 20 or something okay um so we've talked a lot about language models now why should we care about language modeling you know well there's sort of an intellectual scientific answer that says this is a benchmark task right if we what we want to do is build machine learning models of language and our ability to predict what word will come next in the context that shows how well we understand both the structure of language and the structure of the human world that language talks about um but there's a much more practical answer than that um which is you know language models are really the secret tool of natural language processing so if you're talking to any nlp person and you've got almost any task it's quite likely they'll say oh i bet we could use a language model for that and so language models are sort of used as a not the whole solution but a part of almost any task any task that involves generating or estimating the probability of text so you can use it for predictive typing speech recognition grammar correction identifying authors machine translation summarization dialogue just about anything you do with natural language involves language models and we'll see examples of that in following classes including next tuesday where we're using language models for machine translation okay so a language model is just a system that predicts the next word um a recurrent neural network is a family of neural networks which can take sequential input of any length they reuse the same weights to generate a hidden state and optionally but commonly an output on each step note that these two things are different so we've talked about two ways that you could build language models but one of them is rnn's being a great way but rnns can also be used for a lot of other things so let me just quickly preview a few of the things you can do with rnns so there are lots of tasks that people want to do in nlp which are referred to as sequence tagging tasks where we'd like to take words of text and do some kind of classification along the sequence so one simple common one is to give words parts of speech that is a determiner startled as an adjective cat is a noun knocked as a verb um and well you can do this straightforwardly by using a current neural network as a sequential classifier where it's now going to generate parts of speech rather than the next word you can use a recurrent neural network the sentiment classification well this time we don't actually want to generate um and outputted each word necessarily but we want to know what the overall sentiment looks like so somehow we want to get out a sentence encoding that we can perhaps put through another neural network layer to judge whether the sentence is positive or negative well the simplest way to do that is to think well after i've run my lstm through the whole sentence actually this final hidden state it's encoded the whole sentence because remember i updated that hidden state based on each previous word and so you could say that this is the whole meaning of the sentence so let's just say that is the sentence encoding and then put an extra classifier layer on that with something like a softmax classifier um that method has been used and it actually works reasonably well and if you sort of train this model end to end well it's actually then motivated to preserve sentiment information in the hidden state of the current neural network because that will allow it to better predict the sentiment of the whole sentence which is the final task and hence loss function that we're giving the network but it turns out that you can commonly do better than that by actually doing things like feeding all hidden states into the sentence and coding perhaps by making the sentence encoding an element-wise max or an element wise mean of all the hidden states because this then more symmetrically encodes the hidden state over each time step another big use of recurrent neural networks is what i'll call language encoder module uses so anytime you have some text for example here we have a question of what nationality was beethoven we'd like to construct some kind of newer representation of this so one way to do it is to run the current neural network over it and then just like last time to either take the final hidden state or take some kind of function of all the hidden states and say that's the sentence representation and we could do the same thing for the context so for question answering we're going to build some more neural net structure on top of that and we'll learn more about that in a couple of weeks um when we have the question answering lecture but the key thing is what we built so far we use to get sentence representation so it's a language encoder module so that was the language encoding part we can also use rnns to decode into language and that's commonly used in speech recognition machine translation summarization so if we have a speech recognizer the input is an audio signal and what we want to do is decode that into language well what we could do is use some function of the input which is probably itself going to be neural net as the initial hidden state of our rnn lm and then we say start generating text based on that and so it should then we generate word at a time by the method that we just looked at we turn the speech into text so this is an example of a conditional language model because we're now generating text conditioned on the speech signal and a lot of the time you can do interesting more advanced things with recurrent neural networks by building conditional language models another place you can use conditional language models is for text classification tasks and including sentiment classification so if you can condition your language model based on a kind of sentiment you can build a kind of classifier for that and another use that we'll see a lot of next class is for machine translation okay so that's the end of the intro to doing things with recurrent neural networks and language models now i want to move on and tell you about the fact that everything is not perfect and these recurrent neural networks tend to have a couple of problems and we'll talk about those and then in part that'll then motivate coming up with a more advanced recurrent neural network architecture so the first problem to be mentioned is the idea of what's called vanishing gradients and what does that mean well at the end of our sequence we have some overall loss that we're calculating and well what we want to do is back propagate that loss and we want to back propagate it right along the sequence and so we're working out the partials of j4 with respect to the hidden state at time one and when we have a longer sequence we'll be working out the partials of j20 with respect to the hidden state at time one and how do we do that well how we do it is by composition and the chain rule we've got a big long chain roll along the whole sequence um well if we're doing that you know we're multiplying a ton of things together and so the danger of what tends to happen is that as we do these multiplications a lot of time these partials successive hidden states becomes small and so what happens is as we go along the gradient gets smaller and smaller and smaller and starts to peter out and to the extent that it peters out well then we've kind of got no upstream gradient and therefore we won't be changing the parameters at all and that turns out to be pretty problematic um so the next couple of slides sort of um say a little bit about the why and how this happens what's presented here is a kind of only semi formal wave your hands at the kind of problems that you might expect um if you really want to sort of get into all the details of this um you should look at the couple of papers that are mentioned in small print at the bottom of the slide but at any rate if you remember that this is our basic um recurrent neural network equation well let's consider an easy case suppose we sort of get rid of our non-linearity and just assume that it's an identity function okay so then when we're working out the partials of the hidden state with respect to the previous hidden state we can work those out in the usual way according to the chain rule and then if sigma is simply the identity function well then everything gets really easy for us so only the the sigma just goes away and only the first term involves um h at time t minus one so the later terms go away and so um our gradient ends up as wh well that's doing it for just one time step what happens when you want to work out these partials a number of times steps away so we want to work it out the partial of time step i with respect to j um well what we end up with is a product of the partials of successive time steps and while each of those is coming out as wh and so we end up getting wh raised to the elf power and well our potential problem is that if wh is small in some sense then this term gets exponentially problematic i it becomes vanishingly small as our sequence length becomes long well what can we mean by small well a matrix is small if its eigenvalues are all less than one so we can rewrite what's happening with this success of multiplication using eigenvalues and eigenvectors um and i should say that all i can vector values less than one is a sufficient but not necessary condition for what i'm about to say um right so we can rewrite things using the eigenvectors as a basis and if we do that we end up getting the eigenvalues being raised to the eighth power and so if all of our eigen values are less than one if we're taking a number less than one and then raising it to the eighth power that's going to approach zero as the sequence length grows and so the gradient vanishes okay now the reality is more complex than that because actually we always use a non-linear activation sigma but you know in principle it's sort of the same thing um apart from we have to consider in the effect of the non-linear activation okay so why is this a problem that the gradients disappear well suppose we're wanting to look at the influence of time steps well in the future on the representations we want to have early in the sentence well what's happening late in the sentence just isn't going to be giving much information about what we should be storing in the h at time 1 vector whereas on the other hand the loss at time step two is going to be giving a lot of information at what should be stored in the hidden vector at time step one so the end result of that is that what happens is these simple rnn's are very good at modeling nearby effects but they're not good at all at modeling long-term effects because the gradient signal from far away is just lost too much and therefore the model never effectively gets to learn what information from far away it would be useful to preserve into the future so let's consider that concretely um for the example of language models that we've worked on so here's a piece of text um when she tried to print her tickets she found that the printer was out of toner she went to the stationery store to buy more toner it was very overpriced after installing the toner into the printer she finally printed her and well you're all smart human beings i trust you can all guess what the word that comes next is it should be tickets but well the problem is that for the rnn to start to learn cases like this it would have to carry through in its hidden state a memory of the word tickets for sort of whatever it is about 30 hidden state updates and well we'll train on this um example and so we'll be wanting it to predict tickets is the next word and so a gradient update will be sent right back through the hidden states of the lstm corresponding to this sentence and that should tell the model it's good to preserve information about the word tickers because that might be useful in the future here it was useful in the future but the problem is that the gradient signal will just become far too weak out over after a bunch of words and it just never learns that dependency and so what we find in practice is the model is just unable to predict similar long-distance dependencies at test time i've spent quite a long time on vanishing gradients and then really vanishing gradients are the big problem in practice um with using recurrent neural networks over long sequences but you know i have to do justice to the fact that you can actually also have the opposite problem you can also have exploding gradients so if a gradient becomes too big that's also a problem and it's a problem because the stochastic gradient update step becomes too big right so remember that our parameter update is based on the product of the learning rate and the gradient so if your gradient is huge right you've calculated oh it's got a lot of slope here this has a slope of 10 000 then your parameter update can be arbitrarily large and that's potentially problematic that can cause a bad update where you take a huge step and you end up at a weird and bad parameter configuration so you sort of think you're coming up with a to a steep hill to climb and well you want to be climbing the hill to high likelihood but actually the gradient is so sleek steep that you make an enormous update and then suddenly your parameters are over in iowa and you've lost your hill altogether there's also the practical difficulty that we only have so much resolution now floating point numbers so if your gradient gets too steep um you start getting not a numbers in your calculations which ruin all your hard training work um we use a kind of an easy fix to this which is called gradient clipping which is we choose some reasonable number and we say we're just not going to deal with gradients that are bigger than this number um a commonly used number is 20 you know some thing that's got a range of spread but not that high you know you can use 10 100 some we're sort of in that range um and if the norm of the gradient is greater than that threshold we simply just scale it down which means that we then make a smaller gradient update so we're still moving in exactly the same direction but we're taking a smaller step so doing this gradient clipping is important you know but it's an easy problem to solve okay um so the thing that we've still got left to solve is how to really solve this problem of vanishing gradients um so the problem is yeah these rnns just can't preserve information over many time steps and one way to think about that intuitively is at each time step we have a hidden state and the hidden state is being completely changed at each time step and it's being changed in a multiplicative manner by multiplying by wh and then putting it through um and non-linearity like maybe we could make some more progress if we could more flexibly maintain a memory in our recurrent neural network which we can manipulate in a more flexible manner that allows us to more easily preserve information and so this was a idea that people started thinking about and actually they started thinking about it a long time ago in the late 1990s [Music] and huck wright and schmidt hoover came up with this idea that got called long short term memory rnns as a solution to the problem of vanishing gradients i mean so this 1997 paper is the paper you always see cited for lstms but you know actually in terms of what we now understand as an lstm it was missing part of it in fact it's missing what in retrospect has turned out to be the most important part of the modern lstm so really in some sense the real paper that the modern lstm is due to is this slightly later paper by gersh still schmidt hoover and cummins from 2000 um which additionally introduces the forget gate that i'll explain in a minute um yeah so um so this was some very clever stuff that was introduced and it turned out later to have an enormous impact um if i just diverge from the technical part for one more moment that you know for those of you who these days um think that mastering your networks is the path to fame and fortune the funny thing is you know at the time that this work was done that just was not true right very few people were interested in neural networks and although long short-term memories have turned out to be one of the most important successful and influential ideas in neural networks for the following 25 years really the original authors didn't get recognition for that so both of them are now professors at german universities but hawk writer moved over into doing bioinformatics work to find um something to do and guess actually is doing kind of multimedia studies um so um that's the fates of history um okay so what is an lstm so the a crucial innovation of an lstm is to say well rather than just having one hidden vector in the recurrent model we're going to build a model with two hidden vectors at each time step one of which is still called the hidden state h and the other of which is called the cell state now you know arguably in retrospect these were named wrongly because as you'll see when we look at in more detail in some sense the cell is more equivalent to the hidden state of the simple rnn then vice versa but we're just going with the names that everybody uses so both of these are vectors of length n um and it's going to be the cell that stores long-term information and so we want to have something that's more like memory so the meaning like ram and the computer um so the cell is designed so you can read from it you can erase parts of it and you can write new information to the cell um and the interesting part of an lstm is then it's got control structures to decide how you do that so the selection of which information to erase write and read is controlled by probabilistic gates so the gates are also vectors of length n and on each time step um we work out a state for the gate vectors so each element of the gate vectors is a probability so they can be open probability one close probability zero or somewhere in between and their value will be saying how much do you erase how much do you write how much do you read and so these are dynamic gates with a value that's computed based on the current context okay so in this next slide we go through the equations of an lstm but following this there are some more graphic slides which will probably be easier to absorb right so we again just like before it's our current neural network we have a sequence of inputs x and t and we're going to at each time step compute a cell state and a hidden state so how do we do that so firstly we're going to compute values of the three gates and so we're computing the gate values using an equation that's identical to the equation for the simple recurrent neural network but in particular um oops sorry i'll just just say what the gates are first so there's a forget gate um which can we will control what is kept in the cell at the next time step versus what is forgotten there's an input gate which is going to determine which parts of a calculated new cell content get written to the cell memory and there's an output gate which is going to control what parts of the cell memory are moved over into the hidden state and so each of these is using the logistic function because we want them to be in each element of this vector a probability which will say whether to fully forget partially forget or fully fully remember yeah and the equation for each of these is exactly like the simple r and n equation but note of course that we've got different parameters for each one so we've got a forgetting weight matrix w with a forgetting bias and a forgetting multiplier of the input okay so then we have the other equations that really are the mechanics of the lstm so we have something that will calculate a new cell content so this is our candidate update and so for calculating the candidate update we're again essentially using exactly the same simple rnn equation apart from now it's usual to use tanh so you get something that as discussed last time is balanced around zero okay so then to actually update things we use our gates so for our new cell content what the idea is is that we want to remember some but probably not all of what we had in the cell from previous time steps and we want to store some but probably not all of the value that we've calculated as the new cell update and so the way we do that is we take um the previous cell content and then we take its hadamard product with the forget vector and then we add to it the hadamard product of the input gate times the candidate cell update and then for working out the new hidden state we then work out which parts of the cell to expose in the hidden state and so after taking a tan h transform of the cell we then take the hadamard product with the output gate and that gives us our hidden representation and as this hidden representation that we then put through a soft soft max layer to generate um our next output of our lstm or current neural network yeah so um the gates and the things that they're put with are vectors of size n and what we're doing is we're taking each element of them and multiplying them element wise to work out a new vector and then we get two vectors and that we're adding together so this um way of doing things element-wise you sort of don't really see in standard linear algebra course um it's referred to as the hadamard product it's represented by some kind of circle i mean actually in more modern work it's been more usual to represent it with this slightly bigger circle with the dot at the middle as the hadamard product symbol and someday i'll change these slides to be like that but i was lazy and redoing the equations but the other notation you do see quite often is just using the same little circle that you use for function composition to represent hadamard product okay so all of these things are being done as vectors of the same length n and the other thing that you might notice is that the candidate update and the forget import and output gates all have a very similar form the only difference is three logistics and one tanh and none of them depend on each other so all four of those can be calculated in parallel and if you want to have an efficient lstm implementation that's what you do okay so here's the more graphical presentation of this so these pictures come from chris ola and i guess he did such a nice job at producing pictures for lstms that almost everyone uses them these days and so this sort of pulls apart the computation graph of an lstm unit so blowing this up you've got from the previous time step both your cell and hidden recurrent vectors and so you feed the hidden vector from the previous time step and the new input x t into the computation of the gates which is happening down the bottom so you compute the forget gate and then you use the forget gate in a hadamard product here drawn as a actually a time symbol to forget some cell content you work out the input gate and then using the input gate and a regular recurrent neural network like computation you can compute candidate new cell content and so then you add those two together to get the new cell content which then heads out as the new cell content at time t but then you also have worked out an output gate and so then you take the cell content put it through another non-linearity and multi hadamard product with the output gate and that then gives you the new hidden state um so this is all kind of complex but as to understanding why something is different is happening here the thing to notice is that the cell state from t minus 1 is passing right through this to be the cell state at time t without very much happening to it so some of it is being deleted by the forget gate and then some new stuff is being written um to it as a result of using this candidate new cell content but the real secret of the um lstm is that new stuff is just being added to the cell with an addition right so in the simple rnn at each successive step you're doing a multiplication and that makes it incredibly difficult to learn to preserve information in the hidden state over a long period of time is it's not completely impossible but it's a very difficult thing to learn whereas with this new lstm architecture it's trivial to preserve information the cell from one time step to the next you just don't forget it and it'll carry right through with perhaps some new stuff added in um to also remember and so that's the sense in which the cell behaves much more like ram in a conventional computer that is storing stuff and extra stuff can be stored into it and other stuff can be deleted from it as you go along okay so the lstm architecture makes it much easier to preserve information from many time steps right so in particular standard practice with lstms is to initialize the forget gate to a one vector which it's just so that the starting point is to say preserve everything um from previous time steps and then it is then learning when it's appropriate to forget stuff and in contrast it's very hard to get or a simple rnn to preserve stuff for a very long time i mean what does that actually mean well you know i've put down some numbers here i mean you know how what you get in practice you know depends on a million things it depends on the nature of your data and how much data you have and what dimensionality your hidden states are blurry bloody blur but just to give you some idea of what's going on is typically if you train a simple recurrent neural network that its effect of memory its ability to be able to use things in the past to condition the future goes for about seven time steps you just really can't get it to remember stuff further back in the past than that whereas um for the lstm it's it's not complete magic it doesn't work forever but you know it's effectively able to remember and use things from much much further back so typically you find that with an lstm you can effectively remember and use things about a hundred times steps back and that's just enormously more useful for a lot of the natural language understanding tasks that we want to do and so that was precisely what the lstm was um designed to do and i mean so in particular just going back to its name i quite a few people mispassed its name the idea of its name was there's a concept of short-term memory which comes from psychology and it had been suggested for simple rnns that the hidden state of the rnn could be a model of human short-term memory and then there would be something somewhere else that would deal with human long-term memory but well people had found that this only gave you a very short short-term memory um so what um hock wright and schmidt uber were interested in was how we could give um construct models with long short term memory and so that then gave us this name of lstm um lstms don't guarantee that there are no vanishing or exploding gradients but in practice they provide they they don't tend to explode nearly the same way again that plus sign is crucial rather than a multiplication and so they're a much more effective way of learning long distance dependencies okay so despite the fact that lstms were developed around 1997 it was really only in the early 2010s um that the world woke up to them and how successful they were so it was really around 2013 to 2015 that lstms sort of hit the world achieving state-of-the-art results on all kinds of problems um one of the first big demonstrations was for handwriting recognition um then speech recognition but then going on to a lot of um natural language tasks including machine translation parsing um vision and language tasks like image captioning as well of course using them for language models and around these years lstms became the dominant approach for most nlp tasks the easiest way to build a good strong model was to approach the problem with an lstm so now in 2021 actually lstms are starting to be supplanted or have been supplanted by other approaches particularly transformer models um which we'll get to in the class in a couple of weeks time so this is the sort of picture you can see so for many years there's been a machine translation conference and sort of bake off competition called wmt workshop on machine translation so if you look at the history of that in wmt 2014 [Music] there were zero neural machine translation systems in the competition 2014 was actually the first year that the success of um lstms for machine translation was proven in a conference paper um but nothing occurred in this competition um by 2016 [Music] everyone had jumped on ls dms is working great um and lots of people including the winner of the competition was using an lstm model um if you then jump ahead to 2019 um then there's relatively little use of lstms and the vast majority of people are now using transformers so things change quickly in your network land and i keep on having to rewrite these lectures um so quick further note on vanishing and exploding gradients is it only a problem with recurrent neural networks it's not it's actually a problem that also occurs anywhere where you have a lot of depth including feed forward and convolutional neural networks as any time when you've got long sequences of chain rules which give you multiplications the gradient can become vanishingly small as it back propagates um and so generally sort of lower layers are learned very slowly and are hard to train so there's been a lot of effort in other places as well to come up with different architectures that let you learn more efficiently in deeper networks and the commonest way to do that is to add more direct connections that allow the gradient to flow um so the big thing and vision in the last few years has been resnets where the res stands for residual connections and so the way they're made this picture is upside down so the input is at the top um is that you have these sort of two paths that are summed together one path is just an identity path and the other one goes through some neural network layers and so therefore its default behavior is just to preserve the input um which might sound a little bit like what we just saw for lstms um there are other methods that then being dense nets where you add skip connections forward to every laser layer highway nets were also actually developed by schmidhuber and sort of a reminiscent of what was done with lstms so rather than just having an identity connection as a resnet has it introduces an extra gate so it looks more like an lstm which says how much to send the input through the highway versus how much um to put it through a neural net layer and those two are then combined into the output um so essentially this problem occurs anywhere when you have a lot of depth in your layers of neural network but it first arose and turns out to be especially problematic with recurrent neural networks they're particularly unstable because of the fact that you're do you've got this one weight weight matrix that you're repeatedly using through the time sequence okay so we've got we've got a couple of questions um more or less about whether you would ever want to use an rn like a simple rnn instead of an lstm how does the lstm learn what to do with its gates can you opine on those things sure um so i think basically the answer is um you should never use a simple rnn these days you should always use an lstm i mean you know obviously that depends on what you're doing if you're wanting to do some kind of analytical paper or something you might prefer a simple rnn and it is the case that you can actually get decent results with simple rnns providing you're very careful to make sure that things aren't exploding nor vanishing but you know in practice getting simple rnns to work and preserve long context is incredibly difficult where you can train lstms and they will just work so really you should always just use an lstm now wait the second question was um i think there's a bit of confusion about like whether the gates are learned differently oh yeah so the gates the gates are also just learn so if we go back to these equations um you know this is the complete model and when we're training the model every one of these parameters so all of these w u and b's everything is simultaneously being trained by backprop so that what you hope and indeed it works is the model is learning what stuff should i remember for a long time versus what stuff should i forget what things in the input are important versus what things in the input don't really matter so it can learn things like our function words like don't really matter even though everyone uses them in english so you can just not worry about those so all of this is learned and the models do actually successfully learn gate values about what information is useful to preserve long term versus what information is really only useful short term for predicting the next one or two words finally um the uh the gradient improvements due to the so you said that the addition is really important between the new cell candidate and the cell state i don't think at least a couple of students have sort of questioned that so if you want to go over that again that might be useful sure um so what we would like is an easy way for memory to be preserved long term um and you know one way which is what resnets use is just to sort of completely have a direct path from ct minus one to ct and will preserve entirely the history so it's kind of there's a default action of preserving um information about the past long term lstms don't quite do that but they allow that function to be easy so you start off with the previous cell state and you can forget some of it by the forget gate so you can delete stuff out of your memory that's a useful operation and then while you're going to be able to update the content of the cell with this the the right operation that occurs in the plus where depending on the input gate some parts of what's in the cell will be added to but you can think of that adding as overlaying extra information everything that was in the cell that wasn't forgotten is still continuing on to the next time step and in particular um when you're doing the back propagation through time that there isn't um i want to say there isn't a multiplication between c t and c t minus one and there's this unfortunate um time symbol here but remember that's the hadamard product which is zeroing out part of it with the forget gate it's not a multiplication by a matrix like in the simple rnn i hope that's good um okay so there are a couple of other things that i uh wanted to get through before the end i guess i'm not going to have time to do both of them i think so i'll do the last one probably next time so these are actually simple and easy things um but they complete our picture um so we i sort of briefly alluded to this example of sentiment classification where what we could do is run an rnn maybe an lstm over a sentence call this our representation of the sentence and feed it into a soft max classifier to classify for um sentiment so what we're actually saying there is that we can regard the hidden state as a representation of a word in context that below that we have just a word vector for terribly but we then looked at our context and say okay we've now created a hidden state representation for the word terribly in the context of the movie was and that proves to be a really useful idea because words have different meanings in different contexts but it seems like there's a defect of what we've done here because our context only contains information from the left what about right context surely it'd also be useful to have the meaning of terribly depend on exciting because often words mean different things based on what follows them so you know if you have something like red wine it means something quite different from a red light so how could we deal with that well an easy way to deal with that would be to say well if we just want to come up with a neural encoding of a sentence we could have a second rnn with completely separate parameters learned and we could run it backwards through the sentence to get a backward representation of each word and then we can get an overall representation of each word in context by just concatenating those two representations and now we've got a representation of terribly that has both left and right context so we're simply running a forward rnn and when i say rnn here that just means any kind of recurrent neural network so commonly it'll be an lstm and a backward one and then at each time step we're just concatenating their representations um with each of these having separate weights and so then we regard this concatenated thing as the hidden state the contextual representation of a token at a particular time that we pass forward this is so common that people use a shortcut to denote that and they'll just draw this picture with two-sided arrows and when you see that picture with two-sided arrows it means that you're running two um rnn's one in each direction and then concatenating their results at each time step and that's what you're going to use later in the model okay but um so if you're doing an encoding problem like for sentiment classification or question answering using bi-directional rnns is a great thing to do but they're only applicable if you have access to the entire input sequence they're not applicable to language modeling because in a language model necessarily you have to generate the next word based on only the preceding context but if you do have the entire input sequence that bi-directionality gives you greater power and indeed that's been an idea that people have built on in subsequent work so um when we get to transformers in a couple of weeks um we'll spend plenty of time on the bert model where that acronym stands for bi-directional encoder representations from transformers so part of what's important in that model is the transformer but really a central point of the paper was to say that you could build more powerful models using transformers by again exploiting bi-directionality okay there's one tiny bit left on rnn's but i'll sneak it into next class and i'll call it the end for today and if there are other things you'd like to ask questions about you can find me on nooks again and just in just a minute okay so see you again next tuesday