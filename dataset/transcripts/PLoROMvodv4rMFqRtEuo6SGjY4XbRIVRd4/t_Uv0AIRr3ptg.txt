and so today I kind of just want to cover the fundamentals of Pi torch um really just kind of see what are the similarities between pi torch and numpy and python which you guys are used to at this point and see how we can build up a lot of the building blocks that we'll need in order to Define more complex models so specifically we're going to talk today about tensors what are tensor objects how do we manipulate them uh what is auto grad how pytorch helps us compute different gradients and finally how we actually do optimization and how we write the training Loop for our neural networks and if we have time at the end then we'll try and go through a bit of a demo to kind of put everything together and see how everything comes together when you want to solve an actual NLP task all right so let's get started so if you go to the course website there's a notebook and you can just make a copy of this collab notebook and then just run the cells as we go and so to start today we're talking about Pi torch like I said it's a deep learning framework that really does two main things one is it makes it very easy to author and manipulate tensors and make use of your GPU so that you can actually leverage a lot of that capability and two is it makes the process of authoring neural networks much simpler you can now use different building blocks like linear layers and different loss functions and compose them in different ways in order to author the types of models that you need for your specific use cases and so pytorch is one of the two main Frameworks along with tensorflow in this class we'll focus on Pi torch but they're quite similar and so we'll start by importing torch and we'll import the neural network module which is torch.nn and for this first part of the tutorial I want to talk a bit about tensors one thing that you guys are all familiar with now is numpy arrays and so pretty much you can think about tensors as the equivalent in pi torch to numpy arrays they're essentially multi-dimensional arrays that you can manipulate in different ways and you'll essentially use them to represent your data to be able to actually manipulate it and perform all the different Matrix operations that underlie your neural network and so in this case for example if we're thinking of an image one way you can think about it in terms of a tensor is it's a 256 by 256 tensor where it's has a width of 256 pixels and a height of 256 pixels and for instance if we have a batch of images and those images contain three channels like red green and blue then we might have a four-dimensional tensor which is the batch size by the number of channels by the width and the height and so everything we're going to see today is all going to be represented as tensors which you can just think of as multi-dimensional arrays and so to kind of get some intuition about this we're going to spend a little bit of time going through essentially lists of lists and how we can convert them into tensors and how we can manipulate them with different operations so to start off with we just have a simple list of lists that you're all familiar with in this case it's a two by three list and now we want to create a tensor and so here the way we'll create this tensor is by doing torch.tensor and then essentially writing the same syntax that we had before just write out the list of lists that represents that particular tensor and so in this case we get back a tensor object which is the same shape and contains the same data and so now the second thing with the tensor is that it contains a data type so there's different data types for instance there are different varying level of precision floating Point numbers that you can use you can have integers you can have different data types that actually populate your tensor and so by default I believe this will be float32 but you can explicitly specify which data type your tensor is by passing in the d-type argument and so we see here now even though we you know wrote in a bunch of integers they have a decimal point which indicates that they're floating Point numbers and so same thing here we could create another tensor in this case with data type float32 and in this third example you see that we create another tensor we don't actually specify the data type but pytorch essentially implicitly takes the data type to be floating points since we actually passed in a floating Point number into this tensor so pretty much at a high level tensors are like multi-dimensional arrays we can specify the data type for them we can populate them just like numpy rays okay so now great we know how to create tensors we know that ultimately everything that we work with all the data we have is going to be expressed as tensors now the question is what are the functions that we have to manipulate them and so we have some basic utilities that can help us instantiate tensors easily specifically torch.zeros and torch.1s these are two ways to create tensors of a particular shape in this case tensors of all zeros or tensors of all ones and you'll see that this will be very helpful when you do your homeworks typically you'll you'll want to just need to just create a bunch of zero Matrix and it'll be very easy to just specify the shape here without having to write everything out super explicitly and then you can update that tensor as needed another thing you can do is just like we have ranges in Python so if you want to Loop over a bunch of numbers you can specify a range you can also use torch dot a range to be able to actually instantiate a tensor with a particular range in this case we just looped over the numbers one through ten you could reshape this and make it one through five and then six through ten that's another way to be able to instantiate tensors and finally something to note is that when we apply particular operations such as just simple python operations like addition or multiplication by default they're going to be element wise so they'll apply to all the elements in our tensor so in this case we took our tensor I think this one was probably from earlier above and we added two everywhere here we've multiplied everything by two but pretty much the pi torch semantics for broadcasting work pretty much the same as the numpy semantics so if you pretty much have different Matrix operations where you need to batch across a particular Dimension pytorch will be smart about it and it will actually make sure that you broadcast over the appropriate Dimensions although of course you have to make sure that the shapes are compatible based on the actual broadcasting rules so we'll get to that in a little bit when we look at reshaping and how the bra uh how different you know operations have those semantics in this case we have to define the I guess I'm not personally aware of how you would Define kind of a jagged tensor that has unequal dimensions um but typically we don't want to do that because it makes our computation a lot more complex and so in cases where we have you know for instance we have different sentences that we turn into tokens we might have different length sentences in our training set we'll actually pad all the dimensions to be the same because ultimately we want to do everything with Matrix operations and so in order to do that we need to have a matrix of a fixed shape um but yeah that's that's a good point I I'm not sure if there is a way to do that but typically we just get around this by padding okay so now we know how to define tensors we can do some interesting things with them so here we've created two tensors one of them is a three by two tensor the other one is a two by four tensor and I think the answer is written up here but what do we expect is the shape when we multiply these two tensors so we have a three by two tensor and a two by four tensor yeah three by four and so more generally um we can use matte mole in order to do matrix multiplication it also implements batch to matrix multiplication and so I won't go over the entire review of broadcasting semantics but the main gist is that the dimensions of two tensors are compatible if you can left pad the tensors with ones so that the dimensions that line up either a have the same number in that Dimension or B one of them is a dummy Dimension one of them has a one and in that case in those dummy Dimensions Pi torch will actually make sure to copy over the tensor as many times as needed so that you can then actually perform the operation and that's useful when you want to do things like batch dot products or bashed Matrix multiplications and I guess the final Point here is there's also a shorthand notation that you can use so instead of kind of having to type out matte mole every time you can just use the add operator similar to numpy effectively that's kind of where we get into how batching works so for example if you had um let's say two tensors that have um some batch Dimension and then one of them is M by one and the other one is one by n and if you do a batched matrix multiply to those two tensors now what you effectively do is you preserve the batch Dimension and then you're doing a matrix multiplication between an M by one tensor and a one by n so you get something that's the batch Dimension by m by n so effectively they're kind of more I think the full semantics are written out on the pytorch website for how the matrix multiplication works but you're right you don't just have these cases where you have two two dimensional tensors you can have arbitrary number of dimensions and as long as the dimensions match up based on those semantics I was saying then you can multiply it alternatively you can do what I do which is just multiply it anyways and then if it throws an error print out the shapes and kind of work from there that tends to be faster in my opinion a lot of ways but yeah that's a good point all right so yeah let's keep going through some of the other different functionalities here so we can Define another tensor um and kind of one of the key things that we always want to look at is the shape so in this case we just have a 1D tensor of length three so the torch dot size just gives us three in general this is kind of one of the key debugging steps and something that I'll try and emphasize a lot throughout this session which is printing the shapes of all of your tensors is probably your best resource when it comes to debugging it's kind of one of the hardest things to Intuit exactly what's going on once you start stacking a lot of different operations together so printing out the shapes at each point and seeing do they match what you expect is something important and it's better to rely on that than just on the error message that pytorch gives you because under the hood pytorch might Implement certain optimizations and actually reshape the underlying tensor you have so you may not see the numbers you expect so it's always great to print out the shape and so yeah let's uh so again we can always print out the shape and we can have a more complex uh in this case a three-dimensional tensor which is three by two by four and we can print out the shape and we can see all the dimensions here and so now you're like okay great we have tensors we can look at their shapes but what do we actually do with them and so now let's get into kind of what are the operations that we can apply to these tensors and so one of them is it's very easy to reshape tensors so in this case we're creating this 15 dimensional tensor that's the numbers 1 to 15 and now we're reshaping it so now it's a five by three tensor here and so you might wonder well like what's what's the point of that and it's because a lot of times when we are doing machine learning we actually want to learn in batches and so we might take our data and we might reshape it so now that instead of kind of being a long flat and list of things we actually have a set of batches or in in some cases we have a set of batches of a set of sentences or sequences of a particular length and each of the elements in that sequence has an embedding of a particular dimension and So based on the types of operations that you're trying to do you'll sometimes need to reshape those tensors and sometimes you'll want to particularly sometimes transpose Dimensions if you want to for instance reorganize your data so that's another operation to keep in mind I believe the differences view will um view will create a view of the underlying tensor and so I think the underlying tensor will still have the same shape reshape will actually modify the tensor um all right and then finally like I said at the beginning your intuition about pytorch tensors can simply be their kind of a nice easy way to work with numpy arrays but they have all these great properties like now we can essentially use them with gpus and it's very optimized and we can also compute gradients quickly and to kind of just emphasize this point if you have some numpy code and you have a bunch of numpy arrays you can directly convert them into Pi torch sensors by simply catch casting them and you can also take those tensors and convert them back to numpy arrays all right and so one of the things you might be asking is why do we care about tensors what makes them good and one of the great things about them is that they support vectorized operations very easily essentially we can parallelize a lot of different computations and do them for instance across a batch of data all at once and one of those operations you might want to do for instance is a sum so you can take in this case a tensor which is shape five by seven and it looks like that's not working you can take a tensor that's shaped five by seven and now you can compute different operations on it that essentially collapse the dimensionality so the first one is sum and so you can take it and you can sum across both the rows as well as the columns and so one way I like to think about this to kind of keep them straight is that the dimension that you specify in the sum is the dimension you're collapsing so in this case if you take the data and sum over Dimension zero because you know the shape of the underlying tensor is five by seven you've collapsed the zeroth dimension so you should be left with something that's just shape seven and if you see the actual tensor you got 75 80 85 90. you get this tensor which is shape seven alternatively you can think about whether or not you're kind of summing across the rows or something across the columns but it's not just some it applies to other operations as well you can compute standard deviations you can normalize your data you can do other operations which essentially batch across the entire set of data and not only do these apply over one dimension but here you can see that if you don't specify any dimensions then by default the operation actually applies to the entire tensor so here we end up just taking the sum of the entire thing so if you think about it the zeroth dimension is the number of rows there are five rows and there are seven columns so if we sum out the rows then we're actually summing across the columns and so now we only have seven values but I like to think about more just in terms of the dimensions to keep it straight rather than rows or columns because it can get confusing if you're summing out Dimension zero then effectively you've taken something which has some shape that's Dimension Zero by Dimension One to just whatever is the dimension one shape and then from there you can kind of figure out okay which way did I actually sum to check if you were right numpy implements a lot of this vectorization and I believe in the homework that you have right now I think part of your job is to vectorize a lot of these things so the big Advantage with pi torch is that essentially it's optimized to be able to take advantage of your GPU when we actually start building out neural networks that are bigger that involve more computation we're going to be doing a lot of these matrix multiplication operations that it's going to be a lot better for our processor if we can make use of the GPU and so that's where pytorch really comes in handy in addition to also defining a lot of those neural network modules as we'll see later for you so that now you don't need to worry about for instance implementing a basic linear layer and back propagation from scratch and also your Optimizer all of those things will be built in and you can just call the respective apis to make use of them whereas in Python and numpy you might have to do a lot of that coding yourself all right so we'll keep going so this is a quiz except I think it tells you the answer so it's not much of a quiz but pretty much you know what would you do if now I told you instead of you know summing over this tensor I want you to compute the average and so there's there's two different ways you could compute the average you could compute the average across the rows or across the columns and so essentially now we kind of get back to this question of well which dimension am I actually going to reduce over and so here if we want to preserve the rows then we need to actually sum over the second dimension um they're really the first uh zeroth and first so the First Dimension is what we have to sum over because we want to preserve the zeroth dimension and so that's why for row average you see the dim equals one and for column average same reasoning is why you see the dim equals zero and so if we run this code we'll see kind of what are the shapes that we expect if we're taking the average over rows then an object that's two by three should just become an object that's two it's just a one-dimensional almost a vector you can think of and if we are averaging across the columns there's three columns so now our average should have three values and so now we're left with a three a one-dimensional tensor of length three so yeah does that kind of make sense I guess is this General intuition about how we deal with shapes and how some of these operations manipulate shapes so now we'll get into indexing this can get a little bit tricky but I think you'll find that the semantics are very similar to numpy so one of the things that you can do in numpy is that you can take these numpy arrays and you can slice across them in many different ways you can create copies of them and you can index across particular Dimensions to select out different elements different rows or different columns and so in this case let's take this example tensor which is three by two by two and first thing you'll always want to do when you have a new tensor print out its shape understand what you're working with and so I guess uh I may have shown this already but what will X bracket zero print out what happens if we index into just the first element what's the shape of this yeah two by two right because if you think about it our tensor is really just a list of three things each of those things happens to also be a two by two tensor so we get a two by two object in this case the first thing one two three four and so just like numpy if you provide a colon in a particular Dimension it means essentially copy over that dimension so if we do X bracket zero implicitly we're essentially putting a colon for all the other dimensions so it's essentially saying grab the first thing along the zeroth dimension and then grab everything along the other two dimensions if we now take uh just the zeroth along the element along the First Dimension um what are we going to get well ultimately we're going to get now if you look uh the kind of First Dimension were these three things the second dimension is now each of these two rows within those things so like one two and three four five six and seven eight 9 10 and 11 12. so if we index into the second dimension or the First Dimension and get the zeroth element then we're going to end up with one two by six and nine ten and even if that's a little bit tricky you can kind of go back to the trick I mentioned before where we're slicing across the First Dimension so if we look at the shape of our tensor it's three by two by two if we collapse the First Dimension that two in the middle we're left with something that's three by two so it might seem a little bit trivial kind of going through this in a lot of detail but I think it's important because it can get tricky when your tensor shapes get more complicated how to actually reason about this and so I won't go through every example here since a lot of them kind of reinforce the same thing but I'll just highlight a few things just like numpy you can choose to get a range of elements in this case where we're taking this new tensor which is one two one through fifteen rearranged that's a five by three tensor and if we take the zero through third row um exclusive we'll get the first three rows and we can do the same thing but now with slicing across multiple dimensions and I think the final point I want to talk about here is list indexing list indexing is also present in numpy and it's a very clever shorthand for being able to essentially select out multiple elements at once so in this case what you can do is if you want to get the zeroth the second and the fourth element of our Matrix you can just instead of indexing with a particular number or set of numbers index with a list of indices so in this case if we go up to our tensor if we take out the zeroth the second and the fourth we should see those three rows and that's what we end up getting yeah again these are kind of a lot of examples to just reiterate the same point which is that you can slice across your data in multiple ways and at different points you're going to need to do that so being familiar with the shapes that you understand what's the underlying output that you expect is important in this case for instance we're slicing across the first and the second dimension and we're keeping the first uh the zeroth and so we're going to end up getting essentially kind of the the top left element of each of those three things in our tensor if we scroll all the way up here we'll get this one we'll get this five and we'll get this nine because we go across all of this the Earth Dimension and then across the first and the second we only take the first uh the the zeroth element in both of those positions and so that's why we get 1 5 9. and also of course you can you know apply all of the colons to get back the original tensor okay and then I think the last thing when it comes to indexing is conversions so typically when we're writing code with neural networks ultimately we're going to you know process some data through a network and we're going to get a loss and that loss needs to be a scalar and then we're going to compute gradients with respect to that loss so one thing to keep in mind is that sometimes you might have an operation and it fails because it was actually expecting a scalar value rather than a tensor and so you can extract out the scalar from this one by one tensor by just calling dot item so in this case you know if you have a tensor which is just literally one then you can actually get the python scalar that corresponds to it by calling dot item so now we can get into the more interesting stuff one of the really cool things with pytorch is autograd and what autograd is is high torch essentially provides an automatic differentiation package where when you define your neural network you're essentially defining many nodes that compute some function and in the forward past you're kind of running your data through those nodes but what pytorch is doing on the back end is that each of those points it's going to actually store the gradients and accumulate them so that every time you do your backwards pass you apply the chain rule to be able to calculate all these different gradients and pytorch caches those gradients and then you will have access to all of those gradients to be able to actually then run your favorite Optimizer and optimize you know with SGD or with atom or whichever Optimizer you choose and so that's kind of one of the great features you don't have to worry about actually writing the code that computes all these gradients and actually caches all of them properly applies the chain rule does all these steps you can have shocked all of that away with just one call to dot backward and so in this case we'll run through a little bit of an example where we'll see the gradients getting computed automatically so in this case we're going to initialize a tensor and requires grad is true by default it just means that by default for a given tensor python pytorch will store the gradient associated with it and you might wonder well you know why why uh why do we have this you know when we always want to store the gradient and the answer is at train time you need the gradients in order to actually train your network but at inference time you'd actually want to disable your gradients and you can actually do that because it's a lot of extra computation that's not needed since you're not making any updates to your Network anymore and so let's create this right now uh we don't have any gradients uh being computed because we haven't actually called backwards to actually compute um some quantity with respect to this particular tensor we haven't actually computed um those gradients yet so right now the dot grad feature which will actually store the gradient associated with that tensor is not and so now let's just Define a really simple function we have X we're going to define the function y equals 3x squared and so now we're going to call Y dot backward and so now what happens is when we actually print out x dot grad what we should expect to see is number 12. and the reason is that our function y is 3x squared if we compute the gradient of that function we're going to get 6x and our actual value was 2. so the actual gradient is going to be 12. and we see that when we print out X talk grad that's what we get and now we'll just run it again let's set Z equal to 3x squared we call Z dot backwards and we print out X talk grad again and now we see that I may not run this in the right order Okay so here in the second one that I re-rad we see that it says 24. and so you might be wondering well I just did the same thing twice shouldn't I see 12 again and the answer is that by default pytorch will accumulate the gradients so it won't actually rewrite the gradient each time you compute it it will sum it and the reason is because when you actually have back propagation for your network you want to accumulate the gradients you know across all of your examples and then actually apply your update you don't want to overwrite the gradient but this also means that every time you have a training iteration for your network you need to zero out the gradient because you don't want the previous gradients from the last Epoch where you iterated through all of your training data to mess with the current update that you're doing so that's kind of one thing to note which is that that's essentially why we will see when we actually write the training Loop you have to run zero grad in order to zero out the gradient yes so I accidentally ran the cells in the wrong order maybe to make it more clear let me put this one first so this is actually what it should look like which is that we ran it once and I ran this cell first and it has 12. and then we ran it a second time and we get 24. yes so if you have all of your tensors defined then when you actually called out backwards if it's a function of multiple variables it's going to compute all of those partials all of those gradients yeah so what's happening here is that the way Pi torch works is that it's storing the accumulate accumulated gradient at X and so we've essentially made two different backwards passes we've called it once on this function y and we've which is a function of X and we've called it once on Z which is also a function of X and so you're right we can't actually disambiguate which came from what we just see the accumulated gradient but typically that's actually exactly what we want because what we want is to be able to run our Network and accumulate the gradient across all of the training examples that Define our loss and then perform our Optimizer step so yeah even with respect to one thing it doesn't matter because in practice each of those things is really a different example in our set of training examples and so we're not interested in you know the gradient from one example we're actually interested in the overall gradient so going back to this example What's Happening Here is that in the backwards pass what it's doing is you can imagine there's the X tensor and then there's the dot grad attribute which is another separate tensor it's going to be the same shape as X and what that is storing is it's storing the accumulated gradient from every single time that you've called dot backward on a quantity that essentially has some dependency on X that will have a non-zero gradient and so the first time we call it the gradient will be 12 because 6X 6 times 2 12. the second time we do it with Z it's also still 12. but the point is that dot grad doesn't actually overwrite the gradient each time you called out backwards it simply adds them it accumulates them and kind of the intuition there is that ultimately you're going to want to compute the gradient with respect to the loss and that loss is going to be made up of many different examples and so you need to accumulate the gradient from all of those in order to make a single update and then of course you'll have to zero that out because every time you make one pass through all of your data you don't want that next batch of data to also be double counting the previous batches update you want to keep those separate and so we'll see that in a second all right so now we're going to move on to one of the final pieces of the puzzle which is neural networks how do we actually use them in pi torch and once we have that and we have our optimization we'll finally be able to figure out how do we actually train a neural network what does that look like and why it's so clean and efficient when you do it in pi torch so the first thing that you want to do is we're going to be defining neural networks in terms of existing building blocks in terms of existing apis which will Implement for instance linear layers or different activation functions that we need so we're going to import torch.nn because that is the neural network package that we're going to make use of and so let's start with the linear layer the way the linear layer Works in pi torch is it takes in two arguments it takes in the input Dimension and then the output dimension and so pretty much what it does is it takes in some input which has some arbitrary amount of dimensions and then finally the input Dimension and it will essentially output it to that same set of Dimensions except the output dimension in the very last place and you can think of the linear layer as essentially just performing a simple ax plus b by default it's going to um it's going to apply a bias but you can also disable that if you don't want a bias term and so let's look at a small example so here we have our input and we're going to create a linear layer in this case as an input size of four an output size of two and all we're going to do is once we Define it by instantiating it with nn.linear whatever the name of our layer is in this case we called it linear we just essentially apply it with parentheses as if it were a function to whatever input and that actually does the actual forward pass through this linear layer to get our output and so you can see that the original shape was two by three by four then we pass it through this linear layer which has an output dimension of size two and so ultimately our output is two by three by two which is good that's what we expect that's not shape error but you know something common that you'll see is you know maybe uh you decide to you get a little confused and maybe you do let's say two by two you match the wrong dimension and so here we're going to get a shape error and you see that the error message isn't as helpful because it's actually changed the shape of what we were working with we said this was two by three by four under the hood Pi torch has changes to a six by four but if we you know in this case it's obvious because we instantiated it with the shape but if we didn't have the shape then one simple thing we could do is actually just print out the shape and we'd see okay this last Dimension is size four so I actually need to change my input dimension in my linear layer to be size four thank you and you'll also notice on this output we have this grad function and so that's because we're actually Computing and storing the gradients here for our tensor yeah so typically we think of the First Dimension as the batch Dimension so in this case it said n this you can think of as if you had a batch of images it would be the number of images if you had a training Corpus of text it would be essentially the number of sentences or sequences um pretty much that is usually considered the batch Dimension the star indicates that there can be an arbitrary number of dimensions so for instance if we had images this could be a four-dimensional tensor object it could be the batch size by the number of channels by the height by the width but in general there's no fixed number of Dimensions your input tensor can be any number of Dimensions the key is just that that last Dimension needs to match up with the input dimension of your linear layer the two is the output size so essentially we're saying that we're going to map this last Dimension which is four dimensional to now two dimensional so in general you know you can think of this is if we're stacking a neural network this is kind of the input Dimension size and this would be like the hidden Dimension size and so one thing we can do is we can actually print out the parameters and we can actually see what are the values of our linear layer or in general for any layer that we Define in our neural network what are the actual parameters and in this case we see that there's two sets of parameters because we have a bias as well as the actual um the actual linear layer itself and so both of them store the gradients and in this case um you know these are these are what the current values of these parameters are and they'll change as we train the network okay so now let's go through some of the other module layers um so in general nn.linear is one of the layers you have access to you have a couple of other different layers that are pretty common you have 2D convolutions you have transpose convolutions you have batch Norm layers when you need to do normalization in your network you can do up sampling you can do Max pooling you can do lots of different operators but the main key here is that all of them are built-in building blocks that you can just call just like we did with nn.linear and so let's just go I guess I'm running out of time but let's just try and go through these last few layers and then I'll wrap up by kind of showing an example that puts it all together so in this case we can define an activation function which is typical with our networks we need to introduce non-linearities in this case we use the sigmoid function and so now we can Define our our Network as this very simple thing which had one linear layer and then an activation and in general when we compose these layers together we don't need to actually write every single line by line applying the next layer we can actually stack all of them together in this case we can use nn.sequential and list all of the layers so here we have our linear layer followed by our sigmoid and then now we're just essentially passing the input through this whole set of layers all at once so we take our input we call block on the input and we get the output and so let's just kind of see putting it all together what does it look like to define a network and what does it look like when we train one so here we're going to actually Define a multi-layer perceptron and the way it works is to define a neural network you extend the NN dot module class the key here is there's really two main things you have to Define when you create your own network one is the initialization so in the init function you actually initialize all the parameters you need in this case we initialize an input size a hidden size and we actually Define the model itself in this case it's a simple model which consists of a linear layer followed by an activation followed by another linear layer followed by a final activation and the second function we have to Define is the forward which actually does the forward pass of the network and so here our forward function takes in our input X in general it could take in some arbitrary amount of inputs into this function but essentially it needs to figure out how are you actually Computing the output and in this case it's very simple we just pass it into the network that we just defined and return the output and again you could do this more explicitly by kind of doing what we did earlier where we could actually write out all of the layers individually instead of wrapping them into one object and then doing a line by line operation for each one of these layers and so finally if we Define our class it's very simple to use it we can now just instantiate some input instantiate our model by calling multi-layer perceptron with our parameters and then just pass it through our model so that's great but this is all just a full red pass how do we actually train the network how do we actually make it better and so this is the final step which is we have optimization built in to Pi torch so we have this backward function which goes and computes all these gradients in the backward pass and now the only step left is to actually update the parameters using those gradients and so here we'll import the torch.opt-in package which contains all the optimizers that you need essentially this part is just creating some random data so that we can actually decide how to fit our data but this is really the key here which is we'll instantiate our model that we defined we'll Define the atom optimizer um and we'll Define it with a particular learning rate we'll Define a loss function which is again another built-in module in this case we're using the cross entropy loss and finally to calculate our predictions all we do is simply is just call model on our actual input and to calculate our loss we just call our loss function on our predictions and our true labels and we extract the scalar here and now when we put it all together this is what the training Loop looks like we have some number of epochs that we want to train our Network for each of these epochs the first thing we do is we take our Optimizer and we zero out the gradient and the reason we do that is because like many of you noted we actually are accumulating the gradient we're not resetting it every time we call Dot backward so we zero out the gradient we get our model predictions by doing a forward pass we then compute the loss between the predictions and the True Values finally we call law stop backward this is what actually computes all the gradients in the backward pass from our loss and the final step is we call Dot step on our Optimizer in this case we're using atom and this will take a step on our loss function and so if we run this code we end up seeing that we're able to start with some trading loss which is relatively high and in 10 epochs we're able to essentially completely fit our data and if we print out our model parameters and we printed them out from the start as well we'd see that they've changed as we've actually done this optimization so I'll kind of wrap it up here but I think the key takeaway is that a lot of the things that you're doing at the beginning of this class are really about understanding the basics of how neural networks work how you actually Implement them how you implement the backward pass the great thing about Pi torch is that once you get to the very next assignment you'll see that now that you have a good underlying understanding of those things you can abstract a lot of the complexity of how do you do back prop how do you store all these gradients how do you compute them how do you actually run the optimizer and let pytorch handle all of that for you and you can use all of these building blocks all these different neural network layers to now Define your own networks that you can use to solve whatever problems you need