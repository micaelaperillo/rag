so we're now starting um in week three with um lecture five so unfortunately on the last class i i guess i really got behind and went a bit slowly i guess i must just enjoy talking about natural languages too much and so i never really got to the punch line of showing how you could do good things with the neural dependency parser so today for the first piece i'll in some sense be finishing the content of last time and talk about neural dependency parsing which also gives us the opportunity um to introduce a simple feed forward neural net classifier um that will then lead into a little bit of just background things that you need to know about neural networks content because the fact of the matter is there is a bunch of stuff you need to know about neural networks and then after both of those things i'll get into what's really meant to be the topic of today's lecture which is looking at language modeling and recurrent neural networks and that's then going to lead into those two things are important topics that we'll then be talking about really for the whole of next week as well there's a couple of reminders before we get underway the first is that you should have handed in assignment 2 before you joined this class and in turn assignment three is out today and it's an assignment um where you're going to build essentially the new um dependency parser that i'm just about to present in pytorch so part of the role of this assignment is actually to get you up to speed with pytorch so this assignment is highly scaffolded with lots of comments and hints about what to do and so the hope is that by the time you come to the end of it you'll feel fairly familiar and comfortable with pie torch don't forget there was also tutorial on pie torch last week if you didn't catch that at the time you might want to go back and look at the video another thing to mention about the assignments is that assignment three is the last assignment where our great team of tas are happy to look at your code and sort out your bugs for you so maybe take advantage of that but not too much but starting an assignment four for assignments four five and the final project um the tas are very happy to help in general but it's just not going to be their job to be actually sorting out bugs for you you should be looking at your code and discussing ideas and concepts and reasons why things might not work with them okay so if you remember where we were last time i'd introduced this idea of transition based dependency parsers and that these were an efficient linear time method um for giving the syntactic structure of natural language text and that they worked pretty well before neural nets came along and took over nlp again but they had some disadvantages and their biggest disadvantage is that like most machine learning models of that time they worked with indicator features so that means that you are specifying some condition and then checking whether it was true of a configuration so something like the word on the top of the stack is good and it's part of speech is adjective or the next word coming up is a personal pronoun that those are conditions that would be features and a conventional um transition based dependency parser and so what are the problems with doing that well one problem is that those features are very sparse a second problem is the features are incomplete well what i mean by that is depending on what words and configurations occurred in the training data there are certain features that will exist because you sort of saw a certain word preceding a verb and certain features that just won't exist because that word never occurred before a verb in the training data but perhaps the biggest problem and opportunity for doing better with the neural dependency parser is that it turns out that in a symbolic dependency parser computing all these features just turns out to actually be pretty expensive that although the actual transition system that i showed last time is fast and efficient to run you actually have to compute all of these features and what you found was that about 95 of the parsing time of one of these models was spent just computing all of the features of every configuration so that suggests that perhaps we can do better with a neural approach where we're going to learn a dense and compact feature representation and so that's what i want to go through now so this time we're still going to have exactly the same kind of configuration of a stack and a buffer and running exactly the same transition sequence except this time rather than representing the configuration the stack and the buffer by having several million symbolic features we're instead going to summarize this configuration as a dense vector of dimensionality perhaps approximately a thousand and our neural approach is going to learn this dense compact feature representation and so quite explicitly what i'm going to show you now briefly and what you're going to implement is essentially the neural dependency parser that was developed by dante chen in 2014 and to skip to the advertisement right at the beginning as to how this works so well these are the kind of results that you got from it using the measures that i introduced at the last time the unlabeled attachment score whether you attach dependencies correctly um to the right word and the labeled attachment score as to whether you also get the type of grammatical relation of that dependency correct um and so essentially um this chin and manning parser gave a neural version of something like a transition based dependency parser like malt parser in yellow and the interesting thing was that taking advantage of a neural classifier in ways that i'm about to explain that that could produce something that was about two percent more accurate than the symbolic dependency parser and because of the fact that it's not doing all the symbolic feature computation despite the fact that you might think at first that there's a lot of real number math and matrix vector multiplies in a neural dependency parser it actually ran noticeably faster than the symbolic dependency parser because it didn't have um all the feature compute computation the other major approach to dependency parsing that i'm also showing here and i'll get back to at the end is what's referred to as graph based dependency parsing and so that's a different approach to dependency parsing and so these are two symbolic graph-based dependency parsers and in the pre-neural world they were somewhat more accurate than the transition based parses as you could see but on the other hand they were close to two orders of magnitude slower um and so essentially with the chern manning parser we were able to provide something that was basically as accurate as the best graph based dependency parsers which were the best dependency parsers while operating about two orders of magnitude more quickly so how did we do it it was actually a very straightforward implementation which is part of what makes it great for doing for assignment three um but this is how we did it and we got wins so the first win which is what we've already talked about extensively starting in week one is to make use of distributed representations so we represent each word as a word embedding and you've had a lot of experience with that already and so that means when words weren't seen in a particular configuration we still know what they're like because they'll be we'll have seen similar words in the correct configuration um but we don't stop only with word embeddings the other things that are central to our dependency parser are the parts of speech of words and the dependency labels and so what we decided to do is that although those are much smaller sets so the dependency labels are about 40 in number and the parts of speech are of around that order of magnitude sometimes less sometimes more that even within those sets of categories there are ones that are very strongly related so we also adopted um distributed representations for them so for example there might be parts of speech for singular nouns and plural nouns and basically most of the time they behave similarly and there are adjectival modifiers and numerical modifiers so these are just numbers like three four five and again a lot of the time they behave the same that you have both three cows and brown cows okay so everything is going to be represented in a distributed representation so at that point we have exactly the same kind of configuration where we have our stack our buffer and we've started to build some arcs and so the classification decisions of the next transition are going to be made out of a few elements of this configuration so we're looking at the top thing on the stack um the thing second on the stack the first word on the buffer and then we actually added in some additional features that are then to the extent that we've already built arcs for words on the stack that we can be looking at the dependence on the left and right of those words that are on the stack that are already in the sets of arcs and so for each of those things they there is a word there is a part of speech and for some of them there is a dependency um where it's already connected up to something else so for example the left corner of s2 here has an n sub dependency back to the second thing on the stack so we can take these elements of the configuration and can look up the embedding of each one so we have word embeddings part of speech embeddings and dependency embeddings and just concatenate them all together kind of like we did before with the window classifier and that will give us a newer representation of the configuration now there's a second reason why we can hope to win by using a deep learning classifier to predict the next transition and we haven't really said much about that yet so i just wanted to detour and say a little bit more about that um so the simplest kind of classifier that's close to what we've been talking about in neural models is a soft max classifier so that if we have d dimensional vectors x and we have y classes to assign things to um oh sorry y is an element of a set of um c classes to assign things to then we can build a softmax classifier using the softmax distribution that we've seen before where we decide the classes based on having a weight matrix that's c by d and we train on supervised data the values of this w weight matrix to minimize our negative log likelihood loss that we've seen before um a loss is also commonly referred to as cross-entropy loss a term that you'll see in pie torch among other places um so that is a straightforward machine learning classifier and if you've done 229 and you've seen soft max classifiers um but a simple softmax classifier like this um shares with most traditional machine learning classifiers so models include naive bayes models support vector machines logistic regression that at the end of the day they're not very powerful classifiers they're classifiers that only give linear decision boundaries and so this can be quite limiting so if you have a difficult problem like the one i'm indicating in the picture in the bottom left well there's just no way you can divide the green points from the red points by simply drawing a straight line so you're going to have a quite imperfect classifier so the second big win of neural classifiers is that they can be much more powerful because they can provide non-linear classification so rather than only being able to do something like in the left picture um we can come up with classifiers that do something like in the right picture and therefore can separate the green and the red points um as an aside um these pictures i've taken from andre caparti's compnet js software which is a kind of a fun little tool to play around with if you've got a bit of spare time um and so there's something subtle going on here is because our more powerful neural net classifiers at the end of the day what they have at the top of them is a softmax layer and so this softmax layer is indeed a linear classifier and it's still a linear classifier but what they have below that is other layers of neural net and so effectively what happens is that the classification decisions are linear as far as the top softmax is concerned but non-linear in the original representation space so precisely what a neural net can do is warp the space around and move the representation of data points to provide something that at the end of the day can be classified by a linear classifier and so that's what a simple feedforward neural network multi-class classifier does so it starts with an input representation so these are is some dense representation of the input it puts it through a hidden layer h with a matrix multiply followed by non-linearity so that matrix multiply can transform the space and map things around and so then the output of that we can then put into a soft max layer and get out soft max probabilities from which we make our classification decisions and to the extent that our probabilities don't assign one to the correct class we then get some log loss or cross entropy error which we back propagate towards the parameters and embeddings of our model and as the learning that goes on via back propagation we increasingly well learn parameters of this hidden layer of the model which learn to re-represent the input they move the inputs around in an intermediate hidden vector space so it can be easily classified with what at the end of the day is the linear softmax so this is basically the whole of a simple feed for neural network multi-class classifier but um and if we had something like a a visual signal we just sort of feed straight in here real numbers and we've been done but normally with um human language material we actually effectively have one more layer that we're feeding in before that because really below this dense input layer we actually have one hot vectors for what words or parts of speech were involved and then we're doing a lookup process which you can think of as one more matrix multiply to convert the one hot features into our dense input layer okay in my picture here the one other thing that's different is i've introduced a different non-linearity in the hidden layer which is a rectified linear unit and that's what we'll be using now neural dependency parsers um it looks like the picture in the bottom right and i'll come back um to that in a few minutes that's one of the extra neural net things um to talk about okay so our neural net dependency parser model architecture is essentially exactly that um but applied to the configuration of our transition based dependency parser so based on our transition based dependency parser configuration we construct an input layer embedding by looking up on the various elements as i discussed previously and then we feed it through this hidden layer to the softmax layer to get probabilities out of which we can choose what the next action is and it's no more complicated than that um but what we found is um that just simply you you know in some sense using the simplest kind of feed forward neural um classifier could provide a very accurate dependency parser that determines the structure of sentences supporting meaning interpretation the kind of way that i suggested last time indeed you know despite the fact that it was a quite simple architecture in 2014 this was the first successful neural dependency parser and the dense representations especially but also partly the non-linearity of the classifier gave us this good result that it could both outperform symbolic parsers in terms of accuracy and it could outperform them in terms of speed um so that was 2014 just quickly here a couple more slides on what's happened since then so lots of people got excited by the success of this new dependency parser and a number of people particularly at google then cetera about building a bigger fancier transition based neural dependency parser so they explored bigger deeper networks there's no reason to only have one hidden layer you can have two hidden layers um you can do beam search that i briefly mentioned last time another thing that i'm not going to talk about now is adding conditional random field style inference over decision sequences and that then led in 2016 um for a model that they called um parsi mcpa's face which is hard to say with a straight face um which was then about two and a half three percent um more accurate than the model that we had produced but still in basically the same family of transition based parser with the neural net classifier to choose the next transition um the alternative to transition based parsers as graph based dependency parsers and for a graph-based dependency parser what you're doing is effectively considering every pair of words and considering a word as a dependent of root and you're coming up with a score as to how likely is it that big is a dependent of root or how likely is big to be dependent of cat and similarly for every other word for the word sat um how likely is it to be a dependent of root or a dependent of the etc and well to do that well you need to know more than just what the two um words involved are and so what you want to do is understand the context so you want to have an understanding of the context of big what's to the left of what's to the right of it to understand how you might hook it up into the dependency representations of the sentence um and so while they've been previous work in graph based dependency parsing like the mst parser i showed on the earlier results slide it seemed appealing that we could come up with a much better representation of context using neural nets that look at context and how we do that is actually what i'll be talking about in the end part of the lecture and so at stanford um we became interested in trying to work out how to come up with a better graph-based dependency parser using context sorry i forgot this this was showing that um if we can score each pairwise dependency we can simply choose the best one so we can say um probably big is a dependent of cat and to a first approximation we're going to want to choose for each word that it is a dependent of the word that seems most likely to be a dependent but we want to do that with some constraints because we want to get out something that is a tree with a single root as i discussed last time and you can do that by making use of a minimum spanning tree algorithm that uses these scores of how likely different dependencies are okay so then in 2017 another student tim doset and me then worked on um saying well can we now also build a much better neural graph-based dependency parser and we developed a novel method for scoring um neural scoring dependency parses and a graph based model which i'm not going to get into the details of right now but that also had a very nice result because use getting back to graph based parsing we could then build a graph based parser that performed about a percent better than the best of the the google transition based new dependency parsers but i should point out that this is a mixed win because although its accuracy is better these graph-based parsers are just in squared in performance rather than linear time so kind of like the early results i showed they don't operate nearly as quickly when you're wanting to pass large amounts of text with complex long sentences okay so that's everything you need to know about dependency parsers and to do assignment three so grab it this evening and start to work um but i did want to sort of before going on to the next topic just mention a few more things um about neural networks since um some of you know this well already some of you have seen less of it but you know there just are a bunch of things you have to be aware of um for building new networks now again for assignment three essentially we give you everything and if you follow the recipe your parser should work well but you know what you should minimally do is actually you know look carefully at some of the things that this parser does which is questions like how do we initialize our matrices of our neural network what kind of optimizers do we use and things like that um because these are all important decisions and so i wanted to say just a few words about that okay so the first thing that we haven't discussed at all is the concept of regularization so when we're building these neural nets we're now building models with a huge number of parameters so essentially just about all neural net models that work well actually they're full loss function is a regularized loss function so for this um loss function here of j well this part here is the part that we've seen before um where we're using a soft max classifier and then taking a negative log likelihood loss which we're then averaging over the different examples but actually we then stick on the end of it this regularization term and so this regularization term sums the square of every parameter in the model and so what that effectively says is you only want to make parameters non-zero if they're really useful right so the to the extent the parameters don't help much you're just being penalized here um by making them non-zero but to the extent that the parameters do help you'll gain in your estimation of likelihood and therefore it's okay for them to be non-zero in particular um notice that this penalty is assessed only once per parameter it's not being assessed separately for each example okay and having this kind of regularization um is essential to build neural net models that regularize well so the classic problem is referred to as overfitting and what overfitting means is that if you have a particular training data set and you start training your model your error will go down because you'll shift the parameters so they better predict um the the correct answer for data points in the model and you can keep on doing that and it will start keep on reducing your error rate but if you then look at your partially trained classifier and say how well does this classifier classify independent data different test data that you weren't training the model on what you'll find is up until a certain point um you'll get better at classifying independent test examples as well and after that commonly what will happen is you'll actually start to get worse at classifying independent test examples even though you're continuing to get better at predicting the training examples and so this was then referred to as you're over fitting the training examples that you're fiddling the parameters of the model so they're really good at predicting the training examples which aren't useful things that can then predict um on independent examples that you come to at runtime okay um that classic view of regularization is sort of actually outmoded and wrong for modern neural networks um so the right way to think of it for the kind of modern big neural networks that we build is that overfitting on the training data isn't a problem but nevertheless you need regularization to make sure that your models generalize well to independent test data so what you'd like is for your graph not to look like this example with test error starting to head up you'd like to have it at worst case flat line and best case still be gradually dropping it'll always be higher than the training error but it's not actually showing a failure to generalize so when we train big neural nets these days our big neural nets always overfit on the training data they hugely overfit on the training data in fact in many circumstances our neural nets have so many parameters that you can continue to train them on the training data until the error on the training data is zero they get every single example right because they can just memorize enough stuff about it to predict the right answer but in general providing the models are regularized well those models will still also generalize well and predict well on independent data and so for part of what we want to do for that is to work out how much to regularize and so this lambda parameter here is the strength of regularization so if you're making that lambda number big you're getting more regularization and if you're making it smaller you're getting less and you don't want to have it be too big or else you won't fit the data well and you don't want to be too small or else you have the problem that you don't generalize well okay so this is classic l2 regularization and it's a starting point but our big neural nets are sufficiently complex and have sufficiently many parameters that essentially l2 regularization doesn't cut it so the next thing that you should know about and is a very standard good feature for building neural nets is a technique called drop out so dropout is generally introduced as a sort of a slightly funny process that you do when training to avoid feature coat co-adaptation so in dropout what you do is at the time that you're training your model that for each instance or for each batch in your training then for each neuron in the model you drop 50 of its inputs you just treat them as zero and so that you can do by sort of zeroing out elements of um the sort of layers um and then at test time you don't drop any of the model weights you keep them all but actually you have all the model weights because you're now keeping twice as many things as you'd use the training data um and so effectively that little recipe um prevents what's called feature co-adaptation so um you can't you can't have features um that are only useful in the presence of particular other features because the model can't guarantee which features are going to be present for different examples because different features are being randomly dropped all of the time and so effectively dropout gives you a kind of a middle ground between naive bayes and the logistic regression model and the naive bayes models all the weights are said independently in a logistic regression model all the weights are set in the context of all the others and here you are aware of other weights but they can randomly disappear from you it's also related to ensemble models like model bagging because you're using different subsets of the features every time um but after all of those explanations there's actually another way of thinking about dropout which was actually developed here at stanford this paper by percy liang and students um which is to argue that really what dropout gives you is a strong regularizer that isn't a uniform regularizer like l2 that regularizes everything with an l2 last but can learn a feature dependent regularization and so that dropout has just emerged as in general the best way to do regularization for neural nets i think you've already seen and heard this one but just have it on my slides once if you want to have your neural networks go fast it's really essential that you make use of vectors matrices tensors and you don't do things with for loops so here's a tiny example where i'm using time it which is a useful thing that you can use too to see what how fast your neural nets run and different ways of writing that and so when i'm doing this um doing these dot products here i can either do it the dot product in a for loop against each word vector or i can do the dot product with with a single word vector matrix and if i do it in a for loop doing each loop takes me almost a second whereas if i do it with a matrix multiply it takes me an order of magnitude less time so you should always be looking to use vectors and matrices not for loops and this is a speed up of about 10 times when you're doing things on a cpu heading forward we're going to be using gpus and they only further exaggerate the advantages of using vectors and matrices where you'll commonly get two orders of magnitude speed up um by doing things that way yeah so for the backward pass you are running a backward passes before on the dropped out examples right so for the things that were dropped out no gradient is going through them because they weren't present they're not affecting things so in a particular batch you're only training weights for the things that aren't dropped out but then since you for each successive um batch you drop out different things that over a bunch of batches you're then training all of the weights of the model um and so feature dependent regularizer is meaning that how much a feature the different features can be regularized different amounts to maximize performance so back in this model every feature was just so being penalized by taking lambda times at squared values so this is sort of uniform regularization where the end result of this dropout style training is that you end up with some features being regularized much more strongly and some other features being regularized less strongly and how much they be regularized depends on how much they're being used so you're regularizing more features that are being used less but i'm i'm not going to get through into the details of how you can understand that perspective um that's the that's um outside of the context of what i'm going to get through right now so the final bit is i just wanted to give a little bit of perspective on non-linearities in our neural nets so the first thing to remember is you have to have non-linearity so if you're building a multi-layer neural net and you've just got you know w1 x plus b1 then you put it through w to x plus b2 and then put through w3 um x well i guess they're different hidden layers so i should have said x they should be hidden one hidden two hidden three w3 hidden three plus b3 um that multiple linear transformations um compose so they can be just collapsed down into a single linear transformation so you don't get any power as a data representation by having multiple linear layers there's a slightly longer story there because you actually do get some interesting learning effects but i'm not going to talk about that now but standardly we have to have some kind of non-linearity to do something interesting in a deep neural network okay so this there's a starting starting point as the most classic non-linearity is the logistic often just called the sigmoid um non-linearity because of its s shape um which we've seen before in previous lectures so this will take any real number and map it on the to the range of zero one um and that was sort of basically what people used in sort of 1980s neural nets now one disadvantage of this non-linearity um is that it's moving everything into the positive space because the output is always between zero and one so people then decided that for many purposes it was useful to have this variant sigmoid shape of hyperbolic tan which is then being shown in the second picture now you know logistic and hyperbolic tan um they sound like they're very different things but actually as you maybe remember from a math class hyperbolic tan can be represented in terms of exponentials as well and if you do a bit of math which possibly we might make you do on an assignment um it's actually the case that a hyperlog tangent is just a rescaled and shifted version of the logistics so it's really exactly the same curve just squeezed a bit so it goes now symmetrically between minus one and one um well um these kind of transcendental functions like hyperbolic tangent they're kind of slow and expensive to compute right even on our fast computers calculating exponentials is a bit slow so something people became interested in was well could we do things with much simpler non-linearity so what if we used a so-called hard tan h so the hard 10h at some point up to some point it just flat lines at -1 then it is y equals x up until one and then it just flat lines again and you know that seems a slightly weird thing to use because if your input is over on the left or over on the right you're sort of not getting any discrimination in it for things giving the same output but somewhat surprisingly i mean i was surprised when people um started doing this um these kind of models um proved to be very successful and so that then led into what's proven to be kind of the most successful and generally widely used non-linearity and a lot of recent deep learning work which was what was being used um in the dependency powers model i showed is what's called the rectified linear unit or value so a value is kind of the simplest kind of non-linearity that you can imagine so if the value of x is negative its value is zero so effectively it's just dead it's not doing anything in the computation and if its value of x is greater than zero then it's just simply y equals x the value is being passed through um and at first sight this might seem really really weird and how could this be useful as a non-linearity but if you sort of think a bit about how you can approximate things with piecewise linear functions very accurately you might kind of start to see how you could use this to do accurate function approximation with piecewise linear functions and that's what value units have been found to do extremely extremely successfully um so logistic and tan h are still used in various places you use logistic when you on a probability output we'll see 10 h's again very soon when we get to recurrent neural networks um but they're no longer the default when making deep networks that in a lot of places the first thing you should think about trying is relu non-linearities and so in particular um that why part of why they're good is that religion on networks train very quickly because you get this sort of very straightforward gradient backflow because providing you on the right hand side of it you then just getting this sort of constant gradient backflow from the slope one and so they train very quickly the somewhat surprising fact is that sort of almost the simplest non-linearity imaginable is still enough to have a very good neural network but it just is um people have played around with variants of that um so people have then played around with leaky rail use where rather than the left hand side just going completely to zero it goes slightly negative on a but much shallower slope and then there's been a parametric reload where you have an extra parameter where you learn the slope of the negative part um another thing that's been used um recently is this swish non-linearity which looks almost like a value um but it sort of curves down just a little bit there and starts to go up i mean i think it's fair to say that you know none of these have really proven themselves vastly superior there are papers saying i can get better results by using one of these and maybe you can but you know it's not night and day and the vast majority of work that you see around is still just using values in many places okay a couple more things parameter initialization so in almost all cases you must must must initialize um the matrices of your neural nets with small random values neural nets just don't work if you start the matrices off as zero because effectively then everything is symmetry is symmetric nothing can specialize in different ways and and you then get sort of uh you just don't have an ability for a neural net to learn you sort of get this defective solution so standardly you're using some method such as drawing random numbers uniformly between minus r and r for a small value r and just filling in all the parameters with that the exception is with bias weights it's fine to set bias weights to zero and in some sense that's better in terms of choosing what the r value is essentially for traditional neural nets what we want to set that our range for is so that the numbers in our neural network stay of a reasonable size they don't get too big and they don't get too small and whether they kind of blow up or not depends on how many connections there are in the neural network so looking at the fan in and fan out of connections in the neural network and so a very common initialization that you'll see in pi torch is what's called javier initialization named after a person who suggested that and it's working out a value of uh based on um this fan in and fan out of the layers but you can just sort of ask for it say initialize with this initialization and it will this is another area where there have been some subsequent developments so around week five we'll start talking about layer normalization if you're using layer normalization then it sort of doesn't matter the same how you initialize the weights so finally we have to train our models and i've briefly introduced the idea of stochastic gradient descent and you know the good news is that most of the time that if training your networks with stochastic gradient descent works just fine use it and you will get good results however often that requires choosing a suitable learning rate which is my final slide of tips on the next slide but there's been an enormous amount of work on optimization of neural networks and people have come up with a whole series of more sophisticated optimizers and i'm not going to get into the details of optimization in this class but the very loose idea is that these optimizers are adaptive in that they can kind of keep track of how much slope there was how much gradient there is for different parameters and therefore based on that make decisions as to how much to adjust the weights when doing the gradient update rather than adjusting it by a constant amount and so in that family of methods there are methods that include edegrad rms prop adam and then a variants of adam including sparse adam adam w etc the the one called atom is a pretty good place to start um and a lot of the time that's a good one to use and again from the perspective of pie torch when you're initializing an optimizer you can just say please use adam and you don't actually need to know much more about it than that if you are um using simple stochastic gradient send you have to change choose a learning rate so that was the eta value that you multiplied the gradient by for how much to adjust the weights and so i talked about that slightly how you didn't want it to be too big or your model could diverge or bounce around you didn't want it to be too small or else training could um take place exceedingly slowly and you'll miss the assignment deadline um you know how big it should be depends on all sorts of details of the model and so you sort of want to try out some different order of magnitude numbers to see what numbers seem to work well for it training stability but reasonably quickly something around 10 to the minus 3 or 10 to the minus 4 isn't a crazy place to start in principle you can do fine just using a constant learning rate in sgd in practice people generally find they can get better results by decreasing learning rates as you train so a very common recipe is that you halve the learning rate after every k epochs where an epoch means that you've made a pass through the entire set of training data so perhaps something like every three epochs you have the learning rate and a final little note there in purple is when you make a pass through the data you don't want to go through the data items in the same order each time because that leads you just kind of be have a sort of patterning of the training examples that the model will sort of fall into that periodicity of those patterns so it's best to shuffle the data before each pass through it okay there are more sophisticated ways to set learning rates um and i won't really get into those now fancier optimizers like adam also have a learning rate so you still have to choose a learning rate value but it's effectively it's an initial learning rate which typically the optimizer shrinks as it runs and so um you commonly want to have the number it starts off with beyond the larger size because it will be shrinking as it as it goes okay so that's all by way of introduction and i'm now ready to start on language models and rnns so what is language modeling i mean as two words of english language modeling could mean just about anything but in the natural language processing literature language modeling has a very precise technical definition which you should know so language modeling is the task of predicting the word that comes next um so if you have some context like the students open there you want to be able to predict what words will come next is it their books their laptops their exams their minds and so in particular what you want to be doing is being able to give a probability that different words will occur in this context so a language model is a probability distribution over next words given a preceding context and a system that does that is called a language model so as a result of that you can also think of a language model as a system that assigns a probability score to a piece of text so if we have a piece of text then we can just work out its probability according to a language model so the probability of a sequence of tokens we can decompose via the chain rule probability of the first times probability the second given the first etc etc and then we can work that out using what our language model provides as a product of each probability of predicting the next word okay language models are really the cornerstone of human language technology everything that you do with computers that involves human language you are using language models so when you're using your phone and it's suggesting whether well or badly what the next word that you probably want to type is and that's a language model working to try and predict likely next words when the same thing happens in a google doc and it's suggesting a next word or a next few words that's a language model you know the main reason why the one in google docs works much better than the one on your phone is that for the keyboard phone models they have to be very compact um so they can run quickly and not much memory so they're sort of only mediocre language models where something like google docs can do a much better language modeling job query completion same thing there's a language model and so then the question is well how do we um build language models and so i briefly wanted to first again um give the traditional answer um since you should have at least some understanding of how nlp was done without a neural network and the traditional answer that powered speech recognition and other applications for at least two decades three decades really was what were called engram language models and these were very simple but still quite effective idea so we want to give probabilities of next words so what we're going to work with is what are referred to as n grams and so n grams is just a chunk of n consecutive words which are usually referred to as unigrams bigrams trigrams and then four grams and five grams a horrible set of names which would offend any humanist but that's what people normally say and so effectively what we do is just collect statistics about how often different engrams occur in a large amount of text and then use those to build a probability model so the first thing we do is what's referred to as making a markov assumption so these are also referred to as markov models and we decide that the word and position t plus one only depends on the preceding n minus one words so if we want to predict t plus one given the entire preceding text we actually throw away the early words and just use the preceding n minus one words as context well once we made that simplification we can then just use the definition of conditional probability and say all that conditional probability is the probability of n words divided by the preceding n minus one words and so we have the probability of an n gram over the probability of an n minus one gram and so then how do we get these n gram and n minus one gram probabilities we simply take a large amount of text in some language and we count how often the different engrams occur and so our crude statistical approximation starts off as the count of the engram over the count of the n minus one gram so here's an example of that suppose we're learning a foreground language model okay so we throw away all words apart from the last three words and they're our conditioning we look um in some large we use the counts from some large training corpus and we see how often did students open their books occur how often did students open their minds occur and then for each of those counts we divide through by the count of how often students open their occurred and that gives us our probability estimates um so for example if in the corpus students open there occurred a thousand times students open their books occurred 400 times we'd get a probability estimate of 0.4 for books if exams occurred 100 times you get 0.1 for exams and we sort of see here already the disadvantage of having made the markov assumption and have gotten rid of all of this earlier context which would have been useful for helping us to predict um the one other point um that i'll just mention that i confuse myself on is this count of the engram language model so for a foreground language model it's called a foreground language model because in its estimation you're using four grams in the numerator and trigrams in the denominator so you use the the size of the numerator so that terminology is different to the terminology that's used in markov models so when people talk about the order of a markov model that refers to the amount of context you're using so this would correspond to a third order markup model um yeah so someone said is this similar to a naive bayes model sort of naive bayes models you also estimate the probabilities just by counting um so they're they're related and they're sort of in some sense two differences um the first difference or specialization is that naive bayes models um work out probabilities of words independent of their neighbors so what in one part that a naive bayes language model is a unigram language model so you're just using the counts of individual words but the other part of the naive bayes model is you're learning a different set of unigram counts for every class for your classifier um and so um you've then got sort of so effectively a naive bayes model is you've got class specific unigram language models okay i gave this as a simple statistical model for estimating your probabilities with an engram model you can't actually get away with just doing that because you have sparsity problems so you know often will be the case that for many words students open their books or students opened their backpacks just never occurred in the training data that if you think about it if you have something like um ten to the fifth different words even and you want to have then a sequence of four words a problem and they're 10 to the fifth of each there's sort of 10 to the 20th different combinations so unless you're seeing and it's truly astronomical amount of data most forward sequences you've never seen so then your numerator will be zero and your probability estimate will be zero and so that's bad and so the commonest way of solving that is just to add a little delta to every count and then everything is non-zero and that's called smoothing but well sometimes it's worse than that because sometimes you won't even have seen students open theirs and that's more problematic because that means our denominator here is zero and so the division will be ill-defined and we can't usefully calculate any probabilities in a context that we've never seen and so the standard solution to that is to shorten the context and that's called back off so we condition only on open there or if we still don't haven't seen it open there we'll condition only on there or we could just forget all conditioning and actually use a unigram model for our probabilities yeah um and so as you increase the the order n of the engram language model these sparsity problems become worse and worse so in the early days people normally worked with trigram models as it became easier to collect billions of words of text people commonly moved to five gram models but every time you go up an order of conditioning you effectively need to be collecting orders of magnitude more data because of the size of the vocabularies of human languages um there's also a problem that these models are huge um so you basically have to be caught storing counts of all of these word sequences so you can work out these probabilities and i mean that's actually had a big effect in what terms of what technology is available um so in the 2000s decade up till that whenever it was 2014 that there was already google translate using probabilistic models included language models of the engram language model sort but the only way they could possibly be run is in the cloud because you needed to have these huge tables of probabilities but now we have neural nets and you can have google translate just actually run on your phone and that's possible because neural net models can be massively more compact than these old engram language models yeah but nevertheless before we get on to the newer models let's just um sort of look at the example of how these work so it's trivial to train an engram language model because you really just count how often word sequences occur in a corpus and you're ready to go so these models can be trained in seconds that's really good it's not like sitting around for training neural networks so if i train on my laptop a small language model on you know about 1.7 million words as a trigram model i can then ask it to generate text if i give it a couple of words today though i can then get it to sort of suggest a word that might come next and the way i do that is the language model knows the probability distribution of things that can come next um note there's a kind of a crude probability distribution i mean because effectively over this relatively small corpus there were things that occurred once italian and emirate there are things that occurred twice price there were things that occurred um four times company and bank um it's sort of fairly crude and rough but i nevertheless get probability estimates i can then say okay based on this let's take this probability distribution and then we'll just sample the next word so the two most likely was the sample a company or bank but we're um rolling the dice and we might get any of the words that have come next so maybe i sample price now i'll condition on price on the price and look up the probability distribution of what comes next the most likely thing is of and so again i'll sample and maybe this time i'll pick up of and then i will now condition on price of and i will look up the probability distribution of words following that and i get this probability distribution and i'll sample randomly some word from it and maybe this time i'll sample a rare but possible one like gold and i can keep on going and i'll get out something like this today the price of gold per ton while production of shoe lasts and shoe industry the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks step 30 n primary 76 cents a share um so what just a simple trigram model can produce over not very much text is actually already kind of interesting like it's actually surprisingly grammatical right there are whole pieces of it while production of shoe last sense you industry the bank interview intervene just after it can sit and reject an imf demand right it's really actually pretty good grammatical um text so it's um it's sort of amazing that these simple engram models actually can model a lot of human language on the other hand it's not a very good piece of text it's completely incoherent and makes no sense and so to actually be able to generate text that seems like it makes sense we're going to need a considerably better language model and that's precisely what neural language models have allowed us to build as we'll see later okay so how can we build a neural language model and so first of all we're going to do a simple one and then we'll see where we get but to move into a current neural nets might still take us the next time so going to have input sequence of words and we want a probability distribution over the next word um well the simplest thing that we could try is to say well kind of the only tool we have so far is a window-based classifier so what we can say you know what we've done previously either for our named entity recognized in lecture three or what i just showed you for the dependency parser is we have some context window we put it through a neural net and we predict something as a classifier so before we were predicting a location but maybe instead we could reuse exactly the same technology and say we're going to have a window-based classifier so we're discarding the further away words just like in engram language model but we'll feed this fixed window into a neural net so we concatenate the word embeddings we put it through a hidden layer and then we have a softmax classifier over our vocabulary and so now rather than predicting something like location or left arc in the dependency parser we're going to have a soft max over the entire vocabulary sort of like we did with the skipgram negative sampling model in the first two lectures and so we're going to see this choice as predicting what word that comes next whether it produces laptops minds books etc okay so this is a a fairly simple fixed window neural net classifier but this is essentially a famous early model in the use of neural nets for nlp applications um so first a 2000 conference paper and then a somewhat later journal paper joshua bengio and colleagues introduced precisely this model as the neural probabilistic language model and they were already able to show that this could give interesting good results for language modeling and so it wasn't a great solution for neural language modeling but it still had value so it didn't solve the problem of allowing us to have bigger contexts to predict what words are going to come next it's in that way limited exactly like an engram language model is but it does have all the advantages of distributed representations so rather than having these counts for word sequences that are very sparse and very crude we can use distribute distributed representations of words which then make predictions that semantically similar words should give similar probability distributions so the idea of that is if we use some other word here like maybe the pupils open there um well maybe in our training data we'd seen sentences about students but we'd never seen sentences about pupils an engram language model then would sort of have no idea what probabilities to use whereas a new language model can say well pupils is kind of similar to students therefore i can predict similarly to what i would have predicted for students okay so there's now no sparsity problem we don't need to store billions of engram counts we simply need to store our word vectors and our w and u matrices but we still have the remaining problems that our fixed window is too small we can try and make the window larger if we do that w the w matrix gets bigger but that also points out another problem with this model not only can the window never be large enough but w is just a trained matrix and so therefore we're learning completely different weights for each position of context the word minus one position the word minus two the word minus three and the word minus four so that there's no sharing in the model as to how it um treats words in different positions even though in some sense they will contribute semantic components that are at least somewhat um position independent so again for those of if you sort of think back to either a naive bayes model or what we saw with the word to vect model at the beginning the word to vect model or naive bayes model completely ignores word order so it has one set of parameters regardless of what position things occur in that doesn't work well for language modeling because word order is really important in language modeling if the last word is the that's a really good predictor of there being an adjective or noun following where if the word for back is the um it doesn't give you the same information so you do want to somewhat make use of word order but this model is at the opposite extreme that each position is being modeled completely independently so what we'd like to have is a neural architecture that can process an arbitrary amount of context and have more sharing of the parameters while still be sensitive to proximity and so that's the idea of recurrent neural networks and i'll say about five minutes about these today and then next time we'll return and do more about your of recurrent neural networks so for the recurrent neural network rather than having a single hidden layer inside our classifier here that we compute each time for the recurrent neural network we have the hidden layer which often is referred to as the hidden state but we maintain it over time and we feed it back into itself um so that's what the word recurrent is meaning that you're sort of feeding the hidden layer back into itself so what we do is based on the first word we compute a hidden representation kind of like before which can be used to predict the next word but then for when we want to predict what comes after the second word we not only feed in the second word we feed in the hidden layer from the previous word to have it help predict the hidden layer above the second word and so formally the way we're doing that is we're taking the hidden layer above the first word multiplying it by a matrix w and then that's going to be going in together with x2 to generate the next hidden step and so we keep on doing that at each time step so that we're kind of repeating a pattern of creating a next hidden layer based on the next input word and the previous hidden state by updating it by multiplying it by a matrix w okay so in my slide here i've still only got four words of context because it's nice for my slide but you know in principle there could be you know any number of words of context now okay so what we're doing is so um that we start off by having input vectors which can be our word vectors that we've looked up for each word um so sorry yeah so we can have the one hot vectors for word identity we look up our word embedding so then we've got word embeddings for each word and then we want to compute hidden states so we need to start from somewhere h0 is the initial hidden state and h0 is normally taken as a zero vector so this is actually just initialize the zeros and so for working out the first hidden state we calculate it based on the first word it's embedding by multiplying this embedding by a matrix we and that gives us the first hidden state but then you know as we go on we want to apply the same formula over again so we have just two parameter matrices in the recurrent neural network one matrix for multiplying input embeddings and one matrix for updating the hidden state of the network and so for the second word from its word embedding we multiply it by the we matrix we take the previous time steps hidden state and multiply it by the wh matrix and we use the two of those um to generate the new hidden state and precisely how we generate the new hidden state is then by shown on this equation on the left so we take the previous hidden state multiply it by wh we take the input embedding multiply it by w e we sum those two we add on a learn bias weight and then we put that through a non-linearity and although on this slide that non-linearity is written as sigma by far the most common non-linearity to use here actually is a tan h non linearity and so this is the core equation for a simple recurrent neural network and for each successive time step we're just going to keep on applying that to work out hidden states and then from those hidden states we can use them just like in our window classifier to predict what would be the next word so at any position we can take this hidden vector put it through a soft max layer which is multiplying by u matrix and adding on another bias and then making a soft max distribution out of that and that will then give us the probability distribution over next words what we saw here right this is the entire math of a simple recurrent neural network and next time i'll come back and say more about them but this is the entirety of of what you need to know in some sense for the computation of the forward model of a simple recurrent neural network so the advantages we have now as it can process at a text input of any length in theory at least it can use information from any number of steps back we'll talk more about in practice how well it actually works the model size is fixed it doesn't matter how much of a past context there is all we have is our wh and we parameters and at each time step we use exactly the same weights to update our hidden state um so there's a symmetry in how different inputs are processed in producing our predictions um rnns in practice though or these simple rnns and practice aren't perfect so um a disadvantage is that they're actually kind of slow because with this recurrent computation in some sense we are sort of stuck with having to have on the outside of for loop so we can do vector matrix multiplies on the inside here but really we have to do for time step equals 1 to n calculate the successive hidden states and so that's not a perfect neural net architecture and we'll discuss alternatives to that later and although in theory this model can access information any number of steps back in practice we find that it's pretty imperfect at doing that and that will then lead to more advanced forms of recurrent neural network that i'll talk about next time that are able to more effectively access past context okay i think i'll stop there for the day