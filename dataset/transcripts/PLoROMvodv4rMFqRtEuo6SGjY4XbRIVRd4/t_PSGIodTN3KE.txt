okay so for today um we're actually gonna take a bit of a change of pace from what the last couple of lectures or have been about and we're going to focus much more on linguistics and natural language processing and so in particular we're going to start looking at the topic of dependency parsing and so this is the plan of what to go about through today so i'm going to start out by going through some ideas that have been used with the syntactic structure of languages of constituency and dependency and introduce those and then focusing in more on dependency structure i'm then going to look at dependency grammars and dependency tree banks and then having done that we're then going to move back into thinking about how to build natural language processing systems and so i'm going to introduce the idea of transition based dependency parsing and then in particular having developed that idea i'm going to talk about a way to build a simple but highly effective neural dependency parser and so this simple highly effective neural dependency parser is essentially what we'll be asking you to build um in the third assignment so in some sense we're getting a little bit ahead of ourselves here because in week two of the class um we teach you how to do both assignments two and three um but all of this material will come in really useful before i get underway just a couple of announcements so if for a site again for assignment two you don't yet need to use the pi torch framework but now's a good time to work on getting pie torch installed for your python programming assignment three is in part also an introduction to using pytorch it's got a lot of scaffolding included in the assignment um but beyond that um this friday we've got a pie torch tutorial and thoroughly encourage you um to come along to that as well look for it under the zoom tab and um in the in the second half of the thursday of week four we have an explicit class that um partly focuses on the final projects and what the choices are for those but it's never too late to start thinking about the final project and what kind of things you want to do for the final project um so do um come meet with people there are so resources on the course pages about what different tas know about i've also talked to a number of people about final projects but clearly i can't talk to everybody so i encourage you to also be thinking about what you want to do for final projects okay so what i wanted to do today was introduce how people think about the structure of sentences um and to put structure on top of them to explain how human language conveys meaning and so our starting point for meaning and essentially what we've dealt with with word vectors up until now is we have words and words are obviously an important part of the meaning of human languages but for words in human languages um there's more that we can do with them in thinking about how to structure sentences so in particular the first most basic way that we think about words when we thinking about how sentences are structured is we give to them what's called a part of speech we can say that cat is a noun by is a preposition door is another noun cuddly is an adjective and then for the word the um if it was given a different part of speech if you saw any parts of speech in school it was probably you're told it was an article sometimes that is just put into the class of adjectives in modern linguistics and what you'll see in the resources that we use words like that are referred to as determiners and the idea is that there's a bunch of words includes art and but also other words like this and that um or even every which are words sort of occur at the beginning of something like the cuddly cat which have a determinative function of sort of picking out which cats that they're referring to and so we refer to those as determiners but it's not the case that when we want to communicate with language that we just have this word salad where we say a bunch of words we just say you know whatever leaking kitchen tap and um let the other person put it together we put words together in a particular way to express meanings and so therefore languages have larger units of putting meaning together and the question is how we represent and think about those now in modern work um in particular in modern united states linguistics or even what you see in computer science classes when thinking about formal languages the most common way to approach this is with the idea of context-free grammars which you see at least a little bit of in 103 if you've done 103 what a linguist would often refer to as free structure grammars and the idea there is to say well there are bigger units in languages that we refer to as phrases so something like the cuddly cat is a cat um with some other words modifying it and so we'll refer to that as a noun phrase um but then we have ways in which phrases can get larger um by building things and side phrases so the door here is also a noun phrase um but then we can build something bigger around it with the prepositions as a preposition and then we have a prepositional phrase and in general we can keep going so we can then make something like the cuddly cat by the door and then the door is a noun phrase the cuddly cat is a noun phrase by the door as a prepositional phrase but then when we put it all together the whole of this thing becomes a bigger noun phrase and so it's working with these ideas of nested phrases what in context-free grammar terms you'd refer to as non-terminals so noun phrase and prepositional phrase would be non-terminals in the context-free grammar we can build up a bigger structure of human languages so let's just do that for a little bit um to review what happens here so we start off saying okay you can say the cat and the dog and so those are noun phrases and so we want a rule that can explain those so we could say a noun phrase goes to determiner noun and then somewhere over the side we'd have a lexicon and in our lexicon we'd say that dog is a noun and cat is a noun and is a determiner and that is a determiner okay so then we notice you can do a bit more than that so you can say things like the large cat a barking dog so that suggests we can have a noun phrase after the determiner there can optionally be an adjective and then there's the noun and that can explain some things we can say but we can also say the cat by the door or a barking dog in a crate and so we can also put a prepositional phrase at the end and that's optional but you can combine it together with an adjective for the example i gave like a barking dog on the table and so that this grammar can handle that um so then we'll keep on and say um well actually you can use multiple adjectives so you can say a large barking dog or a large barking cuddly cat no maybe not well sentences like that so we have any number of adjectives which we can represent with a star what's referred to as the cleany star so that's good um oh but i forgot a bit actually um for by the door i have to have a rule for producing by the door so i also need a rule that's a prepositional phrase goes to a preposition followed by a noun phrase and so then i also have to have prepositions and that can be in or on or by okay and i can make other sentences of course with this as well like the large crate on the table or something like that or the large crate on the large table okay so i chug along and then well i could have something like talk to the cat and so now i need more stuff so talk is a verb and two is still looks like a preposition so i need to be able to make up something um with that as well okay so what i can do is say i can also have a rule for a verb phrase that goes to a verb and then after that for something like talk to the cat that it can take a prepositional phrase after it and then i can say that the verb goes to talk or walked um okay then i can pause then i can cover those sentences oops um okay so that's that's the end of what i have here um so in this sort of a way i'm hand writing a grammar so here is now i have this grammar and a lexicon and for the examples that i've written i've written down here um this grammar and this lexicon is sufficient um to pass these sort of fragments of showing expansion that i just wrote down i mean of course there's a lot more to english than what you see here right so if i have something like you know the cat walked behind the dog then i need some more grammar rules so it seems then i need a rule that says i can have a sentence that goes to a noun phrase followed by a verb phrase um and i can keep on um doing things of this sort um that's the um one question that um ruthanne asked was about um what the brackets mean and is the first np different from the second um so for this notation on the brackets here i mean this is actually a a common notation that's used in linguistics um it's sort of in some sense a little bit different to traditional computer science notation since the star is used in both to mean zero or more of something so you could have zero one two three four five adjectives somehow it's usual in linguistics that when you're using the star you also put parentheses around it um to mean it's optional so sort of parentheses and star are used together to mean any number of something when it's parentheses just by themselves that's then meaning zero or one um and then four um are these two noun phrases different no they're both noun phrase rules and so in our grammar we can have multiple rules that expand noun phrase in different ways um but you know actually in my example here my second rule because i wrote it quite generally it actually covers the first rule as well so actually at that point i can cross out this first rule because i don't actually need it in my grammar but in general you know you haven't a choice between writing multiple rules for noun phrase goes to categories which effectively gives you a disjunction or working out by various syntactic conventions how to compress them together okay um so that was what gets referred to in natural language processing as constituency grammars um where the standard form of constituency grammar is a context free grammar of the sort that i trust you saw at least a teeny bit of either in cs 103 or something like a programming languages compilers formal languages class there are other forms of grammars that also pick out constituency there are things like tria joining grammars but i'm i'm not going to really talk about any of those now what i actually want to present is a somewhat different way of looking at grammar which is referred to as um dependency grammar which puts a dependency structure over sentences now actually it's not that these two ways of looking at grammar have nothing to do with each other i mean there's a whole um formal theory about the relationships between different kinds of grammars and you can very precisely state relationships um and isomorphisms between different grammars of different kinds but on the surface these two kinds of grammars look sort of different and emphasize different things and for reasons of this sort of closeness to picking out relationships and sentences and their ease of use it turns out that in modern natural language processing starting i guess around 2000 so really in the last 20 years nlp people have really swung behind dependency grammars so if you look around now where people are using grammars in nlp by far the most common thing that's being used is dependency grammars um so i'm going to teach us today a bit about those and for what we're going to build um in assignment three is building using supervised learning and neural dependency parser so the idea of dependency grammar is that when we have a sentence what we're going to do is we're going to say for each word what other words um modify it so what we're going to do is when we say the large crate we're going to say okay well large is modifying crate and that is modifying crate in the kitchen that is modifying kitchen by the door that is modifying door and so i'm showing a modification that depends a dependency or an attachment relationship by drawing an arrow from the head to what's referred to in dependency grammar as the dependent the thing that modifies further specifies or attaches to the head um okay so that's the start of this um well another dependency um that is that well um in look in the large crate that where you're looking is in the large crate so you're going to want to have the large in the large crate as being a dependent of look and so that's also going to be a dependency relationship here um and then there's one final bit um that might seem a little bit confusing to people and that's actually when we have these prepositions um there are two ways that you can think that this might work so if it was something like look in the crate it seems like that is a dependent of crate but you could think that you want to say look in and it's in the crate and give this dependency relationship with the sort of preposition as sort of thinking of it as the head of what was before our prepositional phrase and that's a possible strategy in the dependency grammar but what i'm going to show you today and what you're going to use in the assignment is dependency grammars that follow the representation of universal dependencies and universal dependencies is a framework which actually is involved in creating which was set up to try and give a common dependency grammar over many different human languages and in the design decisions that were made in the context of designing universal dependencies what we decided was that for what in some languages you use prepositions lots of other languages make much more use of case markings so if you've seen something like german you've seen more case markings like genitive and dative cases and in other languages like latin or finnish lots of native american languages you have many more case markings again which cover most of the role of prepositions um so in universal dependencies essentially in the crate is treated like a case marked noun and so what we say is that the in is also a dependent of crate and then you're looking in the crate um so in the structure we adopt um in as a dependent of crate this inn is a dependent of kitchen this buy is a dependent of door and then we have these prepositional phrases in the kitchen by the door and we want to work out well what they modify well in the kitchen is modifying crate right because it's a crate in the kitchen so we're going to say that it's this piece is a dependent of crate um and then well what about by the door well it's not really meaning that's a kitchen by the door um and it's not meaning to look by the door again it's a crate by the door and so what we're going to have is the crate also has door as a dependent and so that gives us our full dependency structure of this sentence okay and so that's a teeny introduction to syntactic structure i'm going to say a bit more about it and give a few more examples but let me just for a moment sort of say a little bit about well why are we interested in syntactic structure why do we need to know the structure of sentences and this gets into how does human languages work so human languages can can communicate very complex ideas i mean in fact you know anything that humans know how to communicate to one another they communicate pretty much by using words so we can structure and communicate very complex ideas but we can't um communicate a really complex idea by one word we can't just you know choose a word like you know empathy and say it with a lot of meaning and say empathy and the other person is meant to understand everything about what that means right we have to compose a complex meaning that explains things by putting words together into bigger units and the syntax of a language allows us to put words together into bigger units um where we can build up and convey to other people a complex meaning and so then the listener doesn't get this syntactic structure right the syntactic structure of the sentence is hidden from the listener all the listener gets is a sequence of words one after another bang bang bang so the listener has to be able to do what i was just trying to do in this example that as the sequence of words comes in that the listener works out which words modify which other words and therefore can construct the structure of the sentence and hence the meaning of the sentence and so in the same way if we want to build clever neural net models that can understand the meaning of sentences those clever neural net models also have to understand what is this structure of the sentence so that they can interpret the language correctly and we'll go through some examples and see more of that okay so the fundamental point that we're going to sort of spend a bit more time on is that these choices of how you build up the structure of a language change the interpretation of the language and a human listener or equally a natural language understanding program has to make in a sort of probabilistic fashion choices as to which words modify i depend upon which other words so that they're coming up with the interpretation of the sentence that they think was intended by the person who said it okay so um to get a sense of this and how sentence structure is um interesting and difficult what i'm going to go through now is a few examples of different ambiguities that you find in natural language and i've got some funny examples some newspaper headlines but these are all real natural language ambiguities that you find throughout um natural language well at this point i should say this is where i'm being guilty of saying language but i'm meaning in english some of these ambiguities you find in lots of other languages as well but which ambiguities for syntactic structure partly depend on the details of the language so different languages have different syntactic instructions different word orders different amounts of word having different forms of words like case markings and so depending on those details there might be different ambiguities so here's one ambiguity which is one of the commonest ambiguities in english so san jose cops kill man with knife so this sentence has two meanings um either it's the san jose cops um who are killing a man and they're killing a man with a knife um and so that corresponds to a dependency structure where the san jose cops are the subject of killing the man is the object of killing and then the knife is then the instrument with which they're doing the killing so that the knife is an oblique modifier for the instrument of killing and so that's one possible structure for this sentence um but it's probably not the right one um so what it actually probably was was that it was a man with a knife and the san jose cops killed the man so that corresponds to the knife then being a noun modifier of the man and then kill is still killing the man so the man is the object of killing and the cops are still the subject and so whenever you have a prepositional phrase like this that's coming further on in a sentence there's a choice of how to interpret it it could be either interpreted as modifying a noun phrase that comes before it or it can be interpreted as modifying a verb that comes before it so systematically in english you get these prepositional phrase attachment ambiguities throughout all of our sentences but um you know to give two further observations on that you know the first observation is you know you encounter sentences um with prepositional phrase attachment um ambiguities every time you read a newspaper article every time you talk to somebody but most of the time you never notice them and that's because our human brains are incredibly good at considering the possible interpretations and going with the one that makes sense according to context the second comment as i said different human languages expose different ambiguities so for example this is an ambiguity that you normally don't get in chinese because in chinese prepositional phrases modifying a verb are normally placed before the verb and so therefore you don't standardly get this ambiguity but you know there are different other ambiguities that you find commonly in chinese sentences okay so this ambiguity you find everywhere because prepositional phrases are really common at the right ends of sentences so here's another one um scientists count whales from space so that gives us these two possible interpretations that there are whales from space and scientists are counting them um and then the other one is how the scientists are counting the whales is that they're counting them from space and they're using satellites to count the sales which is the correct interpretation um that the newspaper hopes that you're getting um and this problem gets much much more complex because many sentences in english have prepositional phrases all over the place um so here's the kind of boring sentence that you find in the financial you news the board approved its acquisition by royal trusco limited of toronto for 27 dollars a share at its monthly meeting and well if you look at the structure of this sentence what we find is you know here's a verb then here's the object noun phrase so we've got the object noun phrase here and then after that what do we find well we find a prepositional phrase another prepositional phrase another prepositional phrase and another prepositional phrase and how to attach each of these is then ambiguous so the basic rule of how you can attach them is you can attach them to things to the left providing you don't create crossing attachments so in principle by royal trusco limited could be attached to either approved or acquisition but in this case by royal trusco limited is that it's the acquirer so it's um a modifier of the acquisition okay so then we have of toronto so of toronto could be modifying raw trusco limited it could be modifying the acquisition or it can be modifying the approved and in this case the of toronto is telling you more about the company and so it's a modifier of royal trusco limited okay so then the next one is for 27 a share and that could be modifying toronto royal trusca limited the acquisition or the approving and well in this case um that's talking about the price of the acquisition so this one is mod go jumps back and this is now a prepositional phrase that's modifying the acquisition and then at the end at its um monthly meeting um well that's where the approval is happening um by the ver by the board so rather than any of these preceding four noun phrases at its monthly meeting is modifying the approval um and so it attaches right back there and this example is kind of too big and so i couldn't fit it in one line but as i think maybe you can see that you know none of these dependencies cross each other and they connect at different places ambiguously so because we can chain these prepositions like this and attach them at different places like this um human language sentences are actually extremely ambiguous um so the number if you have a sentence with um k prepositional phrases um at the end of it where here we have k equals four um the number of parses this sentence has the number of different ways you can make these attachments is given by the catalan numbers so the catalan numbers are an exponentially growing series which arises in many tree-like contexts so if you're doing something like triangulations of a polygon you get catalan numbers if you're doing triangulation and graphical models in cs228 you get catalan numbers but we don't need to worry about the details here the central point is this is an exponential series and so you're getting an exponential number of parsers in terms of the number of prepositional phrases um and so in general you know the number of parsers human languages have is exponential in their length which is kind of bad news um because if you're then trying to enumerate all the parsers it you might fear that you really have to do a ton of work the thing to notice about structures like these prepositional phrase attachment ambiguities is that there's nothing that resolves these ambiguities in terms of the structure of the sentence so if you've done something like looked at the kind of grammars that are used in compilers that the grammars used in compiling compilers for programming languages are mainly made to be unambiguous and to the extent that there are any ambiguities there are default rules that are used to say choose this one particular parse tree for your piece of a programming language and human languages just aren't like that they're globally ambiguous and the listening human is just meant to be smart enough to figure out what was intended so the analogy would be that you know in programming languages um when you're working out what does an else clause modify well you've got the answer that you can either look at um the curly braces to work out what the else clause modifies or if you're using python you look at the indentation and it tells you what the else clause modifies where by contrast for human languages um the it would be um just write down else something it doesn't matter how you do it you don't need parentheses you don't need indentation the human being will just figure out what the else clause is meant to pair up with okay lots of other forms of ambiguities in human languages so let's look at a few others another one that's very common over all sorts of languages is coordination scope ambiguities so here's a sentence shuttle veteran and long time nasa executive fred gregory appointed the board well this is an ambiguous sentence um there are two possible readings of this one reading is that there are two people there's a shuttle veteran and there's a long time nasa executive fred gregory and they were both appointed to the board two people and the other possibility is there's um someone named fred gregory who's a shuttle veteran and long-time lesser executive and they're appointed to the verb one person and these two interpretations again correspond to having different path structures so in one structure we've got a coordination of the shuttle veteran and the long time nasa executive fred gregory coordinated together in one case these are coordinated and then fred gregory specifies the name of the nasa executive um so it's then um specifying um who that executive is where the what in the other one the shuttle veteran and long time nasa executive all together is then something that is a modifier of um fred gregory okay so one time this is the unit that modifies fred gregory in the other one up here just long time nasa executive modifies fred gregory and then that's conjoined together with the shuttle veteran and so that also gives different interpretations um so this is a slightly reduced example of the i mean in um newspaper headlines tend to be more ambiguous than many other pieces of text because they're written in this shortened form to get things to fit and this is an especially shortened form where it's actually left out in the explicit conjunction but this headline says doctor no heart cognitive issues and this was after i guess one of trump it was after trump's first physical and while this was an ambiguity because there are two ways that you can read this you can either read this as saying doctor no heart and cognitive issues which gives you one interpretation instead of that the way we should read it um is that it's heart or cognitive and so then saying no heart or cognitive issues and we have a different narrower scope of the coordination and then we get a different reading okay um i want to give a couple more examples of different kinds of ambiguities another one you see quite a bit is when you have modifiers that are adjectives and adverbs um that there are different ways that you can have things modifying other things um this example is a little bit not safe for work but here goes students get first-hand job experience so this is an ambiguous sentence and again we can think of it as a syntactic ambiguity in terms of which things modify which other things so the nice polite way to render this sentence is that first is modifying hands so we've got first hand um it's job experience so job is a compound noun modifying experience and it's first-hand experience so ex first hand is then modifying experience and then get is the object of sorry first-hand job experience is the object of get and the students are the subject of get but if you have a smuttier mind you can interpret this a different way and in the alternative interpretation you then have hand going together with job um and the the first is then a modifier of experience um and job is still a modifier of experience and so then you get this different path structure and different interpretation there okay one more example in a way this example is similar to the previous one it's sort of having modifier pieces that can modify different things but rather than just being with individual adjectives or individual adverbs is then much larger units such as verb phrases can often have attachment ambiguities so this sentence headline is mutilated body washes up on rio beach to be used for olympic speech volleyball so we have this big verb phrase here of to be used for olympics beach volleyball and then again we have this attachment decision um that we could either say um that that big verb phrase is modifying i is attached to the rio beach or um we could say no no the to be used for olympic speech body volleyball that that is modifying the mutilated body and it's a body that's to be used for the olympic speech volleyball um which gives the funny reading yeah so i hope that's given you at least a little bit of a sense of how human language syntactic structure is complex ambiguous and to work out the intended interpretations you need to know something about that structure in terms of how much you need to understand i mean you know this isn't a linguistics class if you'd like to learn more about human language structure you can go off and do a syntax class but you know we're not really going to spend a lot of time working through language structure but there will be some questions on this in the assignment and so we're expecting that you can be at the level that you can have sort of some intuitions as to which words and phrases are modifying other words and phrases and therefore you could choose between two dependency analyses which one's correct okay um i've spent quite a bit of time on that um so better um keep going okay so the general idea is that knowing this sort of syntactic structure of a sentence can help us with semantic interpretation i mean as well as just generally saying we can understand language it's also used in many cases for simple practical forms of semantic extraction so people such as in biomedical informatics often want to get out particular relations such as protein protein interactions and well here's a sentence the results demonstrated that chi c interacts rhythmically with sas a chi a and kai b um and commonly that people can get out those kind of relationships by looking at patterns of dependency relations with particular verbs so for the interacts verb if you have a pattern of something being the subject and something else being the noun modifier of interacts well that's an interaction relationship but it gets a bit more complicated than that as in this example because often there are conjunctions so you also want to have another pattern where you have also interactions between the subject and the noun modifiers conjunct which will allow us to also find the chi a and kb examples okay um so i've sort of given an informal tour of dependency grammar to just try and uh quickly say a little bit more about formally what a dependency grammar is so in dependency syntax what we say is that the syntactic structure of a sentence consists of relations between pairs of words and it's a binary asymmetric relation i we draw arrows between pairs of words which we call dependencies now normally dependency grammars then type those grammatical relation type those arrows to express what kind of relation that there is and so that they have some kind of taxonomy of grammatical relation so we might have a subject grammatical relation a verbal auxiliary grammatical relation an oblique modifier grammatical relation we have some kind of typology of grammatical relations so and we refer to the arrow as going between the head is the head here and something that is a dependent of it so the subject of a verb is the dependent of the verb or when you have a noun modifier like our sort of cuddly cat we say that um cuddly is a dependent of cat and so cat is the head of cuddly cat and so normally dependencies like in these examples form a tree which is formal it so it's not just any graph with arrows we have an and graph which is connected a cyclic and has a single root so here's the root of the graph and so that gives us a dependency tree analysis dependency grammars have a really really long history so the famous first linguist um was panini um who wrote about the structure of sanskrit um and mainly he worked on the sound system of sanskrit and how sounds change in various contexts which what linguists call phonology and the different forms of sanskrit words sanskrit has rich morphology of inflecting nouns and verbs for different cases and forms but he also worked a little on the syntactic structure of sanskrit sentences and essentially what he proposed was a dependency grammar over sanskrit sentences and it turns out that sort of for most of recorded history when then when people have then um gone on and tried to put structures over human sentences um what they have used is dependency grammars um so there was a lot of work in the first millennium by arabic grammarians of trying to work out the grammar um the structure of sentences and effectively what they used was you know akinned what i've just presented as a dependency grammar so compared to you know 2500 years of history the ideas of having context-free grammars and having constituency grammars is actually a really really recent invention so it was really sort of in the middle of the 20th century that the ideas of um constituency grammar and context free grammars were developed first by wells in the 40s and then by noam chomsky in the early 50s leading to things like the chomsky hierarchy that you might see um cs103 or a formal languages class um so for modern work on dependency grammar using kind of the terminology and um notation that i've just introduced that's normally attributed to lucianteniere who was a french linguist um in around the sort of middle of the 20th century as well dependency grammar was widely used in the 20th century um in a number of places i mean in particular it tends to be sort of much more natural and easier to think about for languages that have a lot of different case markings or nouns like nominative accusative genitive data of instrumental kind of cases like you get in a language like latin or russian and a lot of those languages have much freer word order than english so the subject or object of you know in english the subject has to be before the verb the object has to be after the verb but lots of other languages have much freer word order and instead use different forms of nouns to show you what's the subject or the object of the sentence and dependency grammars can often seem much more natural for those kinds of languages dependency grammars were also prominent at the very beginnings of computational linguistics so one of the first people working computational linguistics in the us was david hayes so the professional society for computational linguistics is called the association for computational linguistics and he was actually one of the founders of the association for computational linguistics and he published in the early 1960s um an early perhaps the first dependency grammar paused sorry dependency parser okay um yeah a little teeny note just in case you see other things when when you have these arrows um you can draw them in either direction you can either draw arrows from the head or to the dependent or from the dependent to the head and actually different people have done one and the other right so the way tenure drew them was to draw them from the head to the dependent and we're following that convention but you know if you're looking at something that somebody else has written um with dependency arrows the first thing you have to work out is are they using the arrow heads at the heads or the dependents um now and not one other thing here is that we a sentence is seen as having the overall head word of the sentence which every other word of the sentence hangs off it's a common convention to add this sort of fake root to every sentence that then points um to the head word of the whole sentence here completed that just tends to make the algorithmic stuff easier because then you can say that every word of the sentence is dependent on precisely one other node where what you can be dependent on is either another word on the sentence or the fake root of the sentence and when we build our parsers we will introduce that fake root okay so um that's sort of dependency grammars and dependency structure i now want to get us back to natural language processing and starting to build parses for dependency grammars but before doing that i just want to say yeah where do we get our data from and that's actually an interesting story in some sense so the answer to that is well what we do is get human beings commonly linguists or other people who are actually interested in the structure of human sentences and we get them to sit around and hand pass sentences and give them dependency structures and we collect a lot of those parsers and we call that a tree bank and so um this is something that really only started happening in the late 80s and took off in a bigger way in the 90s until then no one had attempted to build tree banks lots of people had attempted to build parsers and it seemed like well if you want to build a parser the efficient way to do it is to start writing a grammar so you start writing some grammar rules and you start writing a lexicon with words and parts of speech and you sit around working on your grammar when i was a phd student one of my first summer jobs was spending the summer handwriting a grammar and it sort of seems like writing a grammar is more efficient because you're writing this one general thing that tells you the structure of a human language um but there's just been this massive sea change partly driven by the adoption of machine learning techniques where it's now seen as axiomatic that the way to make progress is to have annotated data namely here a tree bank that shows you the structure of sentences and so what i'm showing here is a teeny extract um from a universal dependencies tree bank and so that's what i mentioned earlier that this has been this effort to try and have a common dependency grammar representation that you can apply to lots of different human languages and so you can go over to this url and see that there's about 60 different languages at the moment which have universal dependencies tree banks um so why are tree banks good i mean it sort of seems like it's bad news if you have to have people sitting around for weeks and months hand passing sentences it seems a lot slower and actually a lot less useful than having somebody writing a grammar which just has um you know a much bigger multiplier factor in the utility of their effort it turns out that although that feel initial feeling seems sort of valid that in practice there's just a lot more you can do with the tree bank so why are tree banks um great you know one reason is the tree banks are highly reusable so typically when people have written grammars they've written grammars for you know one particular parser and the only thing it was ever used in is that one particular parser but when you build a tree bank that's just a use a useful data resource and people use it for all kinds of things so the well-known tree banks have been used by hundreds and hundreds of people and although all tree banks were initially built for the purposes of hey let's help natural language processing systems it turns out that people have actually been able to do lots of other things with tree banks so for example these days psycholinguists commonly use tree banks to get various kinds of statistics about data for thinking about psycholinguistic models linguists use tree banks for looking at patterns of different syntactic constructions that occur um that there's just been a lot of reuse of this data for all kinds of purposes but they have other advantages that i mentioned here you know when people are just sitting around saying oh what sentences are good they tend to only think of the core of language where lots of weird things happen in language and so if you actually just have some sentences and you have to go off and pass them then you actually have to deal with the totality of language um since you're parsing actual sentences you get statistics so you naturally get the kind of statistics that are useful to machine learning systems by constructing a tree bank where you don't get them for free if you hand write a grammar but then a final way which is perhaps the most important of all is you if you actually want to be able to do um science of building systems you need a way to evaluate these nlp systems i mean it seems hard to believe now that you know back in the 90s and 80s when people built nlp parsers it was literally the case um that the way they were evaluated was you said to your friend oh i've built this parser type in a sentence on the terminal and see what it gives you back it's pretty good hey um and that was just the way business was done um whereas what we'd like to know is well as i showed you earlier english sentences can have lots of different parsers commonly can this system choose the right parses for particular sentences and therefore have the basis of interpreting them as a human being would and while we can only systematically do that evaluation if we have a whole bunch of sentences that have been hand passed by humans with their correct interpretations so the rise of tree banks turned parser building into an empirical science where people could then compete rigorously on the basis of look my parser has two percent higher accuracy than your parser in choosing the correct parsers for sentences okay so well how do we build a parser once we've got dependencies so there's sort of a bunch of sources of information that you could hope to use so one source of information is looking at the words on either end of the dependency so discussing issues that seems a reasonable thing to say and so it's likely that issues could be the object of discussing whereas if it was some other word right if you're thinking of making you know outstanding the object of discussion discussing outstanding that doesn't sound right so that wouldn't be so good a second source of information is distance so most dependencies are relatively short distance some of them aren't some of long distance dependencies but they're relatively rare the vast majority of dependencies are nearby and another source of information is the intervening material so there are certain things that dependencies um rarely span so clauses and sentences are normally organized around verbs and so dependencies rarely span across intervening verbs we can also use punctuation and written language things like commas which can give some indication of the structure and so punctuation may also indicate bad places to have long distance dependencies over and there's one final source of information which is what's referred to as valency which is for a head what kind of information does it usually have around it so if you have a noun there are things that you just know about what kinds of dependents nouns normally have so it's common that it will have a determiner to the left the cat on the other hand um it's not going to be the case that there's a determiner to the right cat the that's just not what you get in english on the left you're also likely to have an adjectival modifier that's where he had cuddly but again it's not so likely you're going to have the adjectival modifier over on the right for cuddly so there are sort of facts about what things different kinds of words take on the left and the right and so that's the valency of the heads and that's also a useful source of information okay so what do we need to do using that information to build a parser well effectively what we do is have a sentence i'll give a talk tomorrow on your networks and what we have to do is say for every word in that sentence we have to choose some other word that it's a dependent of where one possibility is it's a dependent of root so we're giving it a structure where we're saying okay for this word i've decided that it's a dependent on on networks and then for this word it's also dependent on networks um and for this word it's a dependent on give so we're choosing um one for each word and there are usually a few constraints so only one word is a dependent of root we have a tree we don't want cycles so we don't want to say that word a is dependent on word b and word b is dependent on word a and then there's one final issue um which is um whether arrows can cross or not so in in this particular sentence we actually have these crossing dependencies you can see there i'll give a talk tomorrow on neural networks and this is the correct dependency parse for this sentence because what we have here is that it's a talk and it's a talk on neural network so the on neural networks modifies the talk um but which leads to these crossing dependencies i didn't have to say it like that i could have said i'll give a talk on your networks um tomorrow and then on your networks would be next to the talk so most of the time in languages dependencies are projective the things stay together so the dependencies have a kind of a nesting structure of the kind that you also see in context-free grammars but most languages have at least a few phenomena where you ended up with these ability for phrases to be split apart which lead to non-projective dependencies so in particular one of them in english is that you can take modifying phrases and clauses like the on neural networks here and shift them right towards the end of the sentence and get i'll give a talk tomorrow on neural networks and that then leads to non-projective sentences um so as a parse is projective if there are no crossing dependency arcs when the words are laid out in their linear order with all arcs above the words and if you have a dependency pass that corresponds to a context-free grammar tree um it actually has to be projective because context-free grammars necessarily have this sort of nested tree structure following the linear order but dependency grammars normally allow non-projective structures to account for displaced constituents and you can't easily get the semantics of certain constructions right without these non-projective dependencies so here's another example um in english with question formation with what's called preposition stranding so the sentence is who did bill buy the coffee from yesterday um there's another way i could have said this it's less natural in english but i could have said um from who did bill from who did bill buy the coffee yesterday in many languages of the world that's the only way you could have said it and when you do that from who is kept together and you have a projective pass for the sentence but english allows and indeed much prefers you to do what is referred to as preposition stranding where you move the who um but you just leave the preposition behind and so you get who did bill buy the coffee from yesterday and so then we're ending up with this non-projective dependency structure as i've shown there okay i'll come back to non-projectivity in a little bit how do we go about building dependency parsers well there are a whole bunch of ways um that you can build dependency parsers very quickly i'll just say a few names and i'll tell you about one of them so you can use dynamic programming methods to build dependency parsers so i i showed earlier that you can have an exponential number of parsers for a sentence and that sounds like really bad news for building a system well it turns out that you can be clever and you can work out a way to dynamic program finding that exponential number of parsers and then you can have an o n cubed algorithm so you could do that you can use graph algorithms and um and i'll say a bit about that later but that may spill into next time um so you can see since we're wanting to kind of connect up all the words into a tree using graph edges that you could think of doing that using using a minimum spanning tree algorithm of the sort that you hopefully saw in cf cs 161 and so that idea has been used for parsing constraint satisfaction ideas that you might have seen in cs221 have been used for dependency parsing but the way i'm going to show now is transition based parsing or sometimes referred to as deterministic dependency parsing and the idea of this is one's going to use a transition system so that's like shift reduce parsing if you've seen shift reduce parsing in something like a compiler's class or a formal languages class that shift and reduce our transition steps and so use a transition system to guide the construction of and so let me just explain about that so let's see um so this was an idea that was made prominent by joachim nivray who's a swedish computational linguist who introduced this idea of greedy transition based parsing so his idea is well what we're going to do for dependency parsing is we're going to part be able to pass sentences by having a set of transitions which are kind of like shift reduce parser and it's going to just work left to right bottom up and pass a sentence so we're going to say we have a stack sigma a buffer beta of the words that we have to process and we're going to build up a set of dependency arcs by using actions which are shift and reduce actions and putting those together this will give us the ability to put path structures over sentences and let me go through the details of this and this is a little bit hairy when you first see it that's not so complex really and this this kind of transition based dependency parser is what we'll use in assignment three so what we have so this is our transition system we have a starting point where we start with a stack that just has the root symbol on it and a buffer that has the sentence that's about to part we're about to pass and so far we haven't built any dependency arcs and so at each point in time we can choose one of three actions we can shift which moves the next word onto the stack we can then do actions that are the reduce actions so two reduce actions to make it a dependency grammar we can either do a left arc reduce or a right arc reduce so when we do either of those we take the top two items on the stack and we make one of them a dependent of the other one so we can either say okay let's make wi a dependent of wj or else we can say okay let's make wj a dependent of wi and so the result of when we do that is the one that's the dependent disappears from the stack and so in the stacks over here there's one less item but then we add a dependency r to our arc set so that we say that we've got either a dependency from j to i or a dependency from i to j and commonly when we do this we actually also specify what grammatical relation connects the two such as subject object noun modifier and so we also have here a relation that's still probably still very abstract so let's go through an example so this is how a simple transition based dependency parser what's referred to as an arc standard transition based dependency parser would pass up i ate the fish so remember these are the different operations that we can apply so to start off with we have root on the stack and the sentence in the buffer and we have no dependency arcs constructed so we have to choose one of the three actions and when there's only one thing on the stack the only thing we can do is shift so we shift and now the stack looks like this so now we have to take another action and at this point we have a choice because we could immediately reduce so you know we could say okay let's just make i a dependent of root and we'd get a stack size of one again but that'd be the wrong thing to do because i isn't um the head of the sentence so what we should instead do is shift again and get i8 on the stack and fish still in the buffer well at that point we keep on passing a bit further and so now what we can do is say well wait a minute now i is a dependent of 8 and so we can do a left arc reduce and so i disappears from the stack so here's our new stack but we add to the set of arcs that we've added that i is the subject of eight okay well after that we could have we could reduce again because there's still two things on the stack but that would be the wrong thing to do the right thing to do next would be to shift fish onto the stack and then at that point we can do a right arc reduce saying that eight is the object of fish and add a new dependency to our dependency set and then we can one more time do a right arc reduce to say that 8 is the root of the whole sentence and add in that extra root relation with our pseudo root and at that point we've reached the end condition so the end condition was the buffer was empty and there's one thing the root on the stack and at that point we can finish so this little transition machine does the parsing up of the sentence but there's one thing that's left to explain still here which is how do you choose the next action so as soon as you have two things or more on the stack what you do next you've always got a choice you could keep shifting at least if there's still things on the buffer or you can do a left arc or you can do a right arc and how do you know what choice is correct and well one answer to that is to say well you don't know what choice is correct and that's why parsing is hard and sentences are ambiguous you can do any of those things you have to explore all of them and well if you naively explore all of them then you do an exponential amount of work to pass the sentence so in the early 2000s um joachim phrase and you know that's essentially what people have done in the 80s and 90s is explore every path um but um in the early 2000s um joachim neverey's essential observation was but wait a minute we know about machine learning now so why don't i try and train a classifier which predicts what the next action i should take is given this stack and buffer configuration because if i can write a machine learning classifier which can nearly always correctly predict um the next action given a stack and buffer then i'm in a really good position because then i can build what's referred to as a greedy dependency parser which just goes bang bang bang word at a time okay here's the next thing run classifier choose next action run classifier choose next action run classifier choose next action so that the amount of work that we're doing becomes linear in the length of the sentence rather than that being cubic and the length of the sentence using dynamic programming or exponential and the length of the sentence if you don't use dynamic programming so for each at each step we predict the next action using some discriminative classifier so starting off he was using things like support vector machines but it can be anything at all like a soft max classifier that's closer to our neural networks and there are either for what i presented um three classes if you're just thinking of the two reduces and the shift or if you're thinking of you're also assigning a relation and you have a set of r relations like 20 relations then that's the sort of 41 moves that you could um decide on at each point and the features are effectively the configurations i was showing before what's the top of the stack word what part of speech is it what's the first word in the buffer what's that words part of speech etc and so on the simplest way of doing this you're now doing no search at all you just sort of take each configuration and turn decide the most likely next move and you make it and that's a greedy dependency parser which is widely used you can do better if you want to do a lot more work so you can do what's called a beam search where you maintain a number of fairly good parse prefixes at each step and you can extend them out further and then you can evaluate later on which of those seems to be the best and so beam search is one technique to improve dependency parsing by doing a lot of work and it turns out that although these greedy transition based parses are a fraction worse than the best possible ways known to pass sentences um but they actually work very accurately almost as well and they have this wonderful advantage that they give you linear time parsing in terms of the length of your sentences and text and so if you want to do a huge amount of parsing they're just a fantastic thing to use because um you've then got an algorithm that scales to the size of the web okay um so i'm kind of a little bit behind so i guess i'm not going to get through all of these slides today and we'll have to finish out the final slides tomorrow but um just to push a teeny bit further i'll just say a couple more on the sort of what never did for dependency parser and then i'll sort of introduce the neural form of that in the next class so conventionally you had this sort of stack and buffer configuration and you wanted to build a machine learning classifier and so the way that was done was by using symbolic features of this configuration and what kind of symbolic features did you use you use these indicator features that picked out a small subset normally one to three elements of the configuration so you'd have a feature that could be something like the thing on the top of the stack is the word good which is an adjective or it could be the thing on the top of the stack is an adjective and the thing that's first in the buffer is a noun or it could just be looking at one thing and saying the first thing in the buffer is a verb so you'd have all of these features and because these features commonly involved words and commonly involved conjunctions of several conditions you have a lot of features and you know having mentions of words and conjunctions and conditions definitely help to make these parsers work better but nevertheless because you had all of these sort of one zero symbolic features that you had a ton of such features so commonly these parsers were built um using something like you know a million to ten million different features of sentences and i mentioned already the importance of evaluation let me just sort of quickly say how these parsers were evaluated so to evaluate a a parser for a particular sentence it was hand our test set was hand passed in the tree bank so we have gold dependencies of what the human thought were right and so we can write those down those dependencies down as statements of saying the first word is the dependent of the second word fire a subject dependency and then the parser is also going to make similar claims as to what's dependent on what and so there are two common metrics that are used one is just are you getting these dependency facts right so both of these dependency facts match and so that's referred to as the unlabeled accuracy score where we're just sort of measuring accuracies which are of all of the dependencies in the gold sentence and remember we have one dependency per word in the sentence so here we have five how many of them are correct and that's our unlabeled accuracy score of eighty percent but a slightly more um rigorous evaluation is to say well no we're also going to label them and we're going to say that this is the subject that's actually called the root this one's the object so these dependencies have labels and you also need to get the grammatical relation label right and so that's then referred to as labeled accuracy score and although i got those two right for that as i guess according to this example actually this is wrong it looks like i got oh no this is wrong there sorry that one's wrong there okay um so i only got two of the um dependencies correct in the sense that i both got what depends on what and the label correct and so my labeled accuracy score is only 40 percent okay um so i'll stop there now for the introduction for dependency parsing and i still have an iou which is um how we can then bring neural nets into this picture and how they can um be used to improve dependency parsing so i'll do that at the start of next time before then proceeding further into neural language models