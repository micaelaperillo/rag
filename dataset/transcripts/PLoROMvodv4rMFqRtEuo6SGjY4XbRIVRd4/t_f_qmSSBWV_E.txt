welcome to cs224n uh lecture seven teen uh model analysis and explanation okay look at us we're here um uh let's start with some course logistics um we have uh updated the policy on the guest lecture reactions um they're all due friday um all at 11 59 pm you can't use late days for this uh so please get the men um watch the lectures they're awesome lectures they're awesome guests um and you get something like half a point for each of them and yeah all three can be submitted up through friday um okay so uh final projects remember that the due date is tuesday it's tuesday at 4 30 p.m uh march 16th and let me uh emphasize that there's a hard deadline on the three days from then friday we won't be accepting for additional points off assignments i'm sorry final projects that are submitted after the 4 30 deadline on friday uh we need to get these graded and get grades in so um it's the end stretch week nine our week 10 is really the lectures are us giving you help on the final project so so this is really the last week of lectures thanks for all your hard work um and for asking awesome questions in lecture and in office hours and on ed and let's get let's get right into it so um today we get to talk about one of my favorite subjects in natural language processing it's model analysis and explanation um so first we're going to do what i love doing which is motivating why we want to talk about the topic at all we'll talk about how you know we can look at a model at different levels of abstraction to perform different kinds of analysis on it we'll talk about out-of-domain evaluation sets so this will feel familiar to the to the robust qa folks um then we'll talk about uh sort of trying to figure out for a given example why did it make the decision that it made it had some input it produced some output can we come up with some sort of interpretable explanation for it and um then we'll look at actually the representations of the models so these are the sort of hidden states the vectors that are being built you know throughout the processing of the model try to figure out if we can understand some of the representations and mechanisms that the model is performing and then we'll actually come back to sort of one of the kind of default states that we've been in in this course which is trying to you know look at model improvements removing things from models seeing how it performs and relate that to the analysis that we're doing in this lecture show how it's not all all that different uh okay um so if you haven't seen this xkcd um now you have and it's one of my favorites i'm going to say all the words so uh person a says this is your machine learning system person b says yup you pour the data into this big pile of linear algebra and then collect the answers on the other side person a what if the answers are wrong and person b just stir the pile until they start looking right and i feel like at its worst deep learning can feel like this from time to time you have a model maybe it works for some things maybe it doesn't work for other things you're not sure why it works for some things and doesn't work for others and you know the changes that we make to our models you know they're based on intuition but frequently you know what are the tas told you know everyone in office hours like ah sometimes you just have to try it and see if it's going to work out because it's very hard to tell um it's very very difficult to understand our models on sort of any level and so today we'll go through a number of ways for trying to sort of carve out little bits of understanding here and there um so so uh beyond it being you know important because it's in the next kcd comic what why should we care about what our models about understanding our models one is that we want to know what our models are doing so here you have a black box black box functions are sort of this idea that you can't look into them and interpret what they're doing you have an input sentence say and then some output prediction maybe this black box is actually your uh your final project uh model and it gets some accuracy now we summarize our models and in your final projects you'll summarize your model with sort of one or a handful of of summary metrics of accuracy or f1 score or bluescore or something but it's a lot of model to explain with just a small number of metrics so what do they learn why do they succeed and why do they fail what's another motivation so we want to we want to sort of know what our models are doing okay but but uh maybe that's because we want to be able to make tomorrow's model so today right when you're building models in this class at the company you know you start out with some kind of recipe that is known to work either at the company or because you have experience from this class um and it's not perfect right it makes mistakes you look at the errors and then over time you know you take what works maybe and then you find what needs changing so it seems like maybe you know adding another layer to the model helped uh and maybe that's that's a nice tweak and the model performance gets better etc and um you know incremental progress doesn't always feel exciting but i want to pitch to you that it's actually very important for us to understand how much incremental progress can kind of get us towards some of our goals so that we can have a better job of evaluating when we ne when we need big leaps when we need major changes because there are problems that we're attacking with our incremental sort of progress and we're not getting very far okay so we want to make tomorrow's model um another thing that's i think very related to and sort of a both a part of and bigger than this field of analysis is model biases so let's say you take your word to vect uh analogies solver you know from um from glove or word to back that is from assignment one and you give it the the analogy managed to computer programmer as woman is two and it gives you the output homemaker this is a real example from the paper uh below um you should be like wow well uh i'm glad i know that now and of course you saw the lecture from yulia svetkov last week you said wow i'm glad i know that now and that's a that's a huge problem what did the model use in its decision what biases is it learning from data and possibly making even worse so that's the kind of thing you can also do with model analysis beyond just making models better according to some sort of summary metric as well and then another thing we don't just want to make tomorrow's model and this is something that i think is super important uh we you know we don't just want to look at that time scale we want to say what about 10 15 25 years from now what kinds of things will we be doing you know what are the limits what can be learned by language model pre-training what are the what's the model that will replace the transformer what's the model that will replace that model what does deep learning struggle to do what are we sort of attacking over and over again and failing to make significant progress on what do neural models tell us about language potentially there's some people who are primarily interested in understanding language better using neural networks cool how are how are our models affecting people transferring power between groups of people governments etc that's an excellent type of analysis what can't be learned via language model pre-training so that's sort of the complementary question there if you sort of come to the edge of what you can learn via language model pre-training is there stuff that we need total paradigm shifts in order to to uh to do well so all of this i mean you know falls under some category of trying to really deeply understand our models and their capabilities and there's a lot of different methods here that we'll go over today and one thing that i want you to take away from it is that they're all [Applause] they're all going to tell us some aspect of the model elucidate some kind of intuition or something but none of them are are we going to say aha i really understand 100 about what this model is doing now um so they're going to provide some clarity but never total clarity and one way if you're trying to decide how you want to understand your model more i think you should sort of start out by thinking about is at what level of abstraction do i want to be looking at my model so the sort of very high level abstraction let's say you trained you know a qa model to estimate the probabilities of start and end indices and you know in a reading comprehension problem or you've trained a language model that assigns probabilities to words in context you can just look at the model as that object so it's just a probability distribution defined by your model you are not looking into it any further than the fact that you can sort of give it inputs and see what outputs it provides so that's like not even who even cares if it's a neural network it could be anything but it's a way to understand its behavior another level of abstraction that you can look at you can dig a little deeper you can say well i know that my network is a bunch of layers that are kind of stacked on top of each other you've got sort of maybe your transformer encoder with your one layer two layer three layer you can try to see what it's doing as it goes deeper in the layers so maybe your neural model is a sequence of these vector representations a third option of sort of specificity is to look you know as much as as at as much detail as you can you've got these parameters in there you've got the connections in the computation graph so now you're sort of trying to remove all of the abstraction that you can and look at as many details as possible and all three of these sort of ways of looking at your model and performing analysis are going to be useful and will actually sort of travel slowly from one to two to three as we go through this lecture okay so we haven't actually talked about any analyses yet uh so we're going to get started on that on that now um and we're starting with the sort of testing our model's behaviors so would we want to see will my model perform well i mean the the natural thing to ask is how does it behave on some on some sort of test set right and so uh we don't really care about mechanisms yet why is it performing this by what method is it making its decision instead we're just interested in sort of the more higher level abstraction of like does it perform the way i want it to perform so so let's like take out take our model evaluation that we are already doing and sort of recast it in the framework of analysis so you've trained your model on some samples from some distribution so you've got input output pairs of some kind so how does the model behave on samples from the same distribution that's a simple question and it's sort of uh you know it's known as you know the in-domain accuracy or you can say that the samples are iid and that's what you're testing on and this is just what we've been doing this whole time it's your test set accuracy or f1 or blue score and you know um so you've got some model with some accuracy and maybe it's better than some model with some other accuracy on this test set right so this is what you're doing as you're iterating on your models in your final project as well um you say well you know on my test set which is what i've decided to care about for now model a does better they both seem pretty good and so maybe i'll choose model a to keep working on maybe i'll choose it if you were putting something into production um but remember back to you know this this idea that it's just one number to summarize a very complex system uh it's not going to be sufficient to tell you how it's going to perform in a wide variety of settings okay so so we've been doing this this is model evaluation as model analysis um now we're going to say what if we are not testing on exactly the same type of data that we trained on so now we're asking did the model learn something such that it's able to sort of extrapolate or perform uh how i want it to on data that looks a little bit different from what it was trained on and we're going to take the example of natural language inference so to recall the task of natural language inference and this is through the multi-nli data set that we're just pulling our definition you have a premise he turned and saw john sleeping in his half tent and you have a hypothesis he saw john was asleep and then you give them both to a model and this is the model that we had before that gets some good accuracy and the model is supposed to tell whether the hypothesis is sort of implied by the premise or contradicting um so it could be contradicting maybe if the hypothesis is you know john was awake for example or he saw john was awake maybe that'd be a contradiction neutral if sort of both could be true at the same time so to speak and then entailment in this case you know it seems like they're saying that you know the premise implies the hypothesis and so um you know you would say probably this is likely to get the right answer since the accuracy of the model is 95 95 of the time it gets the right answer um and we're going to dig deeper into that uh what if the model is not doing what we think we want it to be doing in order to perform natural language inference so in a data set like multi-nli the authors who gathered the data set will have asked humans to perform the task and you know gotten the accuracy that the humans achieved and models nowadays are achieving accuracies that are around where humans are achieving um which sounds great at first but as we'll see it's not the same as actually performing the uh the task more broadly in the right way so what if the models not doing something smart effectively we're going to use a diagnostic test set of carefully constructed examples that seem like things the model should be able to do to test for a specific skill or capacity in this case we'll use hans so hans is the heuristic analysis for analyze systems data set and it's intended to take systems that do natural language inference and test whether they're using some simple syntactic heuristics what we'll have in each of these cases we'll have some heuristic we'll talk through the definition we'll get an example so the first thing is lexical overlap so the model might do this thing where it assumes that a premise entails all hypotheses constructed from words in the premise so in this example you have um the premise the doctor was paid by the actor and then the hypothesis is the doctor paid the actor and you'll notice that in bold here get the doctor okay and then paid and then the actor right and so if you use this this heuristic you will think that the doctor was paid by the actor implies the doctor paid the actor that does not imply it of course and so you know you could expect a model you want the model to be able to do this it's somewhat simple but if it's using this heuristic it won't get this example right next is uh subsequence uh heuristics so here at the prem if the model assumes that the premise entails all of its contiguous sub sequences it will get this one wrong as well so this example is the doctor near the actor danced that's the premise the hypothesis is the actor danced now this is a simple syntactic thing the doctor is doing the dancing near the actor is this prepositional phrase and so the model sort of uses this heuristic oh look the actor dance that's a subsequence entailed awesome then it'll get this one wrong as well and um here's another one that's a lot like subsequence but so if if the premise if the model thinks that the premise entails all complete subtrees so this is like sort of fully formed phrases so the artist slept here is a fully formed sort of it's a sub tree if the artist slept the actor ran and then that's the premise does it entail the hypothesis the actor slept uh no uh sorry the artist slept that does not entail it because this is in that conditional okay i only pause here for some questions before i move on to see how these models do anyone unclear about how this sort of evaluation is being set up nope okay cool okay okay so um so how do models perform that's sort of the question of the hour um what we'll do is uh you know we'll look at these results from the same paper that really released the data set so they took four strong multi-analyze models with the following accuracies so the accuracies here are something between 60 and you know 80 something 80 percent bert over here is doing the best okay and um in domain right in that first sort of setting that we talked about uh you get these reasonable accuracies and that is sort of what we said before about it like looking pretty good and when we evaluate on hans in this setting here we have examples where the heuristics we talked about actually work so if the model is using the heuristic it will get this right and it gets very high accuracies and then if we evaluate the model in the settings where if it uses the heuristic it gets the examples wrong um you know maybe bert's doing like epsilon better than some of the other stuff here but it's uh it's it's it's a very different story okay and you saw those examples they're not complex in our sort of own idea of complexity and so this is why it sort of feels like a clear failure of the system now you can say though that well maybe the training data sort of wasn't didn't have any of those sort of phenomena so the model couldn't have learned uh not to do that and that's sort of a reasonable argument except well you know bert is pre-trained on a bunch of language text so you might hope you might expect you might hope that it does better okay so so we saw that example of um models performing well on examples that are like those that it was trained on and then performing not very well at all on examples that seem reasonable uh but are um sort of a little bit tricky now we're going to take this idea of having a test set that we've carefully crafted and go in a slightly different direction so we're going to have what does it mean to try to understand the linguistic properties of our models does it so that syntactic heuristics question was one thing for natural language inference but can we sort of test how the models whether they think certain things are sort of right or wrong as language models and the first way that we'll do this is we'll ask well how do we think about sort of what humans think of as good language how do we evaluate their sort of preferences about language and one answer is minimal pairs and the idea of a minimal pair is that you've got one sentence that sounds okay to a speaker so this sentence is the chef who made the pizzas is here it's called it's an acceptable sentence at least to me um and then with a small change a minimal change um uh the sentence is no longer okay to the speaker so the chef who made the pizzas are here and um this whoops this should be present tense verbs in english present tense verbs agree in number with their subject uh when they are third person um so chef pizzas okay um and um uh this is sort of a pretty general thing most people don't like this it's a misconjugated verb and so the the syntax here looks like you have the chef who made the pizzas and then uh this arc of agreement and number is requiring uh the word is here to be singular is instead of plural are despite the fact that there's this ver this noun pizzas which is plural closer linearly comes back to dependency parsing we're back okay um and what this this looks like in the tree structure right is well you know chef and is um are attached in the tree um you know chef is the subject of is pizzas is down here in this sub tree and so that subject-verb relationship has this sort of agreement uh thing so um this is a pretty sort of basic and interesting property of language that also reflects the syntactic sort of hierarchical structure of language so we've been training these language models sampling from them seeing that they get interesting things and they tend to seem to generate syntactic content but does it really understand or does it behave as if it understands this idea of agreement more broadly and does it sort of get the syntax right so that it matches the subjects and the verbs um so but language models can't tell us exactly whether they think that a sentence is good or bad they just tell us the probability of a sentence uh so um before we had acceptable and unacceptable that's what we get from humans um and the language models analog is just does it assign higher probability to the acceptable sentence in the minimal pair right so you have the probability under the model of um the chef who made the made the pizzas is here and then you have the probability under the model of the chef maids that made the pizzas are here and you want this probability here to be higher and if it is that's sort of like a a simple way to test whether the model like got it right effectively and just like in hans we can develop a test set with very carefully chosen properties right so most sentences in english don't have terribly complex subject-verb agreement structure with a lot of words in the middle like pizzas that are going to make it difficult so if i say you know um the dog runs sort of no way to get it wrong because there's no this index is very simple um so we can create or we can look for sentences that have these the things called attractors in the sentence so pizzas is an attractor because the model might be attracted to the plurality here and get the conjugation wrong so this is our question can language models sort of very generally handle these examples with attractors so we can take examples with zero attractors see whether the model gets the minimum pairs evaluation right we can take examples with one attractor two attractors you can see how people would still reasonably understand these sentences right the chef who made the pizzas and prep the ingredients is it's still the chef who is and then you know on and on and on it gets rarer obviously but but you can have more and more attractors and so now we've created this test set that's intended to evaluate this very specific linguistic phenomenon um so in this paper here concur it all trained an lcm language model on a subset of wikipedia back in 2018 and they evaluate it sort of in these buckets that are specified by uh the paper um the sort of under introduced subject verb agreement to uh to the nlp field more recently at least and they evaluate it in buckets based on the number of attractors and so in this table here that you're about to see the the numbers are sort of the percent of times that you get this assigned higher probability to the correct uh sentence in the minimal pair so if you were just to do random or majority class you get these errors oh sorry it's the percent of times you get it wrong sorry about that so lower is better um and so with no attractors you get very low error rates so this is 1.3 error rate with a 350 dimensional lstm um and uh you know with uh with one attractor your error rate is higher but actually humans start to get errors with more attractors too um so zero attractors is easy uh the larger the lstm it looks like in general the better you're doing right so the smaller models doing worse okay and then even on sort of very difficult examples with four attractors which try to think of an example in your head like the chef made the pizzas and took out the trash you know sort of has to be this long sentence and the air rate is definitely higher so it gets more difficult but it's still it's still relatively low and so even on these very hard examples models are actually performing subject verb number agreement relatively well very cool okay uh here's some examples that a model got wrong this is actually a worse model than the ones from the paper that was just there but i think actually the errors are quite interesting so here's a sentence the ship that the player drives has a very high speed now this model thought that was less probable than the ship that the player drives have a very high speed my hypothesis right is that it sort of mis analyzes drives as a plural noun for example it's sort of a difficult construction there i think it's pretty interesting um likewise here this one is so is is fun uh the lead is also rather long five paragraphs is pretty lengthy um so here five paragraphs is a singular noun together because it's like a unit of length i guess um but but the model thought that it was more likely to say five paragraphs are pretty lengthy um because it's referring to this sort of five paragraphs as the five actual paragraphs themselves as opposed to a single unit of length describing the lead fascinating okay um maybe questions again um so i guess there are a couple can we do the similar heuristic analysis for other tasks such as q a classification yes um so yes i think that it's easier to do this kind of analysis for like the hans style analysis with uh with question answering um and other other sorts of tasks because you can construct examples that similarly uh you know have these heuristics um [Music] uh and then have the answer depend on the syntax or not you know the actual probability of one sentence is higher than the other of course is sort of a language model dependent thing but um the idea that you can sort of like develop kind of bespoke test sets for various tasks um i think is very very general and um something i think is actually quite uh quite interesting um yes so i won't i'll go on further but i think the answer is just yes so there's another one um how do you know where to find these failure cases maybe that's the right time to advertise linguistics classes sorry uh you're still very quiet over here how do you find what how do you know where to find these failure cases oh interesting yes how do we know where to find the failure cases that's a good question i mean i think i agree with chris that actually you know thinking about what is uh interesting about things in language is one way to do it i mean the kind of the heuristics that we saw in our language model sorry in our nli models with hans you can kind of imagine that they if the model was sort of ignoring facts about language and sort of just doing this sort of rough bag of words with some extra magic then it would do well about as bad as it's doing here and these sorts of ideas about you know understanding that this statement if the artist slept the actor ran does not imply the artist slept is the kind of thing that um maybe you'd think up on your own but also you'd spend time sort of pondering about and and and thinking broad thoughts about in uh in you know linguistics curricula as well so uh anything else chris um so there's also well i guess someone was also saying i think it's about the sort of intervening verbs example or intervening noun sorry example but the data so the data set itself probably includes mistakes with higher attractors yeah yeah that's a good point um yeah because humans make more and more mistakes as the number of attractors gets larger um on the other hand i think that the mistakes are fewer in written text than in spoken maybe i'm just making that up but that's what i think um but yeah it would be interesting to actually go through that test set and see how many of the errors the really strong model makes are actually due to the sort of observed form being incorrect i'd be super curious okay should i move on yep great okay so um so so what does it feel like we're doing when we are kind of constructing these sort of bespoke small careful test sets for various phenomena well it it sort of feels like unit testing and in fact this sort of idea has been brought in brought to the fore you might say an nlp unit tests but for these nlp neural networks and in particular uh the paper here that i'm sitting at the bottom suggests this minimal function a minimum functionality test you want a small test set that targets a specific behavior that should sound like some of the things that we were that we've already talked about but in this case we're going to get even more specific so here's a single test case we're going to have an expected label what was actually predicted whether the model passed this unit test and the labels are going to be this is going to be sentiment analysis here so negative label positive label or neutral or the three options and the unit test is going to consist simply of uh sentences that follow this template i then negation the positive verb and then the thing so if you if you negation positive verb means you negative verb right and so here's an example i can't say i recommend the food the expected label is negative the answer that the model provided and this is i think a commercial sentiment analysis system i was paused so it predicted positive and then i didn't love the flight it the expected label was negative and then the predicted answer was uh neutral and this commercial sentiment analysis system gets a lot of well you could imagine are pretty reasonably simple examples wrong and so um what rbiro all 2020 showed is that they could actually provide a system um that sort of had this framework of building test cases for nlp models two ml engineers working on these products um and uh give them that interface and they would actually find bugs you know bugs being categories of high error right find bugs in their models that they could then kind of try to go and fix and that this was kind of an efficient way of trying to find things that were simple and still wrong with what should be pretty pretty sophisticated neural systems so i really like this and it's sort of a nice way of thinking more specifically about what are the capabilities um in in sort of precise terms of our models and all together now you've seen problems in uh natural language inference you've seen language models actually perform pretty well at the language modeling objective but then you see uh you just saw an example of a commercial sentiment analysis system sort of should do better and doesn't and um this comes to this really i think broad and important takeaway which is uh if you get high accuracy on the in-domain test set you are not guaranteed high accuracy on even what you might consider to be reasonable out of domain evaluations and life is always out of domain and if you're building a system that will be given to users it's immediately out of domain at the very least because it's trained on text that's now older than the things that the users are now saying so it's a really really important takeaway that your sort of benchmark accuracy is a single number that does not guarantee good performance on a wide variety of things and from a what are our neural networks doing perspective one way to think about it is that models seem to be learning the data set fitting sort of the fine-grained sort of heuristics and statistics that help it fit this one data set as opposed to learning the tasks so humans can perform natural language inference if you give them examples from whatever data set you know once you've told them how to do the task they'll be very generally strong at it but you take your mli model and you test it on hans and it got you know whatever that was below chance accuracy that's not the kind of thing that you want to see so it definitely learns the data set well because the accuracy in domain is high but our models are seemingly not frequently learning uh sort of the mechanisms that we would like them to be learning last week we heard about language models and sort of the implicit knowledge that they encode about the world through pre-training and one of the ways that we sought interact with language models was providing them with a prompt like dante was born in mask and then seeing if it puts high probability on the correct continuation which requires you to access knowledge about where dante was born and we didn't frame it this way last week but this fits into the set of behavioral studies that we've done so far this is a specific kind of input you could ask this for multiple types of multiple people you could swap out dante for other people who swapped out born in for i don't know died in or something um and then you can they're like test suites again and so um it's all connected okay so i won't go too deep into sort of the knowledge of language models in terms of world knowledge because we've gone over it some but you know when you're thinking about ways of interacting with your models this sort of behavioral study can be very very general even though remember we're at still this highest level of abstraction uh where we're just looking at the probability distributions that are defined all right so now we'll go into so we've sort of looked at understanding in in fine-grained areas what our model is actually doing um uh what about sort of why for an individual input is it getting the answer right or wrong and then are there changes to the inputs that look fine to humans but actually make the models uh do a bad job so one study that i love to reference that really draws back into our our original motivation of using lstm networks instead of simple recurrent neural networks was that they could use long context so but like how long is your long short term memory and the idea of kindle kindlewell at all 2018 was shuffle or remove contexts that are farther than some k words away changing k and if the accuracy if the if the predictive ability of your language model the perplexity right doesn't change once you do that it means the model wasn't actually using that context i think this is so cool so on the x-axis we've got how far away from the word that you're trying to predict are you actually sort of corrupting shuffling or removing stuff from the from the sequence and then on the y-axis is the increase in loss so if the increase in loss is zero it means that the model was not using the thing that you just removed because if it was using it it would now do worse without it right and so in uh if you shuffle in the blue line here if you shuffle the history that's farther away from 50 words the model does not even notice i think that's really interesting one it says everything past 50 words of this lstm language model you could have given it in random order and it wouldn't have noticed uh and then two it says that if you're closer than that it actually is making use of the word order that's a pretty long memory okay that's really interesting and then if you actually remove the words entirely you have you can kind of notice that the words are missing up to 200 words away so you don't know the order that you don't care about the order they're in but you care whether they're there or not and so this is an evaluation of well do lstms have long-term memory well this one at least has effectively no longer than 200 words of of memory but also no less so very cool um so that's like a general study for a single model it talks about uh it's it's sort of average behavior over a wide range of examples but we want to talk about individual predictions on individual inputs so let's talk about that so um one way of interpreting why did my model make this decision that's very popular is for a single example what parts of the input actually led to the decision and this is where we come in with saliency maps so a saliency map provides a score for each word indicating its importance to the model's prediction so you've got something like bert here you've got bert bert is making a prediction for this mask the mask rushed to the emergency room to see her patient okay and so and the predictions that the model is making is things with 47 it's going to be nurse that's here in the mask instead or maybe woman or doctor or mother or girl okay and then the saliency map is being visualized here in orange according to this method of saliency called simple gradients which we'll get into emergency her and the sep token let's not worry about the step token for now but emergency in her are the important words apparently and the sub token shows up in every sentence so i'm not gonna yeah um right and so these two together are according to this method what's important for the model to make this prediction to mask and you can see you know maybe some statistics biases etc that is picked up in the predictions and then have it mapped out onto the sentence and this is well it seems like it's really helping interpretability um and uh um yeah i think that this is sort of a a very useful tool and actually this is part of a demo from alan nlp uh that allows you to do this um uh yourself for any sentence that you want um so what's this what's this way of making saliency maps we're not going to go there's there's so many ways to do it we're going to take a very simple one and work through why it sort of makes sense um so the sort of issue is how do you define importance right what does it mean to be important to the model's prediction um and here's one way of thinking about it it's called the simple gradient method uh let's get a little formal you've got words x1 to xn okay and then you've got a model score for a given output class so maybe you've got in the birch example each output class was each output word that you could possibly predict um and then you take the norm of the gradient of the score with respect to each word okay so so what we're saying here is the score right is sort of the unnormalized probability um for that for that class okay so you got a single class you're taking the score it's like how likely it is not yet normalized by how likely everything else is sort of um gradient how much is it going to change if i move it a little bit in one direction or another and then you take the norm to get a scalar from a vector so it looks like this so salience of word i you have the norm bars on the outside gradient with respect to x i so that's if i change a little bit locally x i how much does my score change um so the idea is that a high gradient norm means that if i were to change it locally i'd affect the score a lot and that means it was very important to the decision let's visualize this a little bit so here on the y-axis we've got loss just the loss of the model sorry this should be score it should be score and on the x-axis you've got word space and the word space is like sort of a flattening of the ability to move your word embedding in thousand dimensional space i've just plotted it here in one dimension um and now a high saliency thing you can see that the relationship between what should be score and moving the word in word space you move it a little bit on the x-axis and the score changes a lot that's that derivative that's the gradient awesome love it uh low saliency you move the word around locally and uh the the score doesn't change so that's an interpreter the interpretation is that means that the actual you know identity of this word wasn't that important to the prediction because i could have changed it and the score wouldn't have changed now why are there more methods than this because i'm honestly reading that i was like that sounds awesome that sounds great you know there are sort of a lots of issues with this kind of method and lots of ways of getting around them here's one issue it's not perfect because well maybe your linear approximation that the gradient gives you holds only very very locally right so here the gradient is zero so this is a low saliency word because at the bottom of this parabola but if i were to move even a little bit in either direction the score would like shoot up right so is this not an important word like it seems important to be right there as opposed to anywhere else even sort of nearby in order for the score not to go up so but the the simple gradients method won't capture this because it just looks at the gradient which is that zero right there okay but uh if you want to look into more there's a bunch of different methods that are sort of applied in these papers and um you know i think that there's a good tool for the toolbox okay so um that is one way of explaining a prediction and um you know has some issues like why are individual words being scored as opposed to phrases or something like that um and but for now we're going to move on to another type of explanation and i'm gonna check the time okay cool um actually yeah let me pause for a second any questions about this ah i mean earlier on there were a couple of questions one of them was what are your thoughts what are your thoughts on whether looking at attention weights is a methodologically rigorous way of determining the importance that the model places on certain tokens it seems like there's some back and forth in the literature that is a that's a great question and i probably won't engage with that question as much as i could if we had like a second lecture on this i actually will provide some attention analyses and tell you they're interesting and then i'll sort of say a little bit about um uh you know why they can be interesting without being sort of uh maybe um sort of the end-all of analysis of of uh where information is flowing in a transformer for example um i think the debate is something that we would have to get into in a much longer period of time but look at the slides that i show about attention and the caveats that i provide and let me know if that answers your question first because we have quite a number of slides on it and if not please please ask again and we can chat more about it and maybe you can go on great okay so um i think this is a really fascinating question which also gets at what was important about the input but in actually kind of an even more direct way which is could i just keep some minimal part of the input and get the same answer so here's an example from squad you have this passage in 1899 john jacob ashton iv invested 100 000 for tesla okay and then the answer that is being predicted by the model is going to always be in blue in these examples colorado springs experiments so you've got this passage and the question is what did tesla spend astr's money on that's why the prediction is colorado springs experiments the model gets the answer right which is nice and we would like to think it's because it's doing some kind of reading comprehension but here's the issue it turns out based on this fascinating paper that if you just recruit reduce the question to did you actually get exactly the same you actually get exactly the same answer and in fact with the original question the model had sort of a 0.78 confidence you know probability in that answer and uh with the with the reduced question did you get even higher confidence and that if you give a human this they would not be able to know really what you're trying to ask about so it seems like some things are going really wonky here here's another so here here's sort of like a very high level overview of the method in fact it actually references our input sale and same methods nice it's connected so so um you iteratively remove non-salient or unimportant words so here's a quest here's a passage again talking about football um i think yeah and uh and oh nice okay so the question is where did the broncos practice for the super bowl as the prediction of stanford university um and that is correct so again seems nice and now we're not actually going to you know get the model to be incorrect we're just going to say um how can i change this question such that i still get the answer right so i'm going to remove the word that was least important according to a saliency method so now it's where did the practice for the super bowl already this is sort of unanswerable because you've got two teams practicing you don't even know which one you're asking about so why the model still thinks it's so confident in stanford university makes no sense but you can just sort of keep going and you know now i think here the model stops being confident in the answer stanford university but you know i think this is really interesting just to show that if the model is able to do this with very high confidence it's not reflecting the uncertainty that really should be there because you can't know what you're even asking about okay so what was important to make this answer well at least this part these parts were important because you could keep just those parts and get the same answer fascinating um all right so that's sort of the end of the admittedly brief uh section on thinking about uh input saliency methods and similar things now we're going to talk about actually breaking models and understanding models by breaking them okay cool um so if we have a passage here peyton manning became the first quarterback um [Music] something super bowl age 39 past record held by john elway uh again we're doing question answering we've got this question what was the name of the quarterback who was 38 in the super bowl the prediction is correct looks good now we're not going to change the question to try to uh sort of make the question nonsensical while keeping the same answer instead we're going to change the passage um by adding the sentence at the end which really shouldn't distract anyone this is quarterback well-known quarterback jeff dean you know had jersey number 37 in champ bowl so this just doesn't it's really not even related but now the prediction is jeff dean uh for our nice qa model um and so this shows as well that um it seems like maybe there's this like end of the passage bias as to what where the answer should be for example and so not that's this is an adversarial example where we flipped the prediction by adding something that is innocuous to humans and so sort of like the higher level takeaway is like oh it seems like the qa model that we had that seemed good it's not actually performing qa how we want it to even though it's in domain accuracy it was good um and uh here's another example so you've got this this paragraph with a question what has been the result of this publicity uh the answer is increased scrutiny on teacher misconduct now instead of changing the paragraph we're going to change the question in really really seemingly insignificant ways to change the model's prediction so first what h.a and now you've got this type o l been the result of this publicity the answer changes to teacher misconduct likely a human would sort of ignore this typo or something and answer the right answer and then this is really nuts instead of asking what has been the result of this publicity if you ask what's been the result of this publicity the answer also changes and this is the the authors call this a semantically equivalent adversary uh this is pretty rough and in general uh swapping what for what's in this qa model breaks it pretty frequently and so again when you go back and sort of re-tinker how to build your model you're going to be thinking about these things not just the sort of average accuracy um so that's sort of talking about noise our models robust to noise in their inputs our humans robust to noise there's another question we can ask and so you can kind of go to this popular sort of meme passed around the internet from time to time where you have all the letters in these words scrambled you say according to a research uh or at cambridge university it doesn't matter in what order the letters in a word are right and so it seems like you know i think i did a pretty good job there uh seemingly right we got this noise that's a specific kind of noise and we can be robust as humans to reading and processing the language without actually all that much of a difficulty um so that's maybe something that we might want our models to also be robust to and it's it's very practical as well noise is a part of all nlp systems inputs at all times there's just no such thing effectively as having you know users for example and not having any noise um and so there's a study that was performed on some you know popular machine translation models where you train machine translation models french german and czech i think all to english and you get blue scores these blue scores will look a lot better than the ones in your simon 4 because much much more training data the idea is these are actually pretty strong machine translation systems and that's in in domain clean text now if you add character swaps like the ones we saw in you know in that in that sentence about cambridge the blue scores take a pretty harsh dive not very good and even if you take somewhat a somewhat more natural sort of typo noise distribution here you'll see that you're still getting you know 20-ish yeah very high drops in blue score through simply natural noise and so maybe you'll go back and retrain the model on more types of noise and then you ask oh if i do that is it robust to even different kinds of noise these are the questions that are going to be really important and it's important to know that you're able to break your model really easily so you can then go and try to make it more robust okay um now let's see 20 minutes ah some uh now we're going to i guess yeah yeah so now we're going to look at the representations of our neural networks we've talked about sort of their behavior and then whether we could sort of change or observe reasons behind their behavior now we'll go into less abstraction look more at the actual vector representations that are being built by models and we can answer a different kind of question at the very least than with the other studies the first thing is related to the question that was asked about attention which is that um some modeling components lend themselves to inspection now this is a sentence that i chose somewhat carefully actually because in part of this debate right are they interpretable components we'll see but they lend themselves to inspection in the following way you can visualize them well and you can correlate them easily with various properties so let's say you have attention heads in bert this is from a really nice study that was done here where you look at intention heads of burt and you say you know on most sentences this attention head had one one seems to do this very sort of global aggregation simple kind of operation does this pretty consistently that's cool um is it interpretable well maybe right so it's the first layer which means that this word found is sort of uncontextualized and then um you know but in deeper layers the problem is that like once you do some rounds of attention you've had information mixing and flowing between words and how do you know exactly what information you're combining what you're attending to even it's a little hard to tell and saliency methods more directly sort of evaluate the importance of models but it's still interesting to see at sort of a local mechanistic point of view what kinds of things are being attended to so so let's take another example some attention heads seem to perform simple operations so you have the global aggregation here that we saw already others seem to attend pretty robustly to the next token cool next token is a great signal some heads attend to the sep token uh so here you have attending to sep and then maybe some attend to periods maybe that's sort of a you know splitting sentences together and things like that not things that are hard to do but things that some attention heads seem to pretty robustly perform um again now though deep in the network what's actually represented at this period at layer 11 a little unclear a little unclear okay so some heads though are correlated with really interesting linguistic properties so this head is actually attending to noun modifiers so you've got this the complicated language in the huge new law right that's pretty fascinating even if the model is not like doing this as a causal mechanism to do syntax necessarily the fact that these things so strongly correlate is actually pretty pretty cool and so we have in all of these studies is we've got sort of an approximate interpretation and quantitative analysis relating uh like allowing us to reason about very complicated model behavior they're all approximations but they're they're definitely interesting uh one other example is that of co-reference so we saw some work on co-ref reference and um it seems like this head does a pretty okay job of actually matching up co-referent entities these are in red talks negotiations she her and that's not obvious how to do that this is a difficult task and so it does so you know with some percentage of the time um and again it's sort of connecting very complex model behavior to uh to these sort of interpretable summaries of correlating uh properties other cases you can have individual hidden units that lend themselves to interpretation so here you've got a character level lstm language model each row here is a sentence if you can't read it it's totally okay the interpretation that you should take is that as we walk along the sentence this single unit is going from i think very negative to very positive or very positive to very negative i don't really remember but it's you know tracking the position in the line so it's just a linear position unit and uh pretty robustly doing so across all of these sentences so this is from a nice visualization study way back in 2016 way back here's another cell from that same lstm language model that seems to sort of turn on inside quotes so here's a quote and it turns on okay so i guess that's positive in the blue end quote here and then it's negative here you start with no quote negative in the red see a quote and then blue seems again very interpretable also potentially a very useful feature to keep in mind and this is just an individual unit in the lstm that you can just look at and see that it does this very very interesting even farther on this and this is actually a study by some ai and neuroscience researchers is okay we saw that lstms were good at subject verb number agreement um can we figure out the mechanisms by which the lstm is solving the task can we actually get some insight into that and so we have a word level language model the word level language model is going to be a little small but you have a sentence the boy gently and kindly greets the and this cell that's being tracked here so it's an individual hidden unit um one dimension right is actually after it sees boy it sort of starts to go higher and then it goes down to something very small once it sees greets and this cell seems to correlate with the scope of a subject verb number agreement instance effectively so here the boy that watches the dog that watches the cat greets you've got that cell again staying high maintaining the scope of subject until greets and at which point it stops what allows it to do that probably some complex other dynamics in the network but it's still a fascinating i think insight um and yeah this is just you know neuron 1150 in this lstm now so those are sort of all observational studies that you could do by picking out individual components of the model that you can sort of just take each one of and correlating them with some behavior now we'll look at a general class of methods called probing by which we still sort of use supervised knowledge like the knowledge of the type of co-reference that we're looking for but instead of seeing if it correlates with something that's immediately interpretable like a attention head we're going to look into the vector representations of the model and see if these properties can be read out by some simple function to say oh maybe this property was made very easily accessible by my neural network so let's dig into this so the general paradigm is that you've got language data uh that goes into some big pre-trained transformer with fine-tuning and you get state-of-the-art results uh soda means state-of-the-art right and so the question for the probing sort of methodology is like if it's providing these general purpose language representations you know what does it actually encode about language like can we can we quantify this can we figure out what kinds of things is learning about language that we seemingly now don't have to tell it and um so you might have something like a sentence like i record the record that's an interesting sentence and you put it into your your transformer model with its word embeddings at the be at the beginning maybe some layers of self-attention and stuff and you make some predictions and now our objects of study are going to be these intermediate layers right so it's a vector per word or sub word uh for every layer and the question is like can we use these linguistic properties like the dependency parsing that we had way back in the early part of the course um to understand uh sort of correlations between properties and the vectors and these things that we can interpret we can interpret dependency parses so so there are a couple of things that we might want to look for here you might want to look for semantics so here in the sentence i record the record uh i am an agent that's a semantics thing uh record is a patient it's the thing i'm recording okay you might have syntax so you might have the syntax tree that you're interested in that's the dependency parse tree maybe you're interested in part of speech right because you have record uh and record and uh the first one's a verb the second one is a noun they're identical strings does the model sort of encode that one is one and the other is the other um so how do we do this kind of study um so we're going to decide on a layer that we want to analyze and we're going to freeze burt the we're not going to fine-tune bird all the parameters are frozen so we decide on layer two of bert we're gonna pass it some sentences we decide on a on a what's called a probe family and the question i'm asking is can i use a model from my family say linear to decode a property that i'm interested in really well from this layer so it's indicating that this property is easily accessible to linear models effectively so maybe i get i train a model i train a linear classifier right on top of bert and i get a really high accuracy and that's sort of interesting already because you know from prior work and part of speech tagging that if you run a linear classifier on simpler features that aren't burnt you probably don't get as high in accuracy so that's an interesting sort of takeaway but then you can also take like a baseline so i want to compare two layers now so i've got layer one here i want to compare it to layer two i train a probe on it as well maybe the accuracy isn't as good and now i can say oh wow look by layer 2 part of speech is more easily accessible to linear functions than it was at layer 1. so what did that well the self-attention and feed forward stuff made it more easily accessible that's interesting because it's a statement about sort of the information processing of the model okay okay so that's we're going to analyze these layers let's take a second more to think about it and just really give me just a second so if you have the model's representations h1 to ht and you have a function family f that's the subset linear models or maybe you have like a feed forward neural network some fixed set of hyper parameters freeze the model train the probe so you get some predictions for part of speech tagging or whatever that's just the probe applied to the hidden state of the model the probe was a member of the probe family and then the extent that we can predict why is a measure of accessibility so that's just kind of written out not as pictorially okay so i'm not going to not going to stay on this for too much longer and you know it may help in the search for causal mechanisms but it sort of just gives us a rough understanding of sort of processing of the model and what things are accessible at what layer so what are some results here so one result is that bert if you run linear probes on it does really really well on things that require syntax and part of speech named entity recognition actually in some cases approximately as well as just doing the very best thing you could possibly do without without bert so it just makes easily accessible amazingly strong features for these properties and that's an interesting sort of emergent quality of burt you might say it seems like as well that the layers of bert have this property where so if you look at the columns of this of this plot here each column is a task you've got input words at the sort of layer zero albert here layer 24 is the last layer of bur at large lower performance is yellow higher performance is blue and io the resolution isn't perfect but consistently the best place to read out these properties is somewhere a bit past the middle of the model uh which is this is a very consistent rule which is fascinating um and then it seems as well like uh if you look at this function of increasingly abstract or increasingly difficult to compute linguistic properties on this axis and increasing depth in the network on that axis so the deeper you go in the network it seems like the more easily you can access more and more abstract linguistic properties suggesting that that accessibility is being constructed over time by the layers of processing of bert so it's building more and more abstract features which i think is again sort of really interesting result um and now i think yeah one thing that i think comes to mind uh that really brings us back right to day one is um we built intuitions around word to back we were asking like what does each dimension of word to vec mean and the answer was uh not really not really anything but we could build intuitions about it and think about properties of it through sort of these connections between simple mathematical properties of word to vect and linguistic properties that we could sort of understand so we had this approximation just not not 100 true but an approximation that says cosine similarity is effectively correlated with semantic similarity think about even if all we're going to do at the end of the day is fine tune these word embeddings anyway um likewise we had this sort of idea about the analogies being encoded by linear offsets so some relationships are linear in space and they didn't have to be that's fascinating it's this emergent property that we've now been able to study since we discovered this why is that the case in word to vec and in general even though you can't interpret the individual dimensions of of word to back these sort of emergent interpretable connections between approximate linguistic ideas and sort of simple math on these objects is fascinating and so one piece of work that sort of extends this idea um comes back to dependency parse trees so they describe the syntax of sentences um and in a paper uh that i did with uh with chris um [Music] we showed that actually bert and models like it uh make dependency parse tree structure emergent uh sort of more easily accessible than one might imagine in its vector space so if you've got a tree right here the chef who rented the store was out of food what you can sort of do is think about the tree in terms of distances between words so you've got the number of edges in the tree between two words is their path distance so you've got sort of that the distance between chef and was is one and we're going to use this interpretation of a tree as a distance to make a connection with bert's embedding space and what we were able to show is that under a single linear transformation the squared euclidean distance between bert vectors for the same sentence actually correlates well if you choose the b matrix right with the distances in the tree so here in this euclidean space that we've transformed the approximate distance between chef and was is also one likewise the difference between was and store is four in the tree and in my simple sort of transformation of bert space the distance between store and was is also approximately four and this is true across a wide range of sentences and this is like to me a fascinating example of again emergent approximate structure in these very non-linear models that don't necessarily need to encode things so simply okay all right great so um probing studies and correlation studies are i think interesting and point us in directions to build intuitions about models but they're not arguments that the model is actually using the thing that you're finding to make a decision not causal studies this is for probing and correlation studies so in some work that i did around the same time we showed actually that certain conditions on probes allow you to achieve high accuracy on a task that's effectively just fitting random labels and so there's a difficulty of an inter of interpret interpreting what the model could or could not be doing with this thing that is somehow easily accessible it's interesting that this property is easily accessible but the model might not be doing anything with it for example because it's totally random likewise another paper showed that you can achieve high accuracy with a probe even if the model is trained to know that thing that you're probing for is not useful um and there's causal studies that sort of try to extend this work it's much more difficult but read this paper and it's a fascinating line of future work now in my last you know two minutes um i want to talk about or casting model tweaks and ablations as analysis um so we had this improvement process where we had a network that was going to work okay and we would see whether we could tweak it in simple ways to improve it and then you could see whether you could remove anything and have it still be okay and that's kind of like analysis like i have my network do i want it to like is it going to be better if it's more complicated if it's going to be better if it's simpler can i get away with it being simpler and so one example of some folks who did this is they took this idea of multi-headed attention and said so many heads all the head's important and what they showed is that if you train a system with multi-headed attention and then just remove the heads at test time and not use them at all you can actually do pretty well on the original task not retraining at all without some of the attention heads showing that they weren't important you could just get rid of them after training and likewise you can do the same thing for this is our machine translation this is on multi-nli you can actually get away without a large large percentage of your attention heads uh let's see yeah so um another thing that you could think about is questioning sort of the the basics of the models that we're building so we have transformer models that are sort of self-attention feed forward self-attention feed forward but like why in that order with some of the things emitted here and the uh this paper asked this question and said if this is my transformer self-attention feed forward self-attention feed forward etc etc etc uh what if i just reordered it so that i had a bunch of self-attentions at the head and a bunch of feed forwards at the back and they tried a bunch of these orderings and this one actually does uh better so this achieves a lower perplexity on a benchmark and this is a way of analyzing what's important about the architectures that i'm building and how can they be changed in order to perform better so neural models are very complex and they're difficult to characterize and impossible to characterize with a single sort of statistic i think for your test set accuracy especially in domain and we want to find intuitive descriptions of model behaviors but we should look at multiple levels of abstraction and none of them are going to be complete when someone tells you that their neural network is interpretable i encourage you to engage critically with that it's not necessarily false but like the levels of interpretability and what you can interpret these are the questions that you should be asking because it's going to be opaque in some ways almost definitely um and then you know bring this sort of i this this this lens to your model building as you try to think about how to build better models even if you're not going to be doing analysis as sort of one of your main driving goals uh and with that you know good luck on your final projects i realize we're at time um the teaching staff is really appreciative of of your efforts um over this difficult quarter and uh yeah hope so um yeah i guess there's a lecture left on thursday but yeah this is my last one so thanks everyone