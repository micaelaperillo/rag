hello welcome to cs224n today we'll be talking about pre-training uh which is another exciting topic on the road to Modern natural language processing um okay how is everyone doing thumbs up some side thumbs down wow no response bias there all you know all thumbs up oh sorry nice I like that Honesty that's good well um okay so we're now uh what is this week five yes it's week five and um we have a couple so this lecture um the Transformers lecture and then to a lesser extent Thursday's lecture on natural language Generation Um will be sort of the sum of lectures for the assignments you have to do right so assignment five is coming out on uh Thursday um and uh the the topics covered in this lecture and the you know self-attention Transformers and again a little bit of natural language generation will be tested in assignment five and then the rest of the course will go through some really fascinating topics and sort of modern uh natural language processing that should be useful for your final projects and future jobs and interviews and intellectual curiosity and um but uh you know I think that this today's lecture is significantly less um uh technical in detail than last Thursdays on self-attention and Transformers but should give you an idea of this sort of uh world of pre-training and sort of how it helps Define uh natural language processing today um so a reminder about assignment five your project proposals also are due on Tuesday next Tuesday um please do get those in try to get them in on time so that we can give you prompt feedback about your project proposals um and yeah so let's let's jump into it okay so uh what we're going to start with today is um a bit of a technical detail on uh word structure and sort of how we model the input sequence of words that we get so um in when we were teaching word to VEC and uh sort of all the methods that we've talked about so far we assumed a finite vocabulary right so we had a vocabulary V that you define via whatever you've looked at some data you've decided what the words are in in that data and so you know um you have some words uh like hat and learn and uh you know you have this embedding it's in red because you've learned it properly actually let's replace hat and learn with pizza and tasty those are better um and uh and so that's all well and good you see these words uh in your model and you have an embedding that's been learned on your data uh to sort of know what to do when you see those words but when you see some sort of variations maybe you see like tasty and maybe a typo like learn um or or maybe novel items where it's like a word that you know you as a human can understand as sort of this combination this is called derivational morphology uh of like this word Transformer that you know and if I which means you know take this noun and give me back you know a verb that means to make more like that noun to Transformer if I NLP might mean to you know make NLP more like using Transformers and such um and for each of these right this maybe didn't show up in your in your training Corpus and language is uh always doing this right people are always coming up with new words and there's new domains and there's the you know young people are always making new words it's great and so it's a problem for your model though right because you've defined this finite vocabulary and there's sort of no mapping in that vocabulary for each of these things even though their meanings should be relatively well defined based on the data you've seen so far it's just that the sort of string of characters that Define them aren't quite what you've seen and so what do you do well maybe you map them to this sort of universal unknown tokens this is Unk uh right so it's like oh I see something I don't know what I've never seen it before I'm going to say it's always represented by the same token ankh um and so that's been done in the past uh and that's sort of bad right because it's totally in like losing tons of information um but you know you need to map it to something uh and so that this is like a clear problem especially I mean in English it's a problem in many of the world's languages it's a substantially larger problem right so um you know English has relatively simple word structure there's a couple of conjugations for each verb like you know eat eats eaten ate um but in a language uh with much more complex morphology or word structure um you'll have a considerably more complex uh sort of set of things that you could see in the world so here is a a conjugation table for a Swahili verb and it has over 300 conjugations and if I Define the vocabulary to be every unique string of characters uh maps to its own word then every one of the 300 conjugations would get an independent Vector under my model which makes no sense because the 300 conjugations obviously have a lot in common and differ by sort of meaningful uh extents so you don't want to do this I'd have to have a huge vocabulary uh if I wanted all conjugations to show up and that's that's a mistake for efficiency reasons and for learning reasons any questions so far cool okay um and so what we end up uh doing um is we'll look at subword sub word structure sub word modeling so what we're going to do is we're going to say I'm not going to even try to Define what the set of all words is I'm going to Define my vocabulary to include parts of words there where am I oh uh right so um so I'm going to split words into sequences of known sub words and so there's a simple sort of algorithm for this where you start with all characters right so if I only had a vocabulary of all characters and maybe like an end of word symbol um I ha for a finite data set then I could no matter what word I saw in the future as long as I had seen all possible characters I could take the word and say I don't know what this word is I'm going to split it into like all of its individual characters so you won't have this UNC problem you can sort of represent any word and then you're going to find common adjacent characters and say okay A and B co-occur next to each other quite a bit so I'm going to add a new word to my vocabulary now it's all characters plus this new word a b which is a sub word and likewise I'm going so now I'm going to replace the character pair with the new sub word and repeat until you add a lot a lot a lot of vocabulary items through this process of what things tend to co-occur next to each other and so what you'll end up with is a vocabulary a very commonly a co-occurring sort of substrings by which you can build up words and this was originally developed for machine translation but then has been used considerably in pretty much all modern language models so now we have a hat and learn hat and learn so in our sub word vocabulary hat and learn showed up enough that they're their own individual words so that's sort of good right so simple common words show up as a word in your vocabulary just like you'd like them to but now tasty maybe gets split into TAA and then maybe you know in some cases this Hash Hash means like don't add a space next right so TAA and then AAA and then s-t-y right so I've actually taken one sort of thing that seems like a word and in my vocabulary it's now split into three sub word tokens so when I pass this to my Transformer or to my recurrent neural network right the recurrent neural network would take TAA as a as just a single element do the RNN update and then take AAA do the RNA and update and then sty so it could learn to process constructions like this and maybe I can even add more aaas in the middle right and have it do something similar instead of just seeing the entire word tasty and not knowing what it means is that that's feedback yeah uh how loud is that feedback we good okay I think we're fixed great um and so same with Transformer if I maybe Transformer as its own word and then if I and so you can see that you have sort of three learned embeddings instead of one sort of useless unkem betting this is just wildly useful and is used pretty much everywhere variants of this algorithm are used pretty much everywhere in uh like modern NLP questions yes if we have three embeddings for tasty do we just add them together so the question is if we have three embeddings for tasty do we just add them together uh if we want to represent so when we're actually processing the sequence I'd see something like I learned about the TAA AAA sty so it'd actually be totally separate tokens but if I wanted to then say what's my representation of this thing uh depends on what you want to do sometimes you average you average the contextual representations of the three or look at the last one maybe it at that point it's unclear what to do but everything sort of works okay click how do you what how do you know where to split yeah so um you know where to split based on the algorithm that I uh specified earlier for learning the vocabulary so you've learned this vocabulary by just combining commonly co-occurring adjacent strings of letters right so like a b co-occurred a lot so now I've got a new word that's a b um and then when I'm actually walking through and tokenizing I try to split as little as possible so I split words into the maximal uh sort of sub word that takes up the most characters they're algorithms for this uh yeah so like I'm like okay if I want to split this up you know like there's many ways I could split it up and you try to find some approximate like what the best way to split it into the fewest words is yeah do I ask the question is do people make use punctuation in the character set how do people do it yes absolutely so you know sort of from this point on so uh just assume that what text is given to these models is as unprocessed as possible you take it you try to make it sort of clean looking text where you've removed you know HTML tags maybe if it's from the Internet or or whatever um but then beyond that you process it as little as possible so that it reflects as well as possible what people might actually be using this for um so maybe earlier in the course when we were looking at word to VEC maybe we had what might have thought about oh we don't want word to vectors of punctuation or something like that um now everything is just as close as possible to what the text you'd get with people trying to use your system would be so yes uh in practice punctuation and like dot dot dot might be its own word you know and and maybe a sequence of like hyphens because people make big bars across you know tables yeah foreign [Music] could be multiple embeddings versus a single embedding like this is the like system tree those any differently uh the question is does the system treat any differently words that are like really themselves the whole word versus words that are sort of pieces you know the system has no idea they're all just indices into your embedding vocabulary Matrix um so they're all treated equally about really long ones that are I guess relatively common because if you're building up from the same character all the way up what happens then yeah the question is what happens to very long words uh if you're building up some sort of character Pairs and portions of characters uh you know in practice the statistics speak really well for themselves so if a long word is very common it will end up in the vocabulary and uh if it's not very common it won't um there are algorithms that aren't this that do slightly better in various ways um but the intuition that you sort of figure out what the common co-occurring substrings are sort of independent of length almost is is the right intuition to have and so yeah you can actually just look at the Learned vocabularies of a lot of these models and uh you see some long words uh just because they if they showed up a lot I'm curious how does it weigh the uh like the frequency so let's say there's like if ify or at the in your next slide it was like goodbye um at the very last one so if could be really common so how did the weight like the frequency of a sub word versus the length of it like it tries to spread it up into the smallest number but what if it split it up into three but one of them was super common yeah so the question is uh you know if Transformer is a sub word in my vocabulary and if it's a sub word and Y is a sub word and if I as a three letter Tuple is also a sub word how does it choose to like take the you know if I maybe it's not very common uh as opposed to splitting it into more subwords um it's just a choice we choose to try to take the smallest number of sub words because that tends to be uh more of the bottleneck as opposed to the having a bunch of very common very short sub words uh trans you know sequence length is a big problem in Transformers and this seems to be sort of what works although trying to split things into multiple options of a sequence and running the Transformer on all of them is the thing that people have done to see which one will work better but yeah having fewer bigger sub words tends to be the best sort of idea I'm going to start moving on though uh feel free to ask me more questions about this afterward okay so um so let's talk about pre-training from the context of the course so far uh so we at the very beginning of the course we gave you this quote which was you know you shall know a word by the company it keeps this was the sort of thesis of the distributional hypothesis right that the meaning of the word is defined by or at least reflected by what words it tends to co-occur around and we implemented this via word to VEC uh the same person who made that quote had a separate quote actually earlier uh that continues this sort of notion of meaning as defined by context which has something along the lines of well you know since the word shows up in context when we actually use it when we speak to each other the meaning of the word should be defined in the context that it actually shows up in and so uh you know the complete meaning of a word is always contextual and no study of meaning apart from a complete context can be taken seriously so right the big difference here is like at word tovec training time if I have uh the word uh record r-e-c-o-r-d when I'm training word to VEC I get one vector or two but you know one one vector meaning uh record the string um and uh it has to learn by what context it shows up in that sometimes you know it can mean I record I.E the verb or record I.E the noun right but I only have one vector to represent it and so when I use the word embedding of record uh it sort of has this mixture meaning of both of its sort of Senses right it doesn't get to specialize and say oh this part means record and this part means record and so where to back is going to just sort of fail um and and so I can build better representations of language through these contextual uh representations that are going to take things like recurrent neural networks or Transformers that we used before to build up sort of contextual meaning uh so so what we had before were pre-trained word embeddings and then we had sort of a a big box on top of it like a Transformer or an lstm that was not pre-trained right so so you learn via context your word embeddings here and then uh you have a task like sentiment analysis or machine translation or or parsing or whatever and you initialize all the parameters of this randomly and then you train uh to predict your label and the the big difference uh in you know today's work is that we're going to try to pre-train all the parameters so I have my big Transformer and instead of just you know pre-training my word embeddings with word to VEC I'm going to train all of the parameters of the network uh trying to teach it you know much more about language uh that I could use in my in my Downstream tasks so now I'm sort of the the labeled data that I have for say machine translation might need to be smaller I might not need as much of it because I've already trained much more of the network than I otherwise would have if I had just gotten sort of word to back embeddings okay so so here right I've pre-trained this entire sort of structure the word embeddings the Transformer on top everything's been trained via methods that we'll talk about today and so what does this give you I mean it gives you very strong representations of language so the meaning of record and record will be different in the sort of contextual representations that know what where in the sequence it is and what words are co-occurring with it in the specific input then word to back which only has one representation for record independent of where it shows up it'll also be used as strong parameter initializations for NLP models so in all of your homework so far you've worked with uh you know building out a natural language processing system sort of from scratch right like how do I initialize this weight Matrix and we always say oh you know small normally distributed noise like little values you know uh close to zero and here we're going to say well just like we were going to you know use the word to VEC embeddings and those sort of encoded structure I'm going to start maybe my machine translation system from a parameter initialization that's given to me via pre-training and then also it's going to give us probability distributions over language that we can use to to generate and otherwise and we'll talk about this okay so whole models are going to be pre-trained so um all of pre-training is effectively going to be centered around this idea of reconstructing the input so you have an input it's a sequence of text that some human has generated and the sort of hypothesis is that by masking out part of it and tasking a neural network with reconstructing the original input that neural network has to learn a lot about language about the world in order to do a good job of reconstructing the input right so this is now a supervised learning problem just like you know machine translation right I've taken this sentence that just existed Stanford University is located in say Palo Alto California or Stanford California I guess um and I have by removing this you know part of the sentence uh made a label for myself right the input is this sort of broken uh mask sentence and the label is Stanford or Palo Alto so if I give this example to a network and ask it to predict the center thing as it's doing its gradient step on this input it's going to encode information about the co-occurrence between this context Stanford University is located in and Palo Alto so by tasking it with this it might learn say where Stanford is what else might it learn well it can learn things about maybe syntax so I put blank Fork down on the table um here there's only a certain set of words that could go here I put the fork down on the table I put a fork down to the table these are syntactic constraints right so the context shows me sort of What kinds of words can appear in what kinds of contexts the woman walked across the street checking for traffic over blank shoulder any ideas on what could go here right so um this sort of this sort of co uh co-reference between this entity who is being discussed in the world this woman and her shoulder now when I discuss you know this is sort of linguistic concept the word her here is a co-referrent to woman right it's referring to the same entity in the discourse and so the network might be able to learn things about you know like kind of what entities are doing what where uh it can learn things about sort of semantics so if I have I went to the ocean to see the fish Turtle seals and blank then the word that's in the blank should be sort of a member of the class that I'm thinking of as a person writing this sentence of stuff that I see when I go to the ocean and see these other things as well right so in order to do this prediction task maybe I learned about you know the semantics of of uh aquatic creatures okay so what else could I learn I've got overall the value I got from the two hours watching it was the sum total of the popcorn and drink the the movie was blank what kind of task could I be learning from doing this sort of prediction problem sentiment exactly so this is just a naturalistic sort of text that I naturally wrote uh myself um but by saying oh the movie was bad I'm learning about sort of the latent sentiment of the person who wrote This what they were feeling about the movie at the time so maybe if I see a new review later on I can just paste in the review say the movie was blank and if the model generates bad or good that could be implicitly solving the task of sentiment analysis so here's another one Ira went to the kitchen to make some tea standing next to Ira Zuko pondered his Destiny Zuko left the blank okay so in this scenario we've got a world implicitly that's been designed by the person who is creating this text right I've got physical locations in the discourse like the kitchen uh and I've got Zuko uh we've got iros in the kitchen Zuko's next to iro so Zuko must be in the kitchen so what could Zuko leave but the kitchen right and so in terms of you know latent Notions of embodiment and physical location the way that people talk about people you know being next to something and then leaving something could tell you uh stuff about sort of yeah a little bit about how the world works even so here's the secret sequence I was thinking about the sequence that goes one one two three five eight thirteen twenty one uh blank and um you know this is a pretty tough one right this is the Fibonacci sequence right create a model by looking at the a bunch of numbers from the Fibonacci sequence learn to in general predict the next one a question you should be thinking about throughout the lecture okay any questions on these sort of examples of what you might learn from predicting the context okay okay cool um so uh you know a very simple way to think about pre-training is pre-training is language modeling so we saw language modeling earlier in the course and now we're just going to say instead of using my language model just to provide probabilities over the next word I am going to train it on that task right I'm going to actually model the distribution P Theta of the word t given all the words previous and there's a ton of data for this right there's you know there's just an amazing amount of data for this in a lot of languages especially English there's very little data for this in actually most of the world's languages which is a separate problem um but you can free train just through language modeling right so I'm going to sort of do the teacher forcing thing so I have IRL I predict goes I have goes I predict two and I'm going to train my sort of lstm or my Transformer to do this task and then I'm just going to keep all the weights okay I'm going to save all the network parameters um and then once I have these parameters right instead of generating from my language model I'm just going to use them as an initialization for my parameters so I have this pre-training fine tuning Paradigm two steps most of you I think in your well maybe not this year let's say a large portion of you this year in your final projects will be doing the pre-training fine-tuning sort of Paradigm where someone has done the pre-training for you right so you have a ton of text you learn very general things about the distribution of words and sort of the latent things that that tells you about the world and about language and then in step two you've got some tasks maybe sentiment analysis and you have maybe not very many labels you have a little bit of labeled data and you adapt the pre-trained model to the task that you care about by further doing gradient steps on this task so you give it the movie was you predict happy or sad and then um you sort of Continue to update the parameters based on the initialization from the pre-training and this just works exceptionally well I mean unbelievably well compared to training from scratch intuitively because you've taken a lot of the burden of learning about language learning about the world off of the data that you've labeled for sentiment analysis and you're sort of giving that task of learning all this sort of very general stuff to the much more General task of language modeling yes but we didn't have much data in other languages what do you mean by that was it just text in that language yeah labeled in some way so the question is uh you know you said we have a lot of data in English but not in other languages um what do you mean by data that we don't have a lot of in other languages is it just text it's literally just text no annotations because you don't need annotations to do language model pre-training right the existence of that sequence of words that someone has written provides you with all these pairs of input and output input iro output goes input iro goes output two those are all labels sort of that you've constructed from the input just existing but you know in most languages even on the entire internet I mean there's about 7 000 ish languages on Earth and most of them don't have the sort of you know uh billions of words that you might want to to train these systems on uh yeah entire thing are you still only like one vector representation per word the question is if you're pre-training the entire thing do you still learn one vector representation per word you learn one vector representation that is the non-contextual input vector right so you have your vocabulary Matrix you've got your embedding Matrix that is vocabulary size by model dimensionality and so yeah iro has one vector goes has one vector but then the Transformer that you're learning on top of it takes in the sequence so far and sort of gives a vector to each of them that's dependent on the context in that case but still at the input you only have one embedding per word yeah so what's sort of like metric would you use to like evaluate it's supposed to be like General right so but things like application specific metrics which one do you use yeah so the question is what metric do you use to evaluate pre-trained models since it's supposed to be so General um but there are lots of sort of very specific evaluations you could use um it will get into a lot of that in the rest of the lecture uh while you're training it you can use Simple metrics that sort of correlate with what you want but aren't actually what you want just like the probability quality right so you can evaluate the perplexity of your language model just like you would have when you cared about language modeling and it turns out to be the case that better perplexity correlates with all the stuff that's much harder to evaluate like lots and lots of different tasks but also the natural language processing Community has built very large sort of Benchmark Suites of varying tasks to try to get at sort of a notion of generality although that's very very difficult it's sort of ill-defined even and so when you develop new pre-training methods what you often do is you try to pick a whole bunch of evaluations and show that you do better on all of them you know and and that's your argument for generality okay um so so why should this sort of pre-training fine-tuning two-part Paradigm help uh you know this is still an open area of research but the intuitions are all you're going to take from this course so right so pre-training provides some sort of starting uh parameters L Theta so this is like all the parameters in your network right from trying to do this minimum over all possible settings of your parameters of the pre-training loss and then the fine-tuning process takes uh you know your data for fine tuning you've got some labels and it tries to approximate the minimum through gradient Descent of the loss of the fine-tuning task of theta but you start at Theta hat right so you start gradient descent at Theta hat which your pre-training process gave you and then you know if you could actually solve this Min and wanted to it sort of feels like the starting point shouldn't matter but it really really really does it really does um uh so that's and we'll talk a bit more about this later but um the process of gradient descent you know maybe it sticks relatively close to the Theta hat during fine tuning right so um you know you start at Theta hat and then you sort of walk downhill with gradient descent until you hit sort of a valley and that Valley ends up being really good because it's close to the pre-training parameters which were really good for a lot of things this is a cool place where sort of practice and Theory are sort of like meeting where like optimization people want to understand why this is so useful NLP people sort of just want to build better systems um so uh yeah maybe the stuff around Theta hat tends to generalize well if you want to work on this kind of thing you should talk about it yeah the classic waiting to send sticks relatively close but what if we were to use a different Optimizer how would that change their results the question is uh if stochastic gradient descent sticks relatively close what if we use a different Optimizer I mean if we use sort of any common variant of gradient descent like any first order method like atom which we use in this course or add a grad or they all have this very very similar properties um other types of optimization we just tend to not use so who knows ah yeah yeah fine tuning works better than just fine tuning but making article like adding more layers more data yeah uh the question is why does the pre-trained fine-tuned Paradigm work better than just making the model more powerful adding more layers adding more data to just the fine tuning um that's a you know the simple answer is that you have orders of magnitude more data that's unlabeled that's just text that you found then you do you know carefully labeled data and the tasks that you care about right because that's expensive to get it has to be examples of your movie reviews or whatever that you've had someone label carefully um so you have you know something like on the internet uh at least five trillion maybe 10 trillion words of this and you have maybe a million words of your label data or whatever over here so it's just like the it's just the scale is way off um but there's also an intuition that like learning to do a very very simple thing like sentiment analysis um is not going to get you a very general generally able agent in a wide range of settings compared to language modeling so like it's hard to get how to put it even if you have a lot of labeled data of movie reviews of the kind that people are writing today maybe tomorrow they start writing slightly different kinds of movie reviews and your system doesn't perform as well whereas if you pre-trained on a really diverse set of text from a wide range of sources in people it might be more adaptable to seeing stuff that doesn't quite look like the training data you showed it even if you showed it a ton of training data so one of the sort of big takeaways of pre-training is that you get this huge amount of sort of variety of text uh on the internet you have to be very careful I mean you yeah you should be very careful about what kind of text you're showing it and what kind of text you're not because the internet is full of you know um awful text as well um but some of that generality just comes from how hard this problem is and how much data you can show it so much data how do you then train it so that it considers the stuff that you're fine-tuning it with as like more important more sale into a passive Trend if you rather than just one in a billion articles yeah it's a good question so the question is uh given that the amount of data on the pre-training side is orders of magnitude more than the amount of data on the fine tuning side how do you sort of get across to the model that okay actually the fine tuning task is like what I care about so like focus on that um it's it's about the fact that I did this first the pre-training first and then I do the fine tuning second right so I've done I've gotten my parameter initialization from this I've set it somewhere and then I fine tune I move to where the parameters are doing well for this task afterward and so well it might just forget a lot about how to do this because now I'm just asking it to do this at this point uh I should move on I think um but we're going to keep talking about this in in much more detail with more concrete uh elements so um okay so uh let's talk about model pre-training oh wait that did not Advance the slides nice okay let's talk about model pre-training three ways uh in our Transformers lecture uh Tuesday we talked about encoders encoder decoders and decoders and we'll we'll do decoders last because um actually many of the largest models uh that are being used today Are all uh decoders and so we'll have a bit more to say about them um right so let's recall these three so encoders get bi-directional context you have a single sequence and you're able to see the whole thing kind of like an encoder in machine translation encoder decoders have one uh portion of the network that gets bi-directional context so that's like the source sentence of my machine translation system and then they're sort of paired with a decoder that gets unidirectional context so that I have this sort of uh informational masking where I can't see the future so that I can do things like language modeling I can generate the next token of my translation whatever so you could think of it as you know I've got my source sentence here and my partial translation here and I'm sort of decoding out the translation and then decoders only are things like language models we've seen a lot of this so far and there's pre-training for all three sort of large classes of models and how you pre-train them and then how you use them depends on the properties and the productivities of the specific architecture so let's let's look at encoders first um so we've looked at language modeling quite a bit but we can't do language modeling with an encoder because they get bi-directional context right so if I'm down here uh at I and I want to present I want to predict the next word it's a trivial task at the at this level here to predict the next word because in the middle I was able to look at the next word and so I should just know there's nothing hard about learning to predict the next word here because I could just look at it see what it is and then you know copy it over so when I'm training an encoder in something uh for for pre-training I have to be a little bit more clever in practice what I do is something like this uh I take the input and I modify it somewhat I mask out words sort of like I did in the examples I gave at the beginning of class so I blank to the blank right and then I have the network predict uh with this whole you know I haven't built contextual representations so now this Vector representation of the blank sees the entire context around it here and then I predict the word went and then here the word store any questions okay and you can see how this is doing something quite a bit like language modeling but with you know bi-directional context I've removed the Network's information about the words that go in the blanks and I'm training it to reconstruct that so I only have lost terms right I only ask it to actually do the prediction compute the loss back propagate the gradients for the words that I've masked out and you can think of this as you know instead of learning probability of X where X is like a sentence or a document this is learning the probability of X the real document given X tilde which is this sort of corrupted document with some of the information missing missing okay and so maybe we get the sequence of vectors here one per word which is the output of my encoder in blue and then I'd say that for the words that I want to predict y i i draw them this is the Sim means the probability is uh proportional to uh you know my embedding Matrix times um my representation of it so it's a linear transformation of that last thing here so this a plus b is this red portion here and then do the prediction and I train the entire network to do this yes so far do we just do it as we are is there something you can do it the question is do we just choose words randomly to mask out or is there a scheme mostly randomly we'll talk about a slightly smarter scheme in a couple of slides but yeah just mostly randomly uh yeah what was that last part on the bottom um exit the maps version of like if it's the first or the very last sentence uh yeah so so I'm saying that I'm defining X tilde to be this input part where I've got the masked version of the sentence with these sort of words missing and then I'm defining a probability distribution that's the probability of a sequence conditioned on the input being the sort of corrupted sequence the masked sequence okay um so uh this brings us to a very very popular and uh sort of NLP model that you need to know about it's called Bert and it was the first one to popularize this masked language modeling objective um and they released the weights of this pre-trained Transformer that they pre-trained via something that looks a lot like Mass language modeling and so these you can download you can use them via a code that's released by the company hugging face that we have you know continue to bring up many of you will use a model like Birch in your final project because it's such a useful builder of representations of language and context so let's talk a little bit about the details of mass language modeling in Bert first we'd take 15 of the sub word tokens so remember all of our inputs now are subword tokens I'm making I've made them all look like words but just like we saw at the very beginning of class each of these tokens could just be some portion some sub word and I'm going to do a couple of things with it sometimes I am going to just mask out the word and then you know predict the true word sometimes I'm going to replace the word with some random sample of another word from my distribution from my vocabulary and predict the real word that was supposed to go there and sometimes I'm going to not change the word at all and still predict it the intuition of this is the following um if I just had to build good representations of uh in the sort of middle of this network for words that are masked out then when I actually use the model at test time on some real you know review to do sentiment analysis on well there are never going to be any tokens like this so maybe the model won't do a very good job because it's like oh you know I have no job to do here because I only need to deal with the mask tokens by giving it sequences of words where sometimes it's the real word that needs to be predicted sometimes you have to detect if the word is wrong the idea is that now when I give it a sentence uh that doesn't have any masks it actually sort of does a good job of representing all the words in context because it has this chance that it could be asked to predict anything at any time okay so uh the folks at uh at Google who were defining this had a separate additional task that is sort of interesting to think about so this was their their Bert model from their paper they had their position embeddings just like we saw from our Transformers lecture token embeddings just like we saw from the Transformers lecture but then also they had this thing called a segment embedding where they had two possible segments segment a and segment B and uh they had this additional task where they would get a big chunk of text for Segment a and a big chunk of text for Segment B and then they would ask the model is segment b a real continuation of segment a well so the text that actually came next or did I just pick this big segment randomly from somewhere else and the idea is that this should teach the network something some notion of sort of long distance coherence right about sort of the connection between a bunch of text over here and a bunch of text over there turns out it's not really necessary but it's an interesting idea and sort of similar things have continued to have some sort of influence since then but again you should get this intuition that we're trying to come up with hard problems for the network to solve such that by solving them it has to learn a lot about language and we're defining those problems by making simple Transformations or removing information from text that just happen to occur questions yeah the plus signs do we concatenate the vector so do we do an element-wise Edition uh the question is for these plus signs do we concatenate the vectors or do element wise Edition we do element wise Edition uh you could have concatenated them however the one of the big sort of conventions of all these networks is that you always have exactly the same number of Dimensions everywhere at every layer of the network it just makes everything very simple so just saying everything's the same Dimension and then uh doing addition just ends up being simpler yeah the next sentence prediction not necessarily yeah why was the next sentence prediction not necessary I mean it one thing that it does that's a negative is that now um the sort of content the effective context length for a lot of your examples is halved so one of the things that's useful about pre-training seemingly is that you get to build representations of very long sequences of text so this is very short but in practice segment a was going to be something like 250 words and segment B was going to be 250 words and in the paper that sort of let us know that this wasn't necessary they always had a long segment of 500 Words and it seemed to be useful to always have this very long context because longer contexts help give you more information about the role that each word is playing in that specific context right if I see one word it's hard to know if it's just the record it's hard to know what it's supposed to mean but if I see a thousand words around it it's much clearer what its role is in that context is so yeah it cuts the effective context size as one answer um what another thing is that this is actually much more difficult this is a much more recent paper uh that I don't have in the slides but it's been shown since then that these models are really really bad at the next sentence prediction task so it could be that maybe it just like was too hard at the time uh and so it just like wasn't useful because the model was failing to do it at all so I give the link for that paper later why we need to do a next sentence prediction what about just masking and predicting I missed that jump so yeah so the question is why do we need to do next sentence prediction why not just do the masking we saw before that's the thing you seem to not need to do next to this prediction but you know as sort of like his history of the research it was thought that this was useful and the idea is that it required you to develop this sort of pairwise like do these two segments of text interact how do they interact are they related the sort of longer distance notion and many NLP tasks are defined on pairs of things and they thought that might be useful and so they published it with this and then someone else came through published a new model that didn't do that and it and it sort of did better so you know this is just yeah so yeah uh there are intuitions as to why it could work it just didn't it was doing both it was doing both this next sentence so Bert was doing both this next sentence prediction uh evaluate uh training as well as this masking training uh all at the same time and so you had to have a separate predictor head on top of bird a separate predictor sort of classification thing and you know so one detail there is that there's this special word at the beginning of Bert in every sequence that's CLS and you know you can define a predictor on top of that sort of fake word embedding that was going to say is the next sentence real or fake or not yeah okay I'm gonna move on um and so this gets that sort of the question that we had earlier about how do you evaluate these things um there's a lot of different NLP tasks out there gosh and uh you know when people were defining these papers they would look at a ton of different evaluations that had been sort of compiled as a set of things that are still hard for today's systems so are you detecting paraphrases between questions or two quora questions actually the same question that turns out to be hard um you know uh can you do sentiment analysis on this hard data set can you tell if sentences are linguistically acceptable are they grammatical or not um are two sequences similar semantically do they mean sort of vaguely the similar thing um and we'll talk a bit about natural language inference later but that's the task of defining sort of if I say you know I saw the dog that does not necessarily mean I saw the little dog but saying I saw the little dog does mean I saw the dog so that's sort of this natural language inference task and you know the Striking the difference between sort of pre-pre-training days where you had this sort of this row here before you had substantial amounts of pre-training and Bert was just like the field was taken aback in a way that's hard to describe you know very carefully crafted architectures for each individual task where everyone was designing their own neural network and doing things that they thought were sort of clever as to how to define all the connections and the weights and whatever to do their tasks independently so everyone was doing a different thing for each one of these tasks roughly all of that was blown out of the water by just build a big Transformer and just teach it to predict the missing words a whole bunch and then fine tune it on each of these tasks so this was this was just a sea change in the field people were I mean amazed it's a little bit less flashy than chat GPT I'll admit but it's really part of the story that gets us to it you know um okay questions so like uh to get stuff out of the like the during the encode their pre-training stage encoder usually outputs like uh some sort of hidden values how do we correlate those the words that we are trying to test against so the question is you know the the encoder output is a bunch of you know hidden values um how do we actually correlate those values to stuff that we want to predict I'm going to go on to the next slide here to bring up this this example here right so the encoder gives us for each input word token a vector of that token that represents the token in context and the question is you know how do we get these representations and and turn them into uh sort of answers for the tasks that we care about and um the answer comes back to do [Music] something like this uh something like this Maybe wow sure um so when we were doing a pre-training right we had the Transformer that was giving us our representations and we had this little last layer here this little um sort of affine uh transformation that moved us from the encoder's hidden State size to the vocabulary to do our prediction and we just removed this last prediction layer here and let's say we want to do something that is uh classifying the sentiment of the sentence we just pick arbitrarily maybe the last word in the sentence and we stick a linear classifier on top and map it to positive or negative and then fine tune the whole thing okay so so yeah the Bert model uh had uh two different models one was 110 million parameters one was 340 million keep that sort of in the back of your head sort of percolating as we talk about models with with many many more parameters later on it was trained on um uh 800 million words plus that is definitely wrong maybe two point maybe 25 million words but on the order of less than a billion words of text quite a bit still um and it was trained on what was considered at the time to be a whole lot of compute just you know it was Google doing this and they released it and we were like oh who has that kind of compute but Google although nowadays it's not considered to be very much um but fine-tuning is practical and common on a single GPU so you could take the burp model that they'd spend a lot of time training and fine-tune it yourself on your task uh on even sort of a very a very sort of small GPU uh okay so so one question is like well this seems really great why don't we just use this for uh everything um uh-huh yeah uh and the answer is well you know what is the sort of pre-training objective what's the structure of the pre-trained model good for uh Bert is really good for sort of filling in the blanks but it's much less naturally used for actually generating text right so I wouldn't want to use to generate a summary of something because it's not really built for it it's not it doesn't have a natural notion of predicting the next word given all the words that came before it so maybe I want to use Bert if I want a good representation of say a document to classify it give it one of a set of topic labels or say it's toxic or non-toxic or whatever but I wouldn't want to use it to generate a whole sequence uh okay some extensions of Bert so we had a question earlier of whether you just mask things out randomly one thing that seems to work better is uh you uh mask out sort of whole contiguous spans uh because uh sort of the difficulty of this problem is much easier than it would otherwise be because uh sort of this is part of irresistibly and you can tell very easily based on the sort of sub words that came before it whereas uh if I have a much longer sequence it's a trade-off but you know this might be a harder problem and it ends up being better to do this sort of span-based masking than random masking and that might be because sub words make very simple prediction problems when you mask out just one sub word of a word versus all the subwords of a word okay so those this ends up doing much better there's also a paper called the Roberta paper which showed that the next sentence prediction wasn't neces wasn't necessary they also showed that they really should have trained it on a lot more text so Roberta is a drop-in replacement for Bert so if you're thinking of using just use your Brita it's better and it gave us this intuition that we really don't know a whole lot about the best practices for training these things you sort of train it for as long as you're willing to and things do good stuff and whatever um so this is very but it's very difficult to do sort of uh iteration on these models because they're big it's expensive to train them uh another thing that you should know for your final projects in the world ahead is this notion of fine-tuning all parameters of the network versus just a couple of them uh so what we've talked about so far is you pre-train all the parameters and then you fine-tune all of them as well so all the parameter values change uh an alternative which you call parameter efficient or lightweight fine-tuning uh you sort of choose little bits of parameters or you choose the very smart way of keeping most of the parameters fixed and only fine-tuning others and the intuition is that you know these pre-trained parameters were really good and you want to make the minimal change from the pre-trained model to the model that does what you want so that you keep some of the generality some of the goodness of the pre-training so one way that this is done is called prefix tuning prompt tuning is very similar where you actually freeze all the parameters of the network so I've pre-trained my network here and I've never changed any of the parameter values instead I make a bunch of fake sort of pseudo word vectors that I propend to the very beginning of the sequence and I train just them sort of unintuitive it's like these would have been like inputs to the network but I'm specifying them as parameters and I'm training everything to do my sentiment analysis task just by changing the values of these sort of fake words and this is nice because you know I get to keep all the good pre-trained parameters um and then just specify this sort of diff that ends up generalizing better this is a very open field of research but this is also cheaper because I don't have to compute the gradients or I don't have to store the gradients and all the optimizer state with respect to all these parameters I'm only training a very small number of parameters uh yeah it's like make parameters and as if like here but it doesn't make any difference but he's at the end of the beginning in a decoder you have to put them at the beginning because otherwise you don't see them before you process the whole sequence uh yes a few layers I only train the new layers but the question is can we just attach a new layers of the sort of top of this and only train those absolutely this works a bit better another thing that works well sorry we're running out of time um is taking each weight Matrix so I have a bunch of weight matrices in my Transformer and I freeze the weight Matrix and learn a very low ranked little diff and I set the weight matrix's value to be sort of the original Value Plus my my sort of very low rank diff uh from the original one and this ends up being a very similarly useful technique and the overall idea here is that again I'm learning way fewer parameters than I did Via pre-training and freezing most of the pre-training parameters okay encoder decoders so um for encoder decoders we could do something like language modeling right I've got my input sequence here encoder output sequence here and I could say this part is my prefix for sort of having bi-directional context and I could then predict all the words that are sort of in the latter half of the sequence just like a language model and that would work fine um and so this this is something that you could do right you sort of take it along text split it into two give half of it to the encoder and then generate the second half with the decoder uh but in practice what works much better is this notion of span corruption span corruption is going to show up in your assignment five and the idea here is a lot like Bert but uh in a sort of generative sense where I'm going to mask out a bunch of words in the input thank you mask token one me to your party mask token two week and then at the output I generate the mask token and then what was supposed to be there but the mass token replaced it right so thank you then predict for inviting at the output meet your party last week and what this does is that it um allows you to have bi-directional context right I get to see the whole sequence except I can generate the parts that we're missing so this feels a little bit like you mask out parts of the input but you actually generate the output as a sequence like you would in language modeling so this might be good for something like machine translation where I have an input that I want bi-directional context in but then I want to generate an output and I want to pre-train the whole thing so this was shown to work better than language modeling at the scales that these uh Folks at Google were able to test back in 2018 this is still quite quite popular um yeah there's a lot of numbers it works better than the other stuff I'm not going to worry about it um you know there's a fascinating property of these models also so um so T5 was the model that was originally uh introduced with salience band masking and you can think of you know at pre-training time you saw a bunch of things like Franklin D Roosevelt was born in you know blank and you generated out the blank and uh there's this task called um open domain question answering which has a bunch of trivia questions like you know when was Franklin D Roosevelt born and then you're supposed to generate out the answer as a string just like just from your parameters right so you did a bunch of pre-training you saw a bunch of text and then you're supposed to generate these answers and what's fascinating is that this sort of salience band masking method allowed you to pre-train and then fine tune on some examples of questions trivia questions and then when you tested on new trivia questions it would sort of the model would sort of implicitly extract from its pre-training data somehow the answer to that new question that it never saw explicitly at fine tuning time so it learned this sort of implicit retrieval sometimes sometimes you know less than 50 of the time or whatever but you know much more than random chance yeah um and that's just sort of fascinating right so you've sort of learned to access this sort of latent knowledge that you stored up by pre-training and so yeah you just pass it the text when was Roosevelt born and it would pass out an answer and one thing to know is that the answers always look very fluent they always look very reasonable but they're frequently wrong and that's still true of things like chat gbt um yeah okay so that's that's like encoder decoder models um next up we've got decoders and we'll spend a long time on decoders so this is just our normal language model so I get a sequence of hidden States for my decoder the the models this words can only look at themselves not the future and then I predict you know the next word in the sentence and then here again I can you know to do sentiment analysis maybe take the last state for the last word and then predict happier sad based on that last embedding back propagate the gradients the whole network train the whole thing or do some kind of lightweight or or parameter efficient fine tuning like we mentioned earlier so this is our our you know pre-training a decoder and um you know I can just pre-train it on language modeling um so again you might want to do this if you are wanting to generate generate texts generate things uh this is you sort of can use this like you use an encoder decoder but in practice as we'll see a lot of the sort of biggest uh most powerful pre-trained models tend to be decoder only it's not really clear exactly why except they seem a little bit simpler than encoder decoders um and you get to share all the parameters in one big Network for the decoder whereas an encoder decoder you have to split them sort of some into the encoder some into the decoder so for the rest of this lecture we'll talk only about decoders so even and modern things uh the biggest networks do tend to be decoders so we're coming all the way back again to 2018 and the GPT model from openai was a big success it had 117 parameter a million parameters uh it had you know 768 dimensional hidden States and uh it had this vocabulary that was 40 000 ish words that was defined via a method like what we showed at the beginning of class trained on Books Corpus and um you know actually you know GPT never actually showed up in the original paper uh the sort of uh it's unclear what exactly it's supposed to refer to um but uh this model was a precursor to all the things that you're hearing about nowadays uh if you move forward uh oh yeah so if you um so if we wanted to do something like natural language inference right which says you know take these pairs of sentences the man is in the doorway the person is near the door and uh say that these mean that one entails the other the sort of premise entails the hypothesis that I can believe the hypothesis if I believe the premise I just sort of concatenate them together right so give it maybe a start token pass in one sentence pass in some delimiter token pass in the other and then predict uh sort of yes no entailment not entailment fine tuning gbt on this it worked really well um and then you know Bert came after GPT Bert did a bit better it had bi-directional context um but you know it did it did uh sort of an excellent job and then came gpt2 where they focused more on the generative abilities of the network so um right we looked at uh now a much larger Network we've gone from 117 million to 1.5 billion and given some sort of prompt it could generate at the time a quite surprisingly coherent continuation to The Prompt so it's telling this sort of story about uh about scientists and unicorns here um and this size of model is still sort of small enough that you can use on a small GPU and fine-tune and whatever and its capabilities of generating long coherent texts was just sort of um exceptional at the time it was also trained on more data although I don't uh something like 9 billion words of text um and then so after gpt2 we come to gpd3 sort of walking through these models and then we come with a different way of interacting with the models so we've interacted with pre-trained models in two ways so far we've sort of sampled from the distribution that they Define uh We've generated text via like a machine translation system or whatever or you fine-tuned them on a task that we care about and then we take their predictions um but gpt3 seems to have an interesting new ability uh it's much larger and it can do some tasks without any sort of fine-tuning whatsoever uh gbd3 is much larger than gpd2 right so we went from GPT 100-ish million parameters gbd2 1.5 billion cpt3 175 billion much larger uh trained on 300 billion words of text and this sort of notion of in context learning that it could Define or figure out patterns in the training or in the example that it's currently seeing and continue the pattern uh is called in context learnings you got you know the word thanks and I pass in this little arrow and say okay thanks goes to you know mercy and then hello goes to bonjour and then you know they give it all of these examples and ask it um what you know otter should go to and it's learned to sort of continue the pattern and say that this is the translation of otter so now remember this is a single sort of input that I've given to my to my model and I haven't said oh do translation or fine tune it on translation or whatever I've just passed in the input given it some examples and then it is able to to some extent uh do this seemingly complex task that's in context learning uh and here are more examples you know maybe you give it examples of addition and then it can do some uh some simple addition afterward uh you give it in this case this is sort of rewriting typos they can figure out how to rewrite typos in context learning for for machine translation and this was the start of this idea that there were these emergent properties that showed up in much larger models and it wasn't clear when looking at the smaller models that you'd get this sort of new this qualitatively new Behavior out of them right like it's not obvious from just the language modeling signal right gpt3 is just trained on that decoder only just next predict the next word that it would as a result of that training learn to perform seemingly quite complex things as a function of its context um yeah okay one or two questions about that um this should be quite surprising I think right like so far we've said talked about good representations contextual representations meanings of words in context this is some very very high level pattern matching right it's coming up with patterns in just the input data and that one sequence of text that you've passed it so far and it's able to sort of identify how to complete the pattern uh and as you think what kinds of things can this solve what are its capabilities whether it's limitations this ends up being an open area of research sort of what are the kinds of problems that you maybe saw in the training data lab maybe gpt3 saw a ton of pairs of words right they saw a bunch of you know dictionaries bilingual dictionaries in its training data so it learned to do something like this or is it doing something much more General where it's really learning the task in context you know the actual story we're not totally sure something in the middle it seems like it has to be tied to your train data in ways that we don't quite understand but there's also a non-trivial ability to learn new sort of at least types of patterns just from the context so this is a very interesting thing to work on now we've talked a lot about the size of these models so far and as models have gotten larger they've always gotten better we train them on more data um right so gpd3 was trained on 300 billion words of text and it was 175 billion parameters and you know at that scale it costs a lot of money to build these things and it's very unclear whether you're getting the best use out of your money like it's bigger really what you should have been doing in terms of the number of parameters um so you know the cost of training one of these is roughly you take the number of parameters you multiply it by the number of tokens that you're going to train it on the number of words and uh some folks at deepmind I forgot the citation on this some folks at deepmind uh realized through some experimentation that actually gpt3 was just comically oversized right so chinchilla the model they trained is less than half the size and works better but they just trained it on way more data um and this is sort of an interesting sort of trade-off about you know how do you best spend your compute I mean you can't do this more than a handful of times even if you're you know Google um so you know open open questions there as well um another sort of way of interacting with these networks that has come out recently is called Chain of Thought um so the prefix right we saw in the in context learning slide that the prefix can help sort of specify what task you're trying to solve right now and it can do even more so here's standard sort of prompting we have a prefix of examples of questions and answers so you have a question and then an example answer so that's your prompt that's specifying the task and then you have a new question and you're having the model generate an answer and it generates it wrong and Chain of Thought prompting uh says well how about in the example in the demonstration we give we give the question and then we give this sort of decomposition of steps towards how to get an answer right so I'm actually writing this out as part of the input I'm I'm giving annotations as a human to say oh you know to solve this sort of word problem here's how you could think it through ish and then I give it a new question and the model says oh I know what I'm supposed to do I'm supposed to First generate a sequence of steps of intermediate steps and then next say the answer is and then say what the answer is and it turns out and this should again be very surprising that the model can tend to generate plausible sequences of steps and then much more frequently generates the correct answer after doing so relative to trying to generate the answer by itself so you can think of this as a scratch Pad you can think of this as uh increasing the amount of computation that you're putting into trying to solve the problem sort of writing out your thoughts right as I generate each word of this continuation here I'm able to condition on all the past words so far and so maybe it just uh yeah allows the network to sort of decompose the problem into smaller simpler problems which is more able to solve each um no one's really sure why this works exactly either at this point with networks that are this large they're emergent properties are both very powerful and exceptionally hard to understand and very hard you should think uh to trust because it's unclear what its capabilities are and what its limitations are where it will fail so what do we think pre-training is teaching gosh a wide range of things even beyond what I've written in this slide which I mostly wrote two years ago right so it can teach you trivia and syntax and co-reference and maybe some lexical semantics and sentiment and some reasoning like way more reasoning than we would have thought even three years ago um and yet they also learn and exacerbate racism and sexism all manner of biases um there's more on this later but it's uh the generality of this is really I think what's taken many people aback and so increasingly these objects are not just um studied for the sake of using them but studied for the sake of understanding anything about how they work and how they fail uh yeah any questions has anyone tried like a benchmarking like GPT for like programming tasks like how accurate these does etc yeah the question is has anyone tried benchmarking GPT for programming tasks anyone seen how well it does um yes so there's definitely examples of people using GPT uh three four simple programming things and then you know the modern state-of-the-art competitive programming Bots are all based on ideas from language modeling uh and I think I think they're all also based on pre-trained language models themselves like if you just take all of these ideas and apply it to like GitHub uh then you get some very interesting emergent behaviors relating to code uh fallout and so yeah I think all of the best systems use this more or less so lots of benchmarking there for sure through the basis for what like GitHub co-pilot is going to do the question is is this the basis is that what we just mentioned the basis for the GitHub copilot system yes absolutely we don't know exactly what it is in terms of details but it's all these ideas what if you have a situation where you have you know still a large amount of data for you know General data and then you have also a large amount of data for your fine tuning task at one point is it better to train a new model for that pioneering versus you know get data from both yeah the question is what if you have a large amount of data for pre-training and a large amount of data for fine tuning when is it better to do sort of a separate training on just the fine-tuning data um almost never if you have a bunch of data for the task that you care about what's frequently done instead is three-part training where you pre-train on a very broad Corpus then you sort of continue to pre-train using something like language modeling on an unlabeled version of the label data that you have you just like strip the labels off and just treat it all as text and do language modeling on that adapt the parameters a little bit and then do the final stage of fine-tuning with the labels that you want and that works even better this interesting paper called Don't Stop pre-training nice uh final question that's a lot of questions on anyone new new someone knew the question yeah um I was wondering do you know if there's like a lot of instances where a pre-trained model can do some tasks not soon before I even know yeah so are there any instances of where a pre-trained model can do a task that it hasn't seen before uh you know without fine-tuning the question is what is hasn't seen before mean right like uh these models especially gpt3 and similar very large models you know during pre-training did it ever see something exactly like this sort of word problem arithmetic maybe maybe not it's actually sort of unclear it's clearly able to recombine sort of bits and pieces of tasks that it saw implicitly during pre-training we saw the same thing with trivia right like language modeling looks a lot like trivia sometimes where you just read the first paragraph of a Wikipedia page and it's kind of like answering a bunch of little trivia questions about where someone was born and when um but like it's never seen something quite like this and it's actually still kind of astounding how much is able to do things that don't seem like they should have shown up all that directly in the pre-training data quantifying that extent is an open research problem okay that's it let's call it foreign