okay hi everyone um welcome back to we're now in past the halfway point week six of cs 224 n um and so let me just give um a couple of quick announcements first um so today is the day that you have to have done the mid um quarter survey by hundreds of people have but if you haven't this is your last chance um to get the half point for that today is also the day um that final project proposals are due we really encourage you to try and hand them in on time or nearly on time that's really just to help you so we can more quickly give you feedback on final project proposals um and in the background then there's also assignment five you will have seen the message that we're giving you one extra day for that but we do certainly encourage you to be hard at work on assignment five at this point hopefully it's a great exciting opportunity to be learning all the latest stuff about transformers and then today i'm delighted to have our first invited speaker um let me just mention that going along within the half point of participation credit um is you guys writing a reaction paragraph talking about something um that the speaker talks about and their instructions upper for that um on ed um but without further ado let me introduce dante chen um so dante is one of the foremost researchers in question answering and she's particularly well known um in recent work for being one of the co-authors of the roberta paper the spanbert paper and on using dense passage retrieval methods for open domain question answering and as a professor at the um princeton university um but as one other comment um don she once upon a time was the head ta of cs224n um so she's quite familiar um with the context of this class um so i'm really delighted to have done she here to give this lecture on question answering thanks thank you chris for the introduction um for me it's a very good it's a great opportunity for me to come back to cs2201 today and give this lecture also b virtually so question answering the areas that have been working quite a bit in the last few years so today i'm very happy to introduce you some of the fundamentals in this field as well as some cutting edge and save our topics so here is my plan for this lecture so first i will be um give a brief introduction of what is question answering and what kind of problems that people are starting today so i'm going to spend the most of this lecture focus on one type of question answering problems called reading comprehension so this is basically problems of how we build systems to answer questions over a single passive text so i know that many of you are going to do a default project on the stanford question answering data set so understanding this part will be very crucial for your final project so at the end of this lecture i'm hoping to spend like hopefully like 20-ish minutes to talk about a more practical and in my opinion or more exciting problem called open domain question answering so we will try to answer questions over a very large collection of the documents and i so my friends try to quickly go over some of those state art methods in this area okay so let's just get started so first what is the question answering so the goal of question answering is to build systems that can automatically answer questions posed by humans in a natural language question answering all let's say qa in short is one of the earliest mlk tasks and the early systems can even date back to 1960s so here is one example of the early like one of the early um qa systems uh qa systems in back to 1964. so as you can see that this system is trying to answer a question like what do you want it and then finally return on the answer that's the graph so to do this so this system is basically trying to find some kind of text matching between the question and some kind of text segments and by using some kind of dependency analysis i assume that you have already learned the defense parsing in this class and there are many different types of the question answering problems and we can also we can categorize all these question answer problems based on the either information source or the titles of questions or the type of answers so for the information source we can build a system that can put like a condition on um a short passive test or a very large collection of documents or even like a structured database or structured knowledge base basis or even tables or images so for the question type we can also be assistants that can answer like factory questions or non-factory questions or open domain questions or close domain questions or simple questions versus like more complex or compositional questions and for the answer type it can also be like a short segment or text or a paragraph or document or list or even the yes or no questions so just have in mind there's many different types of question answering problems and all these problems may require very different techniques or different data or even different evaluation metrics to divide all these different problems and the question counselor has enabled a lot of the useful real world applications for example today if you just put your question in a search engine like google so for example you can put in your question like where is the deepest lake in the world so you can see that the current system basically found a like a short snippet of text including like lake um by carl barco in siberia calls the distinction of being both the deepest lake in the world and the largest fresh water lake blah blah and then it can actually pinpoint the correct answer which is actually a concise entrepreneur which should be siberia and those kind of systems are also able to handle like more complex questions like how to questions i guess this is probably a question that everyone currently cares about so the question is how can i protect myself from kobe 19. so there isn't really a simple and short answer to this question so you can see that the system actually returns a very long including the best way to prevent the illness is to avoid being exposed to this virus and to help prevent the spirit of kobe 19 you can do the following so actually the this paragraph is actually a summary uh from this cdcsco if you just click this link and read through the article so this is also one like one type of question answering problems and now this is a survey of the use cases for the current digital systems such as alexa or google home so according to this survey result in january 2020 which is one year ago so you can see that also people actually really like to ask questions um this is on this digital assistant so you can see the question is actually the second most used case only ranks after the listening to music and the before the checkered weather instead of time timer so questions have been really useful in this virtual assistants another very famous example of the question answering system is this ibm was from question 3 system so in 1920 or 2011 so this idea was because qa system has been shown to beat two national japanese champions in answering deathly questions so this is kind of this uh like a historical event at least in the lp history so if we look at the era working of this kind of system more closely so you can see that it is actually a very complicated and highly modularized system so if the system is built on both the unstructured text and also the structured data so by looking at the system if you go from the left to right you can see that this system consists of the four stages including the question processing the candidate answer generation and the candidate answer scoring and the confidence merging ranking and then if you look at each stage you can see that there are many different nlp techniques that have been actually included in this complex query system including a question classification parsing relation extraction correference so it's actually there are really lots of the lpcs modules that have been included and now this system has been over 10 years or actually exactly 10 years now and this is actually representing the space art like 10 years ago at that time so we know that this class is about deep learning so today differently has completely really transformed the landscape of the question answering systems so there's no doubt that we can say that almost all the states are personalizing systems today are built on top of the end-to-end training of the diploma networks and approach and language models such as bird so today in this lecture we are also going to learn a lot of these deep learning models in full question answering and this statement is probably also true for almost all the nlp problems that we can see today but we can also argue that question answering is probably one of those fields that we have seen the most recovered remarkable progress in the last couple of years driven by deployment so in this lecture uh i'm i'll be mostly focused on like uh focusing on the text based on textual uh crash answering problems so basically we are trying to answer questions based on the unstructured text so before i get started i jump to that part i also want to quickly point out that there are many other really big questions problems and each of them can be a really like a big star sub field in nlp and they actually have very different challenges and also model designs so one bigger class of this crash answer problem is this knowledge based question stream so basically we want to build question also in systems to answer questions that can answer also questions over a very large database so to solve this problem some approaches need to take this question and convert this question into some kind of larger forms and this kind of logic forms can be executed against this database to give you the final answer and then another class the bigger class of the presentation problem is called visual question answering so it's basically you need to answer questions based on the images so this problem this requires both understanding of the questions and also images and there is actually a very active field between the computer vision and our key so if you are interested with this type of problems i encourage you to check out these problems but i'm not going to dig into these problems today okay so next i'm going to start with the part 2 reading comprehension i just want to quickly check if there any quick questions i can answer before i get started um starts will catch you yeah no i think we could do now okay so yeah so let's talk about the reading comprehension then so reading commemoration is the basic problem that we want to compare a passive text and answer questions about the content so the input of this problem is basically a passive text a question and the goal is to return the answer that actually can answer this question so here's one example so let's tell so here is a passive text and we want to uh answer a question the question is what language your test will study while you score okay so i'm going to pause like 5 10 or 10 seconds and see if people can find the answer to this question based on this passage and you guys um okay what people say to german yeah germany so the answer should be german so basically to answer this question so you need to find this sentence like in 1861 tesla attended this school where he started german arithmetic and a religion and it's only the german the language so the answer to this question should be german okay here is another example again another passive text and the question is which linguistic minority larger hindi or mala um i think yeah five seconds okay so the answer to this question should be hindi so this probably is not a very hard question for humans it's actually a pretty hard question for machines because to get this question correctly so the machines basically to understand that for the hindi like three point three percent of the population speaks of hindi and only like one point twenty seven percent speaks uh mala yellen this language and then also compare these two numbers and the final case uh three percent three point three percent is a bigger number so the answer should be hindi to this question okay so next i'm going to talk a little bit so why do we care about this problem so why do we care about the reading comprehension problem so besides that it actually drives many useful real world practical applications also as i've already shown some examples at the beginning i think there are also two other key reasons so the first reason also besides adaptation the first reasons is so reading comprehension has been also viewed as a very important testbed for evaluating how well computer systems understand human language so this is really just similar to like um how we humans actually test the reading comprehension test to uh to evaluate how well we actually understand about language so this is also the way that we actually um pose questions to test the machines that would understand language understandings of ability so this actually has been formally stated in um back in 1977 by benny leonard in her dissertation so she didn't saying is she says that these questions can be devised to query any aspect of text comprehension so ability to answer questions is the strongest possible demonstration of understanding so that's why reading comprehension can be a very important testbed because we can't divide design very complex complex questions to test that and also i think there's another interesting and important reason that um reading comprehension is important so in the recent few years so many some researchers actually found that okay so for many other nlp tasks that we can also reduce them to a reading comprehension problem so i'm going to give you two examples so one example is really information extraction so basically if we want to um so given on the person like subject barack obama given a relation educated at so we want to fill in what is fill in this question mark and figure out okay where barack obama was advocating that so one one way to solve this problem is basically trying to convert this relation into a question so where did barack obama graduate from and taking a relevant piece of text and then by applying a reading comprehension problem then basically the we can find out the extract the correct answer should be columbia university that is also the output of this information extraction system another example is actually called cementite labeling i'm not sure if i've learned this in the past yet probably not but this is a task of the spanish labeling is trying to taking one sentence and trying to identify the rules for different verbs at least for words in this case in the in one sentence so basically trying to give them one sentence about given one verb finish trying to figure out like who did what you and when and where so by trying to um so it's going to try to figure out um all these like rules um with respect to the verbs so one way to solve this problem is by also by converting all these different roles into questions such as who finished something what did some someone finish and what did someone finish something else so by converting all these kind of like um semantic error relations well we can also apply just apply the ring comprehension problem and give you the correct answer so this is actually a very interesting perspective that reading comprehension can be actually very universally useful to many other tasks so next i'm going to introduce this like a stanford question string dataset cause god so if you are going to do the default final projects you will need to use this data set so cylindrical questions and datasets is actually a supervised reading comprehension dataset so which consists of 100k annotated passage and the answer question also triples so here is one example from this data set and uh i just want to um say that also one important thing to have in mind is that uh so this dataset has consists of 100k annotated examples and this kind of large-scale supervised dataset are also very key key ingredient for the training the effective neural models for reading comprehension so after the data set many other like later data sets have been also collected um basically runs this size around like 100k so 100k is actually very important um to transit neural models so for these data sets so the passages is like a single passage a single paragraph selected from the english wikipedia which usually consists of like 100 to 150 words and the questions are power sourced um basically electronic kind of perking and there is a very important property of this data set is that each answer is a short segment text or we could spend in the passage so as you can see from this example so here are three different questions and each of this um answer can be actually found as a short segment text in the passage so this is actually a pretty interesting property i know it's also important important property of this data set but also just to uh tavis that this is also a limitation because not all the questions can be uh answered use in this way so only the questions that that you can find the answer as a spell in the passage can actually be included in this data set basically but today so this data um yeah i forgot to say so this data set was collected in 2016 by several you know researchers at stanford so it's called stanford question three data sets um today like after four or five years now so school still remains the most popular reading comprehension dataset so it's actually you know very clean on the high quality day set but it's also not as very difficult dataset so today is basically the score data status set has been almost sold and the saves are already exists estimating the human performance and i also want to quickly mention the evaluation for this uh stanford credit counseling data set so there are basically two evaluation metrics to evaluate how well a system can do on this data set the two metrics uh uh let's see if match and a1 score so when you sign match is basically just a binary indicator zero one uh based on measures whether the answer can actually be exactly matched to the gold answer and the eighth line score basically measures kind of some partial credit enough to do the evaluation so basically for the development and testing set there will be like three gold answers collected because um for some questions there might be not just one unique answer so there could be multiple possible answers and the eventual matrix basically takes a pretty good answer and compares or compares the predicted answer to each code answer with some kind of like some articles and also the permutations included and obviously you can computer exact match score and also a fine score by comparing come back comparing the predicted answer to the gold answer and then finally take the next course and um because there are many different examples in the demo or test set and now finally we just take the average of all the examples for the poster exact match and the reference score so by using this evaluation method the estimated human performance is um by the researchers at that time uh estimated by the researchers at the time is the exact match score is 82.3 percent and the f1 score is 91.2 so here is just a quick example so here the question what did tesla do in december 1878 and there are three possible answers so if i see that the first two answers are the same left grass and the third answer is left breath and as uh serve i think that type of fear um all really relations with his family and then you feel in front of prediction is uh spent which is left breath and serve so you can see that the exact there isn't an exact match spot between the predicted answer and any of the answer so the exact match will be zero and the a5 score will be uh taking the max i'm not going to talk about how these are computed so i suggest you check out the original paper so by computing this course and taking the max and the the final is the f1 score will be 0.667 which is a f1 score for this creative answer on this data set so dante one question you might answer is so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of bird as and fine-tuning for it or do it as question answering does one or the other method work better and by how much um that's an interesting question um so i haven't really seen that okay since there has been some claims that okay all the uh tasks can be converted into questions and tasks but i'm not sure there is a really like a very fair comparison let's say a young and anti-recognition and by really converting that into question answering tasks so i don't have to answer to that so the kind of state art and your system is still trying to just change or sequence tagger uh tagging on top of the bird so yeah i don't really have a preset something like that should i continue okay okay so next i'm going to talk about how to build a neural models for reading comprehension in particular how we can build a model to solve this standard questions in datasets called dataset i also want to just quickly mention that um because there are many different papers it actually uses like um different notions to refer the same thing so starting from um so i'm going to use the passage paragraphs in context and also question the query basically interchangeably so they are basically referred to the sentencing because different papers use also different notions so i just want to quickly mention that okay so how can we build a model to solve this problem so let's first formulate this problem so the input of this problem is uh let's take let's take a context or paragraph so c which consists of the n tokens c one to c n and also we take a question q and the question uh consists of m tokens q one to q m so um could be something like around 100 to um between 100 and 200 for spot and the m would be much shorter would be something like 10 or 15. and then because the answer has these constraints as the answer must be your second text in the passage so the output can be just uh written this way so we are going to predict the start and end so start and then end will be arranged um in the range between the one so it is basically just two check points oh sorry two end the point of the answer and um so scott has been collected back in late 2016. so after uh 2016 there have been like visit two families of the models neural models to solve this uh to solving this like stem score data set so the first family basically like uh there are a lot of models that come out during that period between 2016 and 2018. so this my family models figure i always can base models with attention so these are like just like a list of the representing models that come out during that period and including some work that i did when i was a phd student at stanford and now the second the second class models i put here is really there that divided here before the version after birth so after birth came out so almost those other this reading comprehension models were built on like how to find two of the bird models not just for model size for the birth life models so prediction language models and for these kind of reading comprehension problems so here are like two um the sound is uh the illustrations of these two families of the models so on the left is like iostm based models resultation and on the right is on the burton model um and then we need to fine-tune this model for the question for the reading comprehension task so i know that for the so my plan today is first try to talk about to talk about these ios 10 based models so i'm going to spend a little bit more time on this part because i know that um for the default final project you need to implement this model furthest from the scratch so i'm going to work through how to build this model like step by step and hopefully that you can have a good understanding of how this model works and then i'm just going to briefly talk about how to build this use the bird models for the ring comprehension okay so before i start talking about these lstm models i know that you have already learned this sequential sequence models without tension for machine translation so i was um so i want to draw some connections between the machine translation problem the reading comprehension problem because they really share a lot of similarities so first uh so in the machine translation model or this like sequence use sequence model there is a source and package sentence so basically us two sequences so but in our case in this reading comprehension case that we also have two sequences one is a passage and another is a question but the length could be a slightly imbalance because the passage will be much longer than the question but there are essentially also two sequences and then so in the reading comprehension we need to model like which words in the passage are most relevant to the question and then if they're irrelevant to the question so all is also relevant to which which set of the question words so this is basically uh very key important thing that the important thing that we actually need to model and this is actually very similar to the machine translation model that we need to model which words in the source sentence that actually are most relevant to the current packet word so you can imagine that the attention will be also really the key ingredient here is that just like some new sequence 26 model we need to model the attention between the source sentence and the target sentence we also need to model the attention between the passage and the question so this is actually very similar so something that's actually not very similar is for the sigma 6 model we need to build on like a decoder auto regressive decoder to generate the target sentence word by word but in this reading comprehension problem we we don't need to really generate anything so we just take the test and take the question so at least for the school data set we just need to change two class files to predict the start and end positions of the answer so that that part is actually simplified so when you don't need to change the decoder to generate the target sentence okay so next i'm going to talk about one um this model colored by def so it stands for bi-directional attention flow for machine comprehension so it was proposed by means seal and other folks in 2017 so it remains before the ice before the bird came out it remains one of the most popular reading comprehension models and achieved very good performance at that time at least on the small data set so you can see that this model like seems to be pretty complicated but if you um look uh look at this model from the bottom to the top it actually can be decomposed um into many different layers so the next i'm going to just dissect or dissect this model layer by layer and talk about okay what these layers are actually doing and how we can really build this model from the bottom layer to the top layer and then finally transition like model you know enter anyway okay so the first uh chart is actually the bottom three layers um called the character embedding layer word embedding layer and the phrase embedded layer so i just put them together because this is an encoding function so the idea here is that okay let's take the context query or the passage in question we need to encode them separately so to do this so this model basically proposed to use a concatenation of the word embedding as well as the character embedding for each word in the context and the query so for the word embedding is straightforward so if you have you have learned word embeddings so you can just look up the word for the this word like seattle just use the global embedding as a reputation for this word and the for the character embedding part so you basically need to represent each character in this world like seattle and a bypasses to a convolutional neural network with some kind of max pooling operations and then finally you can just get one reputation at the top and then you just concatenate its own uh word embedding and the calculating body so these kind of embeddings have been shown effectively to improve the replication for the unseen or the real words so mathematically mathematically you can see that for each word in the context of query you can just we can just represent this uh rotation as blocks of the embedding and the character embedding and then we just concatenate them and pass this to like all highway networks so i don't write the function here so um so you can just look at orange on paper and the second part um for other we call it the issue very visual word so on the next we are going to pass this like word embedding into two separate bi-directional lstms to separately to produce these contextualized embeddings for both the context and query so let's look at this equation so we take the uh reputation of this word and we just uh basically this is like a one lstm model uh from one direction and this all always can model from another direction so we just need to competent the two um the two header rotations two directions and finally we can get a contextualized rotation for for this for each single word in the context and now we can do the same similar thing for the question um reputation i also want to quickly mention because i mentioned the sequential sequence model the sequence just sigma although we cannot really do this bi-directional stems for the two sequences again like because the decoder is all auto-regressive model so that's why the decoder is usually just implemented as a unidirectional stem but because here we don't really care about the generation so we can just use two bi-directional scans to represent the rotations this is actually very important uh this bi-directional length is actually very important to capture the contacts from both the left and right side okay so the next component is the next layer it's called the attention flow layer so i just call it attention here so the change idea the idea of attention is trying to capture the interactions between the contacts and query and nothing in this paper by that favor they they propose two types of attention so the first type of tension we call the context require attention so the idea is for each context word can we find some most relevant words in the question from the question for the query for the query words so here's one example so here the context context of barack obama is the president of the usa so for each context word we need to find an alignment but define like which words in the question can be actually aligned with this context word so we can see that both barack obama can be aligned to who and the president is aligned to release and the usa alliance united states so busy for each content will try to find the most relevant query words another second type of tension is called query to context or tension so it's very another direction so here the idea is to choose some context words that are most relevant to one of the prior words because the context can be very long so a lot of the context could be just not relevant to the this question so we just run over around several examples you can see that the first thing we need to do is try to locate okay which parts of the sentences in this context can be actually relevant to this question so this type of the query to context or attention is trying to capture so which um which context words actually can be most relevant to to the query to one of the query words so for this example the question is which city is grooming in winter so you because the question is asked about blooming so you can find priority figures okay blue means it speeches is actually very relevant to this question and now we also find this in winter because in winter it also mentions the question so this particular um context was to be also relevant to this question so this um context words could be probably need to capture and uh in this attention that okay this is actually relevant to this question okay so this is actually basically just an intuition of these two types of tension and this is also why this model is called a bi-directional attention flow because there is a contrast required attention and there is also a library to context attention so let me just talk about go through how to actually um uh do this like curve to context attention the contextual periodic tension in this model so the way they do this is first to compute a similarities form for every pair of the contextualized vector ci and for every pair of the question with qj so this is actually the output from the encoding layer so this is already the output from the lstm layers and the wizard is basically just computer similarities form by taking the ci qj and also the element-wise multiplication of the c and qj so they basically just concatenate these three vectors so the output will be a sixth h dimensional vector and they just multiply this to um compute the dot product of another like a learnable vector and then finally just discard this basic can give you one scalar one number the sij which measures how um the similarity between this context where ci and also this question were qj so if you i have so if i learned some attention before so this is actually just one choice of this model so there could be many different ways to define this uh similarity and uh similarity scores so this is basically just one design choice of this model okay so after defining this margins for sij so the context to create attention again like which question words are most relevant to ci so the way to do this is so basically just taking this matrix the similarities for sij for each row we should basically correspond to like one context word for each row um so they are going to compute the soft max for each row and this can give us like um normalization force alpha ij which is our probability distribution over all the question words fij so this is just really similar to all the attention uh mechanisms that you probably have seen in this class so basically for each context we're taking the soft max um um over all the question words and get us um probability distribution and then finally just take the linear combination of the weighted combination of these are tensions for r i j and also the question vector the q j and then finally you can get a vector a i which is actually a 2h dimensional vector so this query context shows quite retention basically just try to capture which questions words are most relevant to each context word so the next five part is a paragraph um sorry the title here sorry uh this is actually the query to context or attention so which means that which context words are relevant to some question words so we don't so a lot of context words would be not relevant to this question so the idea to do this to do this is for each row of this sij this busy just takes a math scores over all the um question words and after taking this back score to compute the softmax over all the context words here so here i actually numerous over all the context words and this can give us like attention another attention score beta i which captures how important this context works is relevant to this question so after computing this beta i so we can again like compute this like a weighted combination by um computing by um somewhere by summing up the beta i and also the context context vector ci and then finally we can get our vector bi which is also another 2h dimensional vector and the final output of this attention function is actually very complicated here is also the design choice of this model so the takes a context measure ci and as it takes ai from this part of the context to query retention and that takes the element from multiplication between the c and ai and also the cnbi and finally take the concatenation and it can give you a produce or h dimensional vector okay maybe i want to pause a little bit and check if there are any questions because it's probably a little bit complicated one question is why is query to context and context of query attention not symmetrical um um that's a good question yes uh so here's essentially the goal you're trying to because if the goal is final goal is trying to find a span in the passage so so to hold the point of the this attention function you know trying to produce a rotation for each single context word in this context so that's um so so we are not trying to generate the question notifications here it's going to try to generate the um context orientations so one so the difference between these two like first let's try to see which questions words are relevant to this context word another part is trying to figure out which context word can be relevant and which context would can be not irrelevant i hope this sounds so answers to a question here's an easier question sort of on the same topic which might help is there a reason why you use both query to context and context to query attention is it sometimes advantageous or okay to use just one that's a good question um the reason is yeah so i'm going to show some obligations partly from this paper so they basically just find the both both directions can really help um by drawing the context curve context so there'll be some relation studies so by using one set is useful but just not as good as using the both directions um right let's see uh in the bottom right we sum over i so why does the i remain in bi is that correct or is there a typo there oh this is not a typo so again i'm sorry so the output yeah i know it will be confusing so also output of this model this com module is to get a reputation for each context word at the end so both the output for ai and bi i is actually um enumerate from like um actually you know if it's over all the context words so bi will be skills um just try to aggregate over all the questions on the over all the context words but the beta i measures the importance of this kind of test words compared to all the context words so both ai and the bi are actually with respect to the context words yes so you can see that here is basically doing some kind of the element wise multiplication so the output of gi will be actually arranged from from the one to an um which is the number of the context words there are lots of questions about this um what is the rationale for the expression for gi how how does one come up with such an expression okay i don't know i guess you'll also try out a lot of things okay so key point here is trying to understand okay so the rules of the contest require attention equality or context rotation so i bet that there could be many different formations to do this i also think the authors have tried many different but uh just what they kind of can come up as and i think that if after week um there could be some other ways to incorporate the both attention but it doesn't have to be written this way i mean one other question would be in the query the context attention why do you do a max inside the soft max yeah um yeah sorry i should have explained this more clearly so here again query to context or attention to try to measure well whether this is the importance of these context words with respect to some um some answer question words so if the so by taking the max for each row in this ace matrix so it's basically trying to see okay which question word um is actually most relevant to uh this context word if this number is still very low that means this there isn't any question words that could be aligned with this context word so that this will just if i take after taking the math if this number is still very low that means this question context vote is not very relevant so so basically just that's why we take the soft max our price of max on top of the max ah i know do you want even more do you want to go on uh i probably should well i don't have a lot of slides but i'm happy on the questions after afternoons yeah maybe you should go on yeah okay so the last part of this model is actually um um the either it's the normal stimulus so it's a two out of five lastly there are two layers smaller layer and output layers so for the modern layer so again for after the attention layer they take some heater rotation um so basic gi which captures the tension between your contacts and the query and then basically just uh passes gi to another two layers of bi-directional stems and the many reasons they do this is the attention layer is basically modeling the interactions between the query and context and now by passing this to another two layers of bi-directional lstms the modern layers is basic modeling they can also first model the interactions using the context words so this is a formulation here um so these are two layers bi-directional stm by taking the gis input and the output will be on the mi which is another 2h dash um dimensional vector for each context word in the in the passage okay so the final is also links so far other players this is just two class friends just trying to predict the start and end positions so by doing this so the first contact is the gi and mi so this would be actually a 10 h dimensional vector and by computing the dot product of another vector called w start and this resulting vector and they can get basically get a score for each position in the context and then you can just apply a soft max and then this will give you a probability that okay what is the probability this partition i will be actually uh based on um the start position of the final answer string and they also have something um another classifier to predict the end position of the answer but they also did something a little bit more complicated so they actually pass the mi to another bi-directional lstm here so they call it m i and they compati and m prime i oh sorry this is the title so this will be wn um so they compute the dot product between wn and this vector and this can give reproduce our own probability uh probability over all the positions which predicts uh um how likely this position will be the end position of the answer so by doing it by passing the mi to another by their character and the reason that they're trying to capture some kind of dependence between the choice of the start and end so you can imagine that start and should shouldn't be um too separate so it shouldn't be could be independent predicted but if they declare that if you incur some kind of dependence between the mi and um just keep starting and pn this can actually perform better okay i'm done with this part of describing the bypass model any quick questions i can answer i think you can actually go on okay okay sorry i forgot to mention this okay the final training loss will be just by taking these two probability distributions and that is basically just the next negative log likelihood of the gold as a gold answer just the start position of the gold answer and the position of um the answer and by um just us um basically taking the product of these two probabilities but you apply them uh apply a lot so this is the sum of the two negative log terms will be the final chain loss and the whole the whole model can be just changing our entry anyway from the encoding layer to attention layer to modern layer and to output layer so this will be just um completes the whole that the whole model of the by that model okay so this model is actually um achieved like uh on the data set it will achieve the 77.3 f1 score so as i mentioned earlier so um 2000 operations started they found the the pulsar attention in two directions are actually important if you usually move the one direction the performance will actually drop quite a bit if we remove the contacts to parallel tension the performance will drop to sixty seven point seven eight wave one score and if you uh remove this part it will drop to a four point eight point four and then also the character embeddings can also help so if you remove the character in balance you you'll get like a 1.9 uh point drop and on the right of this figure you have um this slide you can see a very big table so it's basically all the models that count out at that time between 2016 and 2018 so you can see that um by definition here so you'll achieve the 77.3 f1 score and the basic all the models are actually you know a very similar ballpark so numbers range from like um the highest number here 79.8 until like um after the elmo was introduced the numbers have been actually improved quite a bit so before the elbow basically all the numbers are actually kind of similar so uh each model actually improves our premise previous model by like a 1.02 points and now here is our attention visualization by uh to show that um how this like personalities for the tension actually can capture the similarity between the question words and context words so here example the question where did the super bowl 50 takes play uh place so the issue is actual question word here and each column is matrix basically indicates the attention score the semantic score that that has been learned by this model so you can see that and the on the right is basically trying to uh print out or display so the the context words that have the highest scores so you can see that the where it has been aligned very well with uh at the stadium leva and also the super bowl 50 is basically lined very well with super bowl 50. so this basically really tells that this kind of attention scores can actually capture those merit scores pretty well okay so next i'm going to talk about now how to use the burden model to solve this problem so i know that you have learned the first in the last lecture so i'm not going to repeat this so very quick so bert is basically a deep directional transformer including pre-trained on the large amount of text and in the channel the two training objectives including masculine modeling and the next sentence prediction and this model has a lot of parameters um so the bird base has like 110 million parameters and very large model has 330 million parents so okay so how we can actually use the bird for the uh for reading carbon enhancement so it's actually very easy and very straightforward the idea is to take the question as a segment a so you also in the frictions are two segments for the next sentence prediction task so when you apply the verb on the reading comprehension task you basically just take the question as a second a and take the passage as a segment b and finally the goal is trying to predict true end point in segment b so here's one more concrete example so question is how many parameters does bird large have so you can see that so they basically just um take the question here and then takes a passive here and by putting in the serious token and the acp token and by just contacting the question of passage tokens and also for the questions that you just need to pass the a to a segment embeddings and the for the passage you just need to cut out put in the um the second b embeddings and now finally the training loss is also the same so you basically just um try to maximize the probability of the sum of the natural log likelihood of both the start and positions but here's the way that the computer start and and um probability is slightly different so that's very straightforward so you just pass this um input notation into birth and the bird can give you the hidden vector h i that can actually represent the hidden vector that corresponding to the context word context word ci so you can just introduce another two vectors w star and wn by computing the dot product and then apply the softmax then you can just give you a very similar to what we had before but here is the high just output from the vertical encoder and then we are training on this tool that we start and w and to um for these two probability distribution p start vpn okay so for this model so all the vertical transfers that is actually very large number um if you use basically 110 million parameters as well as the newly introduced parameters h star and h and uh which is if you take the birth base so hidden side will be 768 so it's only like 1 500 new parameters so we just optimized together jointly for this training objective and it actually works really really well this model so if you just take this modbrate model and by just optimizing all the parameters together you can give you very high performance i will show you in a minute i know even the strong even if you use the stronger creation on your are more than like the standard um stronger models than the bird models they can even lead to better performance and on scope and the score that has also become a standard dataset for testing these kind of playtime models let me show you some numbers so here again human performance in 91 and by that is 77.3 and if we just uh do this functioning model so bird base can give you like 88.5 very large can give you 19.9 so you can see that this is a huge jump uh from the badass model to the building models and then finally if you see the even the latest um um pre-translated models include the exxon robot or albert so these models are either like a bigger or these moderate channels bigger covers or the model size are bigger so basically these models can give you give you another like three four point i i find score compared to the very large model so this is already way higher than the estimate f1 score so this just works really well any quick questions i think this may be okay okay so okay so yeah i guess i've been a little bit fast for this bird models but next what so i want to also do a bit of the comparisons between the biodiversity models and the bird models so burning model has many many more planters so it's like either like 110 more million or 330 million branches but that has only like 2.5 million parameters and the by death is built on top of several bi-directional ios teams and while bird is built on top of their transformers so transformer means that um there isn't any recurrence of structural architecture so the transformers are much easier to paralyze and a very key um difference between the bird models and by back models is bird model is a pre-trend but by that model is only built on top of the glove of vectors which is a pre-trend and all the remaining parameters new people learn from this called the data set or the other supervision data set so here it is very clearly that pre-training is a game changer here uh that appreciating basic can just change everything and also give you a very very large boost in terms of the performance but also i want to raise another question so if we don't think of this like um pre-training uh just like by that mother and birth models are really fundamentally different i don't think so because of glory is actually my argument so let's try to see how these two models actually connected especially in terms of model design so by that model essentially they're trying to model the interactions between a question the passage right so both the question to pass it in the passage of question and the first model essentially they're trying to use a self-retention on top of the concatenation of the question passage so this is a transformer model so you should take the question the passage so these are questions in the passage and then you apply many many different layers of the self-attention essentially that this self-attention is able to capture the attention between the context words the attention between the passage was on the question words and the attention from the question to the passive side and also the attention from between um from the question was to another other question words so compared to you by that by definitely trying to model this part but the burden model essentially can capture the tension between all these four parts and uh actually after by that kind of so um just also before the bird can um before the bird came out so he will have been also showing that if you just add a self-potential layer for the passive site so basically you're trying to explicitly model this attention between the passive words and passive words to the depth this account also improve the performance so you can see that um these two models essentially really just trying to model the tension between the passive impression and also the attention between the passwords and the passwords and this is actually what exactly the first model is doing okay um so if there's no further questions um so at this point i talked about um models can do really well on this kind of reading comprehension data set and always talk about pre-training can really change the performance can be again changing reading comprehension i guess don't you add one question first people wonder whether you can do well with a transformer that isn't pre-trained right if you try to build a question-answering system using a transformer rather than rsdns but no pre-training does that work that's a good question yeah it works but you you probably cannot really build a model as big as like 110 million premiums or 200 uh 330 million parameters models so actually there is a model between the um so between uh between the this like family of the ios tn models and burn models called qa net from google so qr net is actually built on top of the transformers with other pre-training so that model actually can perform better than the badass models and other models but they actually underperform the battery models quite a bit so just check it out for qa map okay i will just continue so okay so given free training has been so important so next i will quickly talk about okay question here is that can we actually even design better pre-training objectives for reading comprehension or question answering and the answer is actually yes so this is actually work i did with mendel josh and other folks like one year ago called spambered so think about this so um for the squad and other a lot of extractive uh reading comprehension data set the goal is trying to predict the answer spam from the passage uh as a question so the uh as a answer to this question so there are two key ideas being proposed in spamber the first idea is that instead of using only the masking of individual words we propose that we want to master contiguous extensive words um in the passage because the final answer would be just a segment of text in the passage so we are trying to um so mask out all these um possible answers then from the passage as a training objective and the second idea proposed with spamberg is that because at the end of view because we want to predict unsuspends so we're actually essentially trying to predict two end points um as an answer so the idea here is that can we try to compress the two endpoints of unsuspect um sorry can we try to compress all the information in this span into the two end points so here's the idea is that here let's think about this if we mask out the forwards here and can we try to use the two end points here in this um figure like x4 and x line to predict all the words in the middle so essentially we are trying to pre uh take the two end points and also the position some kind of position encoding and then finally we are going to try to predict all the words in this span so this is why this is called spam bird um so i encourage you to check out our paper and this actually really helps a lot uh at least for the question once again stuff so as you can see from this figure so this is called 1.1 and it's called 2.0 and these are many other question answering data sets as you can see here um so the blue bars here we call google versus actually the um the original checkpoints are released by google for researchers and our birth is actually just uh exactly our re-implementation of the first model but we are having trying to using the same data but we have been trying trying to train this model for slightly longer so it's actually achieved a better performance than the original group so as you can see that the yellow box here is actually standard so spammer can actually uh briefly outperform the blue version overboard um across all the data sets that really tells us that okay even if we don't we are not going to increase the model size we are not going to increase the data by designing better prediction objectives can also be go a long way and do do a much better job in at least in the question answering and reading comprehension data sets okay um so i have like several few slides left in this part so so so far i have to demonstrate that um by using by that model and by using burden models we can get a very good performance on the score data set the resistance number has already exist even the human performance on scope does this means that the reading comprehension is already solved the answer is of course not so let me um just so in the recent last couple of years there's been a lot of evidence showing that the current system still performed poorly on anniversary examples or the examples from the out of domain distributions so here is a very classical example so proposed by robin channel president in 2017. so the idea is that they take a pass passage and take a take a question and they're trying to just insert like a random sentence uh to the end of the paragraph as you can see that this sentence has like even like a nonsense entity in this context drafting here but this sentence actually has like a great uh some left score overlap between the questions it's actually very similar to this question but actually the word numbers have been changed the antennas has been changed and then they found that these kind of adverse examples can actually very easy to fool the current systems and then the final pro and the mix assistant to predict answer to the drafting so the by um here's the table shows that by adding a lot of these adversary examples they found that the performance actually draws a lot of just by that model so it drops from 75.5 to even like 30 percent so for even like this kind of attack the performance will just drop to very low like 4.8 so here's another paper that actually just came out in 2020 so it has me a lot of the evidence showing the similar things that so today we can build a very good reading comprehension data set on individual data on the individual data sets but this season channel one dataset physic can already generalize to other data sets so the diagonal is basically the of this table is basically channel one mod to model one data set and evaluate on the same data set and for all the other numbers in this table but it shows that if you try and run some system on one data set and then you value it on another data set the performance will drop quite a lot so it's basically really kind of generalized from one data set to another data set so finally this is actually a very interesting um result so this small this paper is actually the best paper from acl 2020 is called checklist paper so the idea is that this this authors basically try to propose some kind of test cases to check whether these models can actually really unders um answer some simple questions rather with some specific or particular phenomena they find that by just come up with some really simple questions for example here jeremy is more optimistic than taylor and who is more pessimistic and that's uh they found that a birth large model channel stop and this is can still fill this type of test cases 100 percent of time and now here is another table so you can see that here is another clever example like victoria and alex are friends come on is our agent who's my agent and um so to get this kind of question correctly so it has to understand the pictorial actually refers to a female person and the alice refers to a male person so this model this kind of questions also make some kind of models very large multiple channels for the totally film of this kind of test cases okay um so i have 10 minutes left chris is any question actually answer at this point i think you can go on okay so in the last 10 minutes i'm going to give you a very very brief introduction of what it opened on my question answering and what we are have been trying to do in the last couple years so although the main question answering the problem that um so it's different from the reading comprehension that we don't assume a given passage so here with the assumption that we only have access to a large collection of documents so one example is just taking the whole english wikipedia which has like five million articles so we don't really know where the answer is located and the goal is to return the answer for any open domain questions so this problem so there isn't any single passage so we have to answer questions against a very large collection document or even the whole web documents so this is actually a much more challenging and also more practical problem um so if you look think about the example of google example i showed at the beginning so this will be techniques that will be very useful in the practical applications so the term here open domain is just in contrast to close domains that deal with questions under specific difference under the specific domain yeah okay so how can you solve this type of problem because for the reading comprehension problem we just need to answer questions based on single passage so this is a paper that i wrote in 2017 four years now so it's called the paper it's called reading wikipedia pdf to answer all questions and system called doctor qa so the favorite basically proposed the idea that we can actually solve this problem by using like a retrieval and also a reader framework so idea is that let's take a question okay so here our goal is to try to answer questions uh using like a very large collection of documents such as the wikipedia so the idea is that there's a retrieval and also reader component so the retrieval takes in the question and i try to find out like the smaller number of our documents have to be relevant to this question and this reading model basically trying to read through all the documents that just retrieval return and if i try to find out the correct answer um so formally defined here is that the input is a large collection of documents d and the question q and the output could be our answer string a so we can just decompose this problem into as i just mentioned in the retrieval and reader component so the ritual is basically trying to take the large collection document d and q and try to return a set of documents or set of passages so here the this number k could be very small um could be very small um such as like um well just like a 100 so he's basically trying to pull out a found out like 100 passages or documents um from like of let's say five million documents and finally the reader is basically takes a question and takes this set of the passages and finally found um finally return the answer so the second problem is actually the reading comprehension model that we just learned so lean um just search some 17 paper results so it's actually doing a very same simple thing so the retrieval is just a standard uh information trigger model is a sparse uh pfidf information through a sparse model and the real model is essentially just a neural reading complement comprehension model i just talked about so it's very trend on scott and some other questions three data sets um so this is really the idea is very simple but you're trying to bridge the two things how to how to have bridges this retrieval and also the reader to do this kind of open domain question answering so so i'm just going to quickly go over um some really exciting ideas that um that has been happening um uh in the last two years basically so the first idea is that this retrieval part can be also trend so we can't actually even do this kind of join the training of the retriever on the reader so here is actually so this this idea has been first proposed in canton league's paper in 2019 called legendary retrieval for weekly supervised open number questions so this part is basically the first uh model for reading comprehension and system apart is basically the retrieval model so to get this ritual among working they also try to use the birth to encode the passage and also you called the question and they try to use the dot product between the question notation and passive representation to model how how the relevance so the similarity between the question the passage but this is actually very difficult problem because the scalar scalability of this problem because there are like 20 million passages in the wikipedia so it's actually very hard to model this part but so i encourage you to check out this paper and now also on second table i want to quickly mention it there's also work i did uh last year um it's called the best passive view tool so the idea is actually very similar to the previous paper but the idea is that it's actually much more simplified model and very easy very simple straightforward approach the idea is that we can also really just trend the retrieval part by using two bird models using only the question answer pairs and this motorcycle works really well and then kind of largely all for the traditional ir retrieval models if you see this figure so the blue curve here is a traditional ir approach um like a bm25 approach and uh since the other curve the orange curve is basically training this kind of retrieval using only one thousand question answer pairs so by looking looking at all these different curves basically using different number of training examples so it's actually kind of largely briefly out from the traditional ir models um okay so again really i don't have time to talk about the details of all these approaches so i just encourage you to check out this paper uh papers and i think this result is really exciting um so here is actually a really nice demo so you know the demo is actually um hosted at this website before you check out so again so the database here is the whole wikipedia you can see that if you ask a question who tells harry potter that he's a wizard and the higher in the harry potter series and the system has really found out the correct article should be harry potter film series and then finally give you the current answer which is exactly what you have seen from the google example here so it has the answer it could be the rubrics calgary which is actually the person who tells harry potter that he's a wizard so this is exactly the perfect answer to this question okay i'm going to escape this slide and then finally um very quick so so this is something that came out uh various uh recently that some researchers have demonstrated that maybe you don't even need this retrieval study so you can if you just use a very large language model you can also just do the open domain question answering uh so the way they did this is that i hope that you have learned the t5 model in this class already so they just take a preaching language model g5 and they are trying to find through this model by taking the question and taking the question of the input as an answer as of without any explicit retrieval and they just fine tune this on the data set and they find this model can do pretty well at the testing time by just taking the question and then directly generates answers without the resorting to any like documents or like a retrieval system so this is actually very amazing so this kind of model is also called called crossbow qa systems okay very uh the last life so so just one direction and personally i'm very excited about so this is actually a new direction that basically shows that maybe for the online domain question answering maybe this real model is also not necessary anymore so so this idea was first proposed by museum in 2019 and we recently wrote a paper called dance phrases just to try to demonstrate that maybe you don't even need this like a reader model so instead we can just encode all the phrases in wikipedia using some kind of dance vectors so what you just need to do is just to do this kind of nearest neighbor search in the answer space you just encode all encode all the phrases in wikipedia encodes and using vectors and by taking a question you can just encode this question using a vector and then we can just do the vector the nearest neighbor search and then you can directly give you the answer so this is a new paragraph of this kind of the question answering model so you don't need them you just need a retrieval you don't need a reader so a great advantage for doing this is that so for the perfect reader model essentially you have to run a better model at the inference time this is actually very expensive but you can just get rid of this model you can just do the similarity search you can just do the nearest new research without the running of bird model so this could be very fast and can even run on the cpus without like leading to like a very expensive deep neural network and you can still run very well perform very very well okay finally i hope this works so i actually prepared uh demo for this transferences so i want to show you how this actually works so you can see that after you type this question who won the not nobel prize in peace in 2014 so every time just time for like a little piece of the input question and then the system can basically just find out the relevant um answer in the relevant text passages and then finally on the answer it actually shows up it's actually very fast because it's a bit of real time we don't we don't remember on the birth model so it's just a reassurance here okay i'm actually done this is lecture so that's 5 15 now yeah thanks for joining me today thank you very much dante for that awesome survey of um question answering i guess given that demo at the end people want to know whether you're launching your own search engine soon um but um um at any rate um don chi can stay for a bit to answer questions but not forever but um today um because of you know she doesn't have a stanford login we're going to do questions inside zoom so if you'd like to ask a question if you use the raise hand button we can promote you so that you appear in the regular zoom window and can just ask questions and see each other and if you hang around and don't leave the zoom for more than a few minutes maybe we'll just promote everybody who's still there into um people in the regular zoom um for some bits of discussion but um we'd welcome anyone who'd like to ask a question by asking it themselves at this point okay i've got one volunteer i've got more volunteers should i meet some questions oh should i look at the chat or oh no i mean so there are now four people who've been promoted um there are four people was the first so maybe um he could start by asking a question and then the other people that we've um promoted okay uh so thank you so much for the lecture today uh my question is mainly like if you use like a model like uh for example how uh small can your training um data set be for you to get like reasonable results so the question is how we can try the comprehension and model using only a small number of returning examples yeah i think this is a really good question um especially like the you probably have heard the gpt stream model that you've shown that if you only use a few uh very few examples you can also do the open domain question answering pretty well so i but this kind of model is huge like um what's the number like um how many parameters i forgot in the gpu stream model yeah so it's a very large very cute model but by okay so basically my answer is that if we can leverage a very large and very powerful prediction language model there is a way that we um there is a possibility that we can actually do the question string with um well with only a small number examples and now also there are some other promising directions including like unsupervised questions three so by using some kind of approach like the from the machine uh unsupervised machine translation this kind of idea that can be borrowed and the by um yeah comparison ideas from that um can also work pretty well reasonable reasonably well in unsupervised questions um yeah also i have seen some a lot of works like very busy showing that uh uh synthetic qrdss can also help a lot um in boosting the performance if you don't have enough uh supervised datasets i also have nice examples yeah so my question is it's i guess it's kind of interesting that there's not really that strong of a transfer effect um between data sets that are kind of ostensibly similar so my question is like has there been any research done on how close i guess like the formatting and the uh semantic content of these question answering data sets actually adheres to the data that like burt is pre-trained on um and if so like has there been sort of any effect found between those similarities or differences i it's a question asking like whether there has been like a song okay maybe i can just try to clarify a little bit why the current models can already generalize well from one data set to another days that we've um yeah so i actually really believe that uh most existing question answering dataset or reading comprehension dataset have been collected um from mechanic perk so it's very hard it's very difficult to avoid some kind of artifact or like a simple clues or super visual code that easier let's say not superficial but some simple clues that for the machines to pick up so for let's take those folders examples that so you can see that actually if you look at the data set more closely there has been a lot of examples as a question having like a large overlap in terms of words between the question the passage so the model is actually very good at picking up these kind of clues to get a very high performance on this data set and another data set is called um drop so it's basically about the comparison the two numbers something like that so that's the reason that uh one specialized model that has been checked very well in a one-on-one data set it's very easy to pick up this uh this kind of course and it's very hard to generalize this kind of thing to another deficit what about the natural questions data set doesn't that avoid that objection yeah natural questions will be uh much better but there are some other issues i'm not sure if you have seen that uh there's a recent paper called like a question uh trend test overlap paper so that means a demonstration i used to interrupt so natural questions was a data set that google put out about a year and a half ago maybe um where they were actually taking real questions from google search logs and then finding answers trying to find answers for them in web documents sorry go on dutchy oh i just don't think yeah i think it's definitely natural questions is much better than i said because the questions are natural like you're collected uh i really like real questions that are asking by like the users so it kind of avoids this kind of superficial artifact between the question and passage but there is some other issues that um people like to ask some common questions so if you just do the random spate of the questions into trend diving test and there's a recent paper video showing that there is actually a you know busy model is inevitable that there is a higher overlap between the training staff so if you find this question if one question that you're trying to test uh um in the desktop or the test set that household already appears in the change that that is not really generalization right yeah but this is the more on the like open domain setting not in the reading comprehension section yeah yeah um do you wanna ask a question yes so you mentioned that uh in the last part of the presentation is that the reader model is may not be necessary and you presented the dance phrases which can also work well on cpus so do we know how um how well it performs on the question answering data sets and compared to other compared to other models including birds and sources on gpu of course yeah i just encourage you to check out this paper so this this model is basically performs on par with like the dance pattern retrieval paper uh retrieval model so it's uh either performs on part of these are all the ritual reader models but it is actually right now though so i skipped one slide so so right now the saved art is actually uh dominating by this kind of uh dense passive retrieval positive generating model so this kind of so using a t5 model plus a that's a retrieval this is actually performed really well so i will just say so this can work this is similar um in this block but compared to this kind of generating model we still like a few points behind yeah okay and uh what is the kind of the uh intuition behind the dance phrases apart from like the answers are probably um in close proximity and what if the data says has answers and quest has answers to a specific question like very far uh from the actual information like see the answers to the question may not be may not reside in close proximity to the um to the words in the question so let me just clarify this um okay the goal of this project is trying to um index all the phrases in the wikipedia so i know by under this kind of rotations are built using the training set of the question answering data sets so the assumption is still the distribution of the examples in the different tests that will be similar to the training set for sure does this answer your question like so basically we're still trying to consider all the phrases in wikipedia and that test now we just take the questions and try to complete the dot product okay so if we use say a different dataset that does not present the information using a structure presented in wikipedia then um this model may not work as well as what what do you mean by structure then so uh say if we um lean more towards clearly more towards uh structures like the passages we see in standardized tests where the answers to the question may not be like um may not be in close proximity to where the the information was first introduced oh no so the the answers doesn't have to be seen in the training step so the business are going to taking the training set channel encoder for the phrases and by using and then we apply this encoder to all the free all the places all the a lot of like six meter phrases in this video so so the model is definitely able to generalize from the um training set to all the phrases to use media so it doesn't have to be seeing the things that is this what are you asking i see okay this is actually very so it's actually similar to like the retrieval or the best passive retrieval so you still like um yeah i tried to channel a passive rotation here in the first station but so the rotation is only a trend using the training set of the question answering data sets but um by taking the encoder and then we are going to encode uh all the rotations all the passages of phrases in wikipedia and then we can um expect that this rotation can actually generalize well for the unseen questions yeah so uh so that so the question is um what if the nearest neighbor so she doesn't return the answer so why do you think the nearest neighbor i mean you always can find something right it's just the question is that whether it calls him up or not yes so the sort of question is what do you think the data says that the answer is not close enough then um yeah that's good question i don't know uh if you really come up with something that is really very far away from all the questions that we have been uh seeing in the training center that could be possible i don't know basically depend on uh how the texts are um formatted then uh the nearest neighbor search may not uh work as well as other models so again the question is also the question invitation is also returned by a question code so so the question is whether this question coder can give you something reasonable in that space or not but i don't know yeah so we have been testing a lot of like a random even the imported sentences or even the question doesn't have to be a real question it could be a sentence it doesn't seem to be our problem so far yeah maybe maybe we should give a couple of other people a go and you're allowed to turn your camera on or ask a question if you want um so our next person all right hi uh thank you for taking the time to teach us um my my question is kind of quick so you mentioned work that brought up a set of relatively simple questions that show how brittle or poor the current models can be right i'm curious if that's true yeah yeah exactly exactly did that trigger a big change in the community to improve how to evaluate the models because they're actually doing pretty poorly on some of those right yeah so first these questions are simple in uh in terms of the the wording is very simple the template is very simple but they're still trying to test like a negation or temporal relational preference so the questions are not i mean in terms of the reasoning of the capacity it's not that simple just the wording very simple um i do think um okay so this paper definitely received a lot of attention in the best paper last year at acl the biggest country in the conference um so i think a lot of people are trying to solve the problem i cannot tell you that okay whether we really have a solution to this yeah oh no yeah cool yeah thank you thank you for bringing this one up it's really interesting okay next is hi thanks for taking this time so my question is kind of not relevant but like to build a robust system of question answering in what extent can in-context learning help models to be more robust with respect to different domains oh so like uh basically you provide a template generated by bird and then instead of directly predicting the clauses of text classifications you just use some word to represent that clause okay so i assume that you are actually referred to the in-context learning industry industry stuff like that okay um actually i've been doing something related to dynamic recently uh so but i'm not sure how we can actually use that in context learning in any scene for school type you know problems um yeah so i don't know if that could be robustness or not i even thought you used that technique for the questions three years i see i see thanks and i used to also mention that we can train a retriever without a reader so is there a paper about the current like attempt to do that yeah so so favorite already off so just uh yeah okay okay next hey how's it is uh thanks so much for the uh for the lecture um i'm gonna have a broader question um about the future of nlp um do you think that in order to solve nlp in a sense that you can perform on par with humans on all nlp tasks it's sufficient to only interact with text data whatever do you think will eventually need some sort of sort of experience and common sense that you get um only from seeing and sort of feeling the world and having the sorts of interactions that we as humans have yeah i mean common senses have been very difficult even in the context of constraining common sense is a very yeah now very very important topics that uh still remains i think it still remains on the result uh that's for that part the honest exactly yes i also want to mention that okay so for a lot of the reading comprehension databases or questions that you're seeing that we are pure with people start starting to achieve the human performance but this but we also see that how great of these systems are because yeah i mean they cannot regenerate or solve the easy problems so all these things need to be resolved um [Music] [Music] trying to have a human in the loop um a framework to evaluate these kind of systems just try to uh break the current system come back with some harder questions so um yeah so that means that maybe the kind of static businesses are not good enough to measure the progress so we actually really need some kind of dynamic evaluation and also introduce all more this kind of adverse examples or the um yeah harder questions or something like that are you still going for a couple more questions uh sure i don't want to make sure it's nine or 9 10 p.m uh yeah on the east coast yeah okay um so next is nexus in nerfs 2020 there was this efficient open domain question answering challenge um and from you know from performance it seemed like there was like quite substantial uh decrease versus human accuracy um probably like primarily the quantization and um also some drift that occurred when they were quantizing um so i i recently encountered this paper called uh learnable quantizers uh which essentially learns uh learns like basis representations for the quantizers like jointly with the weights of the network um and while this would be like extremely effective if you were to just like say trains from scratch i was just very curious do you think that like such a there are some way [Music] [Music] um yeah i don't think i i'm really expert to answer that question um yeah i'm not sure if i really have the answer but i also want to quickly mention that yeah quantization has been very very useful technique to make the model smaller right so we have been also exploring the contaction in in the dance phrases project recently because of the storage has been still very uh has been still very large so we are having trying to reduce that story um yeah i'm not sure about the is there a question about the connection between composition and also training uh yeah i'm not sure i'm i've also do that yeah sorry yeah thank you thanks the dante was too modest to mention that she was one of the co-organizers of the efficient qa um shared task um okay next question is hi guys you thanks so much for being here today um so my question is uh a bit different um so one example you gave that call my attention was this alex victoria example the checklist um and i was thinking technically alex was in the wrong answer right it's gender neutral and there wasn't enough context context in the question to determine who it's referring to so my question is how concerned should we be about potentially encoding uh certain biases into these circle labels or how we evaluate them or is that just more of a concern for more open-ended questions um yeah this is definitely very important again like a lot of a lot of people are trying to study okay how much buyers have been coping with models and how we can yeah um yeah i'm not sure if i have a good answer to that um again like i just i want to say like how to do the biasing of the creation language models all these things are very important and um um yeah this is just one so you're talking about this example right so this is just one test case um yeah um yeah i i don't know yeah right yes i guess i'm just a little more about you know who comes up with the test cases right who determines the other yes there is thank you i mean we will we will have more discussion of toxicity in bias coming up very soon including actually thursday's lecture as well as the later lecture um not specifically about qa though um okay next person is right thank you for the lecture um yeah my question is also related to the open domain uh classroom history so um um i was just wondering how much of like uh the uh learning side of um uh domain like sort of generalization um or like domain alignments um techniques can be uh combined with like the language um level like questioning string like to what extent would they work and um like what kind of like language specific design should be leveraged to combine with those to sort of like um if we want like higher performance and stuff like that there's a question about how to generalize between different domains or uh let's talk about how to design uh open domain qa system for different languages i'm not sure so there's like uh some some uh like learning specific designs like um um uh domination alignments and like a future level disentanglement techniques uh that have been that has shown some like uh like uh interesting performance on um other tasks um and i saw that like recently some people also like leverage similar things um like for for for question answering so i was just wondering um like to what extent uh these kind of techniques um come work on um like uh language tasks not just limited question answering but like um mainly um question answering sorry what which which work are you talking about i'm not still not sure what do you mean by this entire conversations or questions right so uh so basically um i mean this is like a little bit more specific for so um so there's this um paper called um uh doing um i forgot the exact name uh okay so i have okay i just want to make sure that we are on the same page so i have things that work instead of trying to learn some kind of in disintegration so i can better generalize to the different domains or adverse examples is this what you're saying yeah yeah and the question is that this technique can be a general general generally applied to personal strength oh um yeah they're just wondering how um to what extent would they work because um i think language has like a lot of like specific things like dependency and other stuff that like these techniques does not like actually um take care of so i was just uh yeah um yeah i'm not sure uh i think we have to try that but that's a interesting point yeah i don't know at least for the work that i've seen so far all are applied or operated at a very simple uh sentence classification task maybe anonymous uh maybe seven or correct so my understanding is as a basic take a level encoder applied to a simple test classification task and take a hidden rotation do some kind of or transformation and make sure that um yeah it can learn some kind of environmental features from the hidden reputation something like that right yeah cool yeah i'm not sure i feel like qa is a more structured task and also handles longer uh sequences um yeah so i don't know if it works uh unless people have tried that yeah thank you thank you okay and then we've got and maybe we should call this the last question hi um i'm just wondering what is like the intuitive difference between solving question answering with uh guaranteed models like t5 versus encoders like bird okay um that's the point oh okay so i'm so i skipped this slide so why this model works so well uh the reason is actually it's not really about the extracting model versus generating model the reason is that they actually um for the extracting model so if the retrieval returns let's say 100 passages so they have to extract the answer from each of the passages and then finally figure out which one is the max what has the highest score but for the generating model essentially they're trying to aggregate all the 100 passages and their reputations together and do their generation um jointly do you understand so essentially taking the 100 stations together to the joint generation instead of only do the extraction from each of the passages so i think that's actually the key difference so that's why this generating model can do really well compared to the extracting models so i also want to mention that okay so if you look at this ig model it's actually um like a comparison dpi and rhg model so ig model is always doing the generative model but they are not doing this kind of aggregation they're just trying to take out a single passage and doing the generation so the rng model actually doesn't perform as well as this model by the way i also want to mention rng model is actually not doing better than dpr because this base model is large model so this these numbers are a little bit confusing so they're actually basically really on par they're basically performed similarly but um so the the key difference between the generating model and the extracting model is that for general models you can actually leverage more com on input passage together and do the generation does this that is that clear or not um yes yeah thanks yeah otherwise you should just check out this paper here so this paper actually started pretty well then why this model can work better than the previous generating models [Music] models to [Music] oh encoders is like you're finding similarities between their encodings uh and then generate models are you like remembering the whole question and you try to retrieve that mainly like when you answer the question okay so for this model there isn't any retrieval so you cannot really find the answer from a question right so this model really has to rely on all the parameters to memorize all the information so by just taking this input it has to just rely on the branches to infer this answer so it's actually very hard too so yeah it's definitely balanced between memory and a generalization i see so um i'm just gonna uh say what i like is it like when you give got this question is embedding it in some space and then using that embedding the generator matches that to like 1882 is that what is going on in there yes the model has like it's very large like a 11 billion parameters so all these the parameters are basically trying to uh yeah memorize a lot of information that has been because the model has been pretty from the text and also has been fine-tuned so the model has been trying to memorize all the information from the text yes um do you want to call it an idol do you want one more question uh either way yeah it's up to you i'm sure i'm happy to take one more question okay let me just do okay one more question from okay uh the first question is about how how easily are these techniques generalized to other languages i like to say languages that are quite different quite different grammatical rules like chinese japanese or arabic or some other languages sort of another question maybe not exactly your domain expertise is there's a lot of interest in modeling user behavior say unlike searching behavior browsing behavior as a sequence using say transformers self-attention um and then you can use that to predict how user can like embed the user as a vector and can predict user's actions how promising do you think that would be i know this may not be your domain expertise but there is a lot of interest in extending these uh plus answering techniques are just encoding techniques embedding techniques to recommend our systems um just want to get your thoughts on okay um the first question is whether these techniques can be generalized to other languages i think it's honestly uh yes and there has been a lot of active research in this fashion but there has been also some constraints that um as a lot of models or systems i described here actually require a lot of them require very strong like a pre-trained language model and also requires a lot of training samples for the pure idss so that would be actually um i would say a bottleneck for many low resource languages right so so it's very hard to collect so many examples for other languages if we have actually i think the techniques can check can be generalized generally applied to other languages as and there has been also a lot of work trying to do like cross-lingual questions through stuff like that