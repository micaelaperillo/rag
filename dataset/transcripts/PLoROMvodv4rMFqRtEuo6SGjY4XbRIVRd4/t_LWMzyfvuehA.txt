hi everyone uh welcome to cs224n we're about two minutes in so let's get started um so today uh we've got what I think is quite an exciting lecture topic we're going to talk about self-attention and Transformers so these are some ideas that are sort of the foundation of most of the modern advances in natural language processing and actually uh sort of AI systems in a broad range of fields so it's a very very fun topic um before we get into that um [Music] okay before we get into that we're going to have a couple of reminders so there are brand new lecture notes uh uh nice thank you yeah um I'm very excited about them um they go into they they pretty much follow along with uh what I'll be talking about today but go into considerably more detail uh assignment four is due a week from today um yeah so the issues with Azure continue um thankfully thankfully um our uh uh Tas especially has tested that this works on collab and the amount of training is such that you know uh you know a collab session will allow you to train uh your machine translation system so if you don't have a GPU use collab we're continuing to work on getting access to more gpus for uh assignment five in the final project uh we'll continue to update you as we're able to um but our you know are the usual systems this year uh are no longer holding because companies are changing their minds about things okay um so our final project proposal uh you have a proposal of what you want to work on for uh your final project we will give you feedback on whether we think it's a feasible idea or how to change it so this is very important because we want you to work on something that we think has a good chance of success for the rest of the quarter that's going to be out tonight we'll have an ad announcement when it is out um and we want to get you feedback on that pretty quickly uh because you know you'll be working on this after assignment five is done really the major core component of the course uh after that is the um is the final project okay any questions cool okay um okay so so let's let's kind of take a look back into what we've done so far in this course and sort of see uh what you know what we were doing in natural language processing what was our strategy if you had a natural language processing problem and you wanted to say take like your best effort attempt at it without doing anything too fancy you would have said okay I'm going to have you know a bi-directional lstm uh instead of a simple RNN right I'm going to use an lstm uh to encode my sentences I get bi-directional context and um if I have an output that I'm trying to generate right I'll have like a unidirectional lstm you know that I was going to generate one by one so you have a translation or a parse or whatever and so maybe I've encoded in a bi-directional LCM The Source sentence and I'm sort of you know one by one decoding out the the target with my unidirectional LCM and then uh also right I was going to use something like attention to give flexible access to memory uh if I you know felt like I needed to do this sort of look back and see where I want to translate from okay and this was just working uh exceptionally well and we we motivated so you know attention through wanting to do machine translation and you have this this bottleneck where you don't want to have to encode the whole sentence Source sentence in a single vector okay and in this lecture we have the same goal so we're going to be looking at a lot of the same problems that we did previously but we're going to use different building blocks we're going to say um you know uh if if 2014 to 2017-ish I was using recurrence uh through lots of trial and error years later uh it was we had these like brand new building blocks that we could plug in sort of you know uh direct replacement for lstms and they're going to allow for just a huge range of much more successful applications and um and so what what are the what what are the issues with the recurrent neural networks we used to use and what are the new systems that we're going to use sort of from this point moving forward okay so um so one of the issues with with a recurrent neural network uh is what we're going to call linear interaction distance so as we know uh you know rnns are unrolled left to right or right to left depending on the language and the direction okay but it encodes the sort of notion of linear locality which is useful because if two words occur right next to each other sometimes they're actually quite related so tasty Pizza they're nearby and in the recurrent neural network right you sort of encode you know tasty and then you sort of walk one step and you encode Pizza um so nearby words do often affect each other's meanings um but you know you have this this problem where very long distance dependencies can take a very long time to interact so if I have the sentence the chef so those are those are nearby those interact with each other and then uh who and then a bunch of stuff like the chef who went to the stores and picked up the ingredients and you know loves garlic um and then was right like I actually have an RNN step right this sort of application of the recurrent weight Matrix and some element-wise non-linearities once twice three times right sort of as many times as there is potentially the the length of the sequence between chef and was right and it's the chef who was so this is a long distance dependency should feel kind of you know related to the stuff that we did in dependency syntax but you know it's quite difficult uh to learn potentially that these words should be related so if you have sort of a lot of steps uh between uh between words um you know it can be difficult to learn the dependencies between them you know we talked about all these gradient problems lstms do a lot better at modeling the gradients uh across long distances than simple recurrent neural networks but it's not perfect um and we already know sort of that this linear linear order isn't sort of the right way to think about about sentences so if I wanted to learn that it's the chef who uh was then you know I might have a hard time doing it because the gradients have to propagate from west to Chef and you know uh really I'd like more direct connection between words that might be related in the sentence or in a document even right if these are going to get much longer um so so this is this linear interaction distance problem we would like words that might be related to be able to interact with each other in the neural networks computation sort of graph uh more easily than uh sort of being linearly far away um yeah so that we can learn these long distance dependencies better and there's a related problem too that again comes back to the recurrent neural networks dependence on the index on the index into the sequence often call it a dependence on time so in a recurrent neural network the forward and backward passes have o of sequence length many so that means just roughly sequence in this case just sequence length many unparallelizable operations so you know we know gpus are great they can do a lot of operations at once as long as there's no dependency between the operations in terms of time that you have to compute one and then compute the other right but in a recurrent neural network you can't actually compute the RNN hidden state for time step 5 before you compute the RNN hidden state for time step four or time step three right and so you get this graph that looks very similar where if I want to compute this hidden state so I've got some word I can I have zero operations I need to do before I can compute this state I have one operation I can do before I can compute this state and as my sequence length grows right I've got okay here I've got three operations I need to do before I can compute the state with the number three because I need to compute this and this and that so there's sort of three unparallelizable operations that I'm sort of glomming you know all the Matrix multiplies and stuff into a single one so so one two three and of course this grows with the sequence length as well so uh down over here so as the sequence length grows I can't parallelize you know I can't just have a big GPU just you know with the with the Matrix multiply to compute this state because I need to compute all the previous States beforehand foreign sort of related problems both with the dependence on time yeah yeah so I have a question on the linear interaction issues I thought that was the whole point of the attention Network and then how maybe um you want during the training of the actual cells that depend more on each other can't we do something like the attention and sort of work our way so the question is uh with the linear interaction distance wasn't this sort of the point of attention that it sort of gets around that can't we use something with attention to sort of help or does that just help so it won't solve the paralyzability problem and in fact everything we do in the rest of this lecture will be attention-based but we'll get rid of the recurrence and just do attention more or less so well yeah it's a great intuition any other questions Okay cool so um so if not recurrence what about attentions even just a slide a slide back um and uh so you know just we're gonna get deep into attention today but just for the second right attention treats each word's representation as a query to access and incorporate information from a set of values so previously right we were in a decoder we were decoding out a translation of a sentence and we attended to the encoder so that we didn't have to store the entire representation of the source sentence into a single vector and here today we'll talk think about attention within a single sentence so I've got this sort of sentence written out here with a you know word one through word t in this case and um right on these sort of integers in the boxes I'm writing out the number of unparallelizable operations that you need to do before you can can compute these so for each word you can independently compute its embedding without doing anything else previously right because the embedding just depends on the word identity and then with attention right if I wanted to build an attention representation of this word by looking at all the other words in the sequence that's sort of one big operation and I can do them in parallel for all the words so the attention for this word I can do for the attention for this word I don't need to sort of walk left to right like I did for an RNN again we'll get much deeper into this but this you should uh have the intuition that it solves the linear interaction problem and the non-parelizability problem because now no matter how far away words are from each other I am potentially interacting right I might just attend to you even if you're very very far away uh sort of independent of how far away you are and I also don't need to sort of walk along the sequence linearly long so I'm treating the whole sequence at once all right so so you know the intuition is that attention allows you to look very far away at once and it doesn't have this dependence on the sequence index that keeps us from parallelizing operations and so now the rest of the lecture we'll talk uh in great depth about attention uh so maybe let's just uh move on okay so let's think more deeply about attention um you know one thing that you might think of with attention is that it's sort of Performing kind of a fuzzy lookup in a key value store so you have a bunch of keys a bunch of values and it's going to help you sort of access that so in an actual lookup table right just like a dictionary in Python for example right very simple you have a table of keys that each key maps to a value and then you like give it a query and the query matches you know one of the keys and then you return the value right so I've got a bunch of keys here and my query matches the key so I return the value simple Fair easy okay good um and in attention uh right so just like we saw before the query matches all keys softly there's no exact match uh you sort of compute some sort of similarity between the key and all of the sorry the query and all of the keys and then you sort of weight the results so you've got to query again you've got a bunch of keys the query to different extents is similar to each of the keys and you will sort of measure that similarity between zero and one through a soft Max and then you know you get the values out you you average them via the weights of the similarity between the key and the the query and the keys you do a weighted sum with those weights and you get an output right so it really is quite a bit like a lookup table but in this sort of soft Vector space you know um mushy sort of sense so I'm really doing some kind of accessing into this information that's stored in the key value store but I'm sort of softly looking at all of the results okay any questions there cool um so so what might this look like right so if I was trying to represent this sentence I went to Stanford's cs224n and learned so I'm trying to build a representation of learned um you know uh I have a key for each word so this is this self-attention thing that we'll we'll get into I have a key for each word a value for each word I've got the query for learned and I've got these sort of these sort of teal-ish bars up top which sort of might say how much you're going to try to access each of the word like so maybe 224n is not that important CS maybe that determines what I learned you know Stanford uh right and then learned maybe that's important to representing itself right so you sort of look across at the whole sentence and build up this sort of soft accessing of of information across the sentence in order to represent learned in context okay so this is just a toy a toy diagram so let's get into the math so we're going to look at a sequence of words that's W1 to n a sequence of words in a vocabulary so this is like you know Zuko made his Uncle T that's a that's a good sequence and for each word we're going to embed it with this embedding Matrix just like we've been doing in this class right so I have this embedding Matrix that goes from the vocabulary size to the dimensionality D so that's each word has a non-contextual right only dependent on itself word embedding and now I'm going to transform each word with one of three different weight matrices so this is often called key query value self-attention so right so I have a matrix Q which is an RD to D so this Maps x i to which is a vector of dimensionality D to another Vector of dimensionality D and uh so that's going to be a query Vector right so it takes an x i and it sort of you know rotates it shuffles it around stretches it squishes it makes it different and now it's a query and now for a different learnable parameter K that's another Matrix I'm going to come up with my keys and with a different learnable parameter V I'm going to come up with my values right so I'm taking each of the non-contextual word embeddings each of these xi's and I'm transforming each of them to come up with my query for that word my key for that word and my value for that word okay so every word is doing each of these roles next I'm going to compute all pairs of similarities between the keys and queries right so in the toy example we saw I was Computing sort of the similarity between a single query for the word learned and all of the keys for the entire sentence in this context I'm Computing all pairs of similarities between all keys and all values because I want to represent sort of all of these sums so I've got this sort of dot product I'm just going to take the dot product between these two vectors right so I've got Qi so this is saying the query for word I dotted with the key for Word J and I get this score which is you know a real value uh might be very large negative might be zero might be very large and positive and so that's like how much should I look at J in this lookup table and then I do the softmax right so I softmax so I say that you know the actual weight that I'm going to look at J from I is softmax of this over all of the possible indices right so it's like the the Affinity between I and J normalized by the infinity between I and all of the possible J Prime in the sequence and then my output is just the weighted sum of values so I've got this output for word I so maybe I is like one for Zuko and I'm representing it as the sum of these weights for all J so Zuko and made and his and uncle and T and the value Vector for that word uh J I'm looking from I to J as much as Alpha i j oh w i you can either think of it as a symbol in vocab V so that's like you could think of it as a one hot Vector in um yeah in this case we are I guess thinking of this so a one hot Vector in dimensionality size of vocab so in in The Matrix e you see that it's uh r d by bars around V that's the size of the vocabulary so when I do e multiplied by w i that's taking e which is d by V multiplying it by W which is V and returning a vector that's dimensionality D so first line it like W1 and that's a matrix where um it has like maybe like a column for every word in that that sentence in each column is a length V yeah usually I guess we think of it as having a I mean if I'm putting the the sequence length index first you might think of having a row for each word but but similarly yeah it's it's n which is the sequence length and then the second dimension would be V which is the vocabulary size and then that gets mapped to this thing which is sequence length by D um why do we learn two different matrices q and K when like Q transpose Qi transpose KJ is really just one Matrix in the middle between that's a great question it ends up being because this will end up being a low rank approximation to that Matrix so it is for computational efficiency reasons although it also I think feels kind of nice and uh in the presentation but yeah what we'll end up doing is having a very low rank approximation to qk transpose and so it you actually do do it like this it's a good question so the curry so I could you repeat that for me the CII so the query of the word um dotted with the key by itself doesn't look like Identity or do they look at these things in particular that's a good question okay let me remember to repeat questions so does eii right for for J equal to I so looking at itself look like anything in particular does it look like the identity is that the question okay so um so right it's unclear actually this question of should you look at yourself for representing yourself well it's it's going to be encoded by the matrices q and K right if I didn't have q and K in there right if those were the identity matrices if Q is identity K's identity then this would be sort of Dot credit with yourself which is going to be high on average like you're pointing in the same direction as yourself but it could be that you know qxi and kxi might be sort of arbitrarily different from each other because Q could be the identity and K could map you to the negative of yourself for example so that you don't look at yourself so this is all learned in practice so you end up it can it can sort of decide by learning whether you should be looking at yourself or not and that's some of the flexibility that parameterizing at SQ and K gives you that wouldn't be there if I just used xi's everywhere in this in this equation I'm going to try to move on I'm afraid because there's a lot to get on but uh we'll keep talking about self-attention and so as more questions come up I can also potentially return back um okay so so this is our basic building block but there are a bunch of barriers to using it as a replacement for for our lstms and so what we're going to do for this portion of the lecture is talk about the minimal components that we need in order to use self-attention as sort of this like very fundamental uh building block so we can't use it as it stands as I've presented it um but because there are a couple of things that we need to sort of solve or fix one of them is that there's no notion of sequence order in self-attention so so you know um what is what does this mean if I have a sentence uh like I'm going to move over here to the Whiteboard briefly and hopefully I'll I'll uh write quite large um if I have a sentence like Zuko made his uncle and uh let's say his uncle made Zuko if I were to embed each of these words right using its embedding Matrix the embedding Matrix isn't dependent on uh the index of the word so this is the word index one two three four versus now his is over here an uncle right and so when I compute the self-attention and there's a lot more in this in the lecture notes that goes through a full example um uh the actual self-attention operation will give you exactly the same representations for this sequence Zuko made his uncle as for this sequence his uncle made Zuko and that's bad because they're sentences that mean different things um and so right it's sort of this this idea that self-attention is an operation on sets like you have a set of vectors that you're going to perform self-attention on and nowhere does like the exact position of the words come into play directly um so uh we're going to encode the position of words uh through the keys queries and values that we have um so you know consider now representing each sequence Index right our sequences are going from one to n as a vector so so don't worry so far about you know how it's being made but you can imagine representing sort of the number one like the position one the position two the position three as a vector in the dimensionality D just like we're representing our keys queries and values and um so these are position vectors uh you know you can if if you were to want to incorporate the information represented by these positions into our self-attention uh you could just add these vectors these Pi vectors to the inputs right so if I have you know this this x i embedding of a word which is the word at position I but really just represents oh the word Zuko is here now I can say that oh it's the word Zuko and it's at position five because you know this Vector represents position five okay so so how do we do this um and we might only have to do this once right so we can do it once uh at the very input to the to the network and then that sort of is sufficient we don't have to do it at every layer because it sort of knows from the input um so so one way in which people have done this is look at these sinusoidal position representations so this looks a little bit like this where you have so these are this is a vector Pi which is in dimensionality D right and um each one of the dimensions you take the value I you modify it by some uh constant and you you pass it to the sine or cosine function and you get these sort of values that vary according to the period uh uh differing periods depending on the dimensionalities D so I've got this sort of a representation of a matrix where D is the vertical Dimension and then n is the horizontal and you can see that they're sort of like oh you know um as I walk along you see the period of the sine function going up and down and each of the dimensions D has a different period And so together you can represent a bunch of different uh sort of position indices and um you know it gives so this intuition that oh maybe period maybe sort of the absolute position of a word isn't as important you've got the sort of periodicity of the Sines and cosines um and maybe that allows you to extrapolate to longer sequences uh but in practice that doesn't work um but this is sort of like an early uh notion that still sometimes used for how to represent position in Transformers and self-attention networks in general um so so that's one idea you might think it's a little bit complicated a little bit unintuitive here's something that feels a little bit more deep learning so we're just going to say oh you know I've got a maximum sequence length of n and I'm just gonna learn a matrix that's dimensionality d by n and that's going to represent my positions I'm going to learn it as a parameter just like I learned every other parameter and what do they mean oh I have no idea but it you know represents position um so um right and be so you just sort of add this Matrix uh to the xi's your input embeddings um and it learns to you know fit to data so whatever representation of position that's linear uh sort of you know index based that you want you can learn and the cons are that well you definitely now can't represent anything that's longer than n words long right no sequence longer than n you can handle because um well you only learned a matrix of this many positions and so in practice you'll get you know a model error if you if you pass a self-attention model something longer than length n it will just sort of Crash and say I can't I can't do this and so this is sort of what most systems nowadays use they're more flexible representations of position including a couple in the lecture notes you might want to look at sort of like the relative linear position or words before or after each other but not their absolute position there's also some sort of representations that that hearken back to our dependency syntax because like oh maybe words that are close in the dependency parse tree should be the things that are sort of close in the uh in the self-attention operation um okay questions in practice do we typically just make n large enough that we don't run into the issue of course having something that could be input longer than him so the question is in practice do we just make n long enough so that we don't run into the problem where we're going to you know look at a text longer than n no in practice it's actually quite a problem uh even today even in the largest biggest language models and uh you know uh you know can I fit this prompt into chat GPT or whatever is the thing that you might see on Twitter I mean these continue to be issues and part of it is because the self-attention operation and we'll get into this later in the lecture it's it's quadratic complexity in the sequence length so you're going to cut you're going to spend N squared sort of memory budget in order to make sequence lengths longer so in practice you know this might be on a large model say 4 000 or so n is four thousand so you can fit four thousand words which feels like a lot but it's not going to fit a novel it's not going to fit a Wikipedia page um you know and so and there are models that do longer uh sequences for sure um and again we'll talk a bit about it but no this this actually is an issue yeah so how do you know that the P that you've learned this Matrix that you've learned is representing position as opposed to anything else the reason is the only thing that correlates this position right so like when I see these vectors I'm adding this P Matrix to my X Matrix the word embeddings I'm adding them together and the words that show up at each index will vary depending on what word actually showed up there in the example but the P Matrix never differs it's always exactly the same at every index and so it's the only thing in the data that it correlates with so you're sort of learning it implicitly like this Vector at index one is always at index one for every example for every gradient update and nothing else uh co-occurs like that yeah so what you end up learning I don't know unclear but it definitely allows you to know oh this word is with this index said this yeah okay yeah just quickly in space um okay so the question is when this is quadratic in the sequence is that a sequence of words yeah think of it as a sequence of words um sometimes there'll be pieces that are smaller than words which we'll go into in next slide in the next lecture but yeah think of this as a sequence of words but not necessarily just for a sentence maybe for an entire paragraph or an entire document or something like that where yeah the tension is based words to words okay cool I'm gonna move on um okay so um right so we have another problem uh another is that you know based on the presentation of self-attention that we've done you know there's really no non-linearities for uh sort of deep learning magic we're just sort of computing weighted averages of stuff um so so you know if I apply self-attention and then apply self-attention again and then and again and again and again you should get uh you should look at the next lecture notes if you're interested in this it's actually quite cool but what you end up doing is you're just re-averageing value vectors together so you're like Computing averages of value vectors and it ends up looking like one big self-attention uh but there's an easy fix to this if you want sort of the traditional deep learning magic and you can just add a feed forward Network to post-process each output Vector so I've got a word here that's sort of the output of self-attention and I'm going to pass it through you know in this case I'm calling it a multi-layer perceptron MLP so this is a vector in Rd that's going to be uh and it's taking in as input a vector in Rd and you know you do the usual uh sort of multi-layer perceptron thing right where you have the output and you multiply it by matrix pass it to a non-linearity multiply it by another Matrix okay and so what this looks like in self-attention is that I've got this sort of sentence the chef who the food and I've got my embedding for it I pass it through this whole big self-attention block right which looks at the whole sequence and sort of incorporates context and all that and then I pass each one individually through a feed forward uh layer right so so this embedding that's sort of the output of the self-attention for the word the is passed independently through a multi-layer perceptron here and that sort of you can think of it as sort of combining you know together uh or processing the result of attention so so there's a number of reasons why we do this um one of them also is that you can actually stack a ton of computation into these feed forward uh networks very very efficiently very paralyzable very good for gpus but but this is what's done in practice so you do self-attention and then you can you know pass it through this sort of position wise feed forward layer right every word is processed independently by this feed forward Network to process the result okay so that's adding our sort of classical deep learning non-linearities for self-attention um and that's an easy fix for this sort of no non-linearities problem in self-attention and then we have a last issue before we have our final minimal self-attention building block with which we can replace rnns and that's that uh well you know when I've been writing out all of these examples of self-attention you can sort of look at the entire sequence right and and uh in practice for some tasks such as machine translation or language modeling whenever you want to define a probability distribution over a sequence you can't cheat and look at the future right uh so you know at every time step I could Define the set of keys and queries and values to only include past words but this is inefficient uh bear with me it's inefficient because you can't parallelize it so well so instead we compute the entire n by n Matrix just like I showed in the slide discussing self-attention and then I mask out words in the future so if this score e i j right and I I computed eij for all n by n pairs of words is equal to whatever it was before if the word that you're looking at at index J is an index that is less than or equal to where you are index I and it's equal to negative infinity-ish otherwise if it's in the future and when you softmax the eij negative Infinity gets mapped to zero so now my attention is weighted zero my my weighted average is zero on the future so I can't look at it what does this look like so in order to encode these words the chef who and maybe the start start symbol there I can look at these words right that's all pairs of words and then I just gray out I I sort of negative Infinity out the words I can't look at so encoding the start symbol I can just look at the start symbol when encoding the I can look at the start symbol and the encoding Chef I can look at start the chef but I you know can't look at who right and so it with this representation of Chef that encode that is you know only looking at start the chef I can define a probability distribution using this Vector that allows me to predict who without having cheated by already looking ahead and seeing that well who is the next word questions so it says for using it in decoders um do we do this for both the encoding layer and the e-coding layer or for the encoding layer are we allowing ourselves to look forward the question is uh it says here that we're using this in a decoder do we also use it in the encoder so that this is the distinction between sort of like a bi-directional lstm and a unidirectional lstm right so wherever you don't need this constraint you probably don't use it so if you're using an encoder right on the source sentence of your machine translation problem you probably don't do this masking because it's probably good to let everything look at each other and then whenever you do need to use it because you have this Auto regressive sort of probability of word one probability of two given one you know three given two in one then you would use this so traditionally yes in decoders you will use it in encoders you will not yes um my question is a lot about philosophical how humans actually generate sentences by having some notion of the probability of future words before they say um the words that or before they choose the words that they are friendly speaking or writing regenerating good question so the question is isn't you know looking ahead a little bit and sort of predicting or getting an idea of the words that you might say in the future sort of how humans generate language instead of the sort of strict constraint of not seeing it into the future is that is that what you're okay so so right um you know trying to plan ahead to see what I should do is definitely an interesting idea um but when I am training the network right I can't if I'm teaching it to try to predict the next word and if I give it the answer it's not going to learn anything useful uh so in practice when I'm generating text maybe it would be a good idea to make some guesses far into the future or have a high level plan or something but in training the network I can't encode that intuition about how humans build uh see like generate sequences of language by just giving it the answer of the future directly at least because then it's just too easy like there's nothing to learn um yeah but there might be interesting ideas about maybe giving the network like a hint as to what kind of thing could come next for example but but that's out of scope for this yeah um yeah question up here so I understand like the like why we want to mask the future for stuff like language models but how does it apply to machine translation like why would we use it there yeah so in machine translation uh I'm gonna come over to this board and hopefully get a better marker nice in machine translation you know I have a sentence like uh I like pizza and I want to be able to uh you know translate it uh Jim uh Pizza nice um right and so uh when I'm when I'm looking at the I like pizza right I get this as the input and so I want self-attention um uh without masking because I want I to look at like and I to look at pizza and like to look at pizza and I want it all and then when I'm generating this right if my tokens are like J M La Pizza um I want to in encoding this word I want to be able to look only at myself and we'll talk about encoder decoder architectures in this uh later in the lecture um but I want to be able to look at myself none of the future and all of this and so what I'm talking about right now in this masking case is masking out you know um with like negative Infinity all of these words so that sort of attention score from to everything else should be uh net to be you know negative Infinity yeah does that answer your question great okay let's move ahead um okay so so that was our last big uh sort of building block uh issue with self-attention so this is what I would call and this is my personal opinion a minimal you know self-attention building block you have self-attention the basis of the method so uh that's sort of here in the red um and maybe we had you know the inputs to the sequence here and then you embed it with that embedding Matrix e and then you add position embeddings right then these three arrows represent using you know the uh the key the value and the query that sort of stylized there this is often how you see these diagrams um right and so you pass it to self-attention uh with the position representation right so that specifies the sequence order because otherwise you'd have no idea what order the words showed up in yeah the non-linearities in sort of the teal feed forward Network there uh to sort of provide that sort of squashing and and sort of uh deep learning expressivity and then you have masking in order to have parallelizable operations that don't look at the future okay so this is sort of our minimal uh architecture and then up at the top above here right so you have this thing maybe you repeat this sort of self-attention and feed forward many times so self-attention feed forward self tension feed forward self tension feet forward right that's what I'm calling this block and then maybe at the end of it you you know predict something I don't know we haven't really talked about that but you know you have these representations and then you predict the next word or you predict the sentiment or you predict whatever so this is like a self-attention architecture okay we're going to move on to the Transformer next so if there are any questions yeah other way around uh we will use masking for decoders where I want to decode out a sequence where I have an informational constraint where to represent this word properly I cannot have the information of the future right yeah okay okay great uh so now let's talk about the Transformers so what I've what I've pitched to you is what I call a minimal self-attention architecture uh and um you know I quite I quite like pitching it that way but really no one uses the architecture that was just up on the slide the previous uh slide it it doesn't work quite as well as it could and there's a bunch of sort of important details that we'll talk about now that goes into the Transformer but what I would hope though to sort of um have you take away from that is that the Transformer architecture as I'll present it now is uh not necessarily the end point of our search for better and better ways of representing language even though it's now ubiquitous and has been for a couple of years so so think about these sort of ideas of of the problems of using self-attention um and maybe ways of fixing some of the issues with Transformers okay so a Transformer uh decoder is how we'll build systems like language models right and so we've discussed this it's like our decoder uh with our self-attention only sort of minimal architecture it's got a couple of extra components some of which I've grayed out here that will go over one by one the first uh that's actually different is that we'll replace uh our self-attention with masking with masked multi-head self-attention this ends up being crucial it's probably the most important uh distinction between the Transformer and this sort of minimal architecture that I've presented so let's come back to our toy example of attention where we've been trying to represent the word learned in the context of the sequence I went to Stanford cs224n and learned um and I was sort of giving these teal bars to say oh maybe intuitively you look at various things to build up your representation of learned um but you know really there are varying ways in which I want to look back at the sequence to see varying sort of aspects of of information that I want to incorporate into my representation so maybe in this way I sort of want to look at Stanford cs224n because like oh it's like entities like it it you learn different stuff at Stanford cs224n than you do it other courses or other universities or whatever right and so maybe I want to look here for this reason and maybe you know there's in another sense I actually want to look at the word learned and I want to look at I you know I went and learned right is he sort of like maybe syntactically relevant words right like it's very different reasons for which I might want to look at different things in the sequence and so trying to sort of average it all out with a single operation of self-attention ends up being maybe somewhat too difficult in a way that will make precise in assignment five nice we'll do a little bit more math uh um okay so uh any questions about this in this intuition um yeah so uh it should be an application of attention just as I've presented it uh right so one independent Define the keys to find the queries to find the values I'll Define it more precisely here but think of it as I do attention once and then I do it again with different like being able different parameters being able to look at different things Etc we do not okay so the question is if we have two separate sets of Weights try to learn say to do this and and to do that how do we ensure that they learn different things uh we do not ensure that they hope that they learn different things and in practice they do uh although not perfectly uh so it ends up being the case that you have some redundancy and you can sort of like cut out some of these but that's sort of out of scope for this but we sort of Hope just like we hope that different sort of dimensions in our feed forward layers will learn different things because of lack of symmetry and whatever that uh we hope that the heads will start to specialize and that will mean they'll specialize even more and yeah okay all right so in order to discuss multi-head self-attention well we really need to talk about the matrices how we're going to implement this in gpus efficiently we're going to talk about the sequence stacked form of attention um so we've been talking about each word sort of individually as a vector in dimensionality D but you know really we're going to be working on these as as big matrices that are stacked so I take you know all of my word embeddings X1 to xn and I stack them together and now I have a big Matrix that is in dimensionality r n by D okay and uh now with my matrices k q and V I can just multiply them sort of on this side of X so X is RN by d k is our d by D so n by D times d by D gives you uh n by D again so I can just compute a big you know Matrix multiply on my whole sequence to multiply each one of the words with my key query and value matrices very efficiently right so this is sort of this vectorization idea I don't want to for Loop over the sequence I represent the sequence as a big Matrix and I just do one big Matrix multiply then the output is defined as this sort of inscrutable bit of math which I'm going to go over visually um so so first we're going to take the key query dot products in one Matrix so we've got um we've got X Cube which is uh RN by D and I've got x k transpose which is our d by n so n by d d by n this is Computing all of the e i JS these scores for self-attention right so this is all pairs of attention scores computed in one big Matrix multiply okay so this is this big Matrix here next I use the softmax right so I softmax this over uh the second dimension the second n Dimension um and I get my sort of normalized scores and then I multiply with XV so this is an N by n Matrix multiplied by an N by D Matrix and what do I get well this is just doing the weighted average right so this is one big weighted average contribution on the whole Matrix giving me my whole self-attention output and r n by D right so I've just restated identically the self-attention operations but computed in terms of matrices so you could do this efficiently on a GPU okay uh so multi-headed attention this is going to give us and it's going to be important to compute this in terms of the matrices which we'll see this is going to give us the ability to look in multiple places at once for different reasons so sort of you know first self-attention looks where this dot product here is high right this x i the Q Matrix the key Matrix uh but um maybe we want to look in different places for different reasons so we actually Define multiple query key and value matrices so I'm going to have a bunch of heads I'm going to have 8 H self-attention heads and for each head I'm going to Define an independent query key and value Matrix and I'm going to say that it's its shape is going to map from the model dimensionality to the model dimensionality over H so each one of these is doing projection down to a lower dimensional space uh this is going to be for computational efficiency and um I'll just apply self-attention sort of independently for each output so this equation here is identical to the one we saw for single-headed self-attention except we've got the sort of L indices everywhere so I've got this lower dimensional thing I'm mapping to a lower dimensional space and then I do have my lower dimensional value Vector there so my output is an r d by H but really you're doing exactly the same kind of operation I'm just doing it h different times and then you combine the outputs so I've done sort of look in different places with the different key query and value matrices and then I to get each of their outputs and then I concatenate them together right so each one is dimensionality d by H and I concatenate them together and then sort of mix them together with the final linear transformation and so uh each head gets to look at different things and construct their value vectors differently and then I sort of combine the result altogether at once okay let's go through this visually because it's at least helpful for me um so uh right it's actually not more costly to do this really than it is to compute a single head of self-attention and we'll see through the pictures so you know we were in single-headed self-attention we computed xq and in multi-headed self-attention we'll also compute X cubed the same way so xq is r and by D and then we can reshape it into our n that's sequence length times the number of heads times the model dimensionality over the number of heads so I've just reshaped it to say now I've got you know a big three axis tensor the first axis is the sequence length the second one is the number of heads the third is this reduced model dimensionality and that costs nothing right and do the same thing for x and V and then I transpose so that I've got the head axis as the first axis and now I can compute all my other operations with the head axis kind of like a batch so what does this look like in uh in practice like instead of having one big xq Matrix that's Model dimensionality D I've got like in this case three x Cube matrices of Model dimensionality D by 3 D by three D by three same thing with the key Matrix here so everything looks almost identical it's just a reshaping of the tensors and now right at the output of this I've got three sets of attention scores right just by doing this reshape and the cost is that well you know each of my attention heads has only a d by H Vector to work with instead of a d dimensional Vector to work with right so I get the output I get these three uh sets of pairs of scores I compute the softmax independently for each of the three and then I have three uh value matrices there as well each of them lower dimensional and then finally right I get my three different output vectors and if I have a final linear transformation to sort of mush them together and I get an output and in summary what this allows you to do is exactly what I gave in the toy example which was I can have each of these heads look at different parts of a sequence for different reasons so this is at a given uh block right like all of these attention heads are for a given Transformer block a next block would also could also have three attention pins the question is uh are all of these four a given block and we'll talk about a block again but this block was this sort of pair of self-attention and feed forward Network so you do like self-attention feed forward that's one block another block is another self-attention another feed forward and the question is are the parameters shared between the blocks or not generally they are not shared you'll have independent parameters at every block although there are some exceptions is it typically the case that you have the same number of like heads at each block or do you vary the number of heads across blocks you have this you definitely could vary it people haven't found reason to there so the question is do you have different numbers of heads across the different blocks uh or do you have the same number of heads across all blocks you know the simplest thing is to just have it be the same everywhere which is what people have done I haven't yet found a good reason to vary it but well it could be interesting it's definitely the case that you know after training these networks you can actually just totally zero out remove some of the attention heads and I'd be curious to know if uh you could remove more or less depending on the like layer index which might then say Oh we should just have fewer but it's again it's not actually more expensive to have a bunch so people tend to instead set the number of heads to be roughly so that you have like a reasonable number of Dimensions per head given the total Model dimensionality D that you want so for example I might want at least 64 Dimensions per head which if D is you know 128 that tells me how many heads I'm going to have roughly so people tend to scale the number of heads up with the model dimensionality excuse by slicing it in different columns you're reducing the rank of the final Matrix right yeah this but that doesn't really have any effect on the results so the question is by having these sort of reduced xq and uh uh XK matrices right this is a very low rank approximation this little sliver in this little sliver defining this whole big matrix it's very low rank is that not bad in practice no I mean again it's sort of the reason why we limit the number of heads depending on the model dimensionality because you you know you want intuitively at least some number of Dimensions so you know 64 is sometimes done 128 something like that um but you know if you're not giving each head too much to do and it's got sort of a simple job you've got a lot of heads it ends up sort of being okay at the very all we really know is that empirically it's way better to have more heads than like one uh yes um I'm wondering have there been studies to see if um information in one of the sets of the attention scores like information that one of them learns is consistent and like um related to each other or so the question is have there been studies to see if there's sort of consistent information encoded by the attention heads and you know yes actually there's been quite a lot of sort of study and interpretability and Analysis of these models to try to figure out what roles what sort of mechanistic roles each of these heads takes on and uh there's quite a bit of exciting results there around some attention heads you know learning to pick out sort of the you know it's like syntactic dependencies or maybe doing like a sort of a global averaging of context um the question is quite nuanced though because in a deep Network it's unclear and we should talk about this more offline but it's unclear if you look at a word 10 layers deep in a network what you're really looking at because it's already Incorporated context from everyone else and it's a little bit unclear active area of research but I think I should move on uh now to uh keep discussing Transformers but yeah if you want to talk more about it I'm happy to um okay so so uh another sort of uh hack that I'm going to toss in here I mean maybe they wouldn't call it hack but you know it's a nice little method to improve things it's called scaled dot product attention so one of the issues with this sort of key query value self-attention is that when the model dimensionality becomes large the dot products between vectors even random vectors tend to become uh large and when that happens the inputs to the softmax function can be very large making the gradients small so intuitively if you have two random vectors in Model dimensionality D and you just dot product them together as D grows their dot product grows an expectation to be very large and so you know you sort of want to start out with everyone's attention being very uniform very flat sort of look everywhere but if some dot products are very large then you know learning will be inhibited and so what you end up doing is you just sort of for each of your heads uh you know you just sort of divide all the scores by this constant that's determined by the model dimensionality so as the vectors grow very large their dot products don't at least at an initialization time so this is sort of like a nice little um you know important but but maybe not uh like yeah it's it's important to know um and uh so that's called scale dot product attention from here on out we'll just assume that we do this you know it's quite easy to implement you just do a little division in all of your uh computations okay so so now in the Transformer decoder we've got a couple of other things that I have un uh faded out here um we have two big optimization tricks or optimization methods I should say really because these are quite important that end up being very important we've got residual connections and layer normalization and in Transformer diagrams that you see sort of around the web they're often uh written together as this ad and Norm box and in practice in the Transformer decoder I'm going to you know apply mask multi-head attention and then do this sort of optimization add a norm then I'll do a feed forward application and then add a norm so you know this is quite important so let's go over these two individual uh components the first is residual connections I mean we've I think we've talked about residual connections before right well it's worth doing it again um uh but you know it's really a good trick to help models train better um so just to recap right we're going to take instead of having this sort of you have a layer uh layer I minus one and you pass it through a thing maybe it's self-attention maybe it's a feed forward Network now you've got layer I I'm going to add the result of layer I uh to this sort of to its input here so now I'm saying I'm just going to compute the layer and I'm going to add in the input to the layer so that I only have to learn the residual from the previous layer right so I've got this sort of connection here it's often written as this it's sort of like oh connection okay right goes around and you should think that the gradient is just really great through the residual connection right like ah you know if I've got Vanishing or exploding gradients Vanishing gradients through this layer well I can at least learn everything behind it because I've got this residual connection where the where the gradient is one because it's the identity um this is really nice and you know it also maybe is like a buy at least at initialization everything looks a little bit like the identity function now right because if the contribution of the layer is somewhat small because all of your weights are small and I have the addition from the input maybe the whole thing looks a little bit like the identity which might be a good sort of place to start and you know there are really nice visualizations I just love this visualization uh right so this is your like lost landscape right so you're gradient descent and you're trying to Traverse the mountains of the Lost landscape this is like the parameter space and down is better in your lost function and it's really hard so you get stuck in some local Optima and you can't sort of find your way to to get out and then this with residual connections I mean come on you just sort of walk down I mean it's not actually I guess really how it works all the time but I really love this it's great okay um so yeah we've seen residual connections we should move on to layer normalization um so layer Norm uh is another thing to help your model train faster um and you know there's the intuitions around layer normalization um and sort of the empiricism of it working very well maybe aren't perfectly like uh let's say connected but you know you should imagine I suppose um that we want to uh say you know this variation within each layer things can get very big things can get very small uh that's not actually informative because of you know variations between um maybe the the gradients or you know I've got sort of weird things going on in my layers that I can't totally control I haven't been able to sort of make everything behave sort of nicely where everything stays roughly the same Norm maybe some things explode maybe some things shrink um and I want to cut down on sort of uninformative variation um in between layers so I'm going to let X and r d be an individual word Vector in the model so this is like at a single a single index one vector and what I'm going to try to do is just normalize it normalize it in the sense of it's got a bunch of variation and I'm going to cut out on everything I'm going to normalize it to unit mean and standard deviation so I'm going to estimate the mean um here across uh so for all of the uh dimensions in the vector so J equals one to the model dimensionality I'm going to sum up the value so I've got this one big word vector and I sum up all the values division by D here right that's the mean I'm going to have my estimate of the standard deviation um again these should say estimates this is my simple estimate of the standard deviation or the values within this one vector and I'm just going to um and then possibly I guess I can have learned uh parameters to try to like scale back out in terms of uh multiplicatively and additively here that's optional we're going to compute this this standardization right we're going to take my Vector X subtract out the mean divide by the standard deviation plus this Epsilon sort of constant if there's not a lot of variation I don't want things to explode so I'm going to have this Epsilon there that's uh close to zero so this part here x minus mu over square root Sigma plus Epsilon is saying take all the variation and sort of normalize it to unit mean and standard deviation and then maybe I want to sort of scale it stretch it back out um and then maybe add an offset beta that I've learned although in practice actually this part and discuss this in the lecture notes uh and practice this part maybe isn't actually that important um but so layer normalization yeah you're sort of you know you can think of this as when I get the output of layer normalization it's going to be sort of look nice and look similar to the next layer independent of what's gone on because it's going to be unit mean and standard deviation so maybe that makes for a better thing to learn off of for the next layer okay any questions for uh residual or layer Norm yes yeah it's a good question when I subtract the scalar mu from the vector x i broadcast mu to dimensionality D and remove mu from Aldi yeah good point thank you that was unclear uh sure is it divided should it be divided by D or from me sorry can you repeat that in the fourth bullet point when you're calculating the mean um is it divided by D or is it or maybe I'm just interested I think it is divided by D yeah these are so this is the average deviation from the mean of all of the yeah yes [Music] mobilized based on the statistics so the question is if I have five words in the sequence do I normalize by sort of aggregating the statistics to estimate mu and sigma across all the five words share their statistics or do it independently for each word this is a great question which I think in all the papers that discuss Transformers is under specified you do not share across the five words which is somewhat confusing to me but so each of the five words is done completely independently um you could have shared across the five words and said that my estimate of the statistics are just based on all five uh but you do not I can't pretend I understand totally why for example per batch of the same position so so a similar question the question is um if you have a batch of sequences right so like just like we're doing batch Based training do you for a single word now we don't share across a sequence index for sharing the statistics we do share across the batch and the answer is no you also do not share across the batch in fact layer normalization was sort of invented as a replacement for batch normalization which did just that and the issue with batch normalization is that now your forward pass sort of depends in a way that you don't like on examples that should be not related to your example and so yeah you don't share statistics across the batch okay cool okay so so now we have our full Transformer decoder and we have our blocks so in this sort of slightly grayed out thing here that says repeat uh for a number of uh encoder or sorry decoder blocks um each block consists of I pass it through self-attention and then my ADD and Norm right so I've got this residual connection here that goes around and I've got the layer normalization there and then a feed forward layer and then another ad and norm and so that sort of set of four operations I apply you know for some number of times number of blocks so that whole thing is called a single block and uh that's it that's the Transformer uh decoder as it is cool so that's a whole architecture right there we've solved things like needing to represent position we've solved things like um not being able to look into the future uh We've solved a lot of different optimization problems you've got a question yes yes Mass to multi-head attention yeah with the dot product scaling with the square root D over H as well yeah so the question is uh how do these models handle variable length inputs um yeah so if you have so so the input to the like GPU forward pass is going to be a constant length so you're going to maybe pad to a constant length and in order to not look at the future the stuff that's sort of happening in the future you can mask out the pad tokens just like the masking that we showed for not looking at the future in general you can just say set all of the attention weights to to zero or the scores to negative Infinity for all of the pad tokens yeah exactly so you can you can uh set everything to this maximum length now in practice so the question was do you set this length that you have everything be be that maximum length I mean you know yes often although you can save computation by setting it to something smaller and uh everything the math all still works out you just have to code it properly so it can handle so you set everything instead of the N you set it all to five if everything is shorter than like five and you save a lot of computation all of the self-attention operations just work so yeah um uh there's one hidden layer in the feed forward yeah okay I should move on got a couple more things and not very much time okay um but I'll be here after the class as well so in the encoder so the Transformer encoder is almost identical but again we want bi-directional context and so we just don't do the masking right so I've got in my multi-head attention here I've got no masking and so it's that easy to make the model bi-directional okay um so that's easy so that's called the Transformer encoder it's almost identical but no masking and then finally we've got the Transformer encoder decoder which is actually how the Transformer was originally presented in this paper attention is all you need um and this is when we want to have sort of a bi-directional network here's the encoder it takes in say my source sentence for machine translation it's multi-headed attention is not masked and I have a decoder to decode out my sentence now but you'll see that this is slightly more complicated I have my masked multi-head self-attention uh just like I had before in my decoder but now I have an extra operation which is called cross attention where I'm going to use my decoder vectors as my queries then I'll take the output of the encoder as my keys and values so now for every word in the decoder I'm looking at all the possible words in the output of all of the blocks of the encoder yes yeah longer because I know initially it was like the keys and the values how do we get like a key in value separated from the output because then we collapse those into the single output uh so we well how sorry how will we get the keys and values out like how do we because when we have the output didn't we collapse like the keys and values into like a single output so the output we capture those yeah the question is how do you get the keys and values and queries out of this sort of single collapsed output now remember the output for each word is just this weighted average of the value vectors for the for the previous words right and then from that output for the next layer we apply a new key query and value transformation to each of them for the next layer of self-attention so it's not actually that you're here yeah you apply the key Matrix the query Matrix to the output of whatever came before it yeah um and so just in a little bit of math right we have um these vectors H1 through each n I'm going to call them that are the output of the encoder right and then I've got vectors that are the output of the decoder uh so I've got these Z's I'm calling the output of the decoder and then I simply Define my keys and my values from the encoder vectors these H's right so I take the H's I apply a key Matrix and a value Matrix and then I Define the queries from my decoder so my queries here so this is why two of the arrows come from the encoder and one of the arrows comes from the decoder I've got my Z's here get my queries my keys and values from the encoder okay uh so that is it I've got a couple of minutes I want to discuss some of the sort of results of Transformers and I'm happy to answer more questions about Transformers after class so um so you know really the original results of Transformers they had this big pitch for like oh look you can do way more computation because of parallelization they got great results in machine translation so you had um you had Transformers sort of doing quite well although not like astoundingly better than existing machine translation systems um and they but they were significantly more efficient to train right because you don't have this this parallelization problem you could compute on much more data much faster and you could make use of faster gpus much more um you know after that there were things like document generation where you had the sort of old standard of sequence to sequence model to the lstms and eventually everything became sort of Transformers all the way down um uh Transformers also enabled this revolution into pre-training which we'll go over uh in lecture next class um and sort of the efficiency the parallelizability allows you to compute on tons and tons of data and so after a certain point sort of on standard large benchmarks everything became Transformer based this ability to make use of lots and lots of data lots and lots of compute just put Transformers Head and Shoulders above lstms in let's say almost every sort of modern advancement in uh in natural language processing um there are many sort of drawbacks and and variants to Transformers you know the clearest one that people have tried to work on quite a bit is this quadratic compute problem so this all pairs of interactions right means that our sort of total computation for each block grows quadratically with the sequence length and in a student's question we heard uh that you know well as as the sequence length becomes long if I want to process you know a whole Wikipedia article a whole a whole novel that becomes quite unfeasible and actually you know that's a step backwards in some sense because for recurrent neural networks it only grew linearly with the sequence length um you know other things people have tried to work on are sort of better position representations because the absolute index of a word is not really you know the best way maybe to represent its position in a sequence um and just to give you an intuition of quadratic sequence length right remember that we had this big Matrix multiply here that resulted in this Matrix of n by n and Computing this is like a you know a big a big cost it costs a lot of memory um and so there's been work uh oh yeah and so you know if you think of the model dimensionality as like a thousand although today it gets much larger then for a short sequence of n is roughly 30 maybe the you know if you're Computing N squared times d uh 30 isn't so bad but if you had something like you know 50 000 then N squared becomes huge and sort of totally infusible so people have tried to sort of map things down to a lower dimensional space to get rid of the sort of quadratic computation but in practice I mean as people have gone to things like gpt3 chat GPT most of the computation doesn't show up in the self-attention so people are wondering sort of is it even necessary to get rid of some attention operations quadratic constraint it's an open form of research whether this is sort of necessary and then finally there have been a ton of modifications for the Transformer uh over the last you know five four ish years and um it turns out that the original Transformer plus maybe a couple of of modifications is pretty much the best thing there is still um there have been a couple of things that end up being important changing out the non-linearities and the feed forward Network ends up being important but it's sort of uh it's had lasting power so far and so it's it's but I think it's it's right for uh people to come through and think about how to sort of improve it in various ways so um pre-training is on Tuesday uh good luck on assignment four and then yeah we'll have the project proposal documents out tonight uh for you to talk about