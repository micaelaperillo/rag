good afternoon folks uh welcome to lecture 18. today we'll be talking about some of the latest and greatest developments in neural nlp where we've come and where we're headed uh chris just to be sure uh are my present and what's visible from this part is it fine you're visible okay uh but none of my presenters right correct okay great thank you um so just as a reminder note that your guest lecture reactions are due tomorrow at 11 59 pm uh great job with the project milestone reports you should have received feedback now if not contact the co-staff i think uh you know we had some last minute issues uh but if that's not resolved please contact us um uh finally the project reports are due very soon uh on the march 16th which is next week there's one question on ed about the leaderboard and uh the last day to summon on the leaderboard is march 19th uh as well okay so for today we'll start by talking about extremely large language models and gpd3 that have recently gained a lot of popularity we'll then take a closer look at compositionality and generalization of these neural models um while transformer models like bird and gpt have really high performance on all benchmarks they still fail in really surprising ways when deployed how can we strengthen our understanding of evaluating these models so they more closely reflect task performance in the real world and then we end by talking about how we can move beyond this really limited paradigm of teaching models language only through text and look at language grappling finally i'll give some practical tips on how to move forward in your neural nlp research and this will include some practical tips for the final project as well okay so uh you know this this beam really kind of captures uh you know what's been going on in the field really and it's it's just that our ability to harness unlabeled data has vastly increased over the last few years and this has been made possible due to advances in not just hardware but also systems and our understanding of like self-supervised uh training so we can use like lots and lots of un-given data um so based on this here's a general representation learning recipe that just works for you know all basically most modalities so the the recipe is uh is basically as follows so convert your data if it's images converted or like it's not uh it's it's really modality agnostic so you take your data if it's images text or videos and you convert it into a sequence of integers and in step two we define a loss function to maximize data likelihood or create a denoising autoencoder loss finally in step three train on lots and lots of data um certain properties emerge only when we scale up model size and this is really the surprising fact about scale so to give some examples of this recipe in action here's gpd3 which can learn to do a really non-trivial classification problem with just two demonstrations and we'll talk more about this soon um another example as we saw in lecture 14 is d5 which does really effective close book qa by storing knowledge in parameters uh finally just so i covered another modality here's a recent text to image generation model with really impressive zero shot generalization okay so now let's talk about gpd3 so how big really are these models uh this table kind of presents some numbers to put things in perspective um so this so we have a collection of models starting with medium-sized lstms which was sort of a staple in pre-2016 nlp all the way to humans who have 100 trillion synapses and some in the middle we have gbt2 with over a billion parameters and gpt3 with over 150 billion parameters and this exceeds the number of synaptic connections in a honeybee brain so obviously anyone with little knowledge of neuroscience and knows that this is not an apples to oranges comparisons uh that this is an apple store in this comparison but the point here is that the scale of these models is really starting to reach astronomical numbers um so here are some facts about gbt3 uh for one it's a large transformer with 96 layers um it has more or less the same architecture as gpd2 with the exception that to scale up attention computation uh it uses these locally banded sparse attention patterns and i really encourage you to look at the paper to understand the details the reason we mentioned this here is because it kind of highlights that scaling up is simply not just changing hyper parameters as many might believe and it involves really non-trivial engineering and algorithms to make computations efficient finally all of this is trained on 500 billion tokens taken from the common crawl toronto books corpus wikipedia [Music] so what's new about gpp3 right so let's let's look at some of the results on the paper first so obviously it does better on language modeling and text completion problems as you can see from this table it does better than gpt2 at language modeling in the pentree bank as well as better on story completion on the story completion data set called ambada to give a flavor of what's to come uh let's take a closer look at this limbaugh story completion data set so the task here is that we're given a short story and we are supposed to fill in the last word um satisfying the constraints uh of the problem can be hard for a language model which could generate a multi-word completion with gpd3 the really new thing is that we can just give a few examples as prompts and sort of communicate a task specification to the model and now gpt3 knows how the completion must be a single word this is a very very powerful paradigm and we give some more examples of this in context learning in a couple more slides so apart from language modeling it's really good at these knowledge intensive tasks like uh close book qa as well as reading comprehension and here we observe that scaling our parameters results in a massive improvement in performance so now let's talk about in context learning uh gbt3 demonstrates some level of fast adaptation to completely new tasks this happens via what's called in context learning as shown in the figure the model training can be characterized as having an outer loop that learns a set of parameters that makes the learning of the inner loop as efficient as possible and with this sort of framework in mind we can really see how a good language model can also serve as a good few short learner so in this segment we will have some fun with gpd3 and look at some demonstrations of this in context learning um [Music] so uh to start up here is an example where someone's trying to create an application that converts language a language description uh to bash one language the first three examples are prompts followed by generated examples from gpd3 uh so it gets a list of running processes right this one's easy probably just involves looking at a hash table some of the more challenging ones that involve copying over um you know some uh spans from the text like the scp example is kind of interesting as well as the harder one to parse grip the scp example comes up a lot uh during office hours so gpd3 knows how to do that here's a somewhat more challenging one where the model is given a description of a database in natural language and it starts to emulate that behavior so the text in bold is sort of the prompt given to the model the prompt includes somewhat of a functional function specification of what a database is so it says that the database begins knowing nothing the database knows everything that's added to it the database does not know anything else and when you ask a question to the database if the answer is there in the database the database must return the answer otherwise it should say it does not know the answer so this is very new and very powerful um and you know the prompt also includes some example usages so when you ask two plus two the database does not know you ask the capital of france the database does not know and then you add in a fact that tom is 20 years old to the database and now you can start asking it questions like where does tom live and as expected it says that the database does not know but now if you ask it what's tom's age uh the database says that tom is 20 years old and if you ask what's my age the database says basically that it does not know because that's not been added so this is really powerful um here's another one uh now uh in this example the model is asked to blend concepts together and so there's a definition of what does it mean to blend concepts so if you take airplane and car you can blend that to your flying car that's essentially you know there's a wikipedia definition of what concept blending it concept blending is along with some examples and now let's look at uh you know some some some problems followed by what gp3 answers so the first one is straightforward two-dimensional space uh blended with 3d space gives 2.5 dimensional space the one that is somewhat interesting is old and new gives recycled um then triangular square gives trapezoid that's also interesting the one that's like really non-trivial is a geology plus neurology used to sediment neurology and i had no idea what this was it's apparently correct um so clearly it it's able to do these very flexible things just from a just from prompt so here's another you know class of examples that gbt3 uh you know gets somewhat right and these are uh these copycat analogy problems which have been really well studied in cognitive science science and the way it works is that i'm going to give you some examples and then ask you to uh you know introduce a function from these examples and apply it to you apply to like new queries so if abc changes to abt what does pqr change to well pqr must change to pqs because the function we've learned is that the last letter must be incremented by one and and this function uh humans can now apply to examples of like you know varying types so like uh p repeated twice q repeated twice r repeated twice much change to be repeated twice q repeated twice and s repeated twice um and it seems like gpd3 is able to get them right uh more or less but uh the problem is that if you if you ask it to generalize to uh you know examples that have increasing number of repetitions then were seen in the prompt it's not able to do that so in this situation uh you ask it to you know make an analogy where um the the the letters are repeated four times and it's never seen that before it doesn't know what to do and so it gets all of these wrong so you know there's a point to be made here about uh just like maybe these prompts are not enough to convey uh you know the function the model should be learning and maybe even more examples that you can learn but the point is that it probably doesn't um it probably it probably does not have the same kind of generalization that humans have and that brings us to sort of the limitations of these modules and some some open questions so just looking at the paper and uh you know passing through the results it seems like the model is bad at logical logical and mathematical reasoning anything that involves doing multiple steps uh of reasoning and that explains why it's bad at arithmetic why it's bad at work problems why it's not great at analogy making and even like traditional textual entailment data sets that seem to require logical reasoning like rte so second most subtle point is that it's unclear how we can uh make permanent updates to the model like maybe if i want to teach a model a new concept that's possible to do it while i'm interacting with the system but once the interaction is over it kind of restarts and does not have a notion of knowledge and it's not that this is something that the model cannot do in principle but just something that's not really been explored [Music] um it doesn't seem to exhibit human-like generalization which is often called systematicity and i'll talk a lot more about that and finally language is situated and gpt3 is just learning from text and there's no exposure to other modalities there's no interaction so maybe the aspects of meaning that it requires are like somewhat limited and maybe we should explore how we can bring in other modalities so we'll talk a lot more about uh these last uh a lot last few limitations the rest of the lecture but maybe i can possibly some questions now if there are [Music] any i don't think there's a big outstanding question but i mean i think some people aren't really clear on you know few shot setting and prompting versus learning and i think it might actually be good to explain that a bit more okay yeah so um so maybe let's let me pick a simple example um let me pick this example here so uh prompting just means that so gpd3 like if you go back to first principles right gbt3 is basically just a language model and what that means is uh given a context it'll tell you what's the probability of of the next word right so if i give it a context uh w1 through wk uh gpd3 will tell me what's the probability of w uh k plus one for you opens the vocabulary so that's that's what a language model is uh a prompt is essentially a context that gets pre-bended before gt3 can start uh generating and what's happening with in context learning is that the uh the context that you append uh that that you that you pre-pen to gp3 are basically xy examples um so that's that's the prompt and the reason why it's also uh it's equivalent to few short learning is because you pre-bend a small number of xy examples so in this case if i just prepend this uh this one example that's highlighted in purple then that's essentially one shot learning because i just give it a single example as context and now like given uh you know given this query which is also appended due to the model it has to make a prediction so um so the input output format is the same as how a few shot learner uh would receive but since it's a language model the training data set is essentially presented as a context so someone is still asking can you be more specific about the in-context learning setups what is the task right so um so let's see maybe i can go to um yeah so maybe i can go to this slide so the task is just that i'm it's a language model so it gets a context which is just a sequence of tokens and the task is just to you know uh uh so you have a sequence of tokens and then the model has to generate given a sequence of tokens and the way you can convert that into an actual machine learning classification problem is that uh so for this example maybe you give it 5 plus 8 equals 13 7 plus 2 equals 9 and then one plus zero equals and now gpd3 can fill in uh you know a number there so that's how you convert it into a classification problem the context here would be these two examples of uh of arithmetic like five plus eight equals thirteen and seven plus two equals nine and then the query is one plus zero equals and then the model since it's just a language model has to fill in one plus zero equals question mark so it fills in something that doesn't have to fill in numbers it could fill in anything and but if it fills in a one uh you know it does the right job so that's how you can take like a language model and do few shot learning with it i'll keep on these questions how is in context learning different from transfer learning so i i guess the like in in context learning i mean you can think of in context learning as being a kind of transfer learning but like transfer learning does not specify the mechanism through which the transfer is going to happen within context learning the mechanism is that the training examples are sort of appended to the model which is a language model just uh you know in order so let's say you have x y x one y one x two y two and these are just appended directly to the model and now it makes prediction on you know some query uh some some queries that are drawn from this data set so yes it is uh it is a sub-category of transfer learning but transfer learning does not specify um exactly how this transfer learning is achieved but in context learning is very specific and says that for language models you can essentially concatenate the training data set and then present that to the language model people still aren't sufficiently clear on what is or isn't happening with learning and prompting so you know another question is so in context learning still needs fine tuning question mark we need to train gpt 3 to do in context learning question mark right so um so there are two parts to this question right so uh so the answer is yes and no so of course the the model is a language model so it needs to be trained so you start with some random parameters and you need to train them but the model is trained as a language model right and once the model is trained you can now use it uh to do transfer learning and the model parameters in in context learning are fixed you do not update the model parameters all you do is that you give it these uh you know small training set to the model which is just appended to the model as context and now the model can start generating from that point on so in this example if 5 minus 8 equals 13 and 7 plus 2 equals 9 are two xy examples in in vanilla transfer learning what you would do is that you would take some great in steps update your model parameters and then make a prediction on one plus zero equals what right but within context learning all you're doing is you just concatenate uh 5 plus 8 equals 13 and 7 plus 2 equals 9 to the model's context window and then make it uh predict what one plus 0 should be equal to maybe we should end for now with one other bigger picture question which is do you know of any research combining these models with reinforcement learning for the more complicated reasoning tasks so that is an excellent question uh there is some recent work on kind of trying to align um language models with human preferences where yes there is like uh you know some amount of fine tuning with reinforcement learning based on like these preferences from humans so maybe you want to do a summer you want to do a summarization problem with gbd3 and the model produces multiple summaries and for each summary maybe you have a reward that is essentially a human preference like maybe i want to include some facts and i don't want to include you know some other uh non-important facts and so i can construct a reward out of that and i can fine-tune the parameters of my language model uh basically using uh reinforcement learning based on this reward which is essentially human preferences uh so there's some very recent work that tries to do this but i'm not sure uh yeah i'm not aware of any work that tries to use reinforcement learning to teach her reasoning i did these models but i think it's a interesting future direction to explore maybe you should go on at this point okay okay so we'll talk a bit more about these last two points uh so systematicity and language grounding um so just to start off like how do you define systematicity so really the definition is that there is a definite and predictable pattern among the sentences that native speakers of a language understand and so there's a systematic pattern among the sentences that we understand what that means is let's say there's a sentence like john loves me right and if a native speaker understands the sentence then they should also be able to understand the sentence mary loves john and closely related to this idea of systematicity is the principle of compositionality and for now i'm going to uh you know ignore the definition by montague and just look at the rough definition and then we can come back to this other like more concrete definition the rough definition is essentially that the meaning of an expression is a function of the meaning of its parts so that brings us to the question are human languages really compositional and here are some examples that you know make us think that maybe uh yes so like if you look at what is the meaning of the noun phrase brown cow so it is composed of the meaning of the adjective brown and and the noun cow um so all things that are brown and all things are a cow take the intersection and get brown cows similarly red rabbits so all things that are red all the things are rather combining them get red and then kick the ball this work phrase can be understood as you have some agent that's you know performing a like kicking operation on the ball uh but this this is not always the case that uh you can like get the meaning of the whole by combining meanings of parts so here we have some counter examples that people often use so like a red herring does not mean all things that are red and all things that are heading and kick the bucket definitely does not mean that there's an agent that's kicking the bucket so uh while these examples like are supposed to be provocative like we think that language is like mostly compositional there's lots of exceptions but for vast majority of sentences that we've never heard before we're able to understand what they mean by piecing together the words that the sentence is composed of and so what that means is that maybe compositionality of representations are helpful prior that could lead to systematicity in behavior um and that brings us to the questions that we ask in the segment are neural representations compositional and the second question is if so do they generalize systematically um so how do you even measure if representations that on your network learns exhibit compositionality um so let's uh let's go back to this definition from logic u which says that compositionality is about the existence of a homomorphism from syntax to something um and to look at that we have this example which is lisa does not skateboard and we have a syntax tree uh corresponding to this example and the meaning of the sentence can be composed in uh according to according to the structure that's decided by the syntax so meaning of lisa does not skateboard it's a function of the meaning of lisa and does not skateboard the meaning of does not skate but is a function of does and not skateboard meaning of not skateboard is a function of north and skateboard so that's good um and so this gives us one way of formalizing how we can measure compositionality in neural representations and so compositionality of representations could be thought of as how well the representation approximates an explicitly homomorphic homomorphic function and learned in a large representation space so what we are going to do is essentially measure if we were to construct a neural network that whose computations are based exactly according to these parse trees how far are the representations of our learnt model from this explicitly compositional uh representation and that gave us some understanding of how compositional the neural networks representations really are uh so to unpack that a little bit uh instead of having um yeah so so instead of having uh denotations we have uh representations uh uh in the node uh and to like kind of be more concrete about that uh we first start by choosing a distance function that tells us how far away two representations are and then we also need a way to compose together two constituents to give us uh sort of the meaning of of the whole and but once we have that we can start by uh we can create like an explicitly compositional function right so what we do is uh we have these uh we have these uh representations at the leaves that are initialized randomly and the composition function that's also initialized randomly and then a forward pass according to this syntax is used to compute the representation of lisa does not skateboard and now once you have this representation you can create a loss function and this loss function measures how far are the representations of my neural network from this second sort of proxy neural network that i've created and then i can uh basically optimize both the composition function and the embeddings of the leaves and then once the optimization is finished i can measure how far was the representation from of my neural net from this explicitly compositional network on a held outside and that then tells me whether the representation of my neural net learnt were actually compositional or not so uh to see how well this works let's look at a plot and um this is relatively uh complex but uh just to unpack this a little bit uh it it it plots uh the mutual information between uh the input that uh the neural network receives versus the representation against this tree reconstruction error that that we were talking about and to give some more background about what's to come uh there is a theory of the which is called the information bottleneck theory which says that uh as a neural network trains uh it first tries to maximize the mutual information between the representation and the input in an attempt to memorize the entire data set and that is called uh that is our memorization phase and then once memorization is done there is a learning or a compression phase where this mutual information starts to decrease and the model is essentially trying to compress the data or consolidate the knowledge in the data into its parameters and what we are seeing here is that as a model learns which is characterized by decreasing major information we see that the representations themselves are becoming more and more compositional and overall we observe that learning is correlated with increased compositionality as measured by this tree reconstruction error so that's really encouraging so uh now that we have a method of measuring compositionality uh of representations in these neural nets uh how do we you know start to create benchmarks now you know that let's see if they are generalizing systematically or not so to do that uh here's a method for taking any data set and splitting it into a trained test split uh that explicitly tests for this kind of generalization so to do that we use this principle called maximizing the compound divergence and to illustrate how this principle works uh we look at this toy example so in this toy example we have a training data set that consists of just two examples and test data set of just two examples um the atoms that are defined as the primitive elements uh so entity words predicates question types so you know in this toy example goldfinger christopher nolan these are all so the primitive elements and the compounds are compositions of these primitive elements so who directed entity would be the composition of the question type did x predicate y and and the predicate direct so here's a basic machinery for producing compositionally challenging splits so uh let's start by introducing two distributions the first distribution is the normalized frequency distribution of the atoms so given any data set if we know what the notion of atoms are we can basically compute the frequency of all of the atoms and then normalize that by the total count and that's going to give us um one one distribution and we can repeat the same thing for the compounds and that'll give us a second uh frequency distribution so uh note that these are just two probability distributions and once we have these two distributions we can essentially define the atom and compound divergence simply as uh this quantity here and where there is the journal coefficient between two categorical distributions the churn of coefficient basis basically measures uh how far two categorical distributions are so just to get a bit more intuition about this uh if we set p to q then the turn off coefficient is one which means these these representations are like maximally similar and then if p is non-zero everywhere q is zero um or or if or if p is zero in all the places where q is zero then the channel coefficient is exactly uh is exactly zero which means that these two distributions are maximally far away and uh the overall goal by uh describing uh this objective is that uh this loss objective is just that we are going to maximize the compound divergence and minimize the atom divergence and so what is the intuition behind doing such a thing so what we want is to ensure that the unigram distribution in some sense is constant between the train and test split so that it's uh so that the model does not encounter any new words but we want the compound divergence to be very high which means that these same words that the model has seen many times must appear in new combinations which means that we are testing for systematicity and so if you do uh if you follow this procedure for a semantic passing data set let's say what we see uh is that as you increase the scale we see that the smaller just does better and better at a compositional generalization but uh just pulling out a quote from this this paper pre-training helps for compositional generalization but doesn't fully solve it and what that means is that maybe as you keep scaling up these models you'll see better and better performance or maybe it starts to saturate at some point in any case we should probably be thinking more about this problem instead of just trying to brute force it so now uh this segment kind of tells us that the way we split a data set you know we can measure for like different kinds of um we can measure like different behaviors of the model and that tells us that maybe we should be like thinking more critically about how we're evaluating models in nlp in general so you know there has been a revolution basically over the last few years in the field where we're seeing all of these large transform models be all of our benchmarks at the same time there's uh you know still not complete confidence that once we deploy these systems in the real world they're going to you know be like they're going to maintain their performance and so it's unclear if these gains are coming from spurious correlations or some real task understanding and so how do we design benchmarks that accurately tell us how well this model is going to do in the real world and so i'm going to give one example of works that try to do this and that's the idea of dynamic benchmarks and what dynamic what the the idea of dynamic benchmarks is basically saying that instead of testing our models on static uh on static test sets we should be evaluating them on an ever-changing dynamic benchmark and there's many recent examples of this and and the idea dates back to a 2017 workshop at emlp and so the overall schematic looks something like this that we start with a training data set and a test data set which is the static uh static order we train a model on that and then once the model is trained uh we deploy that and then have humans create new examples that the model fails to classify and uh crucially we're looking for examples the model does not get tried but humans have no issue figuring out the answer to so by playing this game of whack-a-mole where you know we humans figure out what are sort of the holes in the model's understanding and then add that back into the training data retrain the model deploy it again have humans create new examples we can essentially construct this never-ending uh you know data set this never-ending test set um which can hopefully be a better proxy of estimating real-world performance um so so this is some really cutting-edge research and one of the main challenges of you know this class of works is that it's unclear how much this can scale up because uh maybe after certain after multiple iterations of this whack-a-mole uh humans are just fundamentally limited by creativity so figuring figuring out how to uh you know deal with that is is really an open problem and kind of approaches just use examples from other datasets to you know prompt humans to think more creatively but maybe we can come up with like better like more automated methods of doing this so uh this brings us to sort of the final segment or actually let me stop for questions at this point and see if people have questions here's a question with dynamic benchmark doesn't this mean that the model creator will also need to continually test slash evaluate the models on the new benchmarks new data test new data states uh wait a second sorry um yeah so with with dynamic benchmarks yes it's absolutely true that uh you will have to continuously keep training your model and that's just to ensure that um you know the the reason your model is not doing well on the test set doesn't have to do with like this domain mismatch um and what we're really trying to do is like you know measure how like just come up with a better estimate of the model's performance for the overall task and just trying to get like more and more data so yes to answer to answer your question yes we need to keep like training the model again and again but this can be automated okay so uh i'll move on to sort of uh language grounding so uh in this final step segment i'll talk about how we can move beyond just training models uh on text alone um so many have articulated the need to use modalities other than their ex if we someday want to get at real language understanding and uh this has you know ever since we've had like these big language models you know this there has been sort of a rekindling of this debate and recently there was uh multiple papers on this and so at acl last year there was this paper that argues uh through multiple thought experiments that it's actually impossible to acquire meaning from form alone where meaning refers to the communicative intent of a speaker and form refers to text or speech signals um a more modded version of this was put forward by the second paper where they say that training on only web scale data kind of limits the world scope of models and kind of limits uh the aspects of meanings that the model can actually acquire um and so here's sort of a diagram that i've borrowed from the paper and what they say is uh the era where we were training modernism like supervised data sets uh models were limited in words called one and now that we've moved on to exploiting like unlabeled data we're now in world scope 2 where models just have strictly more signal to get more aspects of the minimum if you mix an additional modalities into this so maybe you make some videos and maybe you make some images then that expands out the world scope of the model further and now maybe it can acquire more aspects of meaning such that now it knows that the lexical item in red refers to you know red images maybe and then if you go beyond that you can have a model that is embodied and it's actually living in an environment where it can interact uh with its data conduct um interventions and experiments and then if you go out uh go even beyond that you can have models that live in a social world where they can interact with other models because after all the purpose of language is to communicate and so she can have like a social world where models can communicate with other models that kind of expands out uh aspects of meaning [Music] and so gpd3 is in world scope queue so there are a lot of open questions in this space so given that there are all of these good arguments about how we need to move beyond text what is the best way to do this at scale um we know that you know babies cannot learn language from watching tv alone for example so there has to be some interventions and there has to be interactions that need to happen but at the same time the question is how far can models go by just training on static data as long as we have additional modalities especially when we combine this with scale and if interactions with the environment are really necessary how do we collect data and design systems that interact minimally or in a cost effective way and then finally put pre-training on text still be useful if any of these other uh if any of these other like um any of these other research directions uh become more sample efficient so if you're interested in learning more about this topic i highly encourage you to take cs224u which is offered in spring they have like multiple lectures on just language okay so in this final segment i'm gonna talk a little bit more about how you can get involved with uh you know nlp and deep learning research and how uh you know how you can make more progress so uh here are some general principles for how to make progress in english and research so i think the most important thing is to just kind of read broadly which means not just read the latest and greatest papers in archive but also read like pre-2010 statistical nlp um learn about the mathematical foundations of machine learning to understand how generalization works so take cs 29 m uh learn more about language which means taking uh classes in the linguistics department in particular i would recommend universe 138 and also take cs224u and finally if you wanted uh if you want to take inspiration from how babies learn then definitely read about child language acquisition literature it's fascinating uh finally learn how to learn your software tools which involves scripting tools uh version control data wrangling uh learning how to visualize quickly with jupiter notebooks and deep learning often involves um you know running multiple experiments with different hyper parameters and different ideas all in parallel and sometimes it can get really hard to keep track of everything so learn how to use experiment management tools like weights and biases and uh finally i'll talk about some really quick final project tips um so firstly let's just start by saying that if your approach doesn't seem to be working please do not panic uh put assert statements everywhere and check if the computations that you're doing are correct use breakpoints extensively and i'll talk a bit more about this uh check if the loss function that you've implemented is correct and one way of debugging that is to see that uh the initial values are correct so if you're doing a kva classification problem then the initial loss should be the natural log of k always always always start by creating a small training data set which has like five to ten examples and see if your model can completely overfit that if not there's a problem with your training loop um check for saturating activations and dead values and often this can be fixed by you know like maybe there's some problems to gradient so maybe there's some problem with the initialization which brings me to the next point check your gradient values see if they're too small which means that maybe you should be using residual connections or lstms or if they're too large then you should use gradient clipping in fact always use gradient clipping um overall be methodical if your approach doesn't work come up with hypotheses or for why this might be the case design oracle experiments to debug it look at your data and look at the errors that it's making and just try to be systematic about everything so um i'll just say a little bit more about uh breakpoints uh so there's this great library called pdb it's like gdp but it's for python so that's why pdb um to create to create a breakpoint just add the line import pdb pdb set trace before the line you want to inspect so earlier today i was trying to play around with uh with the transformers library so uh and i was trying to do question answering so i have a really small training corpus and the context is one morning i shot an elephant in my pajamas how he got into my pajamas i don't know and the question is what did i shoot and to do to solve this problem i basically imported a tokenizer and a birth model um and i you know initialize my tokenizer initialize my model like tokenize my input i set my model into the eval mode and i try to look at the artwork but i get this error and i'm very sad it's not clear what's causing this error and so the best way to look at what's causing this error is to actually put a breakpoint um so right after modular eval i put a breakpoint because i know that that's where the problem is so the problem is in 21 so i put a breakpoint at line 21 and now once i put this breakpoint i can just run my script again and it stops before executing line 21 and at this point i can examine all of my variables so i can look at the token as input because maybe that's where the problem is and lo and behold i see that it's actually a list so it's a dictionary of lists whereas modules typically expect a dodge tensor so now i know what the problem is and that means i can quickly go ahead and fix it and everything just works uh so this just shows that you should use breakpoints everywhere uh if your code is not working and it can just like help you debug really quickly um okay so uh finally i'll say that if you want to get involved with nlp and deep learning research and if you really like the final project uh we have the clips program at stanford and this is a way for undergrads master students and phds who are interested in doing nlp research and want to get involved with the nlp group um so we highly encourage you to apply to clips um and so yeah so i'll uh conclude uh continue today's class spicing that you know we've made a lot of progress uh in the last decade and that's mostly due to you know clever understanding of neural networks data hardware all of that combined with scale we have some really amazing technologies that can do really exciting things and we saw some examples of that today um in the short term uh i expect that we'll see more scaling uh because it just seems to help so perhaps even larger models uh but this is not trivial so you know i i said that before and i'll just say it again scaling requires really non-trivial engineering efforts and sometimes even you know clever algorithms and so we there's a lot of interesting systems work to be done here but in the long term uh we really need to be thinking more about these bigger problems of like systematicity generalization how can we make our models you know learn a new concept really quickly so that's fast adaptation uh and then we also need to you know create benchmarks that we can actually trust so if my model has some performance on some sentiment analysis data set and deployed in the real world that should be reflected in the number that i get from the benchmark so we need to make progress uh in in the way we evaluate models and then also figuring out a way to move beyond text in a more tractable way this is also really essential so yeah that's that's it good luck with your final projects i can take more questions at this point so i answered a question earlier that actually i think you uh could also find on um it was the question of whether you have a large model that's pre-trained on language if it will actually help you in other domains like you apply it to vision stuff uh yeah yeah so i guess uh the answer is actually yes like there was a paper that came out really really recently like just two days ago that just takes uh i think it was gpt too i'm not sure it's like one large transformer model that's featuring on text and like other dialogues definitely apply to images and i think they apply to like uh math problems and some more modalities and show that it's actually really effective at like transfer so if you pre train on text and then you move to a different modality that helps i think part of the reason for that is just that you know across modalities there is a lot of auto aggressive structure that is shared um and i i think one reason for that is that uh language is really referring to the world around it and so you might expect that uh there is you know some there is like some correspondence that's just beyond the autoregressive structure so there's also works that show that uh if you have just text only representations and image only representations you can actually learn a simple linear classifier that can learn to align both of these representations and all of these works are just showing that there's actually a lot more common between mortalities than we thought in the beginning uh so yeah i think yeah it's it's possible to create a text and then fine-tune on your modality of interest and it should probably be effective of course based on what the modality is but yeah for like images and videos it's certainly certainly effective more questions well a couple of questions have turned up one is what's the difference between cs 224 you and this class in terms of the topics covered and focus do you want to answer that one shakar or should i have a go at answering it maybe you should answer this one okay so next quarter um cs224u natural language understanding is co-taught um by chris potts um and bill mccartney um so you know in essence um it's meant to be different that natural language understanding focuses on what its name is um sort of how to build computer systems that understand the sentences of natural language now you know in truth the boundary is kind of complex because um we do some natural language understanding in this class as well and certainly for the people who are doing the default final project um question answering well that's absolutely a natural language understanding task but the distinction is meant to be that you know at least a lot of what we do in this class things like you know the assignment 3 dependency parser or building the machine translation system in assignment 4 that they're in some sense natural language processing tasks where you know processing can mean anything but commonly means you're doing useful useful intelligent stuff with um human language input but you're not necessarily deeply understanding it so there is some overlap in the classes um if you do cs224u you'll certainly see word vectors and transformers again but the emphasis is on doing a lot more with natural language understanding tasks and so that includes things like building semantic parsers so they're the kind of devices that um will you know respond to questions and commands such as an alexa or google assistant will do building relation extraction systems which get out particular facts out of a piece of text of all this person took on this position at this company looking at grounded language learning and grounded language understanding where you're not only using the language but the world context to get information and other tasks that sort i mean i guess you can look at the website to get more details of it i mean you know relevant to this class i mean a lot of people also find it an opportunity to just get further in doing a project in the area of natural language processing that sort of by the nature of the structure of the class since you know it more assumes that people know how to build deep learning natural language systems at the beginning that rather than a large percentage of the class going into okay you have to do all of these assignments although there are little assignments earlier on that there's sort of more time to work on a project for the quarter okay here's one more question that maybe chicago could do do you know of attempts to crowd source dynamic benchmarks eg uses uploading adversarial examples for evaluation or online learning yeah so actually like uh the main idea there is to use crowdsourcing right so in fact there is this bench uh so there is this um platform that was created by bear it's called dyno bench and the objective is just that that to construct this slight dynamically evolving uh benchmark we are just going to offload it to you know users of this platform and you can you know it essentially gives you utilities for like uh deploying your model and then having uh you know humans kind of try to fool the model um yeah so so this is like it's it's basically how the dynamic evaluate the dynamic benchmark uh collection actually works so like you uh deploy a model um on some platform and then you get humans to like fool the system yeah is a question can you address the problems of nlp models not able to remember really long contexts and techniques to infer on really large input length yeah so so i guess like there have been like a few works recently right that kind of tried to scale up transformers to like really large uh context lens uh one of them is like the reformer um and there's also like the transformer excel that was i think the first one to try and do that um i think what is unclear is whether you can combine that with the scale of these gpt-like models and if you see like qualitatively different things once you do that like um and part of it is just that all of this is just like so recent right uh but yeah i think the open question there is that you know can you take these like really long context transformers that can operate over long context combine that with scale of gpd3 and then get models that can actually reason over these like really large contexts um because i guess the hypothesis of scale is that once you train language models on uh at scale it can start to do these things and so to do that for long context we actually need to like have long context transformers that are trained at scale and i i don't think people have done that yet so i'm seeing this other question about language acquisition [Music] because do you have some thoughts on this or maybe i can just do something with that um yeah so the question is um what do you think we can learn from baby language acquisition can we build a language model in a more interactive way like reinforcement learning do you know any of these attempts oh that's that's a big huge question and you know i think the the short non-helpful answer is that there are kind of no answers at the moment you know people have certainly tried to do things at various scales but you know we just have no technology that is the least bit convincing um for being able to replicate the language learning ability of a human child um but after that prologue what i could say is i mean yeah there are definitely ideas to have in your head so you know there are sort of clear results which is that little kids don't learn by watching videos so it seems like interaction is completely key um little kids don't learn from language alone they're in a very rich environment where people are sort of both learning stuff from the environment in general and in particular you know they're learning a lot from what language acquisition risk um researchers refer to as attention which is different what we mean by attention but it means that the caregiver will be looking at the object that's the focus of interest and you know commonly other things as well like sort of you know picking it up and bringing it near the kid and all those kinds of things um and you know babies and young kids get to experiment a lot right so regardless of whether it's learning what happens when you have um some blocks that you stack up and play with them or you're learning language you sort of experiment by trying some things and see what kind of response you get and again that's essentially building on the interactivity of it that you're getting some kind of response to any upfronts you make and you know this is something that's sort of been hotly debated in the language acquisition literature so a traditional chomp skin position is that you know human beings don't get effective feedback you know supervised labels when they talk and you know in some very narrow sense well that's true right it's just not the case that after a baby tries to say something that they get feedback of you know syntax error in english on word four or they get given here's the semantic form i took away from your utterance but in a more indirect way they clearly get enormous feedback they can see what kind of response um they get from their caregiver at every um corner and so like in your question um you were suggesting that well somehow we should be making use of reinforcement learning because we have something like a reward signal there um and you know in a big picture way i'd say hi yeah i agree um in terms of a much more specific way as to well how can we possibly get that to work to learn something with the richness of human language i you know i think we don't have much idea but you know there has started to be some work so people have been sort of building um virtual environments which you know you have your um avatar in and that can manipulate in the virtual environment and there's linguistic input and it can succeed in getting rewards for sort of doing a command where the command can be something like you know pick up the orange block or something like that and you know to a small extent people have been able to build things that work i mean as i as you might be picking up i mean i guess so far at least i've just been kind of underwhelmed because it seems like the complexity of what people have achieved um is sort of you know just so primitive compared to the full complex complexity of language right you know the kind of languages that people have been able to get systems to learn are ones that can yeah do pick up commands where they can learn you know blue cube versus um orange sphere and that's sort of about how far people have gotten and that sort of such a teeny small corner of what's involved in learning a human language one thing i'll just add to that is i i think there are some principles of uh how kids learn that people have tried to apply to deep learning and one example that comes to mind is curriculum learning um where there's like a lot of literature that shows that you know babies uh they tend to pay attention to things that they just that is just slightly challenging for them and they don't pay attention to things that are extremely challenging and also don't pay attention to things that they know how to solve and many researchers have really tried to get curriculum learning to work um and the verdict on that is that it seems to kind of work when you're in like reinforcement learning settings but it's unclear if it's going to work on like supervised learning settings but i still think that it's like underexplored and maybe you know there should there should be like more attempts to kind of see if we can like add in curriculum learning and if that improves anything yeah i agree curriculum learning is an important idea which we haven't really talked about but it seems like it's certainly essential to human learning um and there's been some minor successes with it in the machine learning world but it sort of seems like it's an idea you should be able to do a lot more with in the future as you move from um models that are just doing one narrow task that's trying to do a more general language acquisition process should i attempt this next question as well okay the next question is is the reason humans learn languages better just because we are pre-trained over millions of years of physics simulation maybe we should um pre-train a model the same way so i mean i presume what you're saying is physics simulation um you're evoking evolution when you're talking about millions of years so you know this is a controversial debated big question um so you know again if i invoke chomsky again so noam chomsky is sort of the most famous um linguist in the world um and you know essentially noam chomsky's career starting in the 1950s is built around the idea that little children get such um dubious linguistic input because you know they hear a random bunch of stuff they don't get much feedback on what they say etc that language could not be learned empirically just from the data observed and the only possible assumption to work from is significant parts of human language um uh innate or in the sort of human genome babies are born with that and that explains the miracle by which very little humans um learn amazingly fast how human languages work um now to speak in credit for that idea for those of you who have not been around um little children i mean i i think one does just have to acknowledge you know human language acquisition by live little kids i mean it does just seem to be miraculous right but you go through this sort of slow phase for a couple of years where you know the the kids sort of goose and gars some syllables and then there's a fairly long period where they picked up a few words and they can say juice juice um when they want to drink some juice and nothing else and then it just sort of seems like there's this phase change where the kids suddenly realize wait this is a productive generative sentence system i can say whole sentences and then in an incredibly short period they sort of seem to transition from saying one and two word utterances to suddenly they can say you know daddy come home in garage um pudding bike in garage and you go wow how do they suddenly discover language um so you know so it is kind of amazing but um personally for me at least you know i've just never believed the strong versions of the hypothesis that human beings have much in the way of language specific knowledge or structure in their brains that comes from genetic inheritance like clearly humans do have these very clever brains and if we're at the level of saying being able to think or being able to interpret the visual world that's things that have developed over tens of millions of years and um evolution can be a large part of the explanation and humans are clearly born with lots of vision specific hardware in their brains as are a lot of other creatures but when you come to language you know no one no one knows when language was in a sort of a modern like form first became available because you know there aren't any fossils of people saying you know the word um spear or something like that but you know to the extent that there are estimates based on sort so what you can see of the sort of spread of um proto-humans and their sort of apparent social structures from so what you can find in fossils you know most people guess that language is at most a million years old and you know that's just too short a time for any significant eve for evolution to sort of build any significant structure inside human brains that's specific to language so i kind of think that the working assumption has to be that sort of there's just about nothing specific to language and human brains and you know the most plausible hypothesis not that i know very much about neuroscience when it comes down to it is that humans were being able to repurpose hardware that was originally built for other purposes like visual scene interpretation and memory and that that gave a basis of sort of having all this clever hardware that you could then use for language so you know it's kind of like gpus were invented for playing computer games and we were able to repurpose that hardware to do deep learning we've got a lot of have uh come out at the end okay so this one is answered live um let's see yeah if you could name i guess this is for either of you one main bottleneck as to um uh if we could provide feedback efficiently to our systems like babies are given feedback what's the bottleneck that remains in uh trying to have more human-like language acquisition um i mean i sort of i cannot find on this again or would you start releasing something yeah i was just gonna say that i think it's a bit of everything right like i i think in terms of models um one thing i'll say is that we know that there's more feedback connections and feedback connections in the brain um and we haven't really figured out a way of kind of uh so you know of course we had rnns um you know which sort of implement like you know you can like look through an item that sort of implements a feedback loop but we still haven't really figured out how to you know use that knowledge that the brain has a lot of feedback connections and then apply that to uh like practical practical systems i think on the modeling and like maybe that's one problem um there is like yeah i think curriculum learning is maybe one of them but i think the one that's probably gonna have most bang for buck is really figuring out how we can move beyond text i think there's just like so much of more information that's available that we're just not using and so i think that's where most of the progress might come from like figuring out what's the most practical of going beyond text uh this is what i think okay um let's see uh what are some important nlp topics that we have not covered in this class i do that um you know well sort of one answer is a lot of the topics that are covered in cs224u because you know we do make a bit of an effort to keep them disjoint they're not fully um right so there's sort of lots of topics in language understanding that we haven't covered right so if you want to make um a voice assistant like alexa siri or google assistant well you need to sort of be able to interface with systems apis that can do things like delete your mail or buy you concert tickets and so you need to be able to convert from language into a explicit semantic form that can interact with the systems of the world we haven't talked about that at all um so there's lots of language understanding stuff there's also lots of language generation things so you know effectively for language generation all we have done is neural language models they are great um run them and they will generate language and you know in one sense that's true right like it's just awesome the kind of generation you can do with things like gpt two or three but you know where that's missing is that's really only giving you the ability to produce fluent text where rabbits often produces fluent text that if you actually wanted to have a good natural language generation system you also have to have higher level planning of what you're um going to talk about and how you are going to express it right so that in most situations in natural language you think okay well i want to explain to people something about why it's important to do math classes at college let me think how to organize this maybe i should talk about some of the different applications where math turns up and how it's a really good grounding you know whatever you kind of plan out here's how i can present some ideas right and that kind of natural language generation um we're not doing um any we haven't done any of um yeah i so that's sort of saying more understanding more generation which is most of nlp you can say i mean obviously there are then sort of particular tasks that we can talk about that we either have or haven't not explicitly addressed okay is there has there been any work in putting language models into an environment in which they can communicate to achieve a task and do you think this would help uh with unsupervised learning so again i guess there's been a lot of work on immersion communication um and also self-play where you have like these uh different uh models which are initialized as language models that attempt to communicate with each other to solve some tasks and then you know you have a reward at the end um whether they were able to finish the task or not and then based on that reward you attempt to learn like a communication strategy and this started out as like emergent communication and self-play and then there was like recent work i think it was like i clear last year or the year before that where they showed that if you initialize these models with like uh with like language model pre-training you um basically prevent this problem of like language drift where the language that or the communication protocol that your models end up learning has nothing to do with like actual language um and so yeah i mean from that sense there has been some work um but it's like very limited i think there's like some groups that try to study this but not beyond that okay i mean the last two questions are about gene as well as one question about whether gene smith some correlations from social cues a reward based system i don't know if either of you have opinions about this uh but if you do yeah i mean i don't have anything very deep to say about this question so it's on the importance of social cues as opposed to pure reward based systems well i mean in some sense a social cue you could also regard as a reward that people you know like to um have other people put a smile on their face when you say something um but you know i do think generally um you know not when people are saying what have we not covered another thing that we've barely covered is the social side of language so you know a huge a huge interesting thing about language is it has this very dynamic big dynamic range so on the one hand you can talk about very precise things in language so you can sort of talk about math formulas and steps in a proof and things like that so that there's a lot of precision and language but you know on the other hand you can just sort of emphatically mumble mumble whatever words at all and you're not really sort of communicating anything in the way of a propositional content um what you're really trying to communicate is you know i'm oh i'm thinking about you right now and oh i'm concerned um with how you're feeling or whatever it is in the circumstances right so that a huge part of language use is in forms of sort of social communication between human beings and you know that's another big part of actually building um successful natural language systems right so if you you know if you think negatively about something like the virtual assistants i've been falling back on a lot is you know that they have virtually no ability as social language users right so we're now training a generation of little kids that what you should do is sort of bark out commands as if you were you know serving in the german army in world war ii or something and um that there's none of the kind of social part of how to you know use language um to communicate um satisfactorily with human beings and to maintain a social system and that you know that's a huge part of human language use that kids have to learn and learn to use successfully right you know a lot of being successful in the world is you know you know when you want someone to do something for you you know that there are good ways to ask them for it you know some of its choice of how to present the arguments but you know some of it is by building social rapport and asking nicely and reasonably and making it seem like you're a sweet person that other people should do something for and you know human beings are very good at that and being good at that is a really important skill for being able to navigate the world well you