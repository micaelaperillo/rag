hi everyone uh welcome to the 224n hugging face Transformers tutorial um so this tutorial is just going to be about using the hugging face Library it's really useful in a super effective way of being able to use kind of some off the shelf NLP models specifically models that are kind of Transformer based and being able to use those for either your final project your custom final project or something like that just using it in the future so these are it's a really helpful package to to learn and it interfaces really well with pi torch in particular too okay so first things first is in case there's anything else that you are missing from this kind of like tutorial the hugging face documentation is really good they also have lots of kind of tutorials and walkthroughs as well as other kind of like notebooks that you can play around with as well so if you're ever wondering about something else that's a really good place to look okay so in the collab the first thing we're going to do uh that I already did but can maybe run again is just installing the Transformers python package and then the data sets python package so this corresponds to that hugging face Transformers and data sets um and so those are really helpful the Transformers is where we'll get a lot of these kind of pre-trained models from and the data sets will give us some helpful data sets that we can potentially use for various tasks so in this case sentiment analysis okay and so we'll use a bit of like a helper function for helping us understand what encoding is uh what encodings are actually happening as well so we'll run this just to kind of kick things off an important a few more a few more things okay so um so first what we'll do is this is generally kind of like the step by step for how to use something off of Hocking face so first what we'll do is we'll find some model um from like the hugging face Hub here and note that there's like a ton of different models that you're able to use there's bird there's gpt2 there's T5 small which is another language model from Google um so there are a bunch of these uh different models that are pre-trained and all of these weights are up here in hugging face that are freely available for for you guys to download so if there's a particular model you're interested in you can probably find a version of it here you can also see kind of different types of models on the side as well that for a specific task so if we wanted to do something like uh zero shot classification there are a couple models that are specifically good at doing that particular task okay so based off of what tasks you're looking for there's probably a hugging face model for it that's available online for you to download okay so that's what we'll do first is we'll go ahead and find a model on the hug and face Hub and then um you know whatever you want to do in this case we'll do sentiment analysis and then there are two things that we need next the first is a tokenizer for actually you know splitting your input text into tokens that your model can use and the actual model itself um and so the tokenizer again kind of converts this to some vocabulary IDs these discrete IDs that your model can actually take in and the model will produce some prediction based off of that okay so um so first what we can do is again import this Auto tokenizer and this Auto model from uh for sequence classification so what this will do initially is download some of the you know key things that we need so that we can actually initialize these so what do each of these do so first the tokenizer this Auto tokenizer is from some pre-trained tokenizer that has already been used so in general there's a corresponding tokenizer for every model that you want to try and use in this case it's like cbert so send like something around sentiment and Roberta and then the second is you can import this model for sequence classification as well from something pre-trained on the model Hub again so again this corresponds to sentiment Roberta large English and if we want we can even find this over here um we can find it as um I think English yeah large English so again this is something we can easily find you just copy this string up here and then you can import that okay we've downloaded all of the kind of all the things that we need some kind of like binary files as well and then now we can go ahead and actually you know use some of these inputs right so this gives you some set of an input right this input string I'm excited to learn about hogging face Transformers we'll get some tokenized inputs here from the actual tokenized things here after we pass it through the tokenizer and then lastly we'll get some notion of the model output that we get right so this is kind of some legits here over whatever classification that we have so in this case good or bad and then some corresponding prediction okay and we'll walk through what this kind of looks like in just a second as well a little more depth but this is broadly kind of like how we can actually use these together we'll tokenize some input and then we'll pass these inputs to the model so we'll talk about tokenizers first so um so tokenizers are used for basically just pre-processing the inputs that you get for any model and it takes some raw string to like um essentially a mapping uh to some number or ID that the model can take in and actually kind of understand so tokenizers are either kind of like are specific to the model that you want to use or you can use the auto tokenizer that will kind of conveniently import whatever corresponding tokenizer you need for that model type um so that's that's kind of like the helpfulness of the auto tokenizer it'll kind of make that selection for you um and make sure that you get the correct tokenizer for whatever model you're using so the question is uh does it make sure that everything is mapped to the correct index that the model is trained on the answer is yes so that's why the auto tokenizer is helpful so there are two types of tokenizers uh there's the a Python tokenizer and there's also like a tokenizer fast that the tokenizer fast is written in Rust in general if you do the auto tokenizer it'll just default to the fast one there's not really a huge difference here it's just about kind of like the inference time for getting the model outputs yeah uh so the question is the tokenizer uh creates dictionaries of the model inputs um so I to think it's more like I think the way to think about a tokenizer is like that um like that dictionary almost right so you want to kind of translate almost or have this mapping from the tokens that you can get from like this string and then map that into kind of some inputs that the model will actually use so we'll see an example of that in just a second so for example we can kind of call the tokenizer in any way that we would for like a typical Pi torch model but we're just going to call it on like a string so here we have our input string is hugging face Transformers is great we pass that into the tokenizer almost like it's like a function right and then we'll get out some tokenization so this gives us a set of input IDs so uh to answer the earlier question these are basically the numbers that each of these tokens represent so that the model can actually use them and then a corresponding attention mask for the particular Transformer okay so there are a couple ways of accessing the actual tokenized input IDs you can treat it like a dictionary so hence kind of thinking about it almost as that dictionary form it's also just like a property of the output that you get so there are two ways of accessing this in like a pretty pythonic way okay so what we can see as well is that we can look at the particular the actual kind of tokenization process almost and so this can maybe give some insight into what happens at each step right so our initial input string is going to be hugging face Transformers is great okay the next step is that we actually want to tokenize these individual kind of uh individual words that are passed in so here this is the kind of output of this tokenization step right we get kind of these individual split tokens we'll convert them to IDs here and then we'll add any special tokens that our model might need for actually performing inference on this okay so there's a couple steps that happen kind of like underneath when you use an actual we use a tokenizer that happens at it a few things at a time one thing to note is that for fast tokenizers as well there's another option that you're able to get to so you have essentially right you have this input string you have the number of tokens that you get you might have some notion of like the special token mask as well right so using Char to word is going to give you like the word piece of a particular character in the input so here this is just giving you additional options that you can use for the fast tokenizer as well for understanding how the tokens are being used um in the from the input string okay uh so there are different ways of using the outputs of these tokenizers too so one is that you know you can pass this in and if you indicate that you want it to return a tensor you can also return a pi torch tensor so that's great um in case you need a pie torch tensor which you probably generally want you can also add multiple tokens into the tokenizer and then pad them as however you need so for here for example we can use the pad token as being this kind of like pad bracket almost and giving the token ID is going to correspond to zero right so it's just going to add padding to whatever input that you give so if you need you need your outputs to be the same length for a particular type of model right this will add those padding tokens and then correspondingly gives you like the zeros in the attention mask where you actually need it okay and so the way to do that here is uh you basically set padding padding to be true you can also set truncation to be true as well and so if you ever have kind of like um more uh any other kind of like features of the tokenizer that you're interested in again you can check the hugging face documentation which is pretty thorough for what each of these things do yeah so the the question is kind of looking at um looking at the the hash hash at least and whether that means that we should have like a space before or not so um so here in this case um yeah so the in this case uh we probably don't want like the space before right just because um we uh have like the hugging like I don't know hugging is all one word um in this case um generally like generally the uh for like the tokenizers generally the output that they give is still pretty consistent though um in terms of how the tokenization process works so there might be kind of these like you know instances where it might be contrary to what you might expect for kind of how something is tokenized um in general the tokenization generally works fine um so in most cases kind of like the direct output that you get from the hugging face tokenizer is sufficient foreign okay awesome so one last thing past the adding kind of additional padding is that you can also kind of uh decode like an entire batch at one one given time so if we um look again we have like uh our tokenizer we'll initially have this method called like a batch decode so if we have like the model inputs that we get up here this is the output of passing these sentences or these strings into the tokenizer we can go ahead and just pass like these input IDs that correspond to that into the batch decode and it'll give us kind of this good this decoding that corresponds to all the padding we added in each of the particular kind of like uh words and strings um and if you want to you know ignore all the the presence of these padding tokens or anything like that um you can also pass that into skipping the special tokens gotcha so this gives like a this is a pretty high level overview of the how you would want to use tokenizers I guess in your um in using hugging face so now we can talk about maybe how to use the hugging face models themselves so again this is this is pretty similar to what we're seeing for something like initially using a tokenizer you just choose the specific model type um for your model and then I and you can use that or the specific kind of Auto model class where again this Auto model kind of takes almost the um like the initialization process it takes care of it for you in a pretty easy way without really any too much overhead um so additionally so um for the pre-trained Transformers that we have they generally have the same underlying architecture but you'll have different kind of heads associated with each Transformer so attention head so you might have to train if you're doing some sequence classification or just some other task so hugging face will do this for you and so for this I I will walk through an example of how to do this for sentiment analysis um so if there's a specific context like sequence classification we want to use we can use like this the very specific kind of like Class A hugging face provides so distilbert for sequence classification alternatively if we were doing it using distilbert in like a mass language model setting we use distilbert for Mast LM and then lastly if we're just doing it purely for the representations that we get out of distilled bird we just use like the Baseline model so the key thing here or key takeaway is that there are some task specific classes that we can use from hugging face to initialize so Auto model again is similar to kind of like the auto tokenizer so for this it's just going to kind of load by default that specific model and so in this case it's going to be just like kind of like the basic basic weights that you need for them okay so um so here we'll have basically three different types of models that we can look at one is like an encoder type model which is Bert a decoder type model like gpt2 that's like uh performing these like uh you know generating some text potentially and encoder decoder models so Bart or T5 in this case so again if you go back to kind of the the hugging face Hub there's a whole sort of different um different types of models that that you could potentially use and if we look in the documentation as well so here we can understand some notion of like the different types of classes that we might want to use right so there's some notion of like the auto tokenizer different Auto models for different types of tasks um so here if again if you have any kind of like specific use cases that you're looking for then you can check the documentation here again if you use like an auto model from pre like pre-trained you'll just create a model that's an instance of that or model in this case root model for the burst Burt based case okay Let's uh we can go ahead and start one last thing to note is that like again the particular choice of your model matches up with kind of the type of architecture that you have to use right so there are different these different types of models can perform specific tasks so you're not going to be able to kind of load or use Bert for instance or distill Bert as like a sequence to sequence model for instance which requires the encoder and decoder because distill distilber I only consists of an encoder so there's a bit of like a limitation on how you can exactly use these but it's basically based on like the model architecture itself okay awesome so let's go ahead and get started here um so similarly here we can import so Auto model for sequence classification so again this is we're going to perform some classification tasks and we'll import this Auto model here so that we don't have to reference again just like something like distilbert for sequence classification we'll be able to load it automatically and it'll be all set alternatively we can do distillburt for sequence classification here and that specifically will will require distilbert speed the input there okay so these are two different ways of basically getting the same model here one using the auto model one using just explicitly distiller cool and here because it's classification we need to specify the number of labels or the number of classes that we're actually going to classify for each of the input sentences okay so here we'll get some like a warning here right if you are following along and you print this out because some of the sequence classification classification parameters aren't trained yet and so we'll go ahead and take care of that so here similarly we'll kind of like walk through how to um how to actually you know train some of these models so the first is how do you actually pass any of the inputs that you get from a tokenizer into the model okay well if we get some model inputs from the tokenizer up here and we pass this into the model by specifying that the input IDs are the input IDs from the model inputs and likewise we want to emphasize or we can you know show here and specifically pass in that the attention mask is going to correspond to the attention mask that we gave from these like these outputs of the tokenizer okay so this is option one where you can specifically identify which property goes to what the second option is using kind of a pythonic hack almost which is where you can directly pass in the model inputs and so this will basically unpack almost the keys of like the model inputs here so the model input keys so the input IDs correspond to this the attention mask corresponds to the attention mask argument so when we use this star star kind of syntax this will go ahead and unpack our dictionary and basically map the arguments to something of the same keys so this is an alternative way of passing it into the model both are going to be the same okay so now what we can do is we can actually print out what the model outputs look like so again these are the inputs the token IDs and the attention mask and then second we'll get the actual model outputs so here notice that the outputs are given by kind of these legits here there's two of them we passed in one example and there's kind of two potential classes that we're trying to classify okay and then lastly we have of course the corresponding distribution over the labels here here right since this is going to be binary classification yes it's like a little bit weird that you're going to have like the two classes for the binary classification task and you could basically just choose to classify one class or not um but we do this just basically because of how hugging face models are are set up um and so uh Additionally you know these are the models that we load in from hugging face are basically just Pi torch modules so like these are the actual models and we can use them in the same way that we've been using models before so that means things like lost dot backward or something like that actually will do this back propagation step corresponding to the loss of like your inputs that you pass in so so it's really easy to train train these guys as long as you have like a label you know label for your data you can calculate your loss using you know the pi torch cross entropy function you get some loss back and then you can go ahead and back propagate it you can actually even get kind of the parameters as well um in the model that you're would probably get updated from this this is just some big tensor of the actual embedding weights that you have okay we also have like a pretty easy way for hugging face itself to be able to to calculate the loss that we get so again if we tokenize some input string we get our model inputs we have two labels positive and negative um and then give some kind of corresponding label that we assign to the the model inputs and we pass this in we can see here that the actual model outputs that's that are given by a hugging face includes this loss here right so it'll include the loss corresponding to that input anyways so it's a really easy way of actually calculating the loss just natively in hugging face without having to call any additional things from a pie torch Library and lastly we can actually even use um if we have kind of like these two labels here again for positive or negative what we can do is just take the model outputs look at the legits and see which one is like the biggest again we'll pass that and take it to the ARG Max so that'll give the index that's largest and then that's the output label that the model is actually predicting so again it gives a really easy way of being able to do this sort of like classification getting the loss getting what the actual labels are just from within hugging face okay awesome so um well last thing as well is that we can also kind of look inside the model um in a pretty pretty cool way and also seeing what the attention weights the model actually puts uh the attention weights the model actually has so this is helpful if you're trying to understand like what's going on inside of some NLP model and so for here we can do again I where we're importing our model from some pre-trained kind of pre-trained model model weights in the um the hugging face Hub we want to Output attention set output attentions to true and output hidden states to true so these are going to be the key arguments that we can use we're actually kind of investigating what's going on inside the model at each point in time again we'll set the model to be in eval mode and lastly we'll go ahead and tokenize our input string again we don't really care about any of the gradients here um again so we don't actually want to back propagate anything here and finally pass in the model inputs so now what we're able to do is when we print out the model hidden States so now this is a new kind of property in the output dictionary that we get we can look at what these actually look like here um and sorry this is a massive output so you can actually look at the hidden State size per layer right and so this kind of gives a notion of what we're going to be looking like looking at like what the shape of this is at each given layer in our model as well as the attention head size per layer so this gives you like it the kind of shape of what you're looking at and then if we actually look at the model output itself we'll get all of these different like hidden States basically right so um so we have like tons and tons of these different hidden States we'll have the last hidden State here so the model output is pretty robust for kind of showing you what the hidden state looks like as well as what attention weights actually look like here so in case you're trying to analyze a particular model this is a really helpful way of doing that so what model.eval does is it sorry question is what is the dot eval do um what it does is it basically sets your and this is true for any PI torch module or model is it sets it into quote-unquote eval mode so again for this like we're not really trying to calculate any of the gradients or anything like that that might correspond to um like correspond to some data that we pass in or try and update our model in any way we just care about evaluating it on that particular data point um so for that then it's helpful to set the model into like eval mode essentially to help make sure that that kind of like disables some of like that stuff that you'd use during training time so it just makes it a little more efficient yeah the question was uh it's already pre-changed so can you go ahead and evaluate it yeah you you can so yeah this is just the raw pre-trained model with no no fine tuning so the question is like how do you interpret um these shapes basically uh for the attention head size and then the hidden State size so um so yeah the the key thing here uh is you'll probably want to look at kind of the shape given on the side it'll correspond to like the layer that you're actually kind of like uh looking at so here um like when we call we looked at the shape here we're specifically looking at like the first first one in this list right so this will give us the first hidden layer all right the second gives us a notion of kind of like the the batch that we're looking at and then the last is like so this is like some tensor right 768 dimensional I don't know representation the corresponds there um and then for the attention head size it corresponds to like the actual query word and the keyword for these last two here but yes so um but for this you know we would expect this kind of initial index here right the one to be bigger if we printed out all of the you know all of the layers but we're just looking at the first one here so we can also do this um for um you know actually being able to get some notion of how these different how this actually like looks um and plot out these axes as well so again if we take this same kind of model input which again is like this hugging face Transformers is great we're actually trying to see like what do these representations look like on like a per layer basis so what we can do here is basically we're looking at for each layer that we have in our model right and again this is purely from the model output attentions or the actual outputs of the model so what we can do is for each layer and then for each head we can analyze essentially like what these representations look like and in particular what the attention weights are across each of like the tokens that we have so this is like a good way of again understanding like what your model is actually attending to within each layer so on the side if we look here maybe zoom in a bit we can see that this is going to be like corresponds to the different layers and the top will correspond to these are across the the different attention heads okay this will just give you some notion of like what the weights are so again just to um to clarify so again if we maybe look at the labels sorry it's like a little cut off and like zoomed out but so this y-axis here like these different rows corresponds to the different layers within the model oops um on the x-axis here right we have like the um like the different attention heads that are present in the model as well and so for each head we are able to for each at each layer to basically get a sense of like what how the attention distribution is actually being distributed what's being attended to corresponding to each of like the tokens that you actually get here so if we if we look up again um here as well right we're just trying to look at like basically the model of tensions that we get for each kind of corresponding layer the question is what's the the color key um yellow is like higher higher magnitude and higher value and then darker is like closer to zero so probably very Navy is like zero so what we can do is now maybe walk through like what a fine-tuning task looks like here um and so first like uh in a project you know you're probably going to want to fine-tune a model um that's fine it's a and we'll go ahead and walk through an example of what that looks like here okay so what we can do as well is all right what we can do as well is use some of the um the data sets that we can get from hugging face as well so it doesn't just have models it has really nice data sets and be able to kind of like load that in as well so here what we're going to be looking at is uh looking at like the IMDb data set um and so here again is for sentiment analysis I will just look at only the first 50 tokens or so um and generally so this is this is like a you know a helper function that we'll use for truncating the output that we get and then lastly for actually kind of making this data set we can use the data set dict class from a hugging face again that will basically give us this smaller data set that we can get for the uh for the train data set as well as specifying what we want for validation as well so here what we're going to do for our like mini data set for the purpose of this demonstration is we'll use I make train and vowel both from the IMDb train data set uh we'll Shuffle it a bit and then we're just going to select here 128 examples and then 32 for validation so it'll Shuffle it around it'll take the first 128 and I'll take the LA the next 32. um and then we'll kind of truncate those particular inputs that we get again just to kind of make sure we're efficient and we can actually run this on a CPU okay so next we can do is just see kind of what does this look like it'll just again this is kind of just like a dictionary it's a wrapper class almost of giving you know your train data set and then your validation data set and in particular you can even look at like what the first 10 of these looks like so first like the output so we specify train we want to look at the first 10 entries in our train data set and the output of this is going to be a dictionary as well which is pretty cool so we have some the first 10 test text examples that give the actual movie reviews here um so this is the given in a list and then the second uh key that you get are the labels corresponding to each of these so whether it's positive or negative so here one is going to be a positive review 0 is negative so it makes it really easy to use this for some something like sentiment sentiment analysis okay so what we can do is go ahead and prepare the data set and put it into batches of 16. okay so what does this look like what we can do is we can call the map function that this like uh this small like data set dictionary has so we call map and pass in a Lambda function of what we want to actually do so here the Lambda function is for each example that we have we want to tokenize the text basically so this is basically saying how do we want to you know pre-process this um and so here we're extracting the tokens input IDs that will pass as a model we adding padding and truncation as well we're going to do this in a batch and then the batch size will be 16. hopefully this makes sense okay so um next we're basically just going to um uh do like a little more modification on what the data set actually looks like so we're going to remove the column that corresponds to text and then we're going to rename the column label to labels so again if we see this this was called label we're just going to call it labels and we're going to remove the text column because we don't really need it anymore we just have gone ahead and pre-processed our data into the input IDs that we need okay and lastly we're going to set it the format to torch so we can go ahead and just pass this in pass this into our model or Pi torch model the question is what is labels so um so label here corresponds to like again the first in the context of sentiment analysis it's like just yeah positive or negative and so here we're just renaming the column okay so now we'll just go ahead and see what this looks like again we're going to look at the train set and only these first two things and so um so here now we have the two labels that correspond to each of the reviews and the input IDs that we get corresponding for each of the reviews as well lastly we also get the attention mask so it's basically just taking the what you get out from the tokenizer and it's just adding this back into the data set so it's really easy to pass in the question is we truncated which makes things easy but how do you want to apply like padding evenly so here if we do pass in so first is like you could either manually set some high truncation limit like we did the second is that you can just go ahead and set padding to be true and then basically like the padding is basically uh added I based off of kind of like the longest um like longest sequence that you have yeah so the question is I guess doing it for all of them all the text lists evenly um so again it just like depends on like the size of like the data set you're you're like you're loading in right so if you're looking at particular batches at a time you can just pad within that particular like batch versus like yeah you don't need to like load all the data set into memory pad the entire data set like or like in the same way so it's fine to do it within just batches yeah the question was how does uh how were the input IDs like added and uh yeah the answer is yes it's basically done automatically um so we had to manually remove the text column here and that kind of like this first line here buy it um like if you recall like the outputs of token like at the tokenizer it's basically just the input IDs and the and the attention mask so it just is smart enough to basically aggregate those together um okay the last thing we're going to do is basically just put these so we have this like data set now um that looks great we're just gonna import like a pytorch data loader typical normal data loader and then go ahead and load each of these data sets that we just had I mean specifying the batch size to be 16. okay so that's fine and great and so now for training the model it's basically like exactly the same as what we would do in typical pytorch so again it's like you still want to compute the loss you can back propagate the loss and everything um yeah so it's it's really up to your own design how you do uh how you do the training um so here there's only like a few kind of asterisks I guess one is that you can import specific kind of optim Optimizer types from the Transformers uh package so you can do atom with weight Decay you can get a linear schedule for like the learning rate which will kind of decrease the learning during the learning rate over time for each training step so again it's basically up to your choice but if you look at the structure of like this code right we load the model for classification we set a number of epochs and then however many training steps we actually want to do we initialize our Optimizer and get some learning rate schedule right and then from there it's basically the same thing as what we would do for a typical kind of like Pi torch model right we set the model to train mode we go ahead and pass in all these batches from like the the data loader and then back propagate step the optimizer and everything like that so it's I pretty pretty similar from what we're kind of like used to seeing essentially awesome so that'll go do its thing at some point um okay and so I so that's one potential option is if you really like pie torch you can just go ahead and do that and it's really nice and easy um the second thing is I that hugging face actually has some sort of like a trainer class that you're able to use that can handle most of most of these things so again if we do the kind of like the same thing here this will actually run one star model is done training um like we can create the our you know our data set in the same way as before now what we can what we need to use is like this import of like a training arguments class so this is going to be basically a dictionary of all the things that we want to use when we're going to actually train our model and then this kind of like additional trainer class which will handle the training kind of like magically for us and kind of wrap around in that way okay anyways so if you can okay I think we're missing a directory but um I think yeah pretty straightforward for how you want to train yeah um so for for here at least again there are kind of the two key arguments the first is training arguments so this will specify have a number of specifications that you can actually pass through to it it's where you want to log things for each kind of like device in this case like we're just using one GPU but potentially if you're using multiple gpus what the batch size is during training or the batch sizes during evaluation time how long you want to train it for how you want to evaluate it so this is kind of like evaluating on an Epoch level what the learning rate is and so on so on so again if you want to check the documentation uh you can see that here there's a bunch of different arguments that you can give there's like warm-up steps warm-up ratio like weight Decay there's like so many things um so again it's basically like a dictionary feel free to kind of like look at these different arguments you can pass in but there's a couple key ones here and this is basically this basically mimics the same arguments that we used before in our like explicit Pi torch method here for hugging face okay similarly um what we do is we can just pass this into the trainer and that will take care of basically everything for us so that whole training Loop that we did before is kind of condensed into this one class function um for actually just doing the training so we pass the model the arguments the train data set eval data set what tokenizer we want to use and then some function for computing metrics so for here we pass in this function uh eval and it takes eval predictions as input basically what this does is these predictions are given from the trainer passed into this function and we just can split it into the actual legits and the labels that are predicted or sorry the ground truth labels that we have and then from here we can just calculate any sort of additional metrics we want like accuracy F1 square recolors and whatever you want okay so this is like an alternative way of formulating that training Loop okay uh the last thing here as well is that we can have some sort of callback as well if you want to do things during the training process so after every Epoch or something like that you want to evaluate your model on the validation set or something like that or just go ahead and like dump some sort of output that's what you can use a callback for and so here this is just a login callback it's just gonna log kind of like that information about the the process itself again not super important but in case that you're looking to try and do any sort of callback during training it's an easy way to add it in the second is if you want to do early stopping as well so early stopping will basically um stop your model early as it sounds if it's not learning anything and a bunch of epochs are going by and so you can set that so that you don't waste kind of like compute time or you can see the results more easily the question is is there a good choice for the patient's value um I think it just depends on the model architecture not really I guess this is It's a yeah pretty up to your discretion okay awesome and so the last thing that we do um is just do call trainer.train so if you recall this is just the instantiation of this trainer class we all trainer.train and it'll just kind of go so now it's training which is great it gives us a nice kind of estimate of how long things are taking what's going on what arguments do we actually pass in um so that's just going to run and then likewise hopefully it'll train relatively quickly okay it'll take two minutes we can also evaluate the model um pretty easily as well so we just called trainer.predict on whatever data set that we're interested in so here it's the tokenized data set corresponding about the validation data set okay hopefully we can pop that out soon um and lastly so if we saved anything to our model checkpoints so hopefully this is um saving stuff right now yeah so this is going to be is continuing to save stuff to the folder that we specified and so here in case we ever want to kind of like load our model again from the weights that we've actually saved we just pass in the name of the checkpoint like the relative path here to our checkpoint so notice how we have some checkpoint 8 here right we just pass in the path to that folder we load it back in be tokenized and it's the same thing as as we did before there are a few kind of additional appendices for how to do like different tasks as well there's appendix on generation how to define a custom data set as well how it's a kind of like pipeline um different kind of like tasks together um so uh so this is kind of like uh using some a pre-trained model that you can just use through kind of like the pipeline interface really easily um there's like in different types of tasks like Mass language modeling but um feel free to look at through those at your own time and uh yeah thanks a bunch