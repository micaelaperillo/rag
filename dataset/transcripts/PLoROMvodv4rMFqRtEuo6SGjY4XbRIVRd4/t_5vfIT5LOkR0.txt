so today I'm delighted to introduce our first um invited speaker is Dao Aquila um there has also been um as well as being invited and I'll tell his background um he's also um in the symbolic systems program has been an Adjunct professor and has been involved with some students in that role as well but in his invited role he's originally from the Netherlands where he even learned some logic among other things back in the old days but in more recent times he's been um a prominent um deep learning researcher for a number of years he worked at um Facebook now meta in the fair unit and was involved in various ideas including retrieval augmented Generation Um after that he then spent some time at hugging face he's become interested in looking at multimodal models which is what he's going to be talking about today and we welcome Dara it's great to have you thank you very much foreign yes uh yeah thanks everyone for coming I understand that you get points for being here so you're not really here for me but uh uh thanks for coming anyway so I'm going to talk about multimodal deep learning uh it's gonna have an NLP focus of course that's for discourse but it's also because otherwise I would really be talking for uh many more hours than I have time for here so I'll try to really keep it focused on on the things that I think will be most useful for you to learn and so the first thing you should understand is that this whole concept of multimodality is is kind of ill-defined actually um so if you go to the dictionary you'll see that it means having or involving several modes or modalities or Maxima um and and so what mode here really means is so it could be mode in the very generic sense or it could be a very precise sense of the mode of a statistical distribution um and so depending on the paper you're reading in some cases people really mean this statistical sense in other cases people really mean this sort of very vague concept of a modality where it really means the type of information that you're getting so an example of modality in that case is an image or speech signal or audio in general or even affection so smell or or things like that so in this uh lecture we're just going to focus mostly on text because this is an NLP course and we're going to focus on images mostly as the other modality to to keep it simple all right so why does it matter why do we care about multi-modality um and so there are a couple of really good reasons in general for this uh the the first one is is about faithfulness so if you look at how we humans understand the world how we make sense of what happens in the world uh that is very multimodal uh right so we we perceive the world not just using Vision uh or just audio but we synthesize information across all of these different modalities and that's how we understand the world and each other um there's also a very practical uh argument for doing it it's because the internet is multimodal right so if you go to I don't know uh like Facebook or something like that like it rarely happens that it's just text or just an image there's usually a combination of multiple modalities and then the the final good reason uh that we're just starting to hit now if you're if you're really following where the field is going we're kind of running out of Text data for these large language models so uh one interesting way to uh keep scaling on the data side is to make use of all of these other modalities right so if you can have your language model also watch all of the videos of cats in the world it's going to understand the concept of catch cat much better and that's what we want to have in these models we want them to understand the world in the same way that humans understand it um so right now multimodality is really one of the main frontiers of this New Foundation model uh drives that were all in right now there's a thing called the mcgurk effect let's see if it loads up uh but uh so uh what what we'll see when this loads is uh this guy over here and uh we'll have the same audio effect uh being played so the audio is exactly the same and this man is going to say something like and so you're hearing a b there I think if you look at my mouth because that's what I said uh but if you then change the video to where he says with exactly the same audio you're going to hear the other version um so unfortunately I can't really like swap in the different audio here so you have to trust me for it we might suddenly start hearing a guy saying all right um so um uh multimodal applications so when we have multiple modalities we can do all kinds of uh interesting things and as I said most of the use cases we have on the internet they're all multimodal um and there are some really kind of obvious things we would be interested in if we have information from these different data sources right from different modalities uh so obviously we might want to do retrieval so maybe given a bit of text we want to find the right image or maybe given some image we want to find the right text for it so we can match them up obviously we can also do this in a generative setting so then we have image captioning which you've probably heard of we can do text to image generation so that's image synthesis and so stable diffusion everybody in the audience here has probably seen that then we could do a visual question answering where we have an image and text and then we need to generate some new text we have multimodal classification where we have image syntax and we need to have a label for example whether something is hate speech or not and then in general we want to be able to have a richer understanding of information which means that we combine images and text and then use it for Downstream applications that require better understanding or better Generation Um so this field really is super hot right now uh so there's this uh this nice paper title I predict that this paper is going to do really well in terms of citations just because it has such a sideable title I think a lot of people are not actually going to read it and so I mean I've been in this field for quite a while now and people have been saying this for a really long time I think Chris would agree that so for decades people have been saying that multimodal is the next big thing uh but now it's really true I think all right so uh the outline for uh what we're going to be talking about so first I'm going to tell you a little bit about early models then we're going to do a bit of a deep dive on some of the specifics then we're gonna go over a particular type of fusion contrastive models or Lake Fusion then we're gonna go through a little bit of the history of multimodal foundation models then we're going to talk a little bit about evaluation a little bit about other modalities and then I'll make some predictions for the future and hopefully maybe give you some cool research ideas or things to talk or think about all right so um obviously like there's a lot of work that happened before deep learning but I think if you want to start from like the Deep learning Revolution and what was happening in images and text then a good starting point is for example Wasabi uh or device uh or Richard zoker who you who you've probably heard of has done some really cool early work in this that really pioneered a lot of these ideas uh and the basic uh gist of this is that we have a vision model on the one hand we have a language model so this really I mean the first lecture of this course I think was about word embeddings right so that's just your basic word embedding model and now we need to figure out how to align them in the same multimodal space so the way you do that is you get some sort of similarity metric right a score function or like a kernel function if you're thinking about this from a support Vector machine literature perspective and now you need to figure out uh in a Max margin or margin loss um uh how you want to align these two points in your embedding space right so things that are similar you want to bring them closer together things that are not you want to bring them further apart and if you do that in this multimodal embedding space that means that you can do interesting cross-modal transfer where you can take the word embedding for something like Auto or like horse and then you can find close images in the embedding space to that thing and now now you've solved the retrieval problem so this is a really nice early application and I think a lot of the stuff that I'm going to talk about in the in the early slides you're going to see this income over and over again you're going to see it get kind of reinvented with fancier models but it's basically all the same stuff um so you can do cross modal transfer where you have images and text but you can also combine them together so that you get a multimodal word embedding uh and so this uh just gives you a more accurate representation of how humans understand words word meaning because when we think about the word Moon uh or cat or something we can go to Wikipedia and read that a cat is a small carnivorous mammal that people like to keep as pets or we can just go and look at pictures of cats and now we understand what a cat is right and I would argue actually that for a lot of people the picture of the cats is much closer to the meaning of the concept of cat um so uh some some early work where people were trying to do this is from Rooney at all where they did multimodal distribution or semantics using this very uh elegant approach called bag of visual words so just like who has heard of bag of visual words very few people okay so it's it's surprisingly simple and so I kind of like it it's nicely elegant so you take a picture of a moon in this case I think you can see it in the back too right so uh we use an algorithm like sift uh to find interesting key points so it's sort of where the difference between the pixels and the pixels next to it where that difference is Big those are sort of the spots you want to be looking at and for each of these key points you get feature descriptors so relatively small vectors like 32 dimensional events are kind of on the on the implementation of this and what you can do now with these feature descriptors is you can cluster them using k-means and then you assign every one of these points uh so you can count how often they occur right so in this picture of the Moon we have like uh actually the count is oh yeah so there are three like red dots right so that's why the Red Dot one is three so what that gives you is a uh an idea of the visual words very similar to the original bag of words model that you hopefully have heard about maybe in the first lecture um so that's the visual equivalent of the textual thing um and so if you do this and you then concatenate or you apply sud to fuse the information what you get is a word embedding that is much more representative of human meaning so you know as reflected in the data sets that people used to care about at the time so after that there were a couple of people me included who tried to take these ideas and then really applied deep learning to them so some of the very early versions of this use convolutional neural networks uh and then you can transfer the features from uh your your confnet and you take your word embeddings which you've seen in the first lecture uh and then you can concatenate them now you have a multimodal work vector or you can do something slightly fancier so you've seen the skip gram model you can also try to do skip gram predictions onto image features right so when you see your work like cat in some contexts like the cute little cat said on the Met then when you see cat you also want to predict cat pictures so super easy ideas but it turned out that this gives you much richer work representations uh so that's kind of cool but obviously words are very limited what we really care about is not words but sentences so uh then people started really looking into sentence representations and how can we figure out uh how to get compositional understanding in the sentence representations and how to how do we align that with images um so the loss here is very similar to what we saw with works and pictures but now we just have a sentence encoder right um and so there's some really cool early papers from Andre karapati and Richard Soaker also had some work here um and then you know so the basic idea is just that instead of having these word embeddings we now have an lscm in these papers or some other kind of recurrent neural network or in the case of this one recursive neural network and then we try to align the features together um and so so these three or four papers are actually very important than this one by me is less important but it's still kind of interesting because uh we showed here that grounded sentence representation so if you actually just use this part here as a sentence encoder for NLP tasks the ability to just predict pictures from it already gives you a really good sentence representation right so so just by predicting pictures you can sort of imagine what things look like and that gives you a really good meaning representation which you can then transfer to I don't know sentiment classification or something else um and then of course uh once we have census encoders uh or then we also have decoders and and so when the sequence to sequence architecture came out which you've probably also heard about in this course uh what you can do instead of having a text encoder for like your Source language if you're doing machine translation is you can plug in a confnet uh instead of an lstm encoder and now you can generate captions so that's exactly what people did we used to have all of these fancy diagrams in our papers then where we explain the lstm and how that works probably people don't learn that anymore these days they do yeah very good they might make a comeback I think you know at some point uh Transformers are going to go away we'll see um and uh so uh one of the things that that people figured out in machine translation very early on is that you can do alignment of words between your Source language and your target language and you can do the same thing actually with images right so if you want to align a word in your uh in your generated sequence with something in your picture then you can do the same uh use the same approach for that and that approach of course is called attention right so you know you've learned a lot about detention probably in this course and and so yeah that was one of the the building blocks of these systems as well where you can do very interesting things uh and really see that when it has to generate stop for the stop sign that is really actually looking at the stop sign right so there's a really cool alignment going on there um in these models um and so the the final kind of early model we should talk about a little bit uh is Gans uh who here is sort of Gans okay that's a lot a lot more than bag of visual words I guess that makes sense um and uh so so yeah the basic idea of again is really that you have this generator and discriminator and you want to have the generator uh generate images that the discriminator cannot distinguish uh uh from uh so it cannot distinguish fake and real images right and if you do that you can actually condition that on the piece of text uh and then you can generate images uh using some some uh text prompt right so that's what what uh kind of the the first versions of stable diffusion we're doing things like this and you know it's all the a natural progression to that model um so those were the early models um maybe do people have any like burning questions about this or does this all make sense all right so let's do a bit of a deeper dive then on on in particular on features and fusion so those are really the kind of core building blocks for for all of this multimodal stuff really but before we go there maybe very briefly like if all of this multimodal stuff is cool and sort of useful and and doesn't look that difficult you know like why aren't we all doing multimodal things and so why why do we focus on specific modalities and I think there are a couple of problems just to be aware of uh so one is modalities can sometimes dominate especially text is much more dominant than Vision or audio in many use cases right so uh you can already just have a model that picks up on the tech signal and basically learns to ignore the image completely which actually happened embarrassingly for visual question answering we'll get to that so visual question answering you could do that without actually looking at the picture um the additional modalities can add a lot of noise so it makes your machine learning problem more difficult uh you don't always have full coverage right so as I said if you look at Facebook posts sometimes you have text sometimes you have pictures sometimes you have both but you don't have a guarantee that you always have both so how do you deal with that um in many cases we just really weren't ready it was too complicated to implement stuff and also just in general like how to design your model really to uh to combine all the information is actually quite complicated so in order to to uh you know to maybe drive the home that point home a little bit um so featurizing text I guess we all know how to do that by now especially sort of in the age of Transformers and before in lstm sorry we just said like you have your batch by your secrets so batch size by sequence length by embedding size right so it's always like a 3D tensor and that's how you encode your textual information when you pump it through your neural net um and so with images it's like trickier because you can just kind of look at the patches but then if you do convolutions you're kind of like shifting over the image and then you're aggregating right um and in many cases you don't really want to be this uniform you want to have something that actually looks at the things in the picture right so this is called region features where you would use an object detector as a first step for processing your image and then you would have a confident backbone that encodes the features for that particular sub image like this guys like skateboard or something it has its own like vector representation right um and then in terms of dense features we now also have Vision Transformers so we'll just very quickly go over that to make sure we're on the same page so there are all these models like YOLO is a really good one if you haven't heard of that yet uh so we're at YOLO V7 now I think create I don't know uh so there's a new one coming out every every other like year or something but the basic idea is that we get these bounding boxes uh for things in the images right actually segmentations with the bounding boxes is what people tend to use and they they have labels right so this is labeled like Backpacker or something and so you can do this as a pre-processing step on your image to get a much richer representation of what is really in that image which you can then pump into your system as we'll see later and and so then how you encode the information that is in these little bounding boxes or actually in the image itself in general we just use a standard comp net for that and so this probably feels like super obvious now but uh in 2014 when people were starting to discover this it was really very surprising that you could just use off-the-shelf continent features to really replace the entire computer vision pipeline so people used to do all of this very fancy sophisticated stuff and people you know spend decades on trying to refine this and then it was all thrown away and replaced by a confnet that does all of that stuff for free um and so the cool thing you get there is that you can transfer very easily across different tasks so you don't you can have a very generic confidence and then use it to all kinds of very specialized uh things like spotting buildings in Paris for example or flowers or other stuff um and then of course in the age of Transformers um how far how far we're already quite a while and this is only the first Transformer actually uh in the the slide deck so uh you know we're making good progress uh so Vision Transformers are what we would use these days to encode the images uh where you have these flattened patches and then you would do uh kind of the the standard birth architecture maybe as you would know it from this course and then you do classification right so this is all like a standard Transformer everything standards except now your input here is not words or tokens it's patches of an image and then you classify that all right so then we have a bunch of features and now how do we combine the information right so let's say we have two vectors u and v uh so you know it sounds easy right to how how we could could combine them it turns out that they're actually very many ways to combine them so I I don't think it's it's really useful to go over all the different ways here um but you can do very simple things right so obviously like uh inner product or similarity is what you would use if you want to do cross-modal things so if you want to embed things in the same Vector space uh but you can do sort of fancier uh projections on top or different combinations that are kind of linear uh or you can do multiplicative things where you uh multiply the components element wise or you do some sort of gating over the different features you can do attention you can do fancier buy linear things you can do very fancy compact bilinear things so there there's really a wealth of literature kind of on all the different ways you can combine two vectors and and so uh this is called multimodal fusion and most of the literature on multiple modality is essentially about this question what is the best way to do fusion and that's it um so so I think within that discussion it's maybe useful to distinguish between different levels of fusion so you can do it very early where basically you make sure you have the different features and then you just kind of uh in in the sort of modern sense of attention you would attend to everything in all the features from the beginning you can first treat them separately and then combine them or you can treat them as completely separate and then you only combine the final scores right and so there's the so that's kind of what we would call Early fusion and then sort of my my invention for calling the middle part would be sort of middle fusion and then you have late Fusion uh where you really just combine the scores or the logits but you don't really have any interaction between the information from the different modalities um so you could do really fun stuff with multimodal Fusion so this is a paper I really like film um where uh you have this sort of very special uh feature Maps this sort of f here and it gets gets modulated by a multiplicative Factor so this gamma and an additive sort of bias Vector this beta and you have a different one for every layer of a resnet that is conditioned on some encoding of the thing you're after uh so in this case are there more cubes than yellow things so we have some Vector representation for that and we use that Vector representation to modulate the resnet blocks at every layer of the confident um so you know you can really do very fun things where you're sort of modulating one network with the other one and really try to have them learn uh as much as possible from that all right so um let's talk about late Fusion then so late Fusion is what we would Now call contrastive models uh but the basic idea is that we have this similarity score so we have the two kind of we process the modalities completely independently and then at the very end we do some combination uh and the most famous uh instance of that uh these days is clip so who's heard of clip okay so clip uh from openai and so it's again exactly the same contrast of loss that that we've seen in all these early approaches um it does kind of negative sampling uh but then in batch so you just have a batch you have two things that are aligned right so like this the first piece of text and the first image they are aligned so this is the right answer and I just want to make sure that I rank this thing higher than all the alternatives right and I want to make sure I rank this thing higher than all the Alternatives so it's a very very simple idea uh really really nothing special about this architecture that that was sort of invented here but what made this uh thing so cool was first of all it was Transformers and it was Transformers all the way so your text encoder would be a Transformer and your image encoder would be a vit image encoder so also a Transformer um and it was trained on lots and lots of web data so Alex Radford is really a genius at creating very high quality data sets and he he created I think 300 million image text pairs for this data set trained a bigger model on on it than people used to do and then we got this amazing model out of it um and so so uh moving away from the words there to the sort of texts that you would see on the internet right so the caption uh for an image on the web it's not going to say dog or cat it's going to say a photo of a cat doing something something right so uh that that means that you can do kind of zero shot uh label predictions where you have a photo of uh and then you need to figure out what uh the right label is for a given image using this kind of prompt right so the the thing you know you probably all know about prompting large language models and so you can prompt vision and language models in in very much the same way and do zero shot generalization um so if you want a really really good paper I would recommend that you read this paper this is really one that's going to teach you how to write really good papers it's thorough and it's really worth a very close read I think if you're interested in this view um and so I think when it came out uh actually on imagenet itself it it didn't really outperform resnet right so so you might think oh yeah actually it's not all that special but what really made it special was that it generalized much better to these other data sets right so this uh this resnet thing here is pretty terrible at some of these kind of adversarial versions of imagenet and clip is super robust to that so it's just a way better image encoder in general um so uh very very quickly after clip there was this paper from Google uh using a line um which was basically exactly the same idea uh you know the field is not really that creative at all it's like the same idea but then you just keep like throwing more data and more compute at it and it often works much better so that's what they found here too and 1.8 billion image taxpayers instead of 300 million gives you a better model surprise um but uh so it's still very cool and and what is really cool I think is that there's this organization called lion um uh where uh they've they've started this open source Collective to create really high quality data sets um and so the lie on the initial data set um was uh how many examples in the initial lineup 400 million right he knows I know that he knows um and uh so so now there's a much bigger version of lion uh that's even multilingual and it has five billion examples right so uh stable diffusion was trained on sort of the image the English subset of this thing uh and that's one of the reasons that it's so awesome it's because it's just seen a ton of data uh and that really makes your system a lot better so if you're looking for like the ultimate data set to play around with uh with your own ideas if you have enough compute obviously then you should really look at this data set all right any questions about up until this point nope all right um so that then we'll we'll move on from late Fusion to kind of middle Fusion early Fusion uh and this really is kind of the core uh of what I think a lot of people in the field right now or if you're interested in getting in this field or if you're going to go into industry and you're going to be using this stuff like this is what you should really understand and and again like the idea is sort of Stack onto each other so I've kind of uh sequenced the slides to give you an idea sort of of how the scientists kind of came up with the next step uh and you can really see the architecture just get slightly more and more advanced but basically a lot of it is just more data and more compute uh again um so uh who knows how bird works everybody should raise their heads so um uh yeah so so Bert is kind of so canonical I think everybody kind of gets out Burke works right so I don't think we need a real refresher uh but uh I think you can think and so the reason I have to slide is because I want you to think about if you have a bird model and you have a bunch of images how are you going to turn that Bird model into something multimodal right so so there are a bunch of like obvious things you could do given the kind of features I told you about in the sort of fusion process so you know how are you going to do that does anybody want to like say something like if you're doing classification and then just concatenate it to whatever encoder like maybe an a n or whatever you're training on the data concatenating okay exactly yeah so so you can take take the confnet features and classifier token from bird concatenate them and then classify uh for like a catheter or something like that or whatever the thing is you're interested in yeah yeah so that's one thing you could also like take the confident features and like give them to the bird model in lots of different ways right uh we can use the region features so um and I think a lot of people uh when Burke came out who were working in in vision and language processing were thinking exactly about okay so do we do like middle Fusion late Fusion do we do early Fusion how do we do diffusion um and so there were a lot of papers all coming out basically at around the same time where people were doing versions uh of this because so Bert was really kind of the Innovation and then everybody sort of just plugged it into their own thing because of hugging face Transformers and things like that so um the first thing is uh visual bird um this was one of the very early ones where you have this image and people would do uh object detection on this so you get like a hat and a racket and a shirt and things like that so you can just really take these features and then plug them into your uh your Transformer model and then you you try to like recover the features and so this really is probably like the simplest way to do it right um and so this is what we call a single stream architecture where you have all of these kind of concatenating the the original input features and then putting them through the same Transformer what you can also do and that's something that this this uh model called vilbert did is where you have two different streams so you essentially have these two parallel Transformers but at every layer uh you kind of give them a cross attention right so or co-attention as they call it but it's basically like so you just make sure you have an attention map that spends both and then you just do your full normal Transformer layer again um and then so this you can train just like your regular Bird right so you uh you have your your masked model Mass language model here and here you do sort of some equivalent of that and then you also have your next sentence prediction which you probably remember from your birth lecture um but instead here we're saying okay is this image aligned with this piece of text or not um there's also lexmart I mean there I could go on forever there are like 100 papers that that came out that did this all at the same time so Lexmark had a different cross-modal output encoder a bunch of different uh ways of encoding the positional information right so you could say okay I just have a bunch of bounding boxes that are featureized but I don't care about where they are in the image so it's just kind of like a just a bag of uh bounding boxes or you could say I found it here like this is the particular like top left and and bottom right coordinate and that's what you featurize into your network um you can also do something even dumber and I can say that because this is my paper um where you just take the the image itself you put it through a resnet and then you uh do a little bit of pooling on the final feature maps and you just get give those feature Maps too Bert um and so you then need to distinguish between like your text segment embeddings right and your vision segment embeddings um but so this actually works surprisingly well you don't have to do any uh any additional training you can just take Bert out of the box initially you freeze it you learn to project into bird token space then you unfreeze your resnet and then finally you unfreeze your birth and now you have a very good multimodal classifier on the problem you care about so a lot of these other papers they're doing what they call multimodal pre-training where first you have a bird model and a resnet so they're kind of unimodally pre-trained and then you couple them together and then you have a multimodal sort of intermediate every pre-training step before you fine tune it on the problem you care about uh and what we showed here is that you don't really need that actually in many cases so it's a very strong Baseline um you can also go to the the pixel level completely so so that's what they they did in this other paper called pixel bird where they it's basically exactly mmbt uh so the the previous uh supervised one but here they do do the the multimodal pre-training step and show that I think for vqa it helps a little bit um so there are many of these birds uh doing sort of visual things uh people really tried everything uh here's another one called unider where they they added a bunch of different losses uh we can really talk about this for a very long time uh We're not gonna do that I'm just gonna kind of talk you through some of the more interesting ones so this one I think is quite interesting built because here this is really the first instance where we are completely gone from uh confident features so we don't do any any pre-processing on the image no regime features no backbone that it featurizes the uh the the parts of the image we care about we just have these patches of the image so really integrate we flattened those patches we just pump them into the Transformer straight away so this really is like sort of burnt and vit together in one model and this worked really very well um so that that's been the trend uh so here's a here's a nice uh very long list of all those all of these different models and what they do and and so really the distinctions are just in what is the text encoder that you use so do you use birth or something fancier or better Roberta uh what is your vision encoder so in many cases you have these region features so you would do an rcnn style thing or you could just do a resnet or a vit you have different kinds of fusion so either single or dual stream as we talked about right so visual birth or vilbert different pre-training tasks so Mass language modeling image text matching there's a bunch of like funkier ones you can do so and then finally you can do multimodal pre-training on all of these different data sets that have aligned data um So you you're probably wondering okay so what is what is really the interesting difference between a lot of these and uh so I have another recommended paper that if you're interested in this space you should really take a look at it's also a really well done paper where uh they uh they unmask multimodal pre-training so basically they say if you take all of these little model inventions and you train these different models on exactly the same data in exactly the same way it turns out that they're all basically the same uh so that's a lot of kind of uh you know wasted effort on the part of the field because everybody is saying like oh my model is better but it's actually just because you trained it on different data and there's no real sort of model Innovation uh going on in a lot of these things so I don't mean to sound discouraging or anything like that but you know like uh I think that's why this paper is really nice and really important is because it just shows us what really matters so this is also work that I I did myself uh called Flava uh with uh with my team where we wanted to take these ideas really to the limit so a lot of the things that you've seen now uh so the visual Birds and the vilberts and things like that they're all about multimodal questions so how can we do visual question answering uh something like that where we just have these two modalities we only care about problems that always involve these two modalities and where we want to go and this is this is kind of the basic premise I think of foundation models in general is that we have one model to rule them all right so this one model can consume data from all of these different modalities and you can synthesize across all of these different modalities and then do useful things with that information um so so with flavor that's exactly what we tried to build so we wanted to have one Foundation model that is good at vision and language and computer vision and natural language processing is jointly pre-trained on all of these different data sources so it's also trained on just CC news so common all and book Corpus so it's very good at the sort of things you would expect Bert to be good at it's drained on imagenet for image data so it's good at the things that you would expect is a kind of basic image model to be good at and then you have this PMD data set that we created out of publicly available uh image text pairs that we also train it on so this PMD data set is really just if you take all the data sets that were ever created that have image text pairs that are publicly available so unfortunately the clip data and the Google Align data and all of these data sets they haven't been open source so this is before rely on uh where so now there's a good alternative to this um but so this PMD data set if you combine all of these image taxpayers you get 70 million of them so that's still pretty decent size and then you can take all of this data basically to solve all of these problems that we know we care about in these different fields so you can do multimodal reasoning you can do language understanding you can do visual recognition all with exactly the same model and that's that's a very powerful idea I think if you uh like if you work at a company like Facebook you don't want to have different models for all kinds of different things you want to have one model that you can really use for everything that's going to really make your life a lot easier um so the exact architecture here is that on the one hand we have this image encoder where we take the image we encoded as patches and we just do what we call mass image modeling but it's basically Mass language modeling and then just on the on the image tokens right uh and then on the other side we have the mass language modeling uh on on the language so your regular sort of bird thing and then we have a multimodal part where all of this information gets combined uh so we have a mass multimodal modeling loss term uh where you can also do image text matching so this is like your bird next sentence prediction thing and then we also have a global contrastive loss which is exactly like a clip so if you do all of this stuff it's just all Transformers all the way down and it's sort of a very elegant way I think to combine a lot of this information um and when you do that you get something that can really do a lot of things very well uh so I'm we're not going to talk about that table is just way too many numbers but uh so just trust me we were pretty thorough generating uh the table here and so over 35 different tests if you compare flavor to all kinds of different ablations in terms of clip models then this is just a much better way to to get to this information so I think this is a nice example of like where we're probably going to go with the field uh in in the near future um so the other Trend that we that we see very obviously in the field right now is that everybody cares about generative models right so you know language models and and you know image generative models there's just a trend where we want to be generative we want to move away from this contrastive discriminative uh stuff to the more interesting more richer representations maybe that you get out of generating sequences or or images um so this uh Sim vlm paper was one of the first ones where they really had this separate decoder that was trying to generate or kind of complete captions which they showed gives you a lot richer representations um I think this is actually the current state of the art now it's called coca so a lot of these uh models they all again look very similar uh but in this case now we're starting to really see these text decoders so initially with clip I think that's also what they were trying to go for like open AI being a company that really likes generative models but they couldn't really get it to work and I think so it took us a while as a field to really figure out how to do this the right way um and so right now we're really kind of in the age of language models right and uh so uh one of the interesting things you can do with language models is just keep them Frozen and then learn how to project into the language models so uh the mmbt architecture I talked about where we had this bird model we kind of kept it frozen and we learned to learn to project into the bird token space you can do exactly the same thing but then with a much fancier model uh or something like T5 even where you just have an encoder decoder or some kind of generative part of this you keep that thing Frozen uh and then you learn to project into the token space of that Frozen language model uh and then you can do lots of fun stuff it turns out so what they show in this paper is that you then get few shot Learners uh so all of the things you see with gpt3 where you can just give it some kind of in-context examples and it's gonna figure out binding uh kind of on the fly so it says like this is a accent this is a blicket so what is what is this and then it gives you the answer that is that it's the decks so it really learns in context how you decide the feature mappings which is really kind of solving the the grounding problem that a lot of this multimodal stuff uh started with so I think that's very cool and then uh probably one of the the coolest papers right now or models right now that you might have heard of if you follow the field is flamingo uh out of deepmind where they take a chinchilla language model um and uh so this is really an optimal language model and now you have this Vision encoder uh that encodes multiple different images uh that you can then do reasoning over and then kind of autocomplete so what this gets you is just a much more powerful model because you can do uh you know your generative over lots of different images so like it's really like step wise you can see it right we started off with very simple Transformers and now we're actually at something that that is starting to get pretty complicated because we have these building blocks like a perceiver resampler where we have a a bunch of different images that we featureize and now we need to compress the information because sometimes we have three images sometimes we have five images so we want to make sure that we can compress it so that it's always ready for consumption and by the next layer of the language model and then so this paper again is a really good paper to read because they actually so this is not me this is not my code this comes from the actual paper so they just have the diagram together with the code so that you can really understand what it's doing which I think is is really uh great um and so once you you have your perceiver resampling step what you then do is you do a gated cross attention this is how you implement it um and uh so this gated cross attention you do that before your Frozen language model layer so you really just have a frozen chinchilla language model and you learn to kind of modulate the information that goes into that language model you propagate the gradients all the way back you just don't update the language model so you're really kind of trying to figure out like how am I going to design my signal so that my language model can can do the most with it right how am I going to combine the information so you'll notice that now we do it before the layer right in a lot of other stuff you would do the attention after the layer but here you do it before um so uh karpathy I think more than 10 years ago had this this image it's Barack Obama kind of setting his foot here on the scale to make somebody think uh like uh you know they're they're a lot heavier than they really are uh so this is obviously funny to us um but not to an AI system I think unless it really uh understands the scene and so that's why karpathy uh at the time said this would be a really good visual Turing test like if a system can figure this out then it's actually really smart um and so obviously it's been a bit of a challenge for everybody working in the field than to get something that actually works on this and uh so Flamingo as it turns out kind of gets the joke um but uh yeah so it's it's a bit unclear if it really gets the joke because if you read this conversation it's sort of kind of getting steered in the right direction right but um at least we're making progress let's put it that way um and then so in Flamingo you still have a lot of moving Parts but you can really take this almost through the full extreme where you try to freeze almost everything and you just want to learn this kind of mapping between your image encoder and your language model or your image encoder and your encoder decoder architecture and all you really do is just the projection between the two right so there's this nice model called blip 2 where they they experiment with like opt for the language model in Flint T5 for the encoder decoder architecture and this just gives you amazing results it gives you uh really complex captions and things like that without any real direct supervision on the captions itself which is pretty impressive I think so that just shows you the power of language models in general um so here are some examples uh so it can really do like different things from captioning to reasoning to visual question answering to like like location detection uh so you can have a long conversation with this system this really is is kind of the future where we're going right where we're going to have a chat GPT but it's also going to be able to see the world in a way um and so so I think an interesting thing so you've probably heard of like Chain of Thought prompting and things like that where you ask the language model like let's think step by step um and you can tell a vision and language model uh generate a rationale for why uh why something might be the case so you generate a potential explanation for what your answer might be and then after that you ask it to answer the question and it turns out that if you do that sort of multimodal Chain of Thought prompting then the system gets much better uh and and so you know this was like the new state-of-the-art on science QA or Benchmark like that just because it learns to unpack the information right and so uh I think we're really as a field just starting to figure out what what the potential is of this and I think this paper is where they also showed that multimodal Chain of Thought prompting really gets you pretty amazing results and they show uh very nice results on Raven matrices and like very complicated kind of IQ tests the things that that humans are supposed to be really good at but you have to be a pretty smart human to really be good at this and this system Just Nails it um so you know we're making super fast progress and we started off from a very simple birth model that was able to look at some pictures and now we're getting to these very sophisticated Foundation models so that was my my short history of multimodal foundation models um so how much time do I have left all right okay plenty of time um yeah please questions one of the images they just looked like they were um boxes passed through and kind of no sense of shape in them yeah yeah so so I I think the the history of computer vision has been very similar to the history of natural language processing where we thought we needed all of this structure and all of these different things and it turns out you can just throw it all away and just have a big Transformer over the patches sorry yes [Laughter] you mentioned a couple times like Model T person or what does that mean yeah uh yeah sorry I should have explained that better maybe so it just means that um we are not updating the weights um so uh like if we go to uh this era I think is a nice example so uh we have frozen self-attention so that just means that we when we do a forward pass we go all the way to whatever we want to predict we get some gradients we take them all the way down but we only update the non-frozen layers right so here the gradients actually do get updated but these just never change and so the reason you want to do that is because otherwise you're going to drift way too far right so then you're going to kind of destroy all of the cool stuff your language model has learned because you're just going to focus on on the small data set that you're training it on so you want to preserve the abilities of the language model but you want it to become good at the thing you care about other questions is there a benefit to doing like that really your Metal Fusion as opposed to like only doing leg Fusion I think yeah so so I mean we're going to talk about evaluation next but so it really depends on the the tasks that you care about um and so I would say the earlier is always the better if you can afford it uh and so like clip is very efficient to train it's very late Fusion right at the very end so there's no interaction between the different modalities um and so that's really good if you want to be very efficient and if you want to be like for training it's it's much nicer right uh but if you want to have a richer understanding of the the multimodal signal then you want to do earlier Fusion so it's yeah it's always a trade-off right images are just a lot more data than text so how much more difficult are these to train and um how much bigger does like the image processing have to be compared to uh the language model yeah so so um images are are more complex in a way but but they're also kind of higher bandwidth representations right so there's a lot of kind of like just pixels that our brains just abstract away right it's really about the scene that you're seeing and like you're not really thinking too much about the pixels themselves um so so like John Lagoon likes to say that uh language is just a kind of low bandwidth uh a proxy for a language of thought which is much richer and much higher bandwidth and like he thinks probably visual I'm not so sure um but uh so uh yeah I I don't think that there's necessarily a difference between kind of the scaling laws that you see in these systems um or at least we still have to figure that out we'll kind of talk about that towards the end as well so how to put your bias just like the natural one oh yeah they have terrible biases yeah um so yeah so some people are actually working on on this uh who are in this very room but uh so these models can be very racist also in what they generate or or the kind of predictions they make so uh if you have an Asian basketball player standing sort of like this with a basketball very obviously there then the model will think that he's playing ping pong because he's Asian I'm not joking so uh so so these models uh yeah just like all neural networks right this is really a big problem and one of the the most interesting problems that that you should be working on if you're a student and you want to make a difference is how do we get these systems to be much better at these sorts of things probably examples you show like the model interpret from the content of the image so really we want to understand the content for a video so what actual challenges you might see like what improvements we can make uh to um yeah so so um you're asking about the attention Mass sort of right yeah so you can use the same idea for for videos uh and you just look at the video and and so these systems are are so good now the object detectors are so good you can really track objects kind of real time as they they go through your video and so you can try to check how that aligns with your attention mask in your model um so so a lot of uh like so videos I think are sort of interesting but they're also not really interesting because you can very often just sub-sample images and solve the images rather than having to deal with the complex video um good job all right maybe one more one more question and then we'll go do some evaluations yeah so these multi-mole models when um you only provide let's say you only provide a single source of media to the same only text or vision how does it perform in that case because it's obviously more geared for multi-million cases yeah so I mean that's one of the giant shortcomings of a lot of these models is that they're really just built for multimodal stuff and so what if I don't have an image right uh and so uh I mean that that's why we did Flavor because we want to have one model that can do all of that stuff um and that's why in in mmbt so the supervised multimodal by Transformer we actually have an analysis of like how robust is this model to missing images or missing text uh but uh so I think a lot of a lot of folks working on these early visual bird models that were kind of myopically focused on vqa uh which is actually a great segue to what I want to talk about next uh so so it really depends on the the tasks that you care about as I said right and so I I think if I'm gonna tell you about multimodality I also have to tell you how you're going to check that the multimodal system is actually good at multimodal things and so that's the the topic of evaluation which actually is a super important topic and a lot of people they want to be cool and build big models uh but I I think it should be way cooler to do proper evaluation of these models especially if you're in Academia because you only have limited gpus anyway right so what what what can you do sorry I don't want to rub it in with it no um so um so how do you check well um there's this amazing project uh so like imagenet really changed like the history of deep learning I think and this other data set Coco I think also really changed especially vision and language but also I think Vision uh in general where they uh have just a bunch of main sort of multimodal tasks so these images are very richly annotated with all kinds of different things so like the segmentation of the objects the bounding boxes the labels of the boundary boxes they come with like uh sort of a different pixel granularities it's a huge data set uh it's very fine-grained uh annotated in terms of like the categories that it has and then you have five captions for each of these images um and so this this really was the first data set that unlocked a lot of sort of vision and language processing at scale because you had your picture and you had your caption and now you need to figure out okay how do I give the right caption for this image so that's image captioning or can I retrieve given some piece of text the right image or the piece of uh or the image for the piece of text so there's a bunch of very impactful data sets that do this stuff that we already talked about lion but Coco really is the main one still I think that a lot of people kind of use as the canal core instance of this data set category and then the other thing that people really care about in vision and language processing is visual question answering um and so the there really are a bunch of academic groups who are or have been so focused on this task that they didn't really care about anything else and that's why you see a lot of models that are really optimized just for multimodal and nothing else uh and you can see that kind of reflected in the citation counts as of last night 3 A.M um where uh so the vqh just has way more citations than uh image captioning data sets even right and so what you do here is you just have an image and then people ask very simple questions so annotators right they they ask these simple questions they give the answers and now we want to be able to answer these questions with machines and as I alluded to earlier one of the the kind of embarrassing uh backstories of this data set was that the initial version of the data set was actually found to uh to have images not really matter at all so you could just look at the question then it could have something like how many slices of pizza are there um and so well not in that particular case but in almost all of the data set the right answer for how much or how many questions was too so if you just predicted two to every how much or how many questions you got like 70 accuracy on the accounting category so careful data set uh or evaluation Benchmark design is also really a skill and you really need to think about what you're doing you can't just like set some data aside and evaluate it on it you have to really think about what you're doing um and so there's gqa by by Chris actually which is also just a I I think a a better designed version of this data set maybe so you might want to use that uh these days um they're also kind of um a very targeted data sets that really try to measure one particular thing and I think one of the things we really want to get at with these models is what we would call compositionality right so we want to be able to really take the parts and and reason about the whole and understand the relationships between the different concepts so clever was a very clever data set uh that was designed really to to measure the the compositionality both on the language side and on the vision side so you have to understand the relationships between all of these different objects in the images uh so that's been a pretty impactful data set I think for really uh forcing people to think about compositionality but a lot of these data sets uh really had big problems uh so so one of the problem is you know uh they were too easy uh so vqa is sort of like plateauing out we can talk about that a little bit too it wasn't really realistic so you could solve vqa and that's probably going to make some people's lives better you're all like trying to process the means how I can see everything okay let's get to the memes first then so um so uh obviously so these memes are not actually in the data set um so I could put some really hateful memes about sort of Hitler or something which are in the data set but that would be less fun um so uh these are mean meme examples to kind of uh demonstrate uh how the data set was constructed and and so one of the problems we had as I said like vqa the V didn't really matter what we want to have is a data set if we're if we care about multimodality specifically it's like how do we get a data set that you can only get right if you are good at multimodal reasoning and otherwise you're just going to screw it up um and so this is what we came up with is if you have a meme like this one love the way you smell today I mean that's not very nice if you send this to your friend right um so um but uh so it turns out that that if you just swap out the background now it's a very nice thing to say right uh and like this one is you know I don't know you're maybe a bit weird if you like this but uh there's there's nothing wrong with it right um and so it's the same for this one here like look how many people love you with the Tumbleweed that's really sad and like you know if you if you if you change just one word suddenly it's like a really nice thing to say right um so so if you want to solve this if you want to classify this correctly for the meanness then you have to really understand multimodal reasoning you have to understand the relationship between the image and the text in order to get to the right label right and so it was really constructed by Design to do that um and uh so how we did it exactly is we we use some really uh highly trained annotators and then uh one of the big problems with a lot of these data sets is that uh nobody really knows who owns the meme for example right so somebody makes this meme now they technically own a copyright and so when I made this data set I was working at the Facebook and they were very afraid of copyright things so what we actually had to do is uh we had to pay people to make new memes um and and so so not from stretch so we could show them kind of the actual examples and then they had to try to find images that were were uh kind of corresponding to the original Source image and tried to recreate the meme but now with an image that we could buy from Getty um and and so we gave a lot of money to Getty uh so that we could then release the data set uh to the public so that people could do actually research on this and understand for their multimodal models whether they're good or not um and so we really tried to make it so that we had these benign co-founder benign confounders uh sorry um it's a startup world with co-founders um so um so the co-founder here is obviously that you have your original Meme and then you have uh your confounder where you swap out one of the modalities in here you have the other one right so we had our annotators do that as well uh and so this led to a really nice data set I think uh because it showed some of the intuitions that I think a lot of people under field had which is that multimodal pre-training doesn't really work is that an alarm um so multimodal pre-training doesn't really work uh and so all of this stuff that people have been doing with all their fancy visual work models actually turned out maybe to not really be that useful anyway and so maybe it got you like one point extra right from visual birth to like a different visual birth like less than a point uh just just by doing that multimodal pre-training um so that means like we still have to figure this stuff out right this this data set is far from Salt and we we still have a long way to go despite all of these fancy models and and you know a new paper coming out every week that does something new like we're not there yet um and I think that's encouraging uh especially for you like when you uh you can go out and solve it um so um what we did with this data set is we organized the competition we had 100K in price money uh to try to see what people could come up with um and so there there was a lot of nice work coming out of that and we've really kind of managed to to crank the numbers up by quite a lot um but the the solutions were slightly disappointing so I don't know if you've ever used kaggle but if you want to really win on kaggle you just have to Ensemble the hell out of all of the different models that are in the current state of the art and then you're very likely to win right and so that's that's what happened here um where you know there wasn't really the fundamental breakthrough we had maybe been hoping for so uh that still uh needs to be built I think um so this other data set I just want to kind of briefly talk about so so the theme sort of of this section is like if you make a data set think about it very carefully uh because you can really be very creative with this and really really measure the things you're trying to get at so um this this data set winner ground we were trying to figure out okay how good is clip actually so it looks really amazing and it's way better than things that were previously there but does it understand compositional relationships in the same way that humans would understand it or is it sort of just fitting onto the data distribution and it can be very good at the head of the distribution but it's terrible at detail and you can probably already guess where this is going um but uh so so just to give you an illustration of what is in this data set you would have some plants surrounding a light bulb or you would have eight light bulb surrounding some plants so notice that the words here are exactly the same words but in a different order right so uh and and so the visual depiction of these words is very very different so if your model your contrastive model is actually good at understanding the Visio semantic or the yeah visual linguistic compositionality uh of these these uh these uh examples then then you can get it right but again if it's actually just overfitting on the data distribution that is seen and it just kind of is biased toward what it sees often then it doesn't really get it right and so one paper uh that we use as a source of inspiration for this work is uh this paper here Order word matters pre-training for little uh so we actually found that the order of words doesn't even matter that much for General pre-training very often uh which is also kind of a scary thing right so this is deep learning for NLP we think that you know language is really important but these models can can reason about language even if you shuffle all the words um and so that's that's probably not what we want to have and so that that doesn't tell you something about how great we are as researchers it tells you something about how terrible our evaluation benchmarks are right and that's what we need to fix um so so what we did with this data set here some other nice examples like there's a mug in some grass or there's some grass in a mug like these are very different pictures right and so for us these are trivial like so like you know what's the difference between a truck fire and a fire truck they're pretty pretty important I think also to get that distinction right um so um guess what um state-of-the-art models often perform below random chance so uh uh you know as I said we still have a lot of work to do which is good um and and so when this paper came out that I I think the the reaction was was really nice and uh so when dolly two came out um which so you've probably heard of Dolly too right so it's sort of like stable diffusion but then before stable diffusion um and so this was really the the first model that really showed like just how impressive these generative models can be uh when they're when they're creating images so this is there's a mug in some grass uh you do have to kind of cheat a little bit because you have to add digital art here uh if you if you don't add that then it breaks down completely right uh so it's sort of prone attacking I think or sort of tuning on the test set but okay you know um so this is pretty good right so so it's definitely is better than I think a lot of people would have expected even a couple of years ago um but it's not perfect because uh people on the internet like to take more pictures of spoons than Forks um so if you say there are fewer uh spoons than Forks or there are fewer Forks than spoons it just really like spoons more um you know and so maybe it's like the Matrix or something I don't know but so uh spoons are just nicer so uh so again what you can see here is that these models really are just reflections of the data that they're trained on right um and uh yeah so models are getting better but if you've looked at stable diffusion like it still can't count fingers and things like that right so again uh there's still a lot of a cool work to be done any questions on evaluation no okay so let's let's talk about other modalities then because so we've really just been focused on images and images are great there are lots of images uh on the internet and and so that makes it sort of an obvious thing to focus on it's also I think if you look at our brain like vision is a very dominant modality right so how we understand the world is very Vision driven uh but it that it that doesn't have to be the case so there's all these other interesting problems that involve different modalities and so the most obvious one is just speech or audio right so after after CN comes hearing um and and really we could do another lecture just like this just on speech and audio and there's lots of interesting stuff to talk about obviously we don't have time but uh I'll give you another uh nice example of how Amazing Alec Radford is at creating data sets um so so there's this whisper model that came out of open AI not too long ago which was trained on 680 000 hours of multilingual multitask uh Speech data so speech with transcriptions um and they they trained this very fancy uh thing on there which actually is not very fancy at all it's just the long male spectrogram so how you represent the audio signal and then you feed that into a big Transformer so this is sort of your encoder self-attention here right and then you have your decoder where you have your cross attention and then you just generate the sequence so this is encoder decoder basic Transformer model but your input is uh convolutions one-dimensional convolutions over the log mail spectrogram and so there's lots of papers that do very similar things uh there's there's models like wave to VEC that try to turn the wave signal into vectors or you can discretize it in lots of different ways um so there's a wealth of literature then I think one of the funny observations actually is that you can just reduce audio to Vision anyway right so so that's what you could sort of argue this log mail spectrogram does but so not to toot my own horn but in 27 I I did this paper where we showed that you can just take a real audio sample turn it into a a kind of a spectrogram really just a spectrogram so what does the spectrum of the the audio file look like feed that to a regular content like an Alex net even and then that gives you amazing auditory features so now you can use this to distinguish between violins or guitars and things like that so you know maybe you can just reduce all of this to Vision so one question maybe you could ask is that can we also reduce language division or Vision to language you know that could so that's sort of what people are thinking about today um so we talked about the video there was a question about video so a lot of these ideas also extend pretty directly to video but now you just have more data right so like Flamingo already had a bunch of different images in it you can do Flamingo over videos probably a lot of the images are pretty useless for what you're trying to do with this video model right so they're they're too similar it doesn't really add all that much information so you want to sub-sample the frames so that you get the most useful information out of your video uh and so there's a bunch of approaches that that kind of take the keyframes and then you just do a standard joint vision and language Transformer encoder thing on top of that so this is kind of becoming hopefully by now a very familiar recipe right um and so there's this so Merlot is a nice architecture that does this and then they came up with Merlot Reserve kind of a silly name where they also added audio to this model so this is now a tri-modal model right and so you know for going towards this Foundation model that can consume all of these different modalities uh all in one go and that's really like a clear Trend in the field um another very interesting Direction I think where in the field we were very excited about this for a while but we I think it's it's sort of uh gone now because it's too difficult to create lots of high quality data in this setting but what you can do is you can have simulated environments uh so this is a paper from deepmind from 2017 where they had this agent walk around in the Maze and then it could have natural language instructions they could also generalize to like decks and Blicks and different sort of groundings to the and assignments that that you could do in that environment so this is a super interesting Direction I think in the long term because this is how humans learn language right like we walk around in the world we interact with our environments we have all of these different perceptual observations we synthesize them in our brain we manipulate objects we change our own Viewpoint and that's how we learn everything we know about the world and so our our language is very intricately connected to that world and how we observe it um so I think that that might make a comeback at some point in the future um you can also do other stuff so especially with this kind of conditioning or text that that we're seeing a lot of right so like so you know Dali 2 and stable diffusion and all of these different things and the original again we talked about at the beginning you can do the same thing but now you're generating 3D Point clouds right so this is a 3D Corgi um using a corgi and so this this prompt can probably become much more complex over time and you can do like sort of AutoCAD design and just say like give me a house and it's just going to design the whole house for you uh so you can just like tweak The Prompt and things like that like that that's all coming or or even already here in many cases um so so the final modality I I just briefly wanted to talk about is uh olfactory embeddings sure um and uh so olfaction means smell if you didn't know um and uh so it turns out so my PhD thesis was about grounding uh semantics in uh different perceptual modalities so a lot of my work started in vision and then it's like okay now audio is sort of the obvious next one right so you can learn the meaning of violin and then maybe you can learn that violin like what a violin looks like and what it is and what it sounds like and that's going to give you a richer representation but for a lot of these words well it's actually very primitive to their meaning is what they smell like because uh in our brains that's really one of the core areas of one of the oldest areas in your brain uh so uh what you can try to do if you want to complete all of your perceptual modalities is you can try to build olfactory embedding so it was kind of a joke paper I did but um the funny thing is it actually worked um so uh there's there's a catalog this Sigma Aldrich fine flavors and fragrances catalog where you can look up words like melon and pineapple and then it's going to give you all of the chemical compounds that produce this smell or taste and so if you do that then you can count the occurrences and then you can sort of do SVD or something like that only to to get it to be a bit more of a real embedding model so now you get smell embeddings smell vectors and then you can compute similarity judgments between these these smell so turns out Apple smells like pear uh and you know the chocolate and cocoa and sweet and coffee are sort of related right so you get these clusters of different smells just based off of their chemical compounds so this bag of chemical compounds model gives you a very rich representation and so if you look at all of the words that are concrete enough to have smell right so like if you have a word like democracy in there that doesn't really smell like anything right so you you ignore democracy and you just focus on on the things that smell um or that good smell I guess um and then so the the really interesting thing to me is that you know this is is much more correlated with human similarity judgments than the linguistic vectors we had at the time right so so for a work like apple like you can just get a word Vector like you've learned in your first lecture uh and and so you can do like you know Skip gram and things like that but that that thing is not going to be as correlated with human similarity judgments as this bag of chemical compounds model so that's that's pretty interesting right so even something like smell where maybe we think you know this doesn't really matter if you really want to understand how humans understand language then maybe you want to include this in your foundation model too and but I would start with other modalities all right um okay yeah sorry yeah uh so where to next uh I'll just I think I've already said most of this actually so One Foundation model is going to rule them all um and we'll so I mean there will be many of these but a lot of them are going to have very similar traits I think um we're going to be looking at scaling loss and trying to understand really what is the relationship between the different modalities which one do we want more of that sort of stuff we're going to have retrieval augmentation this thing is going to be really huge if you've heard of rag or if you haven't you should look it up uh so all of these parts of these models can also be multimodal we need way better evaluation and better measurements we already talked about that too and that's all I have thank you [Applause]