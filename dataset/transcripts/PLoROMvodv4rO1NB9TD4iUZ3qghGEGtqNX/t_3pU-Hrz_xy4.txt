all right let's start guys okay so a few announcements before we start so if you have if you need away.he accommodations please let us know if you haven't done that already so you need to let us know by October 31st because we need to figure out the alternate exam date so we'll get back to you about the exact like details around the alternate exam date but let us know by October 31st project proposals are also due this Thursday so to talk to the TAS you talk to us come to office hours alright so today we want to talk about games so so we have started talking about this idea of state based models like the fact that you want to have state as a way of representing everything about everything that we need to plan for the future we talked about search problems already if you have talked about MVPs where we have a setting where we are playing against the nature and and the nature can play like probabilistically and then based on that and we need to respond and today we want to talk about games so so the setup is we have two players playing against each other so we're not necessarily playing against nature which can act probabilistically we're actually playing against another intelligent agent that's deciding for for his own or her own good so so that's kind of the main idea of of games alright so so let's start with an example so this is actually an example that you're gonna use throughout the lecture all right so the example is we have three buckets we have a B and C and then you're choosing one of these three buckets and then I choose a number from the bucket and the question is well well your goal here is to maximize the chosen number and the question is which bucket would you use okay so so how many of you would choose bucket a no one trusts me okay it's how much good how many of you choose B okay so now if people don't trust me how many of you to see okay so so there's a number of people there yeah so so how are you making that decision so the way you are making this decision is if you choose a you're basically assuming that I'm not playing like like trying I'm not trying to get you I might actually give you 50 and if I you fifty that'll be awesome and you have this very large value that you're trying to maximize if you think I'm going to act adversarial and go against you and try to minimize your your number then you're going to choose bucket B right because because worst case scenario I'll choose the lowest number of the bucket and and in buckets be the lowest number is one which is better than minus 50 and minus 5 so if you're assuming I'm trying to like minimize your good then you're gonna choose bucket B and then if you have no idea how I'm playing and you're just assuming maybe I'm acting ad stochastically and maybe I'm like flipping a coin and then based on that deciding like what number to give you you might choose C because in expectation C is not bad right like seems like if you just averaged out these numbers and then pick the average values from ABC a and B and see the average value for a is 0 for B it's 2 and then for C is 5 right so so so if I'm playing in stochastically you might say oh I'm probably going to give you something around 5 so you would pick C okay so so today we want to talk about these different policies that you might choose in these settings and how we should model our opponent and how we formalize these problems at game problems so this is an example that Dad we just started ok so so the plan is to formalize games talk about how we compute values in the setting of games so we're going to talk about expecting Max and minimax and then towards the end of the lecture we're gonna talk about how to make things faster so we're gonna talk about evaluation functions as a way of making things faster which is using domain knowledge to define evaluation functions over notes we're also going to talk about alpha beta pruning which is a more general way of pruning your tree and making things faster yeah all right so that's the plan for today okay so we just defined a scam and the way that to go about this game is to create something that's called a game tree a game tree is very similar to a search tree so this might remind you of search tree where we talked about it like two weeks ago right so so the idea is we have this game tree where we have notes in the industry and each node is a decision point of a player and we have different players here right like I was flying or you were playing you like we have two different people like playing here so these decision notes could be for what one of the players not both of them and then each roots to leaf path is going to be a possible outcome of the game okay so like it could be that I'm choosing minus 50 and then your decision was to pick bucket a so that path is going to give us one possible outcome of how things can go okay so so that is what the tree is basically representing here so the notes and in the first level or the decisions that I was making and in the the first node the roots node are the decisions that you were making in this setting so if you were to formalize this a little bit more we're gonna formalize this problem as a two player zero-sum game okay so so in this class at least like today we are going to talk about two player games where we have an agent and we have an opponent and then we are going to talk about policies and values and for all of those things think of yourself as being the agent so you're playing for the agent you're optimizing for the agent opponent is this opponent that's playing against okay so we're also going to like today we're going to talk about games that are turn-taking games so we're going to talk about things like chess we're not talking about things like rock-paper-scissors we will talk about that actually next time then we have like seeing like simultaneous games where you're playing simultaneously today we are talking about turn-taking settings two-player turn-taking settings full observability we see everything we're not talking about like games like poker where you don't necessarily see like you have partial observation and you don't necessarily see the hand of your opponent full observation to player and also zero-sum games and what zero-sum means is if I'm winning and if I'm getting like ten dollars from winning then my opponent is losing ten dollars so the total utility is going to be equal to zero if I win some amount my opponent is losing the same amount all right so so what are the things that we need when we define games so we need to know the players we have the agent we have the opponent in addition to that you need to define a bunch of things this should remind you of the search lecture or them if you like sure so we might have a start state that's a start we have actions which is a function of states which gives us the possible actions from a state s similar to before you have a successor functions to search problems so successor function takes a state an action and it tells us what's the resulting state you're going to end up and this and you have an is end function which checks if you're in an end state or not and the thing that's different here there are two things that are different here one is this utility function and the utility function basically gives us the agents utility at the end State so one thing to notice here is is that the utility only comes at an end state so after you finish the game like I've played my chest and I have one chest now and this is this chess game and and then I get my utility like as I'm making moves like through my chess game I'm not getting getting any utility it's like you only get the utility at an end state and I no way they are defined me utility as you are defining it for the agent because again we are we're playing from perspective of the agent so so what would be the utility of the opponent - that right so the negation of that would be the utility of opponent I've heard about partially observable Markov decision process this like kind of what it is okay so the question is is this partially observable Markov decision process this is not a partially observable Markov decision processes there are classes that talk about like there's this decision under uncertainty michael Koch endure first class that actually teaches that so you should shoot you should take classes on that this is not partially observable Markov decision process this is fully observable you have two players playing against each other so very different setup so so the question is are there any random this year and and so far I haven't discussed any randomness yet later in the lecture I'll talk actually about the case where there might be a nature in the middle that acts randomly and then how we go about it but so far two players playing against each other okay all right and then the other thing that we need to define when we are defining a game is is the player so so a player is a function of state and basically tells us who is in control like who is playing now so in the game of chess the coos turn is it now and and Ida is a function that we are going to define when we are formally defining a game okay all right so so let's look at an example so we have a game of chess players or white and black let's say you're playing for white so the agent is white the opponent is black and then the state s can represent the position of all pieces and who started this so that is going to what the state is representing so whose players turn it is and the position of all pieces so actions would be all the legal chess moves that player s can take and then is ends basically checks if the state is checkmate or draw that is what is so so then what would the utility be the utility will be will be if you're like you're only going to get it when you win or when you lose or or if there's a draw so the way we are defining it is it's going to be let's say plus infinity if white wins because because the agent is white and and it's going to be zero if there is a draw and then it's going to be minus infinity if black wins yeah so so that was all the things that we would need to define yes why do we have why do we have whose turn it is in this state and so that's one way of actually like extracting the player function so you defy you can define the player function is a player is a function of state so the state already needs to encode whose turn it is so you can kind of extract that this way you said the killer but you probably been negative utility for the the agent is that assuming that both taking the same actions no so so so this is turn-taking right so I take an action and then the opponent takes an action and an agent a connection opponent takes an action and then at the very end of the game then then you get the utility and then the opponent gets in to get negative of that utility but the actions could be very different policies could be very different and we'll talk about how to come up with that so why is the condition like very influential exactly for what wins you get plus infinity but we just black when you trade but if black plays black wins you have you don't have a zero-sum game we'll talk about that next lecture actually a little bit so so I'm talking about zero-sum games here because the algorithms you're talking about are for zero-sum games like we're going to talk about minute minimax type policies where I'm minimizing and the agent that's maximizing so I'll get back to that if I haven't understand sir that like we can talk about it after the class but also next lecture we'll talk about more variations of games so but for now I'm assuming a bunch of simplifying assumptions about the assumption of stuff like you're twins why twins back at zero that's time yeah so this utilities need to add up to zero if white wins maybe white gets ten but black gets minus ten so so like any titles okay all right so and then kind of the characteristics of games that we have already discussed are two main things one is that all utilities are at end state so throughout this path you're not getting in utilities as opposed to like things like MVPs or we were getting rewards like throughout the path but here like the utility only comes in at the very end at the end state and then the other thing about it is that different players are in control at different states right like if you're in state you might not be able to control things in my control things it might be your opponent's turn I knew be able to do anything okay so those are kind of the two main characteristics of games all right so let's look at a game that you're going to play alright so the game is a half a game so we start with the number and and then the player and the players take turn and they can do two things they can either subtract one second decrement N or they can replace n with n over two so they can divide or subtract yeah and the player that's left with zero is going to win okay so that is that is the setup is that so I'll follow that so so let's try to formalize the game and then after that you want to figure out what is a good policy to do it so so right now let's just try to let's just try to formalize this say like what are all the different things for the model R so let's just have a new file we are going to define this game so it's 1/2 in game this alright so if you're initializing with n so we're starting with some number n so what is our state where our state is going to encode whose player turn it is and that number n ok so we have a player let's say our players are either plus 1 or minus 1 that's how I'm defining like whose player this so the start state let's say player plus 1 place with n so so that is plus 1 and N and then we need to define is and ok so what should is n check well we take the state we decouple it into player and number and if the number is equal to 0 then then that is when the game ends that's our ending condition ok how about utility well we get the utility at an end state so again I take a state I decouple it into player and number I make sure that we are in an end state so we search that number is equal to 0 because that kind of defines if you're in an end state or not and then the utility I'm gonna get if I'm winning I'm gonna get infinity if I'm not winning I'm gonna get minus infinity and the way I'm defining that here is by just doing player times infinity because player I'm the agent I'm the player plus 1 the opponent is player a swan that's how like if -1 is winning I'm gonna get minus infinity okay the actions that we can do is we can subtract 1 or we can divide / - I mean subtract and avoid or the main actions and play your display or function again takes a state I'm going to decouple this state into player a number and just return the player that's how I know whose player's turn is and then we need to define the successor function the successor function takes the state and an action and tells us what state you're gonna end up by so again a state I'm going to decouple that into a player and a number and then the actions I can take are two things I can either subtract 1 or I can I can divide by 2 so if I'm subtracting then I'm going to return a new state which is - player because now it's - once turn or +1 so like it's - whoever turned to this now and then I'm going to do number minus 1 if the action is divided we are going to return the new player which is - player and then number / - okay that is it so so we just defined this game all right so so that was my game we were gonna play this game in a little bit but let's quickly before playing it let's talk about what is a solution to a game like what are we trying to do in a game so if you remember MVPs the solution to a game was with a policy so a policy was a function of state it would return the action that you need to take in that state so similar term the piece here we have policies but but the thing is I have two players so policy should depend on the player two so I have pi of P which is the policy of player P and I can define it similar to before it can be a policy as a function of a state and it can return just an action and this would be a deterministic policy like deterministically if I'm in a state the policy is going to tell me what action to take yes we can also define stochastic policies so what's the catholic policies would do is they would take a state and action and then they would return a number between 0 to 1 which is the probability of taking that action so policy PI of a state and action basically will return the probability of player P taking action a in state s so if you remember the bucket example like maybe half the time I would pick the number on the right and half the time I would pick the number under on the left that would be a stochastic policy right I'm not deterministically telling you what the action is I'm coming up with the stochastic way of telling you like what policy I'm falling ok so if you have deterministic policies stochastic policies like in our game we could fall either one of them fantastic policy as the student has been pausing and can you speak up so under what case would you want to stochastic policy versus a deterministic policy so I know what case do you want a stochastic policy is a policy again we'll have covered that a little bit more next time depending on what games you are in like we have some property yourself when stochastic policies are giving us some some properties than the term mystic policies are giving us some other properties right now you're just defining them as things that could exist and we could think our opponent is acting deterministically if we know exactly what they're doing sometimes have no idea maybe you're like I've learned it somehow and I have some randomness there and then I'm gonna do some stochastic policy for how my opponent is going to play against me but we are going to talk about the like what we get out of a stochastic versus deterministic policies a little bit more next time yes all right so okay so now let's okay so now that we know that it's the policy that we want to get let's try to let's try to write up a policy for this game and then I'm gonna define a human policy and what I mean by that is this is going to come from a human that means one of you guys or two of you guys so um so I need two volunteers for this but let's quickly actually write this up so what is a human policy it's just going to get the input from the keyboard so what I'm going to tell you up here is is get the action from from from the keyboard so get the input from the keyboard and that is going to be the action that we are picking remember the actions are either divide or subtract subtract one and if action is valid then return that action that sounds like a good good policy okay so that is a human policy so now what I want to do is I want to have like this game that's actually playing against each other so I want to have policies for my agent my agent is plus one that's going to be a human policy and for my opponent I'm gonna say my opponent is also a human policy so I just want to humans to play against each other and the game is let's say we are starting with 15 so our number that you're starting with is 15 okay all right so that looks right to me so how do we how do we ensure that we are progressing in the game so if you're in an NSA if you're not in an end-state you want to progress so let's print a bunch of things here just print our state okay let's get the player out of the state because again the state encodes the player let's get the policy because because we've defined these policies for both of the players so we can we can get the policy for whoever is playing right now and then the action comes from the policy in that state and then the new state you are gonna end up at is just a successor of the current state and action so that I'm just progressing so so this while loop here just figures out what stage we are in what policy are we following and where are we going to end up at and that's the successor function and another very end I'm just going to print out the utility so that's either plus infinity or minus infinity and that sounds good so all right so let's actually alright so who wants play this okay that's one person your agent your player plus one opponent all three to four I think you reversed white shirt yeah okay so you're minus one all right so let's play this yeah is this large enough yeah okay all right so player 1 player +1 we are at number 15 you want to decrement okay so minus 1 so you are player minus 1 we are at 14 what do you want to do divide okay you have a policy that you know minus 1 oh yeah so I kind of get the point right so wait do they make you lose now sorry my back but when you get the utility at the end and then basically kind of actually does any no I don't know we don't have down motion I was gonna try like another pair but the code is online if you wanna play with it just play with it you'll have one other version playing with an automated policy later all right so okay so we're back here all right so we just saw how we can give some human policies and the human policies playing against each other and again the policy you give it a state an action it gives you a probability or you give it a state and it gives you an action so the term Mystic policy is just an instance of a stochastic policy right so so if you have deterministic policy you can kind of treat it as a stochastic policy where with probability one you're picking you're picking an action so alright so so now we want to talk about how we evaluate a game so so let's say that someone comes in and gives me the policy of an agent and an opponent and I just want to know how good that was and again if you remember the MVP lecture we started with policy evaluation so indemnity lecture we started with this idea of someone gives me the policy if you just want to evaluate how good that is and you're kind of doing an analogous to exactly that someone comes in and tells me that my agent is going to pickpocket eighth that is what my agent is going to just do all the time and someone comes in and says well my opponent is going to access classically and and with probability 1/2 you give me one of those numbers ok so so these are the two policies that we were going to have so the question is how good is this so going back to the to the tree the game tree what is really happening is my agent is going to pick this one right because it's going to pickpocket a so with probability 1 we are going to end up here with probability 0 we end up in any of these other buckets and then my opponent is going to stochastically pick either minus 50 or 50 ok so if my opponent is picking minus 50 or 50 then the value of snowed is just the the expectation of that or it's just going to be zero so 50% of the times it's minus 50 or 50 percent of the times it's 50 then the value of the snow Dussehra and then if my agent is picking picking a then then the value of this node is going to be zero okay so so you kind of can see how the value is going to propagate up from the utility so we have the utilities are the leaf nodes but we could actually compute a value for each one of these notes if I know what the policies are right like if I know who is following what policy I can actually compute these values and go up the tree so in this case I can say a value of the start state if I'm evaluating this particular policy is going to be equal to zero okay all right so someone gave me the policy I evaluated the value of the start State so in general I guess I was just saying earlier this is this is similar to two policy evaluation this is similar to the case that someone gives me the policies and all evaluate what how good the situation is and you can write a recurrence to actually compute that so I'm going to write the recurrence here maybe so you want to compute this value and this value is evaluating a given policy it's a function of state but what is that going to be equal to it's going to be a quarter utility of s if you're in an end state so it's utility of s if we are already in an end state otherwise I have access to the policy of my opponent and policy of my agent so I can just do and expect that sum over all possible actions of us let's say that I'm if player s is agent I'm looking at policy of agent say it's a stochastic policy times V of eval of the successor state successor of SN a and this is if my player is agent so so if is player I'm just gonna write this player of s is equal to agent what happens if my players opponent I'm gonna do the same thing I'm just evaluating I have access to the policy of the opponent I am again just doing going to do a sum over all possible actions of a policy of the opponent this is given to me someone gave this to me of state and action x value of the successor state there's an a and this is the case that my player is the opponent here so this is the recurrence that we were going to just write and it's kind of intuitive again we have seen this entry search shoe like you start with the utilities at the leaf nodes and you just push that back up based on what your policies are and what your policies are telling you like which sides like which and which edges of the tree you are taking what probably okay this make sense all right okay so that was evaluating the game but what if now I want to solve what the agent should do right like I am the agent I care about doing like look like figuring out what my PI agent is I don't know what my PI agent is I need to figure out what sort of policy I should be following and that kind of takes us to this idea of expect Emacs which is basically the idea of if I'm in a scenario where I know what my opponent does so I'm is still assuming what I know what my opponent does what would be the best thing that I should be doing as an agent okay what would be the best thing I should do like if you knew like in the bucket example I was trying I was acting probabilistic Lee what would you do the action that like gives you the maximum so you pick the action that gives you the maximum value because you're trying to maximize your own your own value so so then if that is the case then this recurrence needs to change right this recurrence the way it changes is I'm gonna call this is that new value so I'm gonna just do everything on top of this I'm not rewriting it I'm gonna call this value value of expecting max policy okay so so this value eval I'm not evaluating anything anymore I want to actually figure out what my agent should do so I'm gonna call it expect a max and if I know a policy of my opponent I'm not changing anything here because I know the policy of my opponent I'm just gonna compute this but now I want to figure out what the agent should do and what should the agent do mall the agent should do the thing that maximizes this value so I'm gonna erase this sum with the policy because I don't have that policy and the agent should do the thing that maximizes this value so this should remind you of value iteration so if you remember value iteration in the MVP lecture if you weren't evaluating things right you were trying to maximize our value and that's kind of like analogous to what we are doing here you're trying to figure out what should be the policy that the agent should take that maximizes the value under the scenario that I know what the opponent does so I still kind of know what the opponent does so going back to this example so let's say I know my opponent is acting stochastically what should I do so if my opponent is acting stochastically with probability 1/2 then the values of each one of these buckets are going to be 0 2 and 5 and I'm trying to maximize my own you to my own value so I'm gonna pick the one that gives me 5 and then that's shown with this upward triangle I'm trying to maximize so I'm gonna pick buckets see because I'm trying to maximize under this knowledge that the other agent is stochastic reacting and and then we're calling this the value of expecting max policy and the value of expecting max policy from the start state is equal to 5 right because that's that's you did I think I'm gonna get posture - yes this is assuming I know my opponents policy and then I'm following you Nick I guess so I'm maximizing my own you took my own value knowing that my opponent is following this policy and what the opponent would do an expectation alright so and then this is the this is the recurrence that you would get we would just update the recurrence so if the agent is playing then we maximize the value of expecting max okay all right so okay in general I don't know the policy of my opponent right so in general like I know what gives me this payoff so so if that is the case then what should we do so one thing that we could do is we could assume worst case so one thing that you could do is it could be like oh the opponent is trying to get me in and they're going to play the worst-case scenario and they are trying to minimize my value and and that's the first thing to do and we are going to talk about if that is always the best thing we can do or not a little bit later in the lecture but for now like we could assume that if I know nothing about my opponent I can just assume my opponent is acting adversarially against me so and that kind of introduces this idea of minimax as opposed to expect you know just talking about so so what would minimax so in the case of a minimax policy what I am assuming is I am this agent trying to maximize my my own my own value and then I'm assuming my opponent is acting adversarial so my opponent is really trying to minimize my values and then what that means is from this pocket I'm gonna get minus 50 from this one I'm gonna get one from this one I'm gonna get minus five and under that assumption well I'm gonna pick the second bucket because that gives me the highest the highest value so so that is a minimax policy so how would I change my recurrence if I were to play minimax oh I'm gonna I'm going to call it vo let's look at the V of minimax of a state well the recurrence is going to be over our minimax that we of minimax so let me change that if the agent is playing the agent is still trying to maximize the value so that is all good what if the opponent is playing hmm the opponent is going to minimize right so I don't have access to PI up so what I'm gonna do is I'm gonna remove this and say well the opponent is going to take an action that minimizes the value of the successor of SN a and this is how you would compute the value of minimax policy so what happens like if the adversary or agent is not always adversarial right so in that case you have another stochastic policy that kind of defines what the what the opponent is doing and if you have access to then you can do something similar to expect the max if you don't have access to that maybe you would want to like act worse-case and assume that they're always trying to minimize but but that's some prior knowledge that you have that allows you to track better or maybe evaluate the value better for every state so we will talk about evaluation functions a little bit in the lecture and maybe you look that can inform your evaluation function all right so so so here the value of minimax from the start state is going to be one right does everyone see that so I'm assuming my opponent is acting adversarially so we have minus 51 and minus 5 if I am maximizing then the best thing I can get is 1 and that's how we compute V of minimax and then there is really no analogy to this in MVP setting because in the MVP setting you don't really have this game we don't really have this opponent that's playing against us and what happens is is that this is the recurrence that you're gonna get which is what we already have on the board right okay so so what would the policy be so the policy is just going to be the Arg max of this V of minimax so if you want to know what the policy of your agent should be that's PI max it's the Arg max of V minimax / successor of that state and if you want to know what the policy of your opponent that state s should be well that's argument of me of minimax which is intuitive it's other than that way you can actually figure out what the action should be what the policy what the actual action should be okay all right so let's go back to this example this half in game so what we want to do is you want to actually code up what a minimax policy would do in this setting and maybe we can play with a minimax policy after that okay so what would a minimax policy dudes so it's a policy so it's going to be a function of states it's just give it the state and you're gonna just write this recursion that we have on the board so so we're recursing over the state if you're in an end state then what are we returning just so utility okay so we're returning the utility of that state and there's no actions and then if you're not in an end state then you're either maximizing or minimizing over a set of choices so let's actually like create those choices so they can just call Max and min on them so the choices we're going to iterate over all actions that we have and and what is that going to be exactly well that's going to be doing a recursion over the successor state so we're going to recurse over the successor state so recurs over signal game that successor of state in action and I'm gonna return the action here too because I just want to get the policy later and the successor this district URIs function returns a state in action so I just want to get the state from the first one in the action for the second one okay so if player is plus 1 that's the agent the agent should maximize the choices and if player is minus 1 then then that's the opponent the opponent should try to minimize over these choices and that's pretty much like this recursion that we have on the board and that's our recursive function okay so we're gonna recurse over over our state and that gives us a value and it also gives us gives us an action so let's just print things out so you can refer to them so minimax gives us an action and it tells us this is the value that you can get all right and then it's a policy so let's just return the action okay so now what I'm gonna do is I'm gonna say +1 agent is still a human policy and then it's playing against a minimax policy so alright so let's who wants to play with this it's a little scarier to play with a mini policy alright so let's do this Python alright so you are the agent so your player one you're starting from 15 what do you want to do so you just lost the game why do I know you lost the game oh now it's player minus one point we are at 7 and minimax policy took action minor and says action - and and and it also yeah takes action - so we're at 6 and then the value of the game is minus infinity so you're playing at a minimize policy you're already getting minus infinity so so you just lost you anyone want to try this again you want to try it again subtract okay so you can win right so value is infinity right now and then yeah so and then the minimax policy also did a minus so we are at 13 right now it's your turn you're at 13 you just lost the game again yeah so minus infinity yes yeah actually you need to like alternate between them I think that is the best policy but maybe this get a sense of how this runs the code is on line so just feel free to play with her then figure out what is the best policy to use all right so okay so that was the minimax policy and then this is kind of a recurrence that we get for a minimax policy now what I want to do is I want to spend a little bit of time talking about some properties of this minimax policy and we've talked about two types of policies so far right we've talked about expecting max which is basically saying I as an agent I'm trying to maximize but I know what my opponent is going to do so I'm going to assume my opponent does whatever and then I'm going to maximize based on that so so for example I am following and I'm gonna refer that to a spy of expecting max which means that the agent and everything in red is for the agent everything in blue is for the opponent so I'm gonna say the agent is following this policy which says I'm going to maximize assuming my opponent is doing whatever and here I'm calling pi7 as like some opponent policy it could be like anything but pi7 so let's say that's opponent is playing PI 7 I am going to maximize based on that and and the value we just talked about is the value of expecting max the other value we just talked about is the value of minimax which says I am the agent I am going to maximize assuming the opponent is going to minimize and then the opponent actually is going to minimize and it's going to follow-up I mean okay so so these are the two values you have talked about so far I want to talk a little bit about the properties of this but before that let me sorry oh wait it like kind of like mix the two together we just say like heightened like the probability of typing the minimum for like in expectrum acts I've double probability distribution over like the actions right so they're like why don't we just take the action that like minimizes whatever our reward is and give it a higher weight I didn't fully follow is it are you coming up with a new policy that your thing would be a better policy between like expecting Max and minimax in something so this this this table might kind of address that because it's it's considering four different cases it's actually not considering the two cases so this might actually refer to what you were what you were proposing so so let's actually go through this first and then maybe like if it doesn't answer that so all right so so I want to talk about a setting so this table it's actually not that confusing but it can get confusing so do pay attention to this part all right so do I want to maybe maybe I'll write it over there so I'm gonna use red for agent blue one for left my right okay all right okay and then I'm gonna use before I'm gonna use blue for the opponent policy okay so so then for agent who are going to have pie max right in agent could play pie max what does that mean again I'm going to maximize assuming you're going to minimize an agent could play pie expecting max maybe a policy 7 I'm going to put 7 here which means I'm going to maximize assuming you're going to follow this pie 7 so this is a thing that the agents can do okay and then there are things that my opponent can you I'm gonna write that here my opponent can actually follow pie min which is I'm just going to minimize all my opponent could follow some other policy PI 7 let's say PI 7 in the bucket example right now is just acting as stochastically so half the time pick one number half the time pick another number okay so so that is what we have so I'm gonna draw my actually my tree so we can go over examples of that too so this was the bucket example we started at minus 50 and 50 in bucket a 1 and 3 in bucket B minus 5 and 15 in bucket C ok so this was my buckets example I'm actually going to talk about it so alright so I'm going to talk about a bunch of properties of me of Pi Max and timing which is what we have been referring to as the minimax value okay so I want to talk about this a little bit so the first property that that we can have is is that V of Pi max time in is it is going to be an upper bound of any other value of any other policy pi of I'm gonna just write PI of expecting max for any other policy for the agent assuming that my opponent is playing as a minimizer okay so so what I'm writing so what I'm writing here is is the value is going to be an upper bound of any other value if my agent decides to do anything else under the assumption that my opponent is a minimizer so my opponent is really trying to get me if my opponent is really trying to get me then the best thing I can do is to maximize okay so that's kind of intuitive right that's an upper bound let's look at that example so what is PI V of Pi mix PI max and PI min so so we just talked about that right so if this guy is a minimizer we're gonna get minus 50 here 1 here minus 5 here if this guy is a Maximizer what is the value I'm gonna get get it 1 right I'm gonna go down here and then I'm gonna get one so V of PI max and timing is just equal to 1 that is this value that is just equal to 1 okay what is this saying is that this is going to be greater than maybe the setting where my opponent sorry my my agent is following expecting max and my opponent is still doing timing so so what would this correspond to what would this value correspond to so this is a value which says well I'm going to take an action assuming my opponent is acting as stochastically if my opponent is acting stochastically I'm gonna get zero here I'm gonna get two here I'm gonna get five here if I'm assuming that and I'm trying to maximize my own my own value which trout do I go I'm gonna go at this trout but it turns out that my opponent was not doing that my opponent was actually a minimizer so if my opponent was actually a minimizer and I went this route my opponent is going to give me minus 5 so the value I'm gonna end up getting is minus 5 so this is equal to minus 5 this is equal to minus y so so far I've shown that this guy is greater than this guy all right so that's the first property first property is if my opponent is terrible and is trying to get me best thing I can do is to maximize I shouldn't do anything else okay the second property is is that this is V of Pi knocks again the same be V of Pi Max and pi min is now a lower bound of a setting where your agent is maximizing assuming your opponent is minimizing but your opponent was actually not minimizing your opponent was falling by 7 so so what this says is if you're trying to maximize assuming your agent or your opponent is always minimizing then then you're doing like you'll come up with like a lower bound and if your opponent ends up doing something else you can always just do better than this lower bound okay so what is what is this V you call - well we just showed that is Titus 1 right that is this value okay what does this correspond to so this is value of pi max which is I am going to assume you're trying to get me if I'm going to assume you're trying to get me I'm going to on this route because that is the thing that gives me the highest the highest value but you're not trying to get me or falling pi7 so if you're falling falling by seven you're just going to give me half the time one half the times three and that actually corresponds to two and I'm going to get value two instead of value one so this is actually equal to two in this case and this corresponds to this value in the table which is again the agent is following a Maximizer assuming the opponent is a minimizer ponen was not a minimize their opponent was just following pi seven and this is just equal to two okay so so far the things I've shown are actually very intuitive they seem a little complicated but they're very intuitive what I've shown is that this value of minimax it's an upper bound if you're assuming our opponent is a terrible opponent like it's going to be an upper bound because the best thing I can do is maximize I've also shown it's a lower bound if my opponent is not as bad so so that's that's what I've shown so far secure the opponent's policy is completely innocent yeah so here like because yeah the agent actually doesn't see the opponent where the opponent does right even in the expecting max case it thinks the opponent is going to follow PI seven but maybe the opponent falls at PI 7 maybe not right so so like when we talk about expecting Max and minimax it's always the case that the opponent doesn't actually see what the opponent does but the opponent can't think like the agent can think what the opponent does and I'm going to talk about one more property and this last property basically says if you know something actually goes back to your question if you know something about your opponent right if you know something about your opponent then you shouldn't do that minimax policy you should actually do the thing that has some knowledge of what your opponent to us so so that basically says this we PI max and some PI of opponent you know something about PI opponent you know that opponent is playing PI 7 that is going to be less than or equal to the case where you are following the PI of expect emacs of seven and the opponent actually falls by seven okay so what is this last equality inequality saying well it is saying that the case where you're trying to maximize and you think your opponent is minimizing but your opponent is actually not minimizing the value of that is going to be less than a case where you're maximizing under some knowledge of your opponent's policy and your opponent's policy actually ended up doing that okay so so the first term is always the agent the second term is always the opponent right so so this value we have already computed that that's equal to 2 this value what is this value saying it is saying you are going to maximize assuming your opponent is stochastic so if I'm assuming my opponent is stochastic then I'm assuming that this is 0 this is 2 this is 5 right I am trying to maximize so which one am I every track should I go I should go this route because that gives me 5 so this is the agent thinking the opponent is going to be a stochastic thinking is going to get 5 and it gets here and the opponent actually ends up following PI 7 which is a stochastic thing so so we are actually going to get 5 so so this guy is equal to 5 and this is the last inequality that we have we are PI expecting a max of 7 and pi of 7 is greater than or equal to V of Pi max and PI 7 we just showed this is equal to 5 for this example ok all right the reactions of your points always whether or not if it's the guests is it we say too so if you know something about this the cast to say that's not really like here I knew that the opponent was following this is a casting policy of one half one out I might have known that the opponent is following a deterministic policy and always is picking the left one so I could have like follows like a same expecting max policy under that knowledge it could be anything else but the whole idea of expecting max is I have some knowledge of what the policy of the opponent is it could be a stochastic policy it could be a deterministic policy under that how would I maximize that me not like that bottom right is greater than the bottom always yeah so the question is do we have so we have what is in equality so transitively this guy is always greater than this guy and that kind of makes sense right like you're saying like if you're following expecting max okay so this last one kind of makes sense right it's basically saying if you're following expecting max and you know something about your opponent and your opponent actually ended up doing that so though your value should be greater than pretty much anything right because you knew something about the opponent you play knowing that having that knowledge yes is that just caste clear know what so it's knowing what they're gonna take right click here I knew what the point I knew that half the time they're going to take this one half the time you're going to take the other one and then I use that knowledge right now is the expecting max policy given that your opponent is following atomizer policy given that given if your opponent is following pipe in it is it to do a Maximizer so the expecting that policy is as this policy when here we have a some the expecting max policy assumes your opponent is following PI opponent and assumes that it has access to PI opponent so it ends up doing this some over here we probably were saying so you're saying if PI opponent is actually PI min then do they end up being equal to each other and yeah I guess yeah so you know that you're poor it's becomes minimax right if you know your opponent is following me as I can't minimize every just like all that minimax all right so I'm gonna move ahead a little bit all right so and then this is like what we have already talked about okay so a few other things about modifying this game so so we have okay so we have talked about this game we have talked about properties of this game there's a simple modification one can do which is bring nature in so there was a question earlier which was like is there any chance here and then yeah you can like actually bring chance inside here so so let's say that you have the same game as before you're choosing one of the three bins and then after choosing one of the three wins you can flip a coin and if heads comes then you can move one bin to the left with wraparound so what this means is 50% of the time tails comes you're not changing anything you have this setup 50% of the time you get heads and then in those settings you're just gonna pick like a neighboring bin as opposed to your zombie so so there you're adding this notion of chance here and and it's kind of acting as a new player so so it's not actually the making things that much more complicated so so what happens is in some sense we have a policy of a coin which is nature here right and policy of coin is half the time I get 0 I don't change anything half the time I just get the neighboring bin as opposed to my main bin and then I get this new tree Berber I have like a whole new level for what we're the chance place so we have now we have max nodes we have min nodes we also have these chance nodes here and the chance nodes again like sometimes they take me to the original bucket and then 50% of times they take me to a neighboring bucket but but the whole story like stays the same like nothing changes you can you can still compute value functions you can still push the value functions further up it's the same sort of recurrence nothing fundamental changes just it just feels like there are three things playing now so so then this is actually called expecting minimax so value of expecting minimax here in this case for example is minus two because there is a min node for the opponent there's an expectation node for what nature does and then there is a max node for what the agent should do that's why it's called expecting minimax and then you can actually compute the same value there's like two players I pick up in then you flip the point and then shift it left or notch it to left and then I get to take the number yes well you thought you loved the opponent yeah yeah so there's still two players and then the third coin think yes all right so so yes so the way to formalize this is you have players so you have an agent you have an opponent down you have coin and then the recurrence changes a little bit I guess so so what happens is the recurrence that we have had for minimax was just the Max and min and it would just return us the utility if you're in an nth function and in an end-state now if the if it is the coins term we just do a sum over and expected some of their policy of the coin which is what we were doing expecting minimax but we just have like a new return for one coin place so so everything here kind of follows naturally terms up what we're expecting okay all right so the summary so far is well we've been talking about max notes we've been talking about chance notes like what if you have a coin there and then also these mid notes and and basically we've been talking about composing these sore notes together and creating like a minimax game or or an expecting max game ana value function is we just do the usual recurrence that we have been doing in this class from the expected utility to from the utility to come up with this expected utility value for all the notes that we have so there might be other other scenarios that you might want to think about for example for your projects or like in general there are other variations of games that you might want to think about so what if like the case that you're playing with multiple opponents like so far we have talked about like the two-player setting where we have one opponent and one agent but what if you have multiple opponents like you can think about how the tree changes in those settings or for example like the taking turns aspects of it like is it so if the game is simultaneous versus your turn taking or like you can imagine settings where you have some actions that allow you to have an extra turn so so you have two turns and then the next person takes takes turn so you should think about some of these some of them come up into homework so think about variations of games in general they're kind of fun so to talk a little bit about the computation aspects of this so this is pretty bad are you talking about a game tree which is similar to tree search so we're taking its research approach if you remember it's research like the algorithms we're using like if you have branching factor of B and some depth of D then then in terms of time it's exponential in the order of B to the to D in this case so I'm using D for the number of how do I say this so so that's to D because the play the agent plays and an opponent plays so that's how I'm counting it so every every to D like we have two replies but D that's all right and then in terms of space its order of D in terms of time it's exponential that's pretty bad so for a game of like chess for example the branching factor is around thirty five steps is around 50 so if you compute B to the to D then it goes in the order of like number of atoms and universe that that's not doable we should we are not able to use any of these methods so so how do we make things faster so we should be talking about high things faster so there are two approaches that we are talking about in this class too to make things faster the first approach is using an evaluation function so using an evaluation function what we can do is we can use domain-specific knowledge about the game to define almost like features about the game in order to approximate like the value did this value function at a particular state so I'm gonna talk about that a little bit and then another approach is this approach which is kind of simple and kind of nice which is called alpha beta pruning and and alpha beta pruning approach basically gets rid of part of the tree if it realizes you don't need to go down that tree that part that part of the subtree so so it's a pruning approach that doesn't explore all of the tree only explores parts of the tree so so we're going to talk about both of them alright so evaluation functions so let's talk about that okay so the depths can be really like the breadth and depth of the game can be really large that's not that great so one approach to go about solving the problem is is to kind of limit the depth so instead of like exploring everything in a tree just limits the depth and get to that particular depth and then after that when you get to that depth just call an evaluation function so so if you were to like search the full tree this was the recursion that that we had like we have talked about right this is like if you're doing a minimax approach this is the recursion that you got to do you got over all the states and actions and go over all the tree but if you're using a limited depth tree search approach what you can do is you can basically have this depth need and then decrement D every time you go over an agent an opponent like every time you go down the tree and at some point D just becomes zero so you get to put some particular depth of the tree and when D becomes zero you're going to call an evaluation function on the states that you get okay and this evaluation function is almost of the same form of what future costs maybe we're talking about search problems right so so if you knew exactly what it was that then you were done but you don't know exactly what it is because if you knew that you were to solve like the whole tree search problem but in general you can have some sort of weak estimate of of what what the future costs would be so yeah so an evaluation function eval s is a weak estimate of V minimax of s so it's a weak estimate of your value function okay all right so so analogy of that is future cost in search problems so how do we come up with an evaluation function so we do it in a similar manner to admitted in the learning lecture where we're coming up with with features and and and weights for those features right so so if I'm playing like chess and like the way we play it right like we think about a set of actions that we can take and where we end up at and and based on where we end up at then we kind of evaluate how good that were this right we have some notions of features and how good looking like how good that boards would be from that point on and that allows us evaluate what action to pick right like when we play chess that's kind of what we do we pick a couple of actions and we see how the boards would look like after taking them an evaluation function kind of does the same thing it tries to figure out what are the things that we should care about in a specific game in this case and in chess and then try so I give values to them so so it might be things like the number of pieces we have or mobility of those pieces or if our king is safe or or if you have central control or not so for example for the pieces what we can do is we can look at the difference between the number of pieces we have between what we have and what our opponent has so number of Kings that I have versus number of opponents that I have well that seems really important because if I don't have a king an opponent has a king then now I've lost the game so so you might put like a really large weight for that and you might care about like the differences between the number of ponds or number Queens and other types of pieces that you have on the board so so that allows you to care about to think about how good the board is or number of legal moves that you have a number of legal moves that your opponent has and then that gives you some notion of like mobility of that state ok all right so so summary so far is yeah so this is pretty bad order of B to the to D is pretty bad and an evaluation function basically tries to estimate this the minimax using some domain knowledge and then unlike a estar we actually don't have like any guarantees in terms of like error from from these sort of approximations so but it's an approximation people use it it's pretty good we will talk about it a little bit later next time when we think about like what sort of weights we should we should pick for each one of these for each one of these features so you should think learning when you think about what are the weights we are using all right so okay so now I want to spend a bit of time on alpha beta pruning because this is yeah important okay so alpha beta pruning um yeah the concept of alpha beta pruning is also pretty simple but I think it's one of those things that was it's kind of that table you should pay attention to kind of get what it is happening all right so so let's say that you want to choose between some bucket a and bucket be okay and you want to choose the maximum value and then you know that the values of a fall into like three to five and the values of B fall into five to ten so so they don't really have like any any intersections between each other so so in that case you don't really care about your if you're picking a maximum right you shouldn't care about your bucket a or rest of your bucket a because because you already know that you're above wise you're happy with B you shouldn't even look at a so so kind of the underlying concept of alpha beta pruning is is maintaining a lower bound and upper bound on values and then if the intervals don't overlap then basically dropping part of the subtree that you don't need to work on because there is there is no overlap between there okay so here's an example so let's say we have these max notes and mid notes and you're gonna go down and see three and then this is a mid note so so you're gonna get three here so when I get to the max note here right I know what I know is that the max node is going to get three or higher right that's one one thing that I would know without even looking at anything on the on the other side that I've been looking at the sub tree on the Left I already know that this max no it should get three or higher right your dad okay so so then when I go down to the this min node and I see two here right I know this is a min node it's going to get a value that's less than or equal to 2 less than or equal to 2 does not have any interval with greater than or equal to 3 so I should not worry about that subtree did everyone see that so maybe you're like let me draw that here so that's kind of like the whole concept of what happens in the alpha-beta pruning so I have this max node this was three this was five I found that this guy is three this is a max node whatever it gets it's going to be greater than or equal to three because it's already seen three it's not gonna get any value less than three all right so we know whatever value we are gonna get at this max node is going to be three or higher then I'm gonna go down here and then I see two here right it's a min node whatever it gets is going to be less than or equal to two so less than or equal to 2 is the value that's going to get popped up here I already know less than or equal to 2 has no interval with 3 or greater so I don't even need to worry about this like I like I can completely ignore this side of the tree I don't need to know whatever is happening down here I don't even need to look at that okay cuz cuz I like this value should be greater than applause sorry now minimum so it's a minimum it's a minimum note right so it's going to be your less than or equal it's a mid note so I saw two if I see ten here or twenty here like I'm not gonna pick that like it's two or all right so yeah so if it is 10 or 100 or whatever substrate is there like we're not gonna look at that so that that is kind of the whole concept all right so okay so the key idea of alpha-beta pruning is as we're like an optimal path is going to get to some leaf node that has some utility and that utility is the thing that is going to be pushed up like like and then the interesting thing is if you pick the optimal path the value of the note on that optimal path are all going to be equal to each other like that basically that utility that you're gonna get pushed up all the way to the top so because of that like we need to have like these DS like we can't have settings where we don't have any intersections between the intervals because we know if this rule is if this were to be the optimal path the value on this node should have been the same as the value at this node the same as the value at this node and and so on so if they don't have any intervals then no way that they would have the same value and no way for that path to be the optimal path okay so so that's kind of the reason that it works cuz the optimal path you're gonna have the same value throughout okay so all right so how do we actually do this so the way we do this is we are going to keep a lower bound on max nodes so I'm gonna call that a s here so we are gonna have a s which is a lower bound on max nodes so we're gonna keep track of that you're also going to keep track of BS which is an upper bound on mid notes and then if they don't have any intervals we just drop that subtree if they have intervals we just keep updating a snps okay so so here's an example so let's say that we start with this top node somehow we have found out that this top node should be greater than or equal to 6 right somehow I know it should be greater than or equal to 6 okay so that is my a s value so my a s is equal to 6 it is it is going to be a lower bound on my max node I know that the value the optimal value is going to be something greater than equal to 6 ok then somehow we get to this min node and then we realize that this min node should be less than or equal to 8 so you're here let's say 8 is here you still have some interval you're all good right so the s is going to be equal to 8 right we have an upper bound on the min node and that tells us that upper bound is 8 so the the value the optimal value the value on the optimal path is going to be less than or equal to 8 okay so far so good then somehow I found out that that one is greater than or equal to 3 greater than or equal to 3 should be fine right like greater than or equal to 3 is still greater than or equal to 6 my a s in this case I'm gonna call this s 1 s 2 s 3 is equal to 3 right cuz I know I need to be greater than or equal to 3 what like 6 already does the job right like I don't need to worry about that 3 so so that's all and then for this last node I am at this min node and I realize that ps4 I'm gonna call it B S 4 is equal to 5 and what this tells me is that your value should be less than 5 and less than 5 so I'm going to update less than 8 to less than 5 and now I don't have any inner walls so what that tells me is that path is not going to be the optimal path because there is no interval so we're not going to find this this one number that is going to be the utility and what that tells me is I can actually ignore that whole subtree because because that's not going to be in my optimal path I can I can get rid of it I can ignore it yes so we're ignoring three in a different way so yeah so we're ignoring the value of three because this is already encoded here but we're ignoring the subtree of five like I'm not exploring it like I need to explore things after the three already because I'd like like like with the three of you already had an overlap with the beta so you're looking at with the B value you're looking at the overlap between your upper bound of mid node and lower bound of max node so that interval is the interval you were making sure it still has values in it if the two or three instead we just ignore that anyways because you have something else that yeah yeah so yeah I think so yeah so if you already have like if three where two is that what you're saying yeah so so you want to have non-trivial intervals basically yes yes so like if if you use the same value you still yeah you don't have non-trivial intervals and yeah what are we got six an a300 this is an example that imagines some Holly we'll talk about some examples where we get them so let's talk about one more example where we actually like it these quotes for now just assume somehow we have found this I don't understand why brie is an upper-bound what seems a lower bound so um so then you actual value I'm not showing a full example here so the actual values are coming from somewhere that I'm not talking about yet but oh the one at the top okay oh sorry yeah so the one at the top right so so this is a min note let me note the same accent right so at my mid note I found out that minimum between three and five is three right so max no it is maximizing between three and a bunch of other things that's that's what it's supposed to do right so if it is maximizing between three and a bunch of other things then it's at least going to be three it's not going to be two there's no way for it to be two or it's not going to be zero right because it's it's going to take maximum of three and something else so that's why I'm saying well this value whatever I'm going to get at this max node it's going to be greater than or equal to three X s so now I come down here and I see like I see this - this is a min note so the value here is going to be the minimum between two and whatever is down this tree right so it is going to be at least very bad way that we said it was it's going to be it's going to be two or lower so so what we're getting here is going to be two or lower right so I'm either going to get 2 or 1 or 0 or all that and that's the value that's going to be pushed up here right so that's the value that's going to go down here it's going to be a value that is 2 or lower so if I'm maximizing between 3 and something that is 2 or lower then 3 is enough and I can kind of figure that out based on these intervals and don't look at this side of the tree like like once I have I've seen this - I already feel that there is no no trivial interval between a value that's greater than 3 and a value that's less than 2 so I can just not worry about stuff all right so one quick cutter implementation thing is we talked about these ace a values and B values you can keep track of only one value and that would be this alpha value and beta value where alpha value is just I'm gonna just write it here alpha value right so op of S is the max of a s for all these s Prime's that are listen s yeah so so it's so what this basically says is it remember like when we saw three we said well that's already included like we already knew that that's kind of the same idea so alpha s is just going to be one value in this case it's just going to be six because like when I see three like I don't really care about that three right like I already know I'm greater than six knowing that I'm greater than three is not adding anything so we keep track of one value off of Asaph of s in this case F of s is just equal to six and a similar thing for beta we are going to keep track of beta of s and beta of s is just minimum of BS s and then what I'm writing here is just the ordering of the notes that you have seen so so beta is s fine and then you're looking at the intervals like f of s and F of s and above and beta of SM below and if those intervals don't have any trivial intersections then you can you can prune part of the tree okay so this is more of an implementation thing instead of keeping track of all these assn BS s just keep like one number one alpha and 1 beta okay all right okay so let's look at one other example so all right so I'm gonna just do this example real quick okay so we're gonna start from some top note we're gonna go to this note this is a mid note between nine and seven between nine and seven right so it's a mid note I'm gonna get this guy a seven I'm gonna realize that this max node is going to be something that's at least seven right it's going to be something that's greater than or equal to seven so my alpha there's going to be seven right now I know whatever value I'm gonna get is going to be 7 or higher whatever value to start notice going to get it's got to be 7 or higher so now I come down here I am at a mid note I see a 6 here right I go here it's a min note so whatever we get here is going to be less than or equal to 6 right so it's going to be 6 or something that is lower that tells me my beta is is equal to 6 that tells me whatever I'm getting in that min node is going to be 6 and lower that doesn't have any intersections with my alpha of s so I can just not do anything about this this branch like I don't need to go over like all these other things like I can kind of like ignore like this whole bunch okay all right so now I go back up I go down here I'm at a mid note so remember the way we were computing these beta values we were based on the notice that we have seen previously so I have a new beta now cuz I'm done with this branch right so I need to get here here I have a min between what is it 8 8 and 3 so okay so so I see my maybe let me just rate I see my 8 here it's a min node so it's going to be less than or equal to 8 so my new beta value is going to be 8 my alpha is still 7 because that's for my top note so it's 8 or lower we do have an interval overlapping interval 7 to 8 everything is good so I actually need to go and see what this value is this value is 3 so I get 3 here or like it's exactly equal to 3 so that updates my beta from 8 to 3 we'll have already explored that part of the tree anyways but 3 you don't have an interval if there were a bunch of things below this 3 like I like a nice somehow sound it's not like I wouldn't need to explore it but we don't really have that and then we just find that our optimal value 7 so we just return something okay and we did an explore this giant middle of the tree okay one more slide and enough two more two more quick one quick idea okay so yeah alright so the order of things actually matters so the only thing I want to mention about this idea of pruning is this order of things matter so so when you look at this example remember we didn't explore anything about the ten because we already knew that this value needs to be greater than equal to three these are my buckets right if I swap the buckets like if I just swap the order of buckets I moved the to ten bucket to this side three five pocket to the other side I wouldn't be able to do that I actually need to explore the whole tree because my alpha and beta wouldn't have the same properties so the order that you're putting things on the tree actually matters and you should care about that so worst case scenario our ordering is terrible so we need to actually go over the full tree that's order of B to the to D that's the worst case scenario there are ends of this best ordering where you don't explore like half of it so you can't like if you had that if you if you have a tree where you're you can explore up to like depth ten then with the best order and you can actually explore up to depth like 20 so sorry that's a huge improvement actually so best ordering is going to be order of B to the D and then random ordering turns out to be pretty okay to so random ordering would be order of P to the 2 times 3/4 times D so even if you had a random ordering it would be better than the worst-case scenario and then well how do you figure out what is a good bordering in ordering well we can have this evaluation function remember you're computing the evaluation function and and what you can do is you can order your Super Max nodes you can order the successors by decreasing evaluation function and then for min nodes you can order successors by increasing evaluation functions that allows you to prune as much things as possible all right so with that I'll see you guys next lecture talking about tea new learning