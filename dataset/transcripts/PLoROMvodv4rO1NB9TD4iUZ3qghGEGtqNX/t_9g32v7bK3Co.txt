okay let's start guys okay so I'll plan for two days to catch up so you're a little behind so it's okay so today I want to talk about MVPs Markov decision processes my finest to talk about that for the first hour and then after that I want to talk for ten minutes about the previous lecture so remember like you went over relaxation is kind of quick so maybe we can go over that again and then the last ten minutes I want to talk about the project and and kind of the plan for the project how you should think about it this is coming up so we should start talking about that so this is an optimistic plan though let's see how it goes what this is the current plan okay all right so okay let's get into it so Markov decision processes so let's start with a question let's actually do this just by hand so don't need to go to the website so the question is it's Friday night and you want to go to Mountain View and you have a bunch of options but what you want to do is you want to get to Mountain View with the least amount of time okay which one of these modes of transportation would you use like how many of you would bike no one would like help of people like how many of you would drive this is this is popular mountain view would be good Cal trainers some people would take out rain sounds good uber and lyft we have like a good like distribution why yes yeah the good number of you go on a flight as flying cars are becoming a thing like this could be an option in the future there are a lot of actually startups working on flying cars but but as you think about this problem like the way you think about it is is there a bunch of uncertainties in the world right it's not a necessarily a search problem right you could you could bike and you can get a flat tire and you don't really know that right you have to like kind of take that into account if you're driving there could be traffic if you're taking the Caltrain there are all sorts of delay with the Caltrain and then all sorts of other uncertainties that exist in the world and then you need to think about those so it's not just a pure research problem where you pick your route and then you just go with it right there are there things that can happen and that can affect your decision so and that kind of takes us to Markov decision processes we talked about search problems where everything was at terminus and now you're talking about this next class of state basis functions which are Markov decision processes and the idea of it is you take actions but you might not actually end up where you expect it to because there's this nature around you and there's this world around you that's going to be uncertain and do stuff that you didn't expect okay so so so far we've talked about search problems the idea of it is you start with the state and then you take an action and you deterministically end up in a new state if you remember the successor function successor of SN a would always give us s Prime and we would deterministically end up in s prime so if you have like that graph up there if you start in S and you decide to take this action one you're going to end up in a like there's no other option that's how you're gonna end up in it okay and the solution to these search problems were these paths so we had the sequence of actions because I know if I take action one in action three in action to I know like what is a path that I'm going to end up hide and now with the idea okay so when we think about Markov decision processes that is the setting where we have uncertainty in the world and we need to take that into account so so the idea of it is you start this state you decide to take an action but then you can randomly end up in different states you can randomly end up in s1 prime or ass to prime and again because there is just so many other things that are happening in the world and you need to you need to worry about that randomness and make decisions based on that okay and and this actually comes up pretty much like everywhere in every application so this comes up in robotics so for example if you have a robot that wants to go and pick up an object you decide on your strategy everything is great but like when it comes to actually moving the robot and getting the robot to do the task like the actuators can fail or you might have all sorts of obstacles around you that you didn't think about so there is uncertainty about the environment or uncertainty about your model like your actuators that that you don't necessarily think about and in reality they're affecting your decisions and where you're ending up at this comes up and other settings like resource allocation so in resource allocation maybe you're deciding what to produce what is the product you would want to produce and and that kind of depends on what is the customer demand and and you might not have a good model of that and that's uncertain right it really depends on what products customers want and what they don't and you might have a model but it's not going to be like accurate and and you need to do research allocation under those assumptions of uncertainty about the world similar thing is in agriculture so for example you want to decide what sort of what to plant but but again you might not be sure about the weather if it's gonna rain or if the if the if the crops are going to yield or not so there's a lot of uncertainty in these decisions that we make and and they make these problems to go beyond search problems and become problems where we have uncertainty and we need to make decisions under uncertainty okay all right so let's again another example so this is a volcano crossing example so so we have an island and you're on one side of the island and what we want to do so we're in that black square over there and what we want to do is we want to go from this black square to this side of the island where we have the scenic view and that's gonna give us a lot of reward and happiness so so my goal is to go from one side of the island to the other side of the line but the caveat is here is that there's a small kano in the middle of the island that I need to actually pass okay so and and if I fall into okay now I'm going to get a minus 50 reward more like minus infinity but but for this example like imagine you're getting a minus 50 reward if you fall into the volcano okay so alright so so if I have this thing here on the side so if my slip probability is zero which is I'm sure I'm not gonna fall into it okay now should I cross the island no oh yes well I should cross the island and because I'm not gonna fall right like I'm not gonna fall into that minus 50 sleep probability is zero I'll get to my twenty you reward everything will be great okay but the thing is like we've been talking about how the world is stochastic and SU probability is not going to be zero maybe maybe it is ten percent so if there is ten percent chance of falling into the volcano how many of you would would still across the island good number so the optimal solution is actually shown by these arrows here and yes the optimal solution is still to cross the island like you're you hear we're gonna talk about all these terms but the value here is basically the value you're gonna get at that beginning like stage which is a kind of well we'll talk about it it's expected utility that you're gonna get it's gonna go down cuz there is some probability that you're gonna fall - okay no but still like the best thing to do is to cross the island how about 20% how many of you would do it with 20% so number of people it's less still turns out that the optimal strategy is to cross 30% one person so with 30% that's actually the point that you kind of you'd rather not not cross because there's a soul okay no and it's a large probability you could you could fall into the okay and the value is going to go down okay so these are the types of problems you're gonna we're gonna work with yes so two is like a value the reward that you are going to get that that state and then value you compute that your propagated back you'll talk about that in detail some on how to compute the value okay all right okay so that was just an example so so that was an example of a Markov decision process what we want to do in this lecture is we are going to like again model these types of systems as Markov decision processes then we are going to talk about inference type algorithms so how do we do inference how do we come up with this best strategy path and in the middle I'm go talk about policy valuation which is not an inference algorithm but it's kind of a step towards it and it's basically this idea of if someone tells me this is a policy can I evaluate how good it is and we'll talk about value iteration which tries to figure out what is the best policy that I can take so that's a plan for today then next lecture we are going to talk about reinforcement learning where we don't actually know what the reward is and we don't know what the what the transitions are so that's kind of the learning part of a part of this MDP lectures so Reid is going to actually do the Duda lecture next nix on Wednesday right okay so let's get into let's get into Markov decision processes so we have a bunch of examples throughout this lecture so let's look at another example so all right so actually I do need volunteers for this so in this example we have two rounds and the idea is you can at any point in time you can choose two actions you can either stay or you can quit okay if you decide to quit I'm going to give you $10 actually I'm not gonna give you $10 but imagine I'm gonna give you $10 and then we'll end the game okay and then if you decide to stay then you're gonna get $4 and then I'll roll the dice if I get one or two will end the game otherwise you're going to continue to the next round and you can decide again okay so who wants to play with this okay all right volunteer do you want to stay or quit so that was easy you got your $10 does anyone else wanna play stay oh you got $8 sorry I kind of get the idea here right so you have these actions and then with one of them like if you said to quit you deterministically will get your $10 and you're done with the other one it's it's probabilistic and you kind of want to see which one is better and what would be the best policy to take in this setting so we'll come back to this question you'll formalize this and we'll go over this so okay so then you need to actually compute what is the expected utility right so and that's what we want to do right so so you might say oh I want to I want to stay and then I get my four dollars and I want to quit and then I get 14 and maybe that is the way to go that that could be a strategy but for doing that right like we were going to actually talk about that before doing that we are going to define what would be the optimal policy one other thing that for this particular problem if you're going to keep in mind is I'll talk about in a minute you find a policy but but the policy the way we define it is it's a function of state so if you decide to stay that is your policy if you decide to not stay that is your policy like you're not allowing searching right now like as I talked about this later in a lecture but I'll come back to this problem okay so if you if you decide that your policy the thing you want to do is to just stay keep staying this is the probability of like the total rewards that you're gonna get so you're gonna get four with some probability and then if you're lucky you're gonna get eight and then even if you're luck here you're gonna get 12 and if you're luckier you're gonna get 16 but but the probabilities are going to come down pretty much like really quickly so the thing we care about in this setting is the expected utility right an expectation like if I if I if I run this if I average all these possible paths that I can do what would be the value that I get and for this particular problem it turns out that an expectation if you decide to stay you should get 12 so so you got really unlucky that you got 8 but but in general in expectation you should decide to stay yeah and then we actually want to spend a little bit of time in this lecture thinking about how we get that 12 and then how to go about computing this expected utility and based on that how to decide what policy to use right okay and then if you decide to quit then then expected you chose either it's kind of obvious right because that you're quitting and that's with probability one you're getting ten dollars so you're just going to get ten dollars and that is the expected utility of creating so so when you when I said when you roll a die I said if you get one or two yeah you you say yeah and then if you get the other so the 2/3 of it you continue so so it's a 1/3 2/3 comes from there ok all right I'll come back to this example this is actually the running example throughout this lecture so what the lecture is about okay so let's actually I do wonder finish it an hour that's why maybe I'm rushing things a little bit but we are going to talk about this problem like throughout the class so so don't worry about it if it's not clear at the end of it we can clarify things okay all right so I do want to formalize this problem the way I want to formalize this problem is using an MDP so I want to I want to formalize this at the MA as a Markov decision process maybe I can just use this so in Markov decision processes similar to search problem so you're going to up stakes so in this particular game I'm going to have two states I'm either in the game or I'm out of the game so I'm in an end state where everything we ended you're out of the game you're done okay so so those are my states then when I'm in these states I'm in each of these states I can take an action and if I'm in in-state I can take 2 actions right I can either decide to stay right or I can quit and if I if I decide to stay from in-state that takes me to something that I'm going to call a chance node so a chance node is a node that represents state an action so it's not really like like the blue things are my states but I'm creating these chance nodes as a way of kind of going through this example to see where things are going so so did these blue states are going to be my state I mean s these chance nodes are overstating actions so basically the snail tells me that I started with in and I decided to stay an attached node here basically tells me that I started with in and I decided to quit even though it's deterministic me so I deterministically go there but then from the chance node that's where I'm introducing the probabilities so from the chance node I can like Todd was the clay end up in there different states in the case though it's also deterministic in the case of the quit in this case is deterministic yeah so in the case of the quit we say 41 I'm going to end up in this end state so I'm gonna draw that with the note with the edge that comes from my chance node and I'm gonna save it by what if one I'm going to get $10 and just be done okay what if you're in this state this is actually the state where interesting things can happen with probability 2/3 I'm going to go back to in and get $4 or which probability 1/3 I'm going to end up an end and for $4 so so that is my Markov decision process so so I had maybe you can keep track of a list of things we were defining in this lecture so we just defined States and then we said well we're gonna have these chance nodes because from these chance nodes probabilistic who you're going to come out of them depending on what happens to nature right and this is the decision I've made now nature kind of decides which one we are going to end up at and and based on that we move forward yeah all right so so more formally we have a bunch of things when we define an MVP similar search problems like we now need to define the same set of things so so we have a set of states in this case my states are in an end ok we have a start state I'm starting with in so that's my start state I have actions as a function of States so when I ask what are the actions of the state my actions are going to be stay or quit what are actions of end I don't have anything and state doesn't have any actions that come out of it and then we have these transition probabilities so transition probabilities more formally take a state an action and a new state so as a s Prime and tell me what is the transition probability of that it's 1/3 in this case and then I have a reward which tells me how much was that rewarding four dollars so so I'm defining so when I'm defining my MDP kind of the new things I'm defining is this transition probability it shows me if you're in state s and take action a and you end up in s prime what is the probability of that I'm an in I decide to stay and then I end up in end what's the probability of that that's one-third maybe I'm an in I decide to quit I end up in end what's the probability of that is equal to one okay and then over the same state action state crimes like next States we're going to end up ad we were going to define a reward which tells me how much money did I get or like how how good was that so it was four dollars in this case or or if I decide to quit I got ten dollars and if you remember in the case of search problems you're talking about cost I'm just flipping the sign here we wanted to minimize cost here we want to maximize the reward it's just a more optimistic view of the world I guess so so that is what the rewards are going to be you find on you also have this is end function which again similar to search problems just checks if we are in an end state or not and in addition to that we have something that's called a discount factor it's it's this value gamma you choose between zero and one and I'll talk about this later don't worry about it right now but it's a thing we define for our search problem for our any piece all right so how do I compare this with search again these were the things that we had in a search problem we had the successor function that would deterministically take me to s prime and we had this cost function I would tell me what was the cost of being in state s and taking action a so so the major things that are changed is that instead of a successor function I have transition probabilities these T's that that basically terminal was the probability of starting in s taking action a and ending up in s prime and then the cost just became reward okay so those are kind of the major differences between search and MVP because things are things are not deterministic alright so that was the formalism now I can define any any MVP model any Markov decision process and then one thing just one thing to point out is these transition probabilities to see basically specifies the probability of ending up in in state s prime if you take action a in state s so these are probabilities right so so for example you know like we have done this example but let's just do it under on the slides again if I'm in state in I take action quit I end up an end what's the probability of that one and then if I'm stayed in I take action stay I end up in state in again what's the probability of that I ended up in in again two-thirds and then if I'm stay in I take action stay I end up in end it's probability of that one third yeah and and these are probabilities so what that means is they need to kind of add up to one but one thing to notice is well just what is gonna add up to one like like all of the things in the column are not gonna add up to one the thing that's going to add up to one is if you consider all possible these different X Prime's that you're going to end up at those probabilities are gonna add up to one so so if you look at this table again if you look at deciding and being stay in and taking actions stay then the probabilities that we have for difference s Prime's are two-thirds and one-third and those two are the things that are going to add up to one and in the first case if you're in stay in and you decide to quit then wherever whatever else primes you're gonna end up had in this case it's just the end State those probabilities are gonna add up to one so so more formally what that means is if I'm summing over s primes these new states that I'm going to end up at the transition probabilities need to add up to one because they're basically probabilities that tell me what are the what are the things that can happen if I take an action yeah and then this transition probabilities are going to be non-negative because there are probabilities so that's also another property so usual things alright so so that's a search problem let's actually formalize another search problem this is let's actually try to code this up so what is this search problem this is the problem so remember the problem I have blocks 1 through n what I want to do is I have two possible actions I can either walk from state s to state s plus 1 or I can take the magic tram that takes me from state s to stay to s if I walk that costs 1 minutes okay means reward of that is minus 1 if I if I take the tram that costs 2 minutes and that means that the reward of that is minus 2 okay and then the question was how like how do we want to travel from from 1 to N in the least amount of time so so nothing here is is probable stick yet right so I'm gonna add an extra thing here which says the tram is going to fail with probability 0.5 so I'm gonna decide maybe to take a trial at some point and that tram can can fail with probability 0.5 if it fails I end up in my state like I don't go anywhere and and actually like in this case you're assuming you're still losing 2 minutes so if I decide to take the tram I'm gonna lose 2 minutes maybe you'll fail maybe you will not okay all right so let's try to formalize this so we're gonna take our tram problem from two lectures ago so this is from search one we're gonna just copy that so alright so this was what we had from last time he had this transportation problem and we had all these like algorithms to solve the search problem we don't really need them because we have a new problem so it's just get rid of them and now I just want to formalize an MDP so it's a transportation MDP ok the initialization looks ok start state looks ok I'm starting from 1 is end looks ok so the thing I'm going to change is the first off I need to add this actions function ok so what would actions do it's going to return a list of actions that are potential actions and you give State so I just copy-paste it stuff from down there you just edit so it's going to return a list of valid actions okay so what are the valid actions I can take I can either walk or I can tram so I'm gonna remove all these extra things that I had from before and just keep it to be I'm either walking or I'm taking the tram okay as long as it's a valid state so so that looks right for actions the only thing we had was a successor and cost function so so now we want to just change that and return these transition probabilities and under wort so so it's basically the successor probabilities and reward okay so I'm putting those two together similar to before we had successor and cost now I'm returning probabilities and rewards okay so so what this function is going to return is it's going to return this new state is s Prime I'm going to end up at and the probability value for that under a word of that okay so so given that I'm starting in state s and I'm taking action a then what are the potential s crimes that I can end up at and what are the probabilities of that like then what is T of si s Prime and what is the reward of that what is the reward of si s Prime I want have a function to just return to these so I can call it later okay all right so I need to basically check like for for each one of these actions I can or for action walk what happens for action walk what's new state I'm gonna end up back well I'm gonna end up at s plus one it's a deterministic action so I'm gonna end up there with Parvati one and what's the reward of that minus one because it's one minute costs so it's minus one report then for action tram we kind of do the same thing but you have two options here I can I can end up in 2's tram doesn't fail I end up into s this probability 0.5 that cost that reward of that is minus 2 or the other option is I'm going to end up in state s cuz I didn't go anywhere because we probability point 5 to Tran to fail and that cut that nerve world of that is - - and that's pretty much it that that is my my MVP so I can just define this for a city with let's say ten blocks oh and we need to have the discount factor but we'll talk about that later let's say it's just one for now yeah and they'll use right I'm writing this other states functions were later but that look right just formalized this MVP so let's check if it does the right thing so maybe we want to know what are the actions from state 3 what are the actions from state 3 oh we need to remove this you to a function from before because we don't have it in the folder move that what are the actions from state 3 I have 10 blocks if I'm in state 3 I can either walk or tram right or one of them is fine right so so that did the right thing maybe we want to just check if this successor probability and your horde function does the right thing so maybe maybe we can try that out for state 3 and walk so so for step 3 and action walk then what do we get well we end up in 4 and that it that is with probability 1 with the reward of minus 1 okay let's try that for tram again remember tram can fail so I'm gonna get two things here so these are the things I'm gonna get it for tram I'm going to either end up in 6 with probability 0.5 with the reward of minus 2 or I will not go anywhere I'm still at 3 with probability 0.5 and that is with the reward of minus 2 okay all right so that was just a tram problem and we formalize it as an MDP again the reason it's an MVP here is is that the tram can fail with probability 0.5 so we added that in let me define our transition function and our problem and our reward function okay all right everyone happy with how we are defining MVPs yeah okay pretty similar to search problems except for now we have these probabilities alright so so now I have define an MVP that's great the next question that in general we would like to answer is to give a solution right so there's a question so the Markov part means that you just depends also when you you just depend on the state and this current state like a wavy to find our state you remember our state is sufficient for us to make optimal decisions for the future so the Markov part means that your Markov and you only depends on the current state and actions to end up in that probable equally end up in the next next so yeah so the interesting question who would like to do is well we want to find a solution I want to figure out what is the optimal path to actually solve this problem and again if you remember search problems the solution to search problems was just a sequence of action said that's all I had like a sequence of actions a path that was a solution and the reason that was a good solution was like everything was deterministic so I could just give you the path and then that was what you would fall but in the case of MVPs the way we are defining a solution is by using this notion of a policy so a policy let me actually write that here so so you've defined an MVP but now I want to say well what is a solution of an MVP a solution of an Markov the same process is a policy PI of s so and this policy basically goes from States so it takes any state and it tells me what is the part was a potential action that I would get for that state okay so the policy is a function it's a mapping from each state s in the set of all possible States to to an action in the set of all possible actions okay so in the case of volcano crossing like I can have something like this I can be in state 1 1 and then the policy of that state could be going south or I can be in state 2 1 and a policy for that state is if this was a search problem I would just give it path I would just say go south and then go to go east and go north right so so that would be my solution but but again like if decide that well the policy at 1-1 is to go south there is no reason for you to end up at South right because this thing this thing is probabilistic so the best thing I can do is for every state just tell you what is the best thing you can do for that particular state and and that's why we are defining a policy as opposed to get giving like a full path all right so policy is the thing you're looking for and ideally I would like to find this best policy that would just give me the right solution but in order to get there I want to spend a little bit of time talking about how good a policy would be so and that's kind of this idea of evaluating a policy so so in this middle section I don't want to try to find a policy I just assume you give me a policy and I can evaluate it and tell you how good that is so so that's the plan for the middle section yeah all right everyone happy it so so far all I've done is I've defined an MVP which is very similar to a search problem it's just probabilistic okay so so how would we evaluate a policy so if you give me a policy which basically tells me at every state s take some action then that policy is going to generate a random path right I can get multiple random paths because nature behaves differently and the world is uncertain so I might get a bunch of random paths and then those are all random variables random paths sorry and then for each one of those random paths I can I can define a utility so so what is a utility utility is just going to be the sum of rewards that I'm going to get over that path and I'm calling it as the discount at some of the rewards remember that the scouts will talk about that but but you can't you can just count the future but before now just assume it's just a sum of rewards on that time okay so you took the utility that you're going to get is also going to be a random variable right because if you think about a policy the policy is going to generate a bunch of random paths and and utility is just going to be the sum of rewards of each one of those so it's a random variable so if you remember this example right so I can I can basically have a path that tells me starting in and then stay and then that ends right so so this is one random hat and for this particular on your path throw what is it so see I'm gonna get I'm just gonna get four dollars that's one possible thing that can happen if my if my policy is to let's say stay like there is no reason for for the game to end right here right like I can have a lot of different types of random path I can have a situation where I'm staying three times and an after that ending the game and utility of that is twelve we can have the situation where we have stay stay and end that's the situation it's all like we told you have eight and so on so so you're getting all these utilities for all these random paths so so these utilities are also going to be just random variables okay so I can't really play your arm with the utility that's not telling me anything you know it's telling me something but surrounding variable I can't optimize that so instead we need to define something that you can actually play around with it and and that is this idea of a value which is just an expected utility so so the value of a policy is the expected utility of that policy and and that's not a random variable anymore that's actually like a number and I can I can compute that number I can compute that number for every state and then just play around with value particularly all possible so the question is yeah so when you say value of policy is a policy basically telling me it's a policy basically telling me what what is the strategy for all possible states well you're defining a policy has a function of state right so and value the same thing as a function of state I might ask what is the value of being an in so the value of being in in is is an following and following policies stay is going to be the value of following policies stay from this particular state which is expected utility of that which is which is basically that twelve value there I could ask it for about any other state to so I can be in any other state and then say well what's the value of that and and when we do value iteration and you actually need to compute this value for all states to kind of have an idea of how to get from one state but way to being in state in yeah and the policy given your state in taking the action state yes yeah and that is that is what 12 is okay and 12 like we kind of empirically have seen it's 12 but we haven't like shown how to get 12 yet okay all right so um actually let me write these my list of things so we talked about the policy what else did we talk about we talked about utility so what is utility utility we said it's our rewards so if I get like reward one then I get reward - it's a discount that someone rewards so I'm gonna use this gamma which is that discount that I'll talk about in a little bit times reward 2 plus gamma squared 2 times reward 3 and so on so utilities you give me a random path and I just sum up the rewards of that imagine if gamma is 1 I'm just summing up the rewards if gamma is not 1 on something I'm looking at this discounted song okay so so that is utility but value so this is utility value is just the expected utility okay so you give me a bunch of random path I can compute their utilities I can just sum them up and average them and that gives me value that's a very good question we'll get back there so so so in general in okay if it is a cyclic it is fine but if you have a cyclic graph you want your yama to be less than one and we will talk about that when we get to the convergence of all right okay all right so so let's go to the this particular volcano crossing example so in this case like I can run this game and every time I run it I'm gonna get a different utility cuz like I'm gonna end up in some random path some of them end up in the volcano that's pretty bad right so I get different utility values utilities but the value which is expected utility is not changing really it's just around 3.7 which is just the average of these utilities so I can keep running this getting the different utilities but what value is this one number that I can I can talk about and that's the value of this particular state and that tells me like what would be the best policy that I can take and what's the best amount of utility that I can get from an expectation from that state all right so we've been talking about this utility I've actually written that already on the board so utility is going to be a discounted somewhat rewards and then we've been talking about this discount factor and yeah yield of the discount factor is I might like care about the future differently from how much I care about now so so for example if you give me four dollars today and you give me four dollars tomorrow look if that four dollars tomorrow is the same kind of amount and has the same value to me as today then then I might it's kind of the same idea of having a discount counter of one account and discount of 1 gamma 1 so you're saving for the future the values of things in the future is the same amount if you give me 4 dollars now if you give me four dollars 10 years from now it's going to be 4 dollars I care about it like with four dollars amount and I can just add things up but it could also be the case like you might be in a situation in a particular MVP where you don't care about the future as much maybe you give me four dollars ten years from now and that's that doesn't like I don't have any value for that so so then if that is the case and you just want to live in the moment and you don't care about the values you're gonna get in the future then that's kind of the other extreme when justice gamma this discount is equal to zero so so that is a situation that if I get four dollars in the future that they don't like value like I don't have any value to me they're just like are zero to me so so I only care about right now living in the moment what is them on I'm gonna get and in reality you're like somewhere in between right like we're not just this case we're real living in a moment we're also not this case that that everything is just the same amounts like right now or in the future like in balance life is a setting where we have some discount factor it's it's not zero it's not one it actually discounts values in the future because future maybe doesn't have the same values now but but we still value things and if like four dollars still something in the future and that's where we pick like a gamma that's between zero and one so so that is kind of a design choice like depending on what problem you're in you might want to choose a different gamma it's not really an assessment of risk in that way it depends on the problem it depends on like in the particular problem I do want to get values in the future I have like some sort of long-term like goal that I want to get to and I care about the future like it depends like if you're solving a game versus you're solving like I don't know like a robot manipulation problem like it might just be very different like the scale factors that you would use for a lot of examples we would use in this class you just choose a gamma that's close to one like usually like four for a lot of problems that we end up dealing with gamma is like points nine that's like the usual okay like for usual problems like you might have a very different problem and we don't care about the future so so then we just drop it yes okay so that's a good question so it is gamma a hyper parameter that you need to tune I would say gamma is a design choice it's not a hyper parameter necessarily in that sense that oh I'll pick the right gamma I will do the right thing you would want to pick a gamma that kind of works well with your problem statement and then gamma of zero is kind of young greedy like you were picking like what is the best thing right now and I just don't care about the future ever it doesn't really the Markov property it's just a discount of like you know it's about the reward it's not about how this state affects the next state it's basically affects how much reward you're gonna get or how much do value reward in the future it doesn't it doesn't actually like it's still Markov decision process what you're getting whether it's affecting the reward yeah but it's more called because the iphone's state s and I take action a I'm gonna end up in s Prime and that doesn't depend on like gamma all right so okay so so in this section we've been talking about this idea of someone comes in and gives me the policy so the policy is PI and what I want to do is I want to figure out what's the value of that policy and again value is just expected utility okay so V PI of s is just six but a utility received by following this policy PI from state yes okay so so I'm not doing anything fancy I'm not even trying to figure out what PI is all I want to do is I want to just evaluate if you tell me this is PI how good is that what's the value of that okay so so that's what a value function is so value of a policy is V PI of s okay that's expected utility of starting in some state let me put this here yeah so we PI is the value of the expected utility of me starting in some state us okay and state s has value of PI of s and if phones tells me that well you're following policy PI then I already know from state s the action I'm gonna take is PI of s so that's very clear so I'll take PI of s and if I take PI of s well I'm going to end up in some chance nope okay and that chance note is a state action note it's going to be s and the action I've decided the action is PI of s I have this define this new function this Q function Q PI of Si which is just the expected utility from the chance node okay so so we've talked about value values expected utility from my actual States I'm gonna talk about Q values as expected utilities from the chance notes so after you have committed that you have taken action a and then your following policy PI then what is the expected utility from that point on okay and well what does the expected utility from point on we are in a chance note so many things that can happen because I have like nature is going to play Andros die and anything can happen and they're gonna happen with transition s a s Prime and with that transition probability I'm going to end up in a new state I'm gonna call that s Prime and the value of that state again expected utility of that state is V PI of s prime all right so okay so what are these actually equal to so I've just defined value as expected utility Q value as expected utility from a chance node what are they actually equal to okay so I'm gonna write a recurrence that you're gonna use for the rest of the class so pay attention for five seconds there's question day so they're both of them are expected value yeah just one is just a function off state the other one you've committed to one action and the reason I'm defining both of them is to just writing my recurrence it's gonna be a little bit easier because I have this state action note and I can talk about them and I can talk about pal but if I get branching from these state action notes okay all right so I'm gonna write a recurrence it's not hard but it's kind of the basis of the next like n lectures so pay attention so alright so V PI of s what is that equal to well that is going to be equal to zero if I'm in an end state so if is end of s is equal to true then there is no expected utility that's equal to zero that's an easy case otherwise well I took policy is someone told me take policy is so value is just equal to Q right so so in this case we PI of s if someone comes and gives me policy hi it's just equal to Q PI of s these two are just equal to each other so the next question one might ask is actually let me write this a little closer so I'll have some space so this is equal to Q PI of s so so what is that equal to what is Q PI of Si equal to this is via this so now I just want to know what is Q value key PI of si what is that equal to okay so if I'm right here then there are a bunch of different things that can happen right and I can end up in these different s Prime so if I'm looking for the expected utility then I'm looking for the probability of me ending up in this state times the utility of this state plus the probability of we ending a new state times they told you of that so so that is just equal to sum over all possible s crimes that I can end up at of transition probabilities of Si s prime transition probability of ending of a new state times the immediate reward that I'm gonna get reward of si s prime plus the value here what I care about the discounted value so I'm gonna add gamma V PI of s prime because I'm talking about this this next state okay is this serious okay so this is the recurrence that we were doing in policy evaluation again remember someone came and gave me policy PI so I'm writing this policy PI here someone gave me policy PI I just want to know how good policy PI is I can do that by computing V PI what is reply equal to someone told me it's your following policy PI so it's got to be equal to just Q PI what is Q PI equal to it's just sum of all the like the expectation of all the places that I can end up at that sum over s primes transition probabilities of ending up in s prime times the reward the total reward you are getting which is the immediate reward Plus this counting in my future okay and then following policy Pike I'm starting from the master plan yes sir you promised all right so okay so far so good so so that is how I can evaluate this policy right so so I have these two recurrences if I have these two recurrences I can just replace this guy here and let's imagine we are in the case maybe I can use a different color up here I'm just replacing I'm just replacing this guy right here I don't know if it's worth writing it imagine we are not in an end state if you're not in an in the end of state then we PI of s or what is that equal to that is just equal to sum of transition probabilities si s prime over s Prime's times immediate reward that I'm gonna get plus discounting V PI of s Prime okay so this is kind of a recurrence that I have I literally just combined these two and wrote it in green if you're not in an end state so if you're not in an end State this is the recurrence I have I have a PI here I have a PI on this side too so that is nice and and that is kind of the the Placer I can compute V PI maybe I can do it iteratively or maybe I can actually find a closed form solution for some problems but that is basically what I'm gonna do I have V PI as a function that depends on V PI of s prime and I can just solve for this V PI okay it allows me to evaluate policy PI I haven't figured out a new policy all I have done is evaluating what's a value of pi okay all right okay so let's go back to this example so let's say that someone comes in and tells me about the policy you got to follow is is stay so my policy is to stay okay I want to know I want to just evaluate that I want your policy evaluation when you're doing policy evaluation you got to compute that V PI for all states so let's start with v pi up end well that is equal to 0 because we know V PI at the end state is just equal to 0 now I want to know what's V PI of in okay state in what is that equal to that's just equal to Q PI of in and stay right v pi is just equal to Q PI in mistake so I'm going to replace that that's just equal to 1/3 times immediate rewards which is for cost value of the next state I'm going to end up at which is end in this case plus 2/3 times the immediate reward I'm going to get which is 4 dollars plus value of the state I'm going to end up at which is in okay so so that is just that sum that we have there right the PI of end is 0 so let me just put that 0 there I'm gonna put 0 there I only have one state here to write so that I just have this as a function of this one state in so having an equation I can find the closed form solution of the PI of n I'm just going to move things around a little bit and then I'll find out that V PI of in is just equal to 12 so so that's how you get that 12 that I've been talking about so you just found out that if you tell me the policy to follow stay if that is the policy then the value of that policy from state in is equal to 12 yeah so so the policy is a function of state I only have this one state that's interesting here right that that's on one state is in so I need to win and when I define my policy I need to kind of choose the same policy for that stage right my policy says and in you got either stay or you got either quick quit all right so you can basically do the same thing using an iterative algorithm to so so here like in the previous example it was kind of simple I just solved the closed form solution but in reality like you might have different states and then in with the company it might be a little bit more complicated so we can actually have an iterative algorithm that allows us to find these meat pies so the way we do that is we start with the values for all states to be equal to 0 and this 0 I put here is the first iteration so I'm going to my iterations here so so I'm gonna just initialize all the values for all states to just be equal to zero okay then I'm just gonna iterate for some number of time whatever number I care like I would like to then what I'm gonna do is for every state again remember the value needs to be computed for every state so for every state I'm gonna update my value by the same equation that I have on the board okay and the same equation depends on the value at the previous time step so this is just an iterative algorithm that allows me to compute new values based on previous values that I have had and I served it like everything zero and then I keep updating values of all states and they keep going so basically that equation but think of it as like an iterative update every round so you don't you run this for multiple rounds every round you just update your value okay so like here is just a pictorial you're looking at it imagine you have like five states here you initialize all of them to be equal to zero the first round you're going to get some value are going to update it and then you're going to keep running this and then eventually you can kind of see that the last two columns are kind of close to each other and you have converged to the true value so so you again someone comes and gives you the policy you start with values equal to zero for all the states and then you just update it based on your previous value yeah so how long should we run this well we have a heuristic to kind of figure out how long we should run this particular algorithm one thing you can do is you can kind of keep track of the difference between your value at the previous time step versus this time step so so if the difference is below some threshold you can kind of call it call it done and then say well I've found the right values and then in this case we were basically looking at the difference between value at iteration T versus value I generation t minus 1 and then we're taking the max of that over all possible states because I want the values to be close for all states is this gear so so I'm going to talk about the convergence then you talk about the gamma factor and and the discount factor and basically City and also how long you should run this to get these is also a difficult problem and it depends on the properties of your MVP so if you have an ergodic if you have an hour guard again between this is just should work okay but in general it's a hard problem to answer for general Markov decision problem processes and another thing to notice here is I'm not storing that whole table like the only thing I'm storing is the last two columns of this table because because that's the PI at iteration T and V PI I generation t minus 1 those are like the only things I'm storing because that allows me to compute if I have conversion that kind of allows me to keep going because I only need my previous values to update my values right in terms of complexity but this is going to take order of T times s times s prime well why is that because I'm iterating over T time steps and I'm iterating over all my states and I'm summing over all s Prime's so because of that that's a complexity I yet and one thing to notice here is it doesn't depend on actions it doesn't depend on the size of actions and the reason it doesn't depend on the size of actions is you have given me the policy you're telling me follow this policy so if you've given me the policy then I don't really need to worry about like the number of actions I have okay all right here is just another like the same example that we have seen so at iteration T equal to 1 n is going to get 4 and it's going to get 0 Ida duration 2 it gets a slightly better value and then finally like a duration like 100 let's say we get the value 12 and remember for this particular example like this example we were able to solve it like solve the closed form we have v of policy staying from state n but but you could also run the iterative algorithm and get the same value of 12 the number of actions is the size of s Prime no because you the size of s you might end up in very different different states this depends on your probabilities the size of X prime is actually the size of like size of states is the same thing like it's the worst case scenario you're going from every state to every state just imagine that size of s okay oh it's the summary so far where are we so we have talked about MVPs these are graphs with States and chance notes and transition probabilities and rewards and we have talked about policy as the solution to an MVP which is this function that takes a state and gives us an action okay we talked about value of a policy so value of a policy is the expected utility of that policy so so if you talk about utility like you have these random values before all these random paths that you're gonna get for every policy the value of utility is just an expectation over all those random random variables and so far we've talked about this idea of policy evaluation which is just an iterative algorithm to compute what's the value of a state if you give me some policy like how good is that policy what's the value I'm gonna get at every state all right so that has been all assuming you give me the policy now the thing I want to spend a little bit of time on is figuring out how to find that policy here we only have a stay or quit if you have a different problem that they can learn another actually state way or something trade is going to change the value of the policy because then you have a new action and then you need to update our policies so in this case so far I'm assuming that the set of actions is fixed I'm not like adding new actions right like the way even with search problems like the way we defined search problems or the way we are defining MVPs is I am saying like I'm starting with a set up where states are fixed actions are fixed I have stay and create those are like the only actions I can take the reward is fixed transition probabilities are fixed under that scenario then what is the best the best policy I can take and best policy is just from those set up like they've already defined actions okay next lecture we will talk about unknown settings like when we have transition probabilities that are not known or reward functions that are not known and how we go about learning them and that would be the reinforcement learning lecture so next lecture might address some okay all right so let's talk about value iteration so so that was public evaluation so like that whole thing was evaluation so now what I would like to do is I want to try to get the maximum expected utility and find the set of policies that gets me the maximum expected utility okay so to do that I'm gonna define this thing that's called an optimal value so instead of value have a particular policy I just want to be optimist which is the maximum value attained by any policy so so you might have a bunch of different policies I just want that policy that maximizes the value okay so and that is the Optus so um so let me go back to this to this example so I'm gonna have this in parallel to this example of policy evaluation I want to do value iteration okay so I'm gonna start from state s again state s has V opt s okay that is what I would like to find here I had V Pyrus if I'm looking for we opt of s then I can have multiple actions that can come out of here and I don't know which one to take but like any of if I take any of them if I take this guy that takes me to a chance note of si okay and then I'm looking for Q opt of si and from here it's actually pretty similar to what we had right here so I'm in a chance note anything can happen right nature plays and with some transition probability of a prime I'm going to end up in some new state and its prime and I care about the opt of that so if I'm looking for this optimal policy which comes from this optimal value then I need to find V opt and if I want to find V opt well that depends on what action I'm taking here but let's say I take one of these and if I take one of these I end up in a chance note I have Q opt of the saying that chance note and then from that point on with whatever probabilities I can end up in some s prime okay so I want to write the recurrence for this guy similar to the recurrence that we wrote here it's gonna be actually very similar so okay so I'm going to start with you because that is easier so what is Q opt of si that just seems very similar to this previous case what is that equal to what was q pi q pi was just some of transition probabilities times rewards right so so what is Q opt yeah so so it would just be basically this equation except for I'm gonna replace me pi we'd be opt so so from Q opt I can end up anywhere like based on the transition probabilities so I'm going to sum up over s Prime's and all possible places that I can end up at I'm gonna get an immediate reward which is RSA s Prime and I'm gonna discount the future but the value of the future is V opt of s prime okay so so far so good that's Q opt how about we opt what is that equal to well it's going to be equal to zero if you're in an end state that's similar to before so if his end of S is true then ten eight is zero otherwise I have I have a bunch of options here right I can take any of these actions and I can get any Q opt so reach one should I pick which Q opt should I pick the one that maximizes right which actually I should pick an action from the set of actions of that state that maximizes q opt so so the only thing that has changed here is before someone told me what the policy is I just took the cue of that here I'm just picking the maximum value of Q and that actually tells me what action to pick so what is the optimal policy what should be the optimal policy I'm gonna call it high opt is what is that equal to it's got to be the thing that maximizes V right which is the thing that maximizes this this Q so because that gives me the action so it's going to be the argument of Q opt of SN a where a is in actions okay all right so this one is policy evaluation someone gave me the policy with that policy I was able to compute V I was able to compute Q I was able to write this recurrence then I had an iterative algorithm to do things this is called value iteration this is to find the right policy iteration this is to find the policy how do I do that well I have a value that's for the optimal optimal value that I can get and it's going to be maximum over all possible actions I can take of the Q values and Q values is similar to before so I have this recurrence now and then the optimal policy is just a narc max of Q tiny far exactly like this to eight oh yeah so you could get to ace it so the question is yeah like what if like I have to ace that give me the same thing I can return any of them it depends on your implementation of Mac's so you can return any of them you're five minutes over and be p1 okay so good news is the slice are the same things that I have on the board so so Q opt is just equal to the sum that we've talked about V opt I just add the max on top of Q opt same story okay and then if I want the policy then I just do the arc max of Q opt and that gives me the policy right I can have an again an iterative algorithm that does the same thing it's actually quite similar to the iterative algorithm for policy evaluation I just start setting everything to equal to zero I iterate for some number of times I go over all possible states and then I just update my value based on this new recurrence that has a max so very similar to before I just do this update one thing is the time complexity is going to be order of T times s times a times s fine because now I have this max value over all possible actions so I'm actually iterating over all possible actions versus in policy evaluations I didn't have a chriskiss someone would give me the policy I didn't need to worry about this all right so so let's look at coding this up real quick okay so we have this MVP problem we define it it was a tram problem it was probabilistic everything about it was great so now I just want to do an algorithm section an inference section where I code up value duration and I can call a value duration on this MVP problem to get the best optimal policy okay so I'm going to call value iteration later all right so we initialize so all the values are going to become I might skip things to make this faster so we're gonna initialize all the values to just zero right because all these values are going to be 0 so I defined a state's function so I for all of those the value is just going to be equal to 0 so let's initialize with that then you're just gonna iterate or some number of time and what we want to do is you want to compute this new value given old values so it's an iterative algorithm we have old values you just update new values based on them so what should that be equal to so we iterate over our state's if you're in an end state then what is value equal to 0 right if you're not in an end state then you're just gonna do that it that that recurrence there okay so new value of a state is going to be equal to max of what the Q values okay so new V is just max of Q's of states and actions okay so now I need to define Q what does Q do here of state an action is just equal to that sum over / s Prime's so it's gonna return sum and it's gonna return sum over s Prime's I define this successor probability and report function that gives me new state probability and rewards so I'm gonna iterate over that and then call that up here so given that I have a state in action I can get new state probability and report what are we summing you're summing the probability the transition probabilities times the immediate reward which is reward here times my costs my discount times my V which is the old value of V over s prime over my new state so that is my cue that is my V and that's pretty much done we just need to check for a convergence to check for convergence we kind of do the same thing as before we check if value of V and new V are close enough to each other that we can't call it done I'm gonna skip these parts so you can basically check if V minus nu V are within some threshold for for all states and if they are then V is equal to nu V we need to read the policy so policy is just arc max of Q so I'm gonna make this a little faster so the policy is just going to be well none if you're in an end state and otherwise it's just going to be arc max of our Q values so I'm just writing Arg max here pretty much I'm just returning the action that maximizes the Q and then we need to spend a bunch of time getting the printing working so let me actually get yeah okay all right actually right here so I'm running this function I'm rich I'm I'm writing out actually these are a little shifted weird States values and then PI which is the policy okay so it starts off walk walk walk remember this is the case where we have 50 percent probability of tram failing and with 50 percent probability of translating these are the values we were gonna get and the policies still walk until state five and then take the tram from from state five okay which is kind of interesting because the policy of the search problem was the same thing too okay so the thing we can do is we can actually let me move this little bit forward we can actually define this failed probability which becomes just a variable so you can play around with this if you pick different fail probabilities you're gonna get different policies so for example if you pick a failed probability that is large then probably like the policy is going to be just just walk and never take the tram because the tram is failing all the time but if you decide to take a failed probability that's close to zero then then this is your optimal policy which is close to the search problem so it's basically the solution to the search problem so play around with this the code is online this was just value duration value duration and use on this problem okay so I'm gonna skip this one too alright so yeah and then this is also showing like how over multiple iterations you can kind of get to the get to the optimal optimal value and optimal policy using value duration so in one iterations it hasn't seen it yet so it thinks that the value the optimal value is 1.85 it hasn't updated the values and so like it I don't three iterations it gets better but it hasn't still updated it still thinks it can't get to the other side and remember this is a split probability of 10% but if I get to like I think 10 then it eventually learns the best policy is to get to 20 and the value is 13 point 68 and if you go even like higher iterations after that point it's just fine-tuning so the values are around 13 so you can play around you the okay no problem okay so when does this converge so if your discount factor is less than 1 or your MVP graph is a cyclic then this is going to converge so if MVP graph is a cyclic that's kind of obvious you're just doing dynamic programming over your full thing so so that's going to that's going to convert if you have cycles you you want your your discount to be less than 1 because if your if you have cycles and your discount is let's say 1 and let's say you're getting zero rewards from then you're never going to change you're never going to move you move from your state you're always going to be stuck in your state and if you have nonzero rewards you're going to get this unbounded reward and keep going because you have cycles and and it's just going to end up becoming numerically so so just a good rule of thumb is pick a gamma that's less than 1 then then you kind of get this convergence property ok all right so yeah summary so far is we have mdps now we've talked about finding policies rather than paths policy evaluation is just a way of computing like how good a policy is and the reason I talk about policy evaluation is there's this other algorithm called policy iteration which uses policy evaluation and we didn't discuss that in the class but it's kind of like a quick not equivalent but you could use it in a similar manner as value iteration it has its pros and cons and so policy evaluation U is used in those settings do not leave please we have more stuff to cover we have value iteration which computes this optimal value which is the maximum expected utility okay and next time you're going to talk about reinforcement learning and that's gonna be awesome so talk about unknown rewards alright so that was MVPs doing inference and and kind of defining them I'm going back to the last lecture just to kind of talk about some of the stuff that we didn't cover last time okay all right so if you remember last night we were talking about search problems so big future search problems where we don't have probabilities and we talked about a store as a way of just making things faster and we talked about this idea of relaxations which was a way of finding good heuristics so a store had this heuristic heuristic was an estimate of future cost we wanted to figure out how to find these heuristics like how do we go about finding this heuristic and one idea was just to relax everything that allows you to come up with an easier search problem or just easier problem and that helps you to find what the heuristic is okay so so we talked about this idea of removing constraints and when you remove constraints then you can end up in nice situations like in some settings you have a closed form solution in some other settings you have just an easier search problem and you can solve that and in some other settings you have like independent sell problems so when you remove constraints then then you have this easier problem you can solve that easier problem and that gives you a heuristic you're not done yet right you're you have a heuristic you take that heuristic and then change your costs and and just run uniform cost search on your original problem so so solving an easier problem is like you're not done when you're solve the easy a problem it just helps you to find the thing that helps for the origin problem so it's kind of like a multi-step thing so examples of that is if you have walls remove all the walls you have an easier problem if you solve that easier problem that gives you a heuristic and in this case it's like when you knock down these walls that easier problem you have a closed-form solution for it you don't need to do anything fancy you don't need to do uniform cost search any of that you just compute them in height and distance and and that gives you a heuristic with that heuristic you go and solve your original problem that was one example another example is when you remove constraints you have an easier search problem so you don't have closed form solutions but you have an easier search problem so you might have a really difficult search problem with a bunch of constraints that are hard to do remove the constraints so when you remove the constraints you have a relaxed problem which is just the original problem without the constraint that's a search problem you can solve that search problem using uniform cost search or dynamic programming and and solving that allows you to find the heuristic again you're not done yet right you take the heuristic and then you go to the original problem change the cost and I'm drawing uniform hazard and just one quick kind of example here was when you're computing these relaxed problems the thing you want to find is the future costs of this this relaxed problem and to do that you have this easier search problem you still need to run uniform cost search or dynamic programming in this case if you decide to run uniform cost search remember uniform cost search computes past cost in this case I really want to compute future costs so you need to do a bunch of engineering to get that working in this particular case the relaxed problem you need to reverse it because when you reverse it past cost of the reversed relaxed problem becomes future cost of the relaxed problem if that makes sense so so the way I'm reversing this is I'm basically saying start to stay this N and state is 1 and my walk action takes me to s minus 1 instead of s plus 1 and my Tran action takes me to s over 2 instead of s times 2 and the whole reason I'm doing that is is that the past cost of this new problem is the future cost of the non reversed version ok because I need to use uniform cost search here so I run my uniform cost search that gives me a heuristic and that heuristic gives me this future cost of the relaxed problem and everything will be great another example is I can have independent sub problems using my new RipStik so in this case like they have these tiles they technically cannot overlap instead what we are allowing is you're allowing them to overlap so if they allow them to overlap I have eight independent subproblems that I can solve these sub problems give me heuristics and I can just go with them okay so so these were just a bunch of examples and kind of the key idea was reducing edge like when we are coming up with this relaxed problems you're reducing edge costs from infinity to some finite cost okay so I'm getting rid of walls before I couldn't cross like it was infinity cost of that was infinity but if I get rid of the wall I'm making it a finite cost so this type of method this is a general framework so the point I want to make is generally you can talk about the relaxation of a search problem so if you have a search problem P a relaxation of a search problem I'm gonna call that P R ap R L is going to be a problem or the cost of the relaxation for any state action is less than or equal to cost of state and action I'll take questions afterwards all right so so that is a relaxed problem okay so the cool thing about that is if you're given a relaxed problem then you can pick your heuristic to be the future cost of the relaxed problem and that is called the relaxed heuristic okay so so this is kind of a recipe a general framework like if someone asks you find a good heuristic find a relaxed problem future cost of the relaxed problem is a heuristic and the cool thing about that is it turns out that that that that future cost of the relaxed problem mature deciding to be a heuristic is also consistent because we talked about all these consistency properties and and how you want to find a heuristic to be consistent for the solution to be correct and how in the world am I going to find a consistent heuristic well here is one here is one way of finding consistent heuristics pick your problem make it relaxed making it relaxed means that pick a cost that's less if we can pick a relaxed problem where the cost is less than the cost of the original problem and then future cost of that relaxed problem is just going to be your heuristic and it's going to be consistent so proof of that is two lines skip that and and the quick think about this like what knows about this is there's a trade-off here there's a trade-off between efficiency and tightness so sure like making things relaxed and removing constraints it's kind of fun right you have this easier problem and you just solve it and everything is great about it but it's not like there is kind of a trade-off between how tight you want your heuristic to be like you shouldn't remove too many constraints because if you remove too many constraints then your heuristic is not a good estimate of future cost remember your heuristic is supposed to be an estimate of future cost so so if it is not a good estimate of future cost and it's not tight then it's not that great so so there is a balance between how much you're removing you're considering your constraints and and how that makes finding the heuristic easier versus the fact that you want your heuristics to be tight and be close to your future costs so so don't remove everything leave some constraints and then solve it and you can also do things like if you have two heuristics that are both consistent you can take the max of that and you can take the max of that it's a little bit more restrictive maybe maybe that is closer to your future costs and that is then you can actually show the max of that is also consistent okay so we talked about like relaxation say a start what a quick thing I want to mention because that wasn't very clear last time it's structured this perceptron we talked about that a little bit too and we talked about convergence of that so quick things on that structured perceptron actually converges there was this question that if we have if that if you have a patch that is let's say walk tram and and we end up recovering another path that is tram walk is that bad is that good well turns out that the cost of both of these paths are the same thing so if I end up getting this path that's perfectly fine to write like that that is also with the same optimal weights in the example that we have shown in a tram example I don't think we are able to get two paths that look like this because of the nature of the example so so in general things to remember from structures perceptron is it does converge it does converge in a way that it can recover the two Weiss but it doesn't necessarily get the exact double use as we saw last time right like you might get two and four you might get foreign air so guys as long as you have the same relationships that that is enough but but you're going to be able to get to actualize and it does convert so with that project conversation is going to be next time do take a look at do take a look at the website so all the information on the project is on the website so you started thinking about it look at the project page and that has something