let's start guys okay so we're gonna continue talking about games today and just quick announcement the project proposals are due today I think you all know that all right so let's good tomorrow right okay yeah today is not Thursday yeah tomorrow for a second I thought it's Thursday all right so let's talk about games so we started talking about games last time we formalized them we talked about none we talked about zero-sum two-player games that were turn-taking right and we talked about a bunch of different strategies to solve them like the minimax strategy or the expecting max strategy and today we want to talk a little bit about learning in the setting of games so what does learning mean how do we learn those evaluation functions that we talked about and then towards the end of the lecture we want to talk a little bit about variations of the game the games you have talked about so how about you have how about the cases where we have simultaneous games or nonzero-sum games so that's a that's a plan for today so I'm gonna start with a question that you're actually going to talk about it towards the end of the lecture but it's a good motivation so think about a setting where we have a simultaneous two player zero-sum game so it's a two player zero-sum game similar to the games we talked about last time but it is simultaneous so you're not taking turns you're playing at the same time and an example of that is rock-paper-scissors so can you still be optimal if you reveal your strategy so we'll say you're playing with someone if you tell them what your strategy is can you still be optimal that's the question exactly what you're going to play you won't be successful too huge for a zero-sum real-time so I was using a larger scale I think you could still be successful if that approach is like superior to the others rates so it's not so the answer was about the size of the game so rock-paper-scissors being small versus versus not being small so so the question is more of a motivating thing people talk about this a lot of details towards the end of the class it's actually not the size that matters is a type of strategy that you play that matters just to give you an idea but like the reason that we have put this I guess at the beginning of the lecture is intuitively when you think about this you might say no I'm not gonna tell you what my strategy is right because if I say I'm gonna play like scissors you'll know what to play but this has an intuitive answer that you're gonna talk about towards the end of the lecture so just more of a motivating example don't think about it too hard all right so so let's do a quick review of games so so last night we talked about having an agent an opponent playing against each other so and you were playing for the agent and the agent was trying to maximize their utility so they were trying to get this utility the example we looked at was agent is going to pickpocket a bucket B or bucket C and then the opponent is going to pick a number from these buckets they can either pick minus 50 or 51 or 3 or minus 5 or 15 and then if you want to maximize your your utility as an agent then you can potentially think that your opponent is trying to trying to minimize your utility and you can have this minimax game kind of playing against each other and based on that decide what to do so we had this minimax tree and based on that the utilities that are gonna pop up or minus 51 and minus 5 so if your goal is to maximize your utility you're gonna pick bucket be the second bucket because that's the best thing you can do assuming your opponent is a minimizer so so that was kind of the setup that we started looking at and the way we thought about solving this game why was by writing a recurrence so so we had this value this is V which was the value of a minimax at state s and if you're at the utility so if you're at an end state you're gonna get utility of s right like if you get to the end state we get the utility because we get the utility only idea at the very end of the game and if the agent is playing we the recurrence is maximize V of the successor States and if the opponent is playing want to minimize the value of the successor States so that was the recurrence we started with and and we looked at games that were kind of large like the game of chess and if you think about the game of chess the branching factor is huge the depth is really large it's not practical to you to do the recurrence so we we started talking about waste for speeding things up and one way to speed things up with this idea of using an evaluation function so do their recurrence but only do it until some depth so don't go over the full tree just do it until some depth and then after that just call an evaluation function and hopefully your evaluation function which is kind of this weak estimate of your value is going to work well and give you an idea what to do next so so instead of the usual recurrence what we did was we decided to add this knee here and this D right here which is the depths that until which we are exploring and then we decrease the value of depth after an agent an opponent place and then my depth is equal to zero we just call an evaluation function so intuitively if you're playing chess for example you might think a few steps ahead and when you think a few steps ahead you might think about how the board looks like and kind of evaluates that based on the features that that that board has and based on that you might you might decide to take various actions so similar type of idea and then the question was well how are we going to come up with a solution function like where is this evaluation function coming from and then one idea that that we talked about last time was it can be handcrafted the designer can come in and sit down and figure out what is a good evaluation function so in the game of chase and test example is you have this evaluation function that can depend on the number of pieces you have the mobility of your pieces maybe the safety of your king central control all these various things that you might care about so the difference between the number of Queens that you have and your opponent's number of Queens these are things these are features that you care about and and potentially a designer can come in and say well I care about nine times more than I care about how many pawns I sew so a hand like you can actually hand design these things and write down these weights about how much you care about this so I'm using terminology from the learning lecture right I'm saying we have wait here and we have features here and someone can comment just handcraft well one other thing we can do is instead of hand crafting it we could actually try to learn this evaluation function so so we can still have to have two features right we can still say well I care about the number of kings and queens and these are things that I have but I don't know how much I care about them and I actually want to learn that evaluation function like what the weights should be so to do that I can write my evaluation function eval of s as this me as a function of state parameterize by pie weights doubles and and my goal is to figure out what these w's what these weights are and ideally I want to learn that from some data ok so so we're going to talk about how learning is applied to the game setting and specifically the way we are using learning for these game settings is to just get a better sense of what this evaluation function should be from some data so so the questions you might have right now is well how does we look like where does my data come from because I find if you know where your data comes from and your V is then all you need to do is to come up with a learning algorithm that takes your data and tries to figure out what your V is so so we're going to talk about that at the first part of knowledge and that kind of introduces to this this temporal difference learning which you're gonna discuss in a second it's very similar to key learning and then towards the end of the class we'll talk about simultaneous games and nonzero symbols all right so so let's start with this V function I just said well this new function could be parameterize by a set of weights I set up double use and the simplest form of this V function is to just write it as a linear classifier as as a linear function of a set of features double use time space and these these are the features that are hind coded and someone writes them and then and then I just want to figure out what W sir so this is the simplest form but in general did this this V function doesn't need to be a linear classifier it can actually be any supervised learning model that you have discussed in the first few lectures it can be a neural network it can be anything even more complicated than network that just does regression so we can basically any model you could use in supervised learning could be placed here as I see function so all I'm doing is I'm writing this P function as a function of state and a bunch of parameters those parameters in the case of linear classifiers are just w's and in the case of the neural network there are WS and these fees in this case of like one layer alright so let's look at an example so let's think about an example and I'm gonna focus on the linear classifier way of looking at this just for simplicity so okay let's pick a game so we're gonna look at backgammon so this is a very old game it's a two-player game the way it works is you have the red player and you have the white player and each one of them have these pieces and what they want to do is they want to move all their pieces from one side of the board to the other side of the board it's a game of chance you can actually like roll two dice and based on the outcome of your dice you move your pieces various various amounts to two various columns there bunch of rules so your goal is to get all your pieces off the board but if you have only like one piece and your opponent like gets on top of you they can push you to the bar and you have to like start again there are a bunch of rules about it read about it on Wikipedia if you're interested but you're gonna look at a simplified version of it so in this simplified version I have player O and play your X and I only have four columns I have column 0 1 2 & 3 and in this case I have four of each one of these players and and the idea is we want to come up with features that we would care about in this game of backgammon so so what are some features how do you think might be useful remember the learning lecture how do we come up with like future templates third is still down with the color but it's a mistake so maybe like the location of the X's and O's the number of them yeah yeah so like what idea is you have all these knowledge about the boards so maybe we should like care about the location of the X's maybe we should care about like where the O's are how many pieces are on the board how many pieces are off the board so similar type of way that we would come up with features in the first few lectures we were basically we would do the same thing so a feature template set of feature templates could look like this like number of X's or OS in column whatever con being equal to some value or a number of excess zeros on the bar may be fraction of excesses or OS that are removed whose turn it is so these are all like potential features that it could be so for this particular board here are what those features would look like so if you look at number of OS in column 0 equal to 1 that's equal to 1 remember we were using these indicator functions to be more general so like here again we are using this indicator functions you might ask number of O's on a bar that's equal to one fraction of O's that are removed so I have four pieces two of them are already removed so that's one half number of X's in column one equal to 1 that's one number of X's and columns three equal to three that's one it's to stern so that's a cool okay so so we have a bunch of features these features kind of explain what the sport looks like or how good this world is and what we want to do is we want to figure out what it what are the weights that we should put for each one of these features and how much we should care about each one of these features so that is the goal of learning here okay all right so okay so that was my model right so far I've talked about this vs of W I defined it as a linear classifier predictor W times features and now the question is where do I get data like where it's because if I'm doing learning I got to get data from somewhere so so what idea that we can use here is we can try to generate data based on our current policy PI agent or PI opponent which is based on our current estimate of what we use right so currently I might have some idea of what this V function is it might be a very bad idea of what V is but that's okay I can just start with that and starting with that V function that I currently have what I can do is I can I can call our max of V over successors of SN a to get a policy for my agent remember this was how we were getting policy in a minimax setting policy for the opponent is just argument of that V function and then when I call these policies I get a bunch of actions I get a sequence of like states based on based on how we're following these policies and that is some data that I can actually go over and try to make might be better and better so so that's kind of how we do it we call these policies we get bunch of episodes we go over them to make things better and better so that's kind of the key idea um one question you might have at this point is is this deterministic or not like do I need to do something like Epsilon greedy so in general you would need to do something like epsilon greedy but in this particular case you don't really need to do that because we have again it like we have this guy that you're actually rolling the dice and by rolling the dice you are getting random different different random paths that that we might take so that would take us different states so we kind of already have this this element of random this year that does some of the exploration for us you just like yes so why if someone greedy what I mean here is do I need to do extra exploration am I gonna get stuck like in particular set of states if I don't do exploration and in this particular case because we have this randomness we don't really need to do that in general you might imagine having some sort of epsilon greedy to take us explore a little bit more okay so then we generate episodes and then from these episodes we want to learn okay his episodes look like state action reward states and then they keep going until like if you get a full episode one thing to notice here is is the reward is going to be 0 throughout the episode until the very end of end of the game right like on till we end the episode and we might get some reward at that point or we might not but but the reward throughout is going to be equal to 0 because we were playing a game right like you're not getting any rewards available and if you think about each one of these small pieces of experience si RS prime you can try to learn something from each one of these pieces of experience okay so so what you have is actually going bored maybe what you have here is you have a piece of experience it's not like s hey you get some reward maybe it is zero that's fine if it is zero and you go to some s 5 through that so let's take an action and you get a reward and you get a reward you go to something from that and you have some prediction right your prediction is your current like your current V function so your predict is going to be this V function at the state s prime meter eyes with W and this is what your already like you kind of know right now this is your current estimate or what he is and this is your prediction I'm writing the prediction as a function of W right because it depends on W and then we had a target that your try to get to and my target which is kind of acts as a label it's going to be equal to my reward the reward that I'm getting so it's kind of the route so if you look at this V of s and W well what's kind of Polish to reward plus I'm gonna write discount factor yeah del V of s prime right so so my target the thing that I'm trying to like get to is the reward plus gamma V of s prime so we're playing games in games gamma is usually one I'm gonna keep it here for now but I'm gonna drop it at some point so you don't need to really worry about gamma and then one other thing to notice here is I'm not writing target as a function of W because target acts kind of like my label right if I'm if I'm trying to do regression here how get is my label it's kind of the ground truth thing that I'm trying to get to so I'm gonna treat my target that's just like a value I'm not writing it as function of W all right so so what do we try to do usually like when you're trying to do learning yeah a prediction we have a target what do I do minimize say yeah so what is there so I can write my error as potential you squirt or so I'm gonna write 1/2 of prediction of W minus target squirt this is my square there I want to minimize that so with respect to W okay how do I do that I can take the gradient what is a gradient equal to this is simple right so you do two gets cancelled gradient is just this guy prediction of w- target times the gradient of this inner expression the gradient of this inner expression with respect to w is the gradient of prediction with respect to w minus zero plus target is treating it as a number okay let me move this up so now I have the gradient what algorithm should I use I can use gradient descent right so I'm going to update my W how do I update it I'm going to move in the negative direction of my gradient using some learning rate a de times my gradient my gradient is prediction of W minus target times gradient of prediction of W all right so that's actually what's on the slide so the objective function there's prediction - target squared gradient we just took that it's prediction - target times gradient of prediction and then the update is just this this particular update probably move in a negative direction of the gradient this is this is what you guys have seen already all right so so far so good um so this is the TD learning algorithm this is all it does so temporal difference learning what it does is it picks like these pieces of experience si R s prime and then based on that pieces of experience it just updates W based on this gradient descent update difference between prediction and target times a gradient of V so what happens if I have if I have this this linear function may be let me write let me write this in the case that I have a linear linear function so what if my V of s W is just equal to W dot feel this LS so what happens to my update - ADA what is prediction right although you don't feel this little horn what is target did you find it up there it's the reward you're getting the immediate reward you're getting plus gamma times V of s prime W which is w dot V of s prime times gradient of your prediction which is what's the illness so I just I just wrote up this in the case of a linear predictor case what you learn you know where the difference is between you two yeah so this is very similar to Q learning they're very minor differences that you'll talk about actually at the end of this section comparing it to Q line all right so so I want to go over an example it's kind of like a tedious example but I think it helps going over that and kind of saying why it works especially in the case that the reward is just equal to zero like throughout an episode so it kind of feels funny to use this algorithm and make it work but it works so I want to just go over like one example of this so I'm gonna show you one episode starting from s 1 to some other state and and I have an episode I start from some state I get some features of that state again these features are by just evaluating those hand-coded features and I'm just gonna start what W should I start with 0 let me just initialized over u to be equal to 0 ok right how do I update my W maybe let me just write it in this so so this is I want to write in as simple or not the simpler form we're just in the other form so W the way we're updating it is previous W - ADA times prediction - target I'm going to use P and T for prediction - target times V of s this is update you are doing ok yeah that's right okay so so what is my prediction with my prediction W dot C of s 0 what is my target so for my target I need to know what state I'm ending up at I'm gonna end up at 1 0 in this episode and I'm gonna get a reward of 0 so what is my target my target is reward which is 0 plus double times V of s prime that is zero because W is equal to zero so my target is equal to zero my P minus P is equal to zero so P minus C is equal to zero this whole thing is 0 W stays the same so in the next kind of step that we use just okay I'm gonna move forward so what is prediction here 0 x 0 prediction is 0 what is target I haven't yeah it's 0 because I haven't got any anything any word yet about 1/2 so yeah so target is going to be a reward which is 0 plus 0 times whatever state of V of s prime that I'm at so that's equal to 0 P minus C is equal to 0 it's kind of boring so at this point W haven't changed W is equal to 0 what is my prediction prediction is equal to 0 that's great what is target equal to so I'm gonna end up in an end state where I get 1 0 and I get a reward of 1 so this is the first time I'm getting a reward which is my target to be my target is reward 1 plus 0 times 1 0 which is 0 so my target is 1 so what this tells me is I'm predicting 0 but my target is 1 so I need to push my w's a little bit up to actually address the fact that this is this is this is equal to 1 so P minus C is equal to minus 1 so I need to do an update maybe I'll do that update here so how am i updating it so I'm doing starting from zero zero minus my ADA is 0.5 that's what I allowed it like I put it I defined it to be my prediction - target is minus 1 what is fee of s P of s is 1 2 right so what should my new W be for this an equal to point 5 and then 1 X I'm just doing arithmetic here so my new W is going to become 0.5 and 1 at the end of this one episode so I just did a 1 episode 1 full we're w0 throughout and then at the very end when I got a reward then I updated my W because I realized that my prediction and target or not the same thing okay so now I'm gonna I'm gonna start a new episode and the new episode I'm starting is going to start with this particular W and in the new episode even though the rewards are gonna be 0 throughout so like we're actually going to update our w's this is amazing think about the same features right you could have like I said up yeah depends on what source or feature you can you could use like really not representative features like if you really want S 4 and s 1 s 9 to differentiate between them we should pick features that differentiates between them but if you are kind of the same and have the same sort of characteristics it's fine to have it's going to yeah you will never converge and that kind of tells you that that entry in your future victory you don't care about that or it's always like it's always staying the same and if it is always zero it doesn't matter like what the weight of that entry is so in general we wanna have feature is that or differentiating and you're losing something so for the second row I'm not gonna write it up because that takes time so so okay so let's start with a new episode we started this one again but now I'm starting with this new W that I have so I can compute the prediction the prediction is one I can compute my target it's point five and what we realize here is we overshoot it so before prediction was zero target was one who are under shooting we fixed our w's but now you're over shooting so we to fix that variation on the relationship between the teachers and elites they always have to be the same dimension and what should we be thinking about that would make a good feature for updating the link specifically like so okay so first off yes they need to be always the same there is dimension because you're doing this dot product between them the feature selection you don't necessarily think of it as like how am i updating the weights you think of the feature selection as is it representative of how good my board is is it for example in the case of backgammon or is it representative of how good I'm navigating so so it should be a representation of how good your state is and then it's yeah it's usually like hand designed right so so it's not necessarily you shouldn't think of it as how is it helping my weights as you think of it as how is it representing how good my state is the blackjack example you have a threshold of 21 and then your threshold intent if you're using the same feature extraction for both how does that affect the generalizability of the model the agent yeah so so you might choose do two different features and one of them might be more like so so there is kind of a trade-off right you might get a feature that actually the friendships between different states very well but then that that makes learning longer that makes it not as generalizable and then the idea on the other hand you might get a feature that's pretty generalizable but but then it might not do these specific things that you would want to do or these differentiating factors about it so picking features it's an art right so all right so let me let me move forward because we have a bunch of things coming up okay so I'll go over this real quick then so we have the W's right so so we now update the W based on this new value and kind of similar thing you have a prediction you have a target you're still over shooting so you still need to update it and then once you update it to a point 25 and point 75 and it kind of stays there you're happy okay all right so so this was just an example of TD learning but this is the update that you have kind of already seen right and then a lot of you pointed out that this is this is similar to Q learning already right this is actually pretty similar the update is very similar like we have these gradients and same way that we have in q-learning and and we are looking at the difference between prediction and target same way that we were looking at in cue learning but there are some minor differences so the first difference here is that q-learning operates on the cue function q function is a function over state and actions here we are operating on a value function right on V and V is only a function of state right and part of that is actually because in the setting and setting of a game you already know the rules of the game so we kind of already know the actions you don't need to worry about it as much the same way that you're worrying about it in cue learning the second difference is Q learning is an auth policy algorithm so so the values based on this estimate of the optimal policy which is this Q opt it's based on this optimal policy but in the case of TD learning it's a non policy the values based on this exploration policy which is based on a fixed pie and sure you're updating the PI but you're going with whatever PI you have and and kind of running with that I keep updating it okay so that's another difference and then finally like in Q learning you don't need to know the MVP transitions so you don't need to know this transition function as transition from SI to s Prime but in the case of TD learning you need to know the rules of the game so you need to know how the successor function OSN a works so so those are some kind of minor differences but from like a perspective of like how your update works it is pretty similar to what Q is all right so so that was kind of this idea of I have this evaluation function I want to learn it from data I'm gonna generate data from that generated data I'm gonna update my W so that's what we've been talking about so far and the idea of learning using learning to play games is not a new idea actually so so in 50s samuel looked at a checkers game program so where he was using ideas from self play and ideas from like similar type of things we have talked about using really smart features using linear evaluation functions to try to solve the checkers program so a bunch of other things that he did included adding intermediate rewards so kind of threw out like sui to get to the end point he added some intermediate words use alpha-beta pruning and some search heuristics and and it was kind of impressive like what he did in 50's like he ended up having this game that was playing like I was reaching like human em amateur level of play and he only used like 9k of memory which is like really impressive if you think about it so so this idea of learning in games is old people have been using it in the case of backgammon this was around 90s when tesora came up with with an algorithm to solve the game of backgammon so he specifically used this TV Holanda for them which is similar to the TV learning that we have talked about it has this lambda temperature parameter that that kind of tells us how good states are like as they get far from the reward he didn't have any any intermediate rewards usually dumb features but then he used neural networks which was kind of cool and he was able to reach human experts play and kind of gave us and this kind of gave us some insights into how to play games and how to solve like these really difficult problems and then more recently we have been looking at the game of growth so in 2016 we had alphago which was using a lot of expert knowledge in addition to ideas from Monte Carlo tree search and then in 2017 yet alphago zero which wasn't using even expert knowledge it was all like based on self play it was using dumb features neural networks and then basically the main idea was using Monte Carlo tree search to try to solve this really challenging difficult problem so I think in this section you're gonna talk a little bit about alphago 0-2 you're attending section all right so the summary so far is we've been talking about parameterizing these evaluation functions using using features and the idea of TD learning is to look at this error between our prediction and our target and try to minimize that error and find better double use as we go through so um alright so that was learning and in games so now I want to spend a little bit of time talking about other variations of games so so the setting where we take our games two simultaneous games from turn-based and then the setting or we go from zero song nonzero-sum alright okay simultaneous games so um all right so so far we have talked about turn-based games like chess where you play and an X player plays and you play a next play in place and minimax key and strategies seem to be pretty okay but it comes to solving these time bases but not all games are turn-based right like an example of it is rock-paper-scissors you're all playing at the same time everyone is playing simultaneously the question is how do we go about solving a simultaneous so let's search with a game that is a simplified version of rock-paper-scissors this is called a two finger Mora game so the way it works is we have two players player a and player B and each player is going to show either one finger or two fingers and then you're playing at the same time and then the way it works is if both of the players show one at the same time then player B gives two dollars to play or a if both of you show two at the same time player B gives player a four dollars and then if you show different numbers like 1 or 2 or 2 or 1 then player a has to give they'll give 3 dollars to 2 player B ok does that make sense so can you guys talk to your neighbors and play the same [Music] all right so what was the outcome how many of you are in the case or a chose one Danby chose one oh yeah one ok Whomper here a chose one we chose to perfect like four people played so I chose to be chose one okay - and then - in - all right so so you can kind of see like a whole mix of strategies here happening and this is the game that you're going to talk about a little bit and think about what would be a good strategy to use when you're solving this this simultaneous game yeah all right so um all right so let's formalize this yeah player a and player B you have these possible actions of showing one or two and then you're gonna use this this payoff matrix which is which represents a utility if a chooses action a and B chooses action B so so before we had this this value function right before the itis value function over over our state here now we have this value function that is that is again from the perspective of a agent a so remember like before we were thinking about value of function you were looking at it from the perspective of the first player the Maximizer player the agent now I'm looking at all of these scares from the perspective of a player so so I'm trying to like get good things for a yeah and then this is like a one step game - right so so like you're just playing and then you see what you get so so we're not talking about it repeated games you're playing you see what happens okay so so we have this V which is V of a and B and and this basically represent ace utility if agent a plays a and if agent B plays and this is called and then you can represent this with a matrix and that's why it's called a payoff matrix I'm gonna write that payoff matrix here so payoff matrix a you me here agent a can show one or can show two agent B can show one or can show to right if both of us show one at the same time agent a gets two dollars if both of us show two at the same time agent a gets four dollars otherwise agent a has to pay so agent a gets minus three dollars and again the reason I only like to talk about one way is we are still in the setting of zero-sum games so whatever we age in a gets agent B gets negative of that right so so if agent a gets foreign dollars agent B is paying minus four dollars so I'm just writing one B from perspective of agent a and this is called the payoff matrix all right so so now we need to talk about what does a solution mean in this setting so so what is a policy in this setting and then the way we refer to them in this case are as strategies so we have pure strategy which is almost like the same thing as as deterministic policies so a pure strategy is just a single action that you decide to take so you have things like period strategies your strategies the difference between pure strategy and an deterministic policy is if you remember a deterministic policy again is a function of States right so so it's a policy as a function of state it gives you an action here we have like a one move game right so it's just that one action and we call it pure strategy we have also this other thing that's called mixed strategy which is equivalent to two stochastic policies and what a mixed strategy is is is a probability distribution that tells you what's the probability of you choosing so so fewer strategies are just actions ace and then you can have things that are called mixed strategies and they are probabilities of our choosing actually okay all right so here is an example so if you say well I'm gonna show you one I'm going to always show you one then the if you can you can write that strategy as a pure strategy that says I'm always be probable do you want to show you one and probably what is zero show you two so so let's say the first column is for showing one the second column is for sure so so this is a pure strategy that says always I'm going to show you one if I told you well I always I'm gonna show you two then I can write that strategy like this right with probability 1 I'm always showing you 2 I can also come up with a mixed strategy mix strategy with me I'm gonna flip a coin and if I get 1/2 I'm gonna give you I find if I get heads I'm gonna show you one if I get tails I'm gonna show you two and then you can write that as this and this is going to be a mixed strategy you could totally play that to your in the simultaneous game you could just bring chance in and be like half the time I'm gonna show you one half the time I'm going to show you two based on chance everyone happy with mixed strategies and your strategies alright so so how do we evaluate a value of a game so so remember in previous lecture and like in the MVP lecture even we were talking about evaluating if someone gives me the policy how do I evaluate how good that is so the way we are evaluating that is again by this value function me and we are gonna write this value function as a function of PI a and PI B you know I'll just write that up here or I'm gonna erase this so I'm gonna say a value of agent a following PI a and agent be following PI B what is that equal to well that is going to be the setting where PI a chooses action a PI B chooses action be x value of choice a and B summing over all possible aims okay so so let's look at an actual example for this so so for this particular case of two-finger more game let's say someone comes in and says I'm gonna tell you what PI a is policy of agent a is just always show one and policy of agent B is this this mixed strategy which is half the time show one half the time show show two and and the question is what is the value of these two policies how do we compute that well I'm gonna use my payoff matrix right so so 1 times 1 over 2 times the value that we get 1 which is equal to 2 so it's 1 times 1 1 over 2 times 2 plus 0 times 1 over 2 times 4 plus 1 times 1 over 2 times minus 3 the value that I get is minus 3 plus 0 times 1 over 2 times minus 3 and well what is that equal to what is that equal to there are two zeroes here that's minus 1 over 2 okay so I just computed that the value of these two policies is going to be minus 1 over 2 and again this is a front perspective of agent a and it kind of makes sense right if agent a tells you I'm gonna always show you 1 then probably agent and an agent 2 is following this mixed strategy agent a is probably losing an agent a is losing minus 1 over 2 that opens of a whole set of new questions and you're not discussing this class so that introduces repeated games so you might be interested in looking at what happens in repeated games in this class right now we were just talking about this one step one play we're playing like zero-sum game and we're playing like Jose rock-paper-scissors and you just play once like you might say oh what happens if you play you're like 10 times then you're building some relationship and weird things can happen and so so that introduces a whole new class oh all right so so the value is equal to minus 1 over 2 okay all right so so that was a game value so so we just evaluated it right if someone tells me it's Phi a and PI B I can evaluate it I can know how good PI a and PI B is from the perspective of agent a okay so what do we want to do like when we saw we want to try to solve games all we want to do is from the agent ace perspective you want to maximize this I want to get as much money as possible and its values from my agent a perspective so I should be trying to maximize this agent mean you should be trying to minimize this like thinking minimax agent B should be making minimizing this agent a should be maximizing this yeah that's what we want to do but the challenge here is we're playing simultaneously so we can't really use the minimax tree we can remember any minimax 3 like in that setting we had sequential place and then quick like wait for agent a to play and then after that play and that would give us a lot of information here we are playing simultaneously so what should we do ok so what should we do so I'm gonna assume we can play sequential e so that's what I want to do for now so so I'm gonna limit myself to pure strategies so maybe I'll I'll come over here so right now I'm gonna focus only on pure strategies I won't just consider a setting very limited setting and see what happens and I'm gonna assume oh what if what if we were to play sequentially what would happen how bad would it be if we were to play sequentially so so we have the setting where player a place goes first what do you think we do you think like if player a goes first is that better for player a or is that worse for player i worse for player II ok so that's probably what's gonna happen find out so player a was trying to maximize right this me player B was trying to minimize right and then each of them have a have actions of either showing 1 or showing 2 this is player a B I can shoot one or show one or two right if you do want if we show one one player a gets what $2 is that right that's right and otherwise for your it gets - three dollars if you have two two player a gets four dollars so okay so so now if we have this sequential sitting if you're playing minimax then player B's going second player B is going to take the minimizer here so player B is going to be like this one and in this case player B is going to be like this one what should player a do well in both cases player a is getting minus three dollars it doesn't actually matter play you're a could do any of them and pull your a at the end of the day is going to get minus three dollars and this is a case where a player a goes first what if player a goes second second soso then player B is going first player B is minimizing and then play your a is maximizing and we have the same values here okay so this is this is player a going second player a going second tries to maximize so we'd like to pick these ones player B's is here player B wants to minimize so player B is going to be like okay if you're going second I'd rather I'd rather show you one because by showing you one I'm losing less if I show you two I'm losing even more so and then in that setting we're gonna get to so player a is going to get two dollars all right so that was kind of intuitive if you have fewer strategies it looks like if you're going second that should be better so so going second is no worse it's the same or better and that basically can be represented by this minimax relationship right so so agent a is trying to maximize so so in this second case in the second case we are maximizing second over our actions of via a and B and player B is going first so this is going to be greater than or equal to the case where player a is going first sorry no not me that makes sense so I'm gonna just write these things that you're learning throughout on the side of the board maybe up here so what did we just learn we learned if we have fewer strategies if we have pure strategies right going second is better that sounds intuitive and right okay so far says it okay so the question that I want to try to think about right now is what if we have mixed strategies what's gonna happen if you have mixed strategies are we going to get the same thing look if you have mixed strategies it's going second better or is it worse or is it the same so that's a question in charge at okay so so let's say player a comes in and play your a says well I'm gonna reveal my strategy to you what I'm gonna do is I'm gonna flip a coin depending on what it comes I'm either show you're going to show you one or I'm going to show you two that's what I'm gonna tell you tell you that's what I'm gonna do so so what would be the value of the game under that setting so the value of the game would be maybe all right it's here so the value of PI a and PI B PI ay is already this mixed strategy of 1/2 1/2 right is going to be equal to PI is this actually alright so what is that going to be equal to its going to be PI B times 1 right type so it's going to be PI be choosing one times one happy probability 1/2 agent is also picking one if it is one one you're gonna get to write plus I be choosing one PI a with 1/2 choosing one and you're gonna get - $3 so choosing - you're gonna get - $3.00 plus PI be choosing 2 times 1/2 PI a choosing do - you're gonna get $4 plus PI be choosing 2 times pi a choosing 1 and that's - - so I just like iterated all the four options that we can get here under the policy of PI be choosing one or two and then PI is always just are following this mixed strategy so well what is this equal to that's equal to minus 1 over 2 pi B of 1 plus 1 over 2i b of 2 so that's the value ok so again the setting is someone came in agent a came in AJ told me I'm following this mixed strategy this is gonna be the thing I'm gonna do what should I do as an agency what should I do as an agent Lee so ok so now it's so quick so you always have to do one but why why is that well well if agent a comes and tells me well this is the thing I want to do I should try to minimize value of agent a right so what I'm really trying to do as agent B is to minimize this right cuz I don't want agent a to get anything so if I'm minimizing this and some sense I'm trying to come up with a policy that minimizes this is a probability so it's like a positive number I have like positive part and negative part here the way to minimize this is to put as much weight as possible for this side and as little as possible for this side so that tells me that never show too and always show one does anyone see that so so the best thing that I can do as agent 2 is to follow a pure strategy that always shows 1 and never shows okay so this was kind of interesting right like if someone comes in and tells me this is the thing this is a mixed strategy I'm gonna follow I'll have a solution in response to that and that solution is always going to be a pure strategy actually so that's I hope cool all right so so this is actually what's happening in a more general case I'm gonna make a lot of generalizations in this lecture so I show one example and I generalize it but if you're interested in details of it we can talk quite at offline so yeah so the setting is for any fixed mixed strategy pie a so-so pie a told me what their mixed strategy is it's a fixed mixed and mixed strategy what I should do as agent P's I should minimize that value I should pick PI B in a way that minimizes that value and that can be attained by pure strategy so the second thing that I've learned here is if player a plays plays mixed strategy makes strategy player B has an optimal pure strategy that's kind of interesting right okay so so in this case also we haven't decided what the policies should be yet right like behave you have started you still we have still been talking about the setting we're PI it like agent a comes in and tells us what their policy is and we know how to respond to it it's going to be a pure strategy so now I want to figure out what is this this policy like what it what should be this mixed strategy actually so so I want to think of it more generally so so I want to go back to those two diagrams and actually modify those two diagrams in a way where you talk about it a little bit more generally maybe yeah I'll just modify these okay so so let's say that okay and I'm going to think about both of the settings so let's say it again player a is deciding to go first player a is going to follow and make a mixed strategy so this is all we know but we don't know what mix strategy play player a is going to decide to do to follow a mixed strategy this is player a player ace maximizing player a is falling a mixed strategy the way I'm writing at mixed strategy there's more generally saying player a is gonna show one a bit probability P and is going to show two with probability 1 minus P or generally like some something value okay and then after that it's player B Stern we have just seen that player B the best thing player B can do this is to do a pure strategy so player B is either 100% is going to pick one or a hundred percent is going to pick turns then like so the thing is that the strategies are probabilities right so there are values from 0 to 1 and then you kind of always end up with this negative turn that you're trying to eliminate as negative as possible and this positive term that you're trying to get as positive as possible and that's kind of intuitively why you end up with a period strategy and Mercury strategy what I mean is you always end up like putting as much possible like 1 like all your probabilities on the negative term and nothing on the positive term because you're trying to minimize this so that's kind of like intuitively why you are getting this here's to do so so you wouldn't get one so silver that's what I mean so like if you would have never get like 1/2 + 1 if you get 1/2 and 1/2 that's that's a mixed strategy that's not a pure strategy and I'm saying you wouldn't get a mixed strategy because you would always end up in this setting that to minimize this you end up pushing all your probabilities is negative 1 all right so so all right so let me go back to this so alright so we have this setting or a player a goes first play you're a is following a mixed strategy with P and 1 minus P player B is going to follow a period strategy either 1 or 2 I don't know which one right so what's gonna happen is if you have 1 1 and then then that is going to give me 2 value 2 all right so it's 2 times P I'm trying to write a value here by writing it wait is it 2 times P plus yeah 1 minus P times 3 right so if we probability 1 minus P this guy's gonna pick 2 but this guy picks one you're gonna get - three - three okay and then for this side if with probability 1 minus P a is gonna show two if I am gonna show - then I'm gonna get four so it's 4 times 1 minus P and we 1200 TP designs are gonna show one I'm going to show 2 so that is minus 3 all right so what are these equal to so this is equal to 5 P minus 3 that is equal to minus 7 P plus 4 right so so I'm talking about this more general cases in this more general case player a comes in clear is playing first and it's following a mixed strategy but doesn't know what P they should choose they're choosing a P and 1 minus P here and then player B has to follow a pure strategy that's what we decided and then under that case we either get 5p minus 3 and minus MP possible what should player B do here this is player B and this min node what should player B do which we should player B pick one or two it should player B should pick a thing that minimizes between these two so player B is going to take the minimum of 5p minus 3 and minus 7 P plus 4 what should play your ad what should player a do I'm thinking minimax right so when you think about the minimax play you're a it's maximum maximizing the value so player is going to maximize the value that comes up here so player is going to maximize that and also I'm saying clear air you needs to decide what PDR picking so they're gonna pick a P that maximizes that these computations yeah so these are the four different things in my payoff matrix so I'm saying is with probability P a is going to show me one right and I'm gonna go down this other route where B is also choosing one so one like both of us are showing one then I'm gonna get two right so I'm gonna get $2 so that's where the two dollars comes from times probability P with probability 1 minus P a is going to show me - I'm gonna show one that's - $3 times probability 1 minus P so so that's how and for this particular branch I know the payoff is going to be 5 P minus 3 that make sense and then for this side again like with probability 1 minus P is going to show me - if it is both of them - I'm gonna get $4 that's why it's 4 times probability 1 minus P your probability P is going to show me one so that's why I'll lose $3 that's -3 times probability P so that's minus 17 so and then and then the second player what they're gonna do is they're going to minimize between these two values they're gonna pick one or two they're going they're deciding should I pick one or should I pick two and the way they are deciding that is by trying to pick pick one or two based on which one minimizes I'm writing it like using this variable P that's not decided yet and this variable P is the thing that player a needs to decide so what what P should tell you're a decide employer a should decide repeated maximizes so I'm writing like literally a minimax relationship here yeah all right so the interesting thing here is this 5p minus 3 is some line right with with positive slope this is 5p minus and this minus 7 p + where is another line - 7 P plus 4 it's another line negative slope what is the minimum of this where is going to be the minimum of this happening anymore of these two lines where they meet each other right this is going to be a minimum so so the period I'm going to pick is going to be actually the P where the value of P where these two are equal to each other and that turns out to be at I don't know what it is 7 over 12 or something actually I don't know where this what is this value yeah so it's going to happen at 7 over 12 and the value of it is minus 1 over 12 right so ok so let's recap ok what did I do so I'm talking about the simultaneous game but I'm relaxing it and making it sequential I'm saying a is going to play first base playing second the thing that's gonna happen is ace playing first is deciding to choose a mixed strategy so he's deciding to say maybe 1/2 1/2 well maybe the a doesn't want to say 1/2 1/2 once to come up with some other probabilities so the thing is deciding is should I pick one and with probability P and should I pick 2 with probability 1 minus P and what should that P be so so what is the probability I should be picking 1 so that's what a is trying to decide here ok so whatever a decides with P and 1 minus P ends up in two different results and based on them me is trying to minimize that when B is trying to minimize that B is minimizing between these two linear functions these two linear functions meet at one point that is the points that this thing is going to be minimized and that actually corresponds to a p-value when a nice to maximize this this is I know but this is requires a little bit of thinking but any clarification questions any see a lot of boss faces so yeah and then yeah the interesting point is exactly right yeah so a is 2 by the way of losing so even in this case where a is trying to come up with the best mixed strategy he could do the best mixed strategy a is doing is shown show 1 with probability 7 over 12 and show 2 with probability 5 over 12 this comes from here even under that scenario aces is losing minus 1 over 2 ok all right ok so also I haven't solved a simultaneous game yeah that's right like I have talked about the setting where a place first so what if B plays first so I'm gonna swap this what if he plays first so a goes second B plays first I'm gonna modify this one now ok he goes first a is going second he is going to start is going to reveal this strategy his strategy the strategy that B is going to reveal is also again I'm gonna with probability P show you 1 the probability 1 minus P show you show you 2 then a place is trying to maximize and a has to play a pure strategy because of that like the best thing I can do is go into the appear strategy always going to the Eider showing 1 or 2 and a is deciding which one but doesn't know yet and the values here are going to be exactly the same thing as third so there are five point five P minus three minus seven P plus four all right so what's happening here so so in this case a is playing second what a likes to do is a likes to maximize between 5p minus 3 and minus 7 P plus 4 that's what a likes to do he is going second sorry is going first so then B has to minimize that and pick it P that minimizes that okay so these two are exactly the same two lines but now I'm picking the maximum of them the maximum of these two lines end up being exactly the same point as before ends up being exactly the same P is before and giving you exactly the same value as before so so this is also equal to minus 1 over 12 so what this is telling me is if you're playing a mixed strategy even if you reveal your best to make strategy at the beginning it doesn't matter it actually doesn't matter if you're going first or second so like in the more a game and you were playing if you were playing get make strategy and you would tell your opponent this is the thing I'm gonna do and this is a mixed strategy actually like anything what's the optimal thing like like you didn't matter like if they don't know it or not like you just still get the same value so again you get 5p minus 3 and minus 7 P plus 4 and then now you're minimizing or a maximum of these two lines maximum of these two lines ends up being at the same point and you pick a P that kind of maximizes that and you get the same value so this is called the Von Neumanns theorem so this whole thing that you just did over this one example there's a theorem about it that says for every simultaneous two player zero-sum game with a finite number of actions the order of place doesn't matter so he is playing second or B's playing first the values are going to be the same thing if you're minimizing over a maximum we're maximizing minimum of that value it's going to be the same thing okay so this is kind of the third thing that we just learned which is one Newman's theorem which says if I'm writing a modification of simpler shorter version of it safe playing makes strategy order of play doesn't matter and remember if you play mixed strategy your opponent is going to play fewer strategy because this is like this is the first point right if you play mixed strategy your opponent is going to follow a pure strategy I don't want to like you for doing that board or anything one of the two answers like look valid I'll disappear one one like it will be either one or two and then in that case the second music so in this case yeah so so the thing is these two end up being equal so the way to it doesn't it doesn't matter because your way for you to maximize this is going to be the point where the two end up being equal so the two branches like if you actually plug in P equal to 7 over to 12 here like these two values end up being equal I'm not an interpretation they're actually equal and the reason that they end up being equal is you are trying to minimize the thing that this guy is trying to maximize so you're trying to pick the P that actually makes this thing equal so no matter what your opponent's does like you're gonna like get the best thing that you can do so so yeah like think of it like this okay so I'm player a I'm still I still have a choice my choice is to pick a P I want to pick a P that I'm not gonna like lose as much what P should I pick I should pick a P that makes these choices the same because if I pick a P that makes this one higher than this one of course the second player is going to make me lose and then go down the routes that's that's better for the second player so the best thing that I can do here is make these two as equal as possible so then the second player whatever they choose choose one or two like it's gonna be the same thing it's going to be those does that make sense no expectations we multiplied by P and one was easier saying like oh so in expectation you're saying when you're choosing P yeah yeah so I'm just I'm treating P as a variable that I'm deciding right like peas the thing I gotta be deciding so I'm player a I gotta be citing a P that's not gonna be too bad for me like let's say I would pick a P that doesn't make these things equal let's say I don't know I would pick your P that makes this guy I don't know 10 and this makes this guy 5 the second player is of course going to make me lose and of course is going to like pick the thing that's going to be the worst thing for me so the best thing I can do is I can like make both of them I don't know 7 so it's not gonna be as bad so so that's kind of the idea all right so we move forward because there's so much things happening all right so it's okay so the kind of key idea here is revealing your optimal make strategy does not hurt you which is kind of a cool idea the proof of that is interesting if you're interested in look at the notes you can use linear programming here the reason kind of the intuition behind it is if you're playing mixed strategy the next person it has to play pure strategy and you have n possible options for that keyword strategy so that creates n constraints that you're putting in for your optimization you end up with a single optimization with n constraints and then you can use like linear programming duality to actually solve it so you could compute this using linear programming and that's kind of the one LS here so so let's summarize what we have talked about so far so so we have talked about these simultaneous games and and we've talked about the setting where we have pure strategies and we saw that if you have pure strategies going second is better right going second is better if you're just telling you like what's the pure strategy you're using right so that was none of the first point couple and then if you're using mixed strategies it turns out it doesn't matter if you're going first or a second you're telling them what you're mixed best mixed strategy is and they're going to respond based on that so and that's the Von Neumanns minimax there all right so next ten minutes I want to spend a little bit of time talking about non zero sum games so so far we have talked about zero-sum games where it's either minimax I get some reward you get the negative of that or vice versa there are also these other things called collaborative games where we were just supposed to maximizing something so we both get like money out of it and that's kind of like a single optimization it's a single maximization you can think of it as playing search in real life you're kind of somewhere in between that and I want to motivate that by an example so I want to be that but by this idea of prisoner's dilemma how many of you have heard of prisoner's dilemma ok good ok so the idea of prisoner's dilemma is you have a prosecutor who asks a and B individually if they will testify against each other or not ok if both of them testified and both of them are sentenced to five years in jail if both of them refused and both of them are sentenced to one year in jail if one testifies then he or she gets out for free and and then the other one gets ten years sentence play with your partner real quick okay so let's look at the payoff matrix so I think you kind of have an idea of how the game works so so you have two players a or B each one of you have an option you can either testify or you can refuse to testify so you can you can testify and you can refuse to testify and I'm going to create this payoff matrix this payoff matrix is going to have two entries now in each one of these in these cells and then why is that because we have a non zero sum game before our payoff matrix only had one entry because this was for player a player B would just get negative of that but now for your a and B are getting different values so if both of us testify then both of us get five years jail right so hey gets five years in jail he gets five years right if both of us refuse and gets one year jail he gets one year jail one year one year gee and then if it is a setting or one of us testifies the other one refuses one of us gets Nero the other one gets ten years jail so if I refuse to testify then I get ten years and then B gets zero and then in this case a gets 0 and B gets ten yeah so the payoff matrix is now going to be for every player we are gonna have a payoff matrix so so now we have this this v value function which is a function of a player for policy a and policy B will be the utility for one particular player because you might be looking the idea from perspective of different players okay so the one known as minimax theorem it doesn't really apply here because we don't have the zero sum game but you actually get something a little bit weaker and that's the idea of Nash equilibrium so a Nash equilibrium is set of policies PI star a and PI star B so that no player has an incentive to change their strategy so what does that mean so what that means is if you look at the value function from perspective employer a value phone from perspective player a at the Nash equilibrium at PI star a and PI star B is going to be greater than or equal to value of any other policy PI a if you fix pipe and at the same time the same thing is true for value of me so for agent p value of B Atma and Nash equilibrium is going to be greater than or equal to a value of B at any other PI B if if PI a fixes their Falls okay so what does that mean in this setting do we have a Nash equilibrium here so let's say I start from here I start from a equal to minus 10 B equal to 0 can I get this better and I make this better I did I flip them I only flipped right 0 minus n minus 10 0 okay so let's say I start from here can I get can I get this better and I make this better I start from this cell a gets 0 years of jail that's pretty good he gets 10 years of ill that's not that great so he has an incentive to change that right like he has an incentive to actually move in this direction right so B has an incentive to get 5 years jail instead of 10 years similar thing here what if you start here a has 1 year jail B has 1 year jail they have an incentive to change this now and get 0 years jail he has an incentive to change this and get 0 and we end up with this cell where like you don't have any incentive to change our strategy so we have one Nash equilibrium here and that one Nash equilibrium here is both of us are testifying and both of us are getting 5 years jail it's kind of interesting because there is like a socially better choice to have here I like both of us like if both of us were with your fuse like you would each had one year jail but that's not gonna be Nash equilibria all right so there's a theorem which is Ash's existence theorem which basically says if any finite player game with finite number of actions if you have any finite for your game we find out number of actions then there exists at least one Shaku Libyan and then this is usually one mixed strategy Nash equilibria on mixed strategy Nash equilibrium in this case it's actually a pure strategy Nash equilibria but but in general there's at least one Nash equilibrium if you have game this one okay all right so so let's look at a few other examples two-finger Maura what would be a Nash equilibrium for that so we just actually solved the dad's using the minimax oneness minimax theorem right so so it would be if you're playing it makes strategy of seven over 12 and 5 over 12 you might you might kind of modify your two finger Mora game and make it collaborative so in a collaborative setting and what that means is we both get two dollars or we both get four dollars or we both lose three dollars so so a collaborative two finger morai game it's not a zero-sum game anymore and and you have two Nash equilibria so you would have a setting where a and B both of them play one and the values two or a and B both of them play two values 4 and then prisoner's dilemma it's the case where both of them passive life we just just saw that on the world alright okay so the summary so far is we've talked about simultaneous zero-sum games we talked about this one knowing mins minimax theorem which has like multiple minimax strategies and the single game value like we had a single game value because it was zero-sum but in the case of non zero sum games we would have something that's slightly weaker that's Nash's existence theorem we would still have multiple Nash equilibria we could have multiple Nash equilibria but we have multiple we also have multiple game values from depending on whose perspective you're looking at so this kind of was just a brief like short introduction to game theory and econ there is a huge literature around different types of games in game theory and economics if you're interested in that take classes and yeah there are other types of games too like security games or resource allocation games that have some characteristics that are similar to things you have talked about you're interested in any of them maybe you can take a look at them would be useful for projects and with that I'll see you guys next time