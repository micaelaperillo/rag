homework 1 hard deadlines section tomorrow we're going to go through the backpropagation example which I went through very briefly in the last lecture talk about nearest neighbors which I did in one minute and also they're going to talk about scikit-learn which is this really useful tool for doing machine learning which might be useful for your final projects good to know so please come to section alright let's do a little bit of review of where we are so we've talked about we're talking about machine learning in particular supervised learning where we start with feature extraction where we take examples and convert them into a feature vector which is more amenable for a learning algorithm we can take either linear predictors or neural networks which gives us scores and the score is either defined via a simple dot product between the weight vector and the feature vector or some sort of more fancy nonlinear combination the end of the day we have these model families that gives us you know score functions which then can be used for a classification or regression we also talked about loss functions as a way to assess the quality of a particular predictor so in linear classification we had the zero M loss and the hinge loss as example of loss functions that we might care about the training loss is an average over the losses on individual examples and to optimize both of this we can use the stochastic gradient algorithm which takes an example X Y and computes the gradient on that particular example and then just updates know the weights based on that ok so hopefully this should be all no review ok so now I'm gonna ask the following question you know let's be a little bit philosophical here so what is the true objective you know of machine learning so how many of you think it is to minimize the error on the training set show of hands no one this is what we've been talking about right we've been talking about minimizing your on training sets okay well maybe that's maybe that's not right then what about minimizing training error with rigor ization because rigorous a ssin is probably a good idea how many of you think that's that's a goal what about minimizing error on a test set okay seems like it's closer right you know test sets test accuracies or things maybe you care about what about minimizing ear on unseen future examples okay so the majority of you think that's the right answer and what about learning about machines that's a true objective who doesn't want to learn about machines that's actually the true objective No so the correct answer is minimizing error on unseen future examples so I think all of you have an intuition that we are doing some machine learning we're learning on data but we really care about is how this predictor performs on in the future because we're gonna deploy this in a system and it's going to be the future it's gonna be unseen and then but then okay so then how do we do you think about all these other things no training said regularization in test sets so that's going to be something we'll come back to later okay so there's two topics today I want to talk about generalization which is I think a pretty subtle but an important thing to keep in mind when you're doing machine learning and then we're gonna switch gears and talk about unsupervised learning where we don't have labels but we can still do something so we've been talking about training loss right we've you know I've made a big deal about you write down what you want and then you're optimize so the question is like is this training loss a good objective function well let's take this literally suppose we really wanted to just minimize the training loss what would we do well here's here's an algorithm for you so you just store your training examples okay and then you're gonna define your predictor is as follows so if you see that particular example in your training set then you're just going to output the output that way you saw in the training set and then otherwise you just gonna stick phone these are gonna crash right so this is great it minimizes the training error perfectly it gets zero loss assuming your training examples don't have conflicts but you know you're all laughing because this is clearly a bad idea so somehow purely following this minimizing training set objective our objective is not really the right thing um so this is an example a very extreme example of overfitting so overfitting is this phenomena that you see where you have some data and usually the data has some noise and you are trying to fit a predictor but you're really trying too hard right so if you're fitting this green squiggly line you are fitting the data and getting zero training error but you're kind of missing the big picture which is you know this black curve or incorrect regression some of you probably seen examples where you have a bunch of points usually with noise and if you really try hard to fit the points and you can't get zero error but you're kind of missing this general trend and overfitting can really kind of bite you if you're not careful so let's try to formula formalize this a little bit more how do we assess whether a predictor is good because we can't measure it we can't really know optimize it okay so the key idea is that we really care about error on unseen future examples okay so this is great as you know aspiration to write down but the question is how do we actually you know optimize this right because it's the future and it's also unseen so you know our definition we can't get a handle didn't have so typically what people do is the next best thing which is you gather a test set which is supposed to be representative of the types of things you would see and then you guard it carefully and make sure you don't touch it too much right because you know what happens if you start looking at a test set and know what the worst case that you train on a test set right so you know the test set being a surrogate for unseen future examples just completely goes away right and even if you start looking at any of your you know really try and optimize it you can get you know two into this overfitting regime right so really be careful of that and I want to emphasize that the test set is really a surrogate for what you actually care about so don't blindly just you know try to make test accuracy numbers go up at at all cost okay okay so but for now let's assume we have a test set though you know we have to work with so you know there's this kind of really peculiar thing about machine learning which is this leap of faith right you the training algorithm is only operating on a training set and then all of a sudden you go to these unseen examples or the test set and you're expected to do well so you know why is there why would you expect that you know to happen and as I alluded to on the first day of class there's some kind of actually pretty deep of mathematical reasons for why this might happen but you know rather getting into the math I just kind of want to give you a maybe intuitive picture of how to think about this this gap okay so remember we have this picture that it's of all predictors so these are all the functions that you possibly want in your wildest dreams and then when you define a feature extractor or a neural net architecture or any sort of no structure you're basically saying hey I'm only interested in these set a function not all functions okay and that learning is trying to find some element of the class of functions F that you've set out to fine okay so there's a decomposition which is useful so let's take a at this point G so G is going to be the best function in this class the best predictor that you can possibly go so if some Oracle came and set your neural net weights to something how well could you do okay so now there's two gaps here one is the approximation error the approximation error is the difference between f star which is the true predictor so this is you know the thing that always gets the right answer and G which is the best thing in your class okay so this really measures how good is your hypothesis class remember last time we said that we want hypothesis class to be expressive if you only have linear you know functions and your data looks sinusoidal then that's not expressive enough to capture the the data okay so the second part is estimation error this is the difference between the best thing in your hypothesis class and the the function you actually find right and this measures how good is a learned predictor and relative to the potential of the hypothesis class you define this hypothesis classes here are things that I'm willing to consider but at the end the day based on a finite amount of data you you can't get to G you only can kind of estimate you know some you do learning and you get to some F hat so in the kind of more mathematical terms if you look at the error of the thing that your learning algorithm actually returns minus the error of the your best thing possible which you know many cases is you know zero then this written as follows so all I'm doing is minor subtracting era of GM adding era of G so this is the same quantity as this but then I can look at these two terms so the estimation error is the difference in error between the thing that you're learning algorithm produces - the best thing in the class G and then the difference approximation error is the difference between error of G and error of F sorry okay so this is going to be useful as a way to kind of conceptualize the different trade-offs right so you know just to kind of explore this a little bit suppose I increase the hypothesis class size right so I add more features or I you know added increase the dimension on my neural networks what what happens so the approximation error will go down and why is that because we're taking a minimum over a larger set so what G is always the minimum possible error over the set F and if I make the set larger I have just more possibilities of driving there down okay so the approximation error is going to go down but the estimation error is going to go up right as I make my hypothesis class more expressive and that's because it's harder to estimate something more complex so I'm leaving a kind of a vague right now there's a mathematical way to formalize this which you can ask me about offline okay so you can see this kind of tension here right you really want to make your hypothesis class large so you can drive down the approximation error but you don't want to make it too large that it becomes impossible to estimate okay so now we have this kind of abstract framework what are some kind of knobs we can turn how do we control the size of the hypothesis class so we're going to talk about two centric classes of types of ways so strategy 1 is dimensionality so remember for linear classifiers a predictor is specified by a weight vector so this is d numbers right and we can change D we can make these smaller by removing features we can make the larger by adding features and pictorially you can think about as reducing D as reducing the dimensionality of your of your hypothesis class so if you were in three dimensions you have three numbers three degrees of freedom you have this kind of ball and you if you remove one of the dimensions now you have this you know ball or circle in two dimensions okay so concretely what this means is you can manually add you know this is a little bit heuristic you can add features if they seem to be no helping and remove features if they don't help so you can you know kind of modulate the dimensionality of your weight vector or there are also automatic feature selection methods such as your boosting or l1 regularization which are outside the scope of this this class could be if you take a machine learning class you'll learn more about this the stuff but the main point is that you can determine by setting the number of features you have you can vary the expressive power of your hypothesis okay so the second strategy is looking at the norm or the norm or the length of a vector so this one is maybe a little bit less you know obvious so again for linear predictors the weight vector is just a DNA match from vector and you look at how long this vector has so what is and the length pictorially looks like this so if you have let's say all the weight vectors in know each W can be about as a point as well so this circle contains all the wave vectors up to a certain length and if by making the smaller now you're considering you know a smaller number of wave vectors okay so at that level it's perhaps intuitive so what does this actually look like so let's suppose we're doing one dimensional linear regression here's the board and we're looking at X Y so remember what and in one dimension we're all we're looking at is you know W is just a single number right and the number represents the slope of this line so by saying um no let's draw some slopes here okay so by saying that the weight vector or the weight is a small magnitude that's basically saying the slope is no smaller or closer to zero so if you think about you know slope equals let's say this is slope equals one so W equals one so anything-anything let's say less than 1 or greater than minus one is fair okay and now if you reduce this to half now you're looking at a kind of a smaller window here and if you keep on reducing it now you're basically converging to you know essentially very flat and constant functions okay so you can understand this two ways one is just that the total number of possible weight vectors you're considering is just shrinking because you're putting more constraints they have to be you know smaller from this picture you can also think about it as what you're really doing is making the function you know smoother right because a flat function is the kind of the smoothest you know function it doesn't kind of vary too much and a complicated function is one that can go you know very jump up very steeply and you know for quadratic functions can also come down really quickly so you get kind of very Wiggly functions those are tend to be more complicated okay any questions about this so far yeah what if we have like like instructors within the data sets that sense of track that sense we kind of like not over fit we're really just kind of like these drinking ourselves like a parenthesis or a set of like distributions that we say okay the state of must have like come from like something normal must have come from something reasonable but by saying that we're like not really capturing the full the full like scope apart so the question is if there's particular structure inside your data set for example if some things are sparse or you know low rank or something you know how do you capture that with a regularization realization but you have like perhaps not not even just like president lakes watch like this like if you have like calls a models inside decided between year like parameters like how would you like what a recognization like impede some of those relations yeah so so all this is kind of very generic right you're not making any assumptions about there's what the classifier is or the features so they're kind of like big hammers that you can just apply so if you have models where you have more structure or domain knowledge or if you which will see you know for example if you have you know Bayesian networks later in the class then there's much more you can do and this is just kind of you know two techniques for as a kind of generic way of controlling for overfitting yeah [Music] this approach is actually putting constraints on each element in factor W like the magnitude of it versus the other one that's actually how many elements are in yeah so um so let's look at W here so let's say you're in three dimensions so W is w1 w2 w3 um so the first method just says okay let's just kill some of these elements and make it smaller this one is saying that I mean formally it's looking at the squared values of each of these and looking at the square root that's what the norm is so it's saying that each of these should be you know small according to this particular metric yeah yeah so that's why I'm going get to so this is just kind of giving you intuition for in terms of hypothesis classes and how you want them to be small how do you actually implement this you know there's several ways to do this but the most popular way is to add regularization and buy regularization what I mean is take your original objective function which is trained loss of W and you just add this penalty term so lambda is called the regularization strength it's just a positive number let's say let's say 1 and this is the squared length of W ok so what this is doing is by adding this it's saying that ok optimizer you should really try to make trained loss small but you should also try to make this small as well okay and there's a if you study convex optimization there's kind of this to the duality between this this is called the Lagrangian form where you have a penalized objective where you add a penalty on the wave vector and the constraint form where you just say that I want to minimize training loss with subject to the norm of W being less than some value but this is more than kind of a typical one that you're going to see in in practice okay so here's objective function great how do i optimize it yeah I CW there and paint losses further minimizing yeah so it's important that these be the same W and you're optimizing the Sun so the optimizer is gonna make these trade-offs if it says oh okay I can drive the training loss down but if this is shooting up then that's not good and they'll try to balance these two it's it's basically saying try to fit the data but not at the expense of having huge weight vectors so if there's another way to say it is that kind of think about Occam's razor it's saying if there's a simple way to fit your data then you should just do that instead of finding some really complicated weight vector that fits your data so prefer simple solutions okay so once you have this objective you know we have a standard crank we can turn to turn this into algorithm you can just do gradient descent and the you know if you just take the derivative of this then you have this gradient and then you also have lambda W which is the gradient of this term so you can understand this is basically you're doing gradient descent as we were doing before and now all you're doing is you're shrinking the weights towards zero by lambda so lambda is a regularization strength if it's large that means you're trying to really kind of push down on the magnitude of weights so the great optimizer is basically going to say hey I'm going to try to step in the direction that makes the training law small but then I'm going to also you know push the weights towards zero okay in neural nets literature this is also known as so weight decay and you know optimization statistics it's known as l2 regularization because this is you know the Euclidian or two norm okay so here here is another strategy which intuitively gets at the same idea but it's in some sense you know more crude so it's called early stopping and the idea is very simple you just stop early instead of going and training for 100 iterations you just train for you know 50 ok so why why does this why is this a good idea um the intuition is that you know if you start with the weights at zero so that's the smallest you can make the norm of W right so every time you update on the training you know set generally the norm goes up you know there's no guarantee that it will always go up but generally this is what happens so if you stop early that means you you're giving less of an opportunity for the norm to your grow grow so fewer updates translates to generally lower norm you can also make this formal mathematically but you know the connection is not as tight as the explicit regularization from the previous slide okay so the lesson here is you know try to minimize the training error but don't try you know too hard yeah question depends on how we initialize the weights question is does this depend on how we initialize the weights most of the time you're going to initialize the weights from you know some sort of weights which is kind of a baseline either zero or you know for neural nets maybe like random vectors around zero but they're pretty small weights and usually the weights grow from you know outside from small to large there's other cases where if you think about you know pre-training you have a pre-trained model you start with some ways and then you do gradient descent from that then you're saying basically don't go too far from your initialization right so the question is why aren't we focusing on minimizing the train loss or why focus on W it's always going to be a combination so the optimizer is still trying to push down on the training loss by taking these gradient updates I noticed that the the cranium with respect to the Reg riser actually doesn't come in here it kind of comes in explicitly through the fact that you're stopping it early but it's always kind of a balance between minimizing the training loss and also making sure your class classifier weights doesn't get too complicated yeah [Music] I decide what value of lambda or intersect yeah so the question is how you decide the value of T here and how you decide the value of lambda so we pull up hyper parameters and I'll talk a little bit more about that later okay so here's the kind of the general philosophy that you should have any machine learning is you should try to minimize the training error because really that's the only thing you can do that's your data and that's you know you have your data there but you should try to do so in a way that keeps your hypothesis small so trying to minimize the training set error but don't try too hard I guess as that is the lesson here okay so now going back to the question earlier if you notice through all these my presentation there's there's all sorts of properties of the learning algorithm you know which feature you have which regularization parameter you have the number of iterations the step size for gradient descent these are all considered hyper parameters so so far they're just magical values that are given to the learning algorithm and the learning algorithm runs with them but someone has to set them and how do you set them yeah you can ask me I don't know the answer to that ok so here here's an idea so let's choose a high parameters to minimize the training error so how many of you think that's a good idea okay not too many so why is this a bad idea yeah you can overfit right so suppose you took lambda and you say hey you know let's choose a lamb that I will minimize the training error okay and the learning algorithm says well okay you know I want to make this day go down what is this doing in the way let's just settle I'm to the zero and then I don't have to worry about this so it's kind of you know cheating in a way and also early stopping would say like don't stop just keep on going because you're always going to drive the training error lower and lower okay so that's not good so how about choosing high parameters to minimize the test error I mean you say yeah it's a good idea yeah nuts not so good it turns out so why and this is again stressing the point that the test error is not the thing you care about because what happens when you look at it we try to use a test set then it becomes an unreliable estimate of the actual unseen error because if you're tuning hyper parameters on a test set that means that it's no longer it becomes less and less unseen and less future [Music] yeah so we could do cross validation which I'll describe in a second okay so I want to emphasize this point when you're doing your final project you have your test set you have it sitting there and you should not be you know failing way that too much or else it becomes less reliable okay so you can't use a test set so what do you do so here's idea behind a validation set is that you take your training set and you sacrifice some amount of it maybe it's you know 10% maybe it's 20% and you use it to estimate the tester so this is a validation set right the tests they're set is you know off to the side it's locked in a safe you know it's you're not gonna touch it and then you're just gonna tune hyper parameters on the validation set and use that to guide your model of development yeah [Music] the proportion itself it's a hyper hyper parameter no I you yeah you don't usually don't do that I mean usually it's how you choose it is kind of this balance between you want the validation set to be large enough so it gives you reliable estimates but you also want to use most of your data for training yeah yeah so how do you choose the hyper parameters so the the answer is you try particular values so you for example try let's say lambda equals point zero one and point one one and then you run your algorithm and then you look at your validation error and then you just choose the one that has the lowest yeah it's pretty crude yeah way to choose it to choose like the linear rates and I get a hyper parameters without just doing like a like a like a search alert you try this one then try this one yeah so how is there a better way to search for hyper parameters you could do your grid search generally is fine random sampling is fine there's fancier things based on Bayesian optimization which might give you some benefits but it's actually the jury's out on that and they're more complicated there's also you can use better learning algorithms which are less sensitive to the step size so you don't have to nail is like 0.1 works but black point one one doesn't so you don't you don't want that but in all the high-level answer is that there's no real kind of principled way of like here's a formula that lambda equals and you just evaluate that formula and you're done because there's this is you know the kind of the I don't know the dirty side of machine learn there's always this tuning that needs to happen to get your good results yeah question tricky kind of is this process usually automated or so the question is is this process automated increasingly it becomes much more automated so it requires a fair amount of compute right because usually if you have a large dish that even training one model might take all the while and now you're talking about you know training let's say 100 models so it can be very expensive and there's things that you can do to make it faster but I mean in general I would advise that don't hype a pro mattoon kind of blindly especially when you're kind of learning the ropes I think do we are kind of manually and getting intuition for what step size like effective step sizing algorithm is still valuable to have and then once you kind of get a hang of it then maybe you can automate but I wouldn't try to automate - yeah yeah so the question is if you change the hyper parameters a little bit and that causes your training or model performance to change quite a bit does that mean your model is not robust yeah it means your model is probably not as robust and sometimes you actually don't choose to have a hyper premise at all and you still get varying you know model performances so yo you should always check that first because there could be just inherent randomness especially if you're doing neural networks it could get stuck in local optimum there's all sorts of you know things that can happen okay final question in now move on I could they optimal have a burger it just went there converging to certainly so how do you choose a optimal hyperparameters so you basically have like a for loop that says for lambda in you know point one point zero one one and whatever values for T equals you know something you train on this all these training examples - validation and then you test the model on the validation you get a number and you just use whichever setting gives you the lowest number I'm sorry we do know the numbers it's not just like we usually you just have to be in the ballpark you don't have to get like 99 versus 100 the things I would just advise is like you know let's say what kind of orders of magnitude because if it if it really matters like being down to the precise number then you probably have other things to worry about okay let's move on so what I'm gonna do now is go through a kind of a sample problem right because I think the the theory of machine learning and the practice is a are actually kind of quite different in terms of the types of things that you have to think about so here's a simplified named entity recognition problem so named entity is this recognition is this Y popular task in NLP where you're trying to find names of people and locations of organizations so the input is a string where which has you know a particular potential you name with the left and right contacts words okay and the goal is to predict whether this X contains you know either a person which is plus one or not okay so so here's the the recipe for success when you're doing your final project or something you get a dataset it have if it hasn't been already split split it into training validation and test and lock the test set away and then first I would try to look at the data to get some new intuition you know I'll always remember you want to make sure that you understand near your data don't just immediately start coding up the most fancy algorithm you can think of and then you repeat you implement some you know feature maybe you change the architecture of your network and then you tune some you set some hyper parameters and you run the learning algorithm and then you look at the the training error and validation error rates to see you know how they're doing if you're under fitting or overfitting in some cases you can look at the weights further than your classifiers and for neural nets it might be a little bit harder and then you I would recommend look at the predictions of your model I always have I always try to log as much information as I have you can so that you can go back and understand what the model is you know trying to do and then you brainstorm some improvements and you kind of do this until you either are happy or you run out of time and then you run it on the final test set and you get your final iterates what you put in your report okay so let's go through an example of what this might look like so this is going to be based on the code base for the sentiment homework so okay so here's where we're starting we're reading a training set let's look at this training set so there are you know 7,000 lines here each line contains the label which is minus 1 or plus 1 along with the input which is going to be you know remember the left context the actual entity and the right okay all right so you also have a development or validation set and what this code does is it's going to learn a predictor which takes the training set and a feature extractor which we're gonna fill out and then it's going to output either both though the weights and some air analysis which we can look to use to look at the predictions and finally there's this test which I'm gonna not do for now okay so so the first thing is let's define this feature extractor so this feature extractor is V of X and we're going to use the sparse you know map representation of features so the feed is there's this really nice commune structure called default dict so this is kind of like saying you have a you know a your map but you can you know access it and if the element isn't there then you return 0 okay Sophie it goes that and then return field okay so this is the simplest feature vector you can come up with the dimensionality is 0 because you have no features ok so but you know we can run this and see how we do on this ok so let's run this ok so over a number of iterations you can see that learning isn't do anything because there's no white stuff update ok so but you know it doesn't crash which is good ok so I'm getting 72% error which is you know pretty bad but I haven't really done anything so that's to be expected ok where my window go ok so now let's start defining some new features ok so remember what is X X is something like took Mauritius in two right so there's this this entity and left and right so let's break this up so I'm going to tokens equals x split so that's going to give me a bunch of tokens and then I'm going to define left entity right equals so this is the token 0 is the left that's going to be took tokens 1 through minus 1 is going to be everything until the last soak in and then tokens minus 1 is the last one ok so now I can define a feature template so remember a good nice way to go about it is to find your future template so I can just say entity is and blank that's how I would have write written it as a feature template in code this is actually pretty no transparent it's saying I'm defining a feature which is going to be 1 for this you know feature template so entity is going to be some value I plug it in I get a particular feature of value or feature name and I'm gonna set that feature name tip you have the feature value 1 okay so let's run this ok so let's go over here run it whoops so entity is a list so I'm going to just going to turn into a string okay so now I'm getting what happened so the the training error is pretty low right I'm basically fitting the training error pre trains that pretty well but no notice I I don't remember I don't care about the training sir I care about the tester so just one note it says test here but it's really the validation Chicago change that it's just whatever non training said you passed it okay so this is still a twenty percent error which is not great okay so at this point remember I want to go back and look at some you know get some intuition for what's going on here so let's look at the weights okay so this is the weight vector that's learned so for this weight feature the weight is one and all of these are 1 and this know corresponds to the names that the people names that have been seen that training time because whenever I see a person name that I'm going to you know give that feature a 1 so I can get that training example right if you look at the bottom these are the entities which are not people that's okay so this is sanity check that it's doing what it's supposed to do so the nice thing about with these kind of really interpretive that you can kind of almost compute the what the weight should be in your in your head yeah each or every almost every example the German yeah yeah yeah so I have look essentially one feature for every entity which is almost you know number yeah so there's a 3900 so we're gonna change those but we'll get we're not there okay so okay so the other thing we want to look at is the error analysis okay so this shows you here's an example Eduardo Romero the ground truth is positive but we predicted minus one and why do we predict minus one it's because this feature has weight zero and why does it have way zero because we never saw this name at training time okay but you do get some right we saw Senate at training time and we rightly predicted that was minus one okay but you know you look at these errands you say okay well you know this is maybe the we should add more features okay so if you look remember this new example here maybe the context helps right because if you have governor blank then you probably know it's a person because only people can you know be governors so let's add a feature so I'm gonna add a feature which is left is left and for symmetry I'll just add right is right okay so this defines them indicated features on you know the context so in this case would be tooken into okay so now if three feature templates let's go and train this model and now I'm down to I just love in percent error okay so I'm making some progress oops let's look at the error analysis okay so now I'm getting this correct and let's look at what else am I getting wrong so Smith is blamed you know Felix mantilla and you know again it hasn't seen this execs actually maybe did see this string before but it so got it wrong you know I think there's a kind of a general intuition though that well if you have you know Felix you know even if it you've never seen Felix mantilla if you see Felix something you know chances are probably as a person not always but as as we you know noted before features are not meant to be like deterministic rules they're just pieces of information which are useful so let's go over here and we want to define let's say a feature for every possible word that's in in entities a word and entity remember entity is a list of tokens which occurred between left and right and I'm gonna say entity contains word okay so now let's run this again and now I'm down to you know 6% error which is you know a lot better if you look at the error analysis so I think the maybe the Felix example and now I get this right and you know what else what else can I do so you know what I'm kind of this general strategy here I'm following here is you know which is not always the necessary the right one but you start with kind of very very specific features and then you try to kind of generalize you know as you go so how can I generalize this more right so if you look at worker so Kurdistan right if your word ends in Stan or then I mean maybe it's less likely to be a person I actually don't know but you know maybe like suffixes and prefixes are helpful too so I'm going to add features let's say entity contains prefix and then I'm going to let's say just you know heuristic we look at the first four tokens and suffix the last four tokens and then run this again and now I'm down to four percent error okay okay I'm probably gonna you know stop right now at this point you can actually run out on your test set and we get you know four percent error as well yeah oh yeah I guess this was all planned out so that the test error would go down but actually more often than not you're at a feature that you really really think should help but it doesn't help for whatever reason so yeah yeah some of the time you do it doesn't move that's kind of probably them more of the time but sometimes it can go up if you add a really you know bad future so the more features you add generally the trainer will go down right so all the algorithm a knows is like it's driving training are down so it doesn't know that it doesn't know generalize yeah okay so this is definitely the happy path I think when you go and actually do machine learning it's going to be more often than not the tests error will not go down so don't get too frustrated just keep on trying yeah we expect to keep optimizing after like are you expected to optimize after 5% error it's it really depends you know there's kind of a limit to every data set so data sets have noise so sometimes you you should definitely not optimize below the noise limit so one thing that you might imagine is for example you have an Oracle which let's say it's human agreement like if your data set is annotated by humans and if humans can team agree like three percent of the time then you can't really do better than three percent of the time as a general rule there are exceptions [Music] okay any other questions yeah do all your training happy energy - into a real application say in the end then you town or test that you find it's not good oh yeah what happens if you actually you try on the test set and it's not good that's you just say that it's not good so there's many things that could happen one is that your test set might actually be different for whatever reason maybe it was collected in a different day and your performance just doesn't hold up on that test set in that case well that's your test error right remember the test error is just if you didn't look at it it's really an honest representation of how good the small is and if it's not good well that's just the true there was a your model it's not that good in some cases there's some like bug like something was missed processed in a way and it wasn't really fair so you know there are cases where you wanted like investigative it's like way off the mark if I had gone like 70 percent error then maybe something was wrong and you would have to go investigate but if it's in the ballpark and whatever it is that's kind of what you have to deal with right so what you want to do also is make sure your validation error is kind of representative of your if your test error so that you don't have you know surprises at the end of a day right I mean it's I think fine to run it on a test set just to make sure that there's no catastrophic problems but the the kind of aggressive tuning on a test that is something that would you know have warned against yeah and these are the scanners you should split the data into train about and validation of testing German like what percentage of your day that you should allocate to each one just randomize daily yeah so question is how do you split into train validation it depends on how large your data set is so generally people you know shuffle the data and then randomly split it into test validation and train maybe let's say like 80 percent 10 percent 10 percent just as a kind of a there are cases where you don't want to do that there's cases where you for example want to train on the past and test on the future because that simulates the more realistic settings remember the test set is meant to kind of be representative as possible of the situations that you would see in the real world so the question is the data set was labeled there's 7,000 of them I personally did not label this data set this is a standard data set that someone labeled you know sometimes these data sets come from you know crowd workers sometimes they come from you know experts yeah sometimes they come from grad soon it's actually a good exercise to go in label I've labeled a lot of data you know also in my life yeah exactly okay let's go on so switching gears now let's talk about unsupervised learning so so far we talked about supervised learning where the training set contains input output pairs so you're given the input and this is the output that your predictor should output but you know this is very timely we were just talking about how fully labeled data is very expensive to obtain because you know the 7,000 is actually not that much you know you can often have you know hundred thousand or even a million examples which you do not want to be sitting down and annotating yourself so here's another possibility so unsupervised learning non-super by learning the training data only contains inputs and unlabeled data is much cheaper to obtain in certain situations so for example if you're doing text classification you have a lot of text out there people write a lot on the internet and you can easily download you know gigabytes of text and all that is on labeled and you can do something with it that would be you know you turn that it's gold or something and also images videos and so on you know it's not always possible to obtain unlabeled data for example if you have you know some device that is producing data and you only have one of that device that you built yourself then you know you're not gonna be able to get that much data but we're going to focus on a case where you do have basically infinite amount of data and you want to do something with it so here's some examples I want to share with you this is a classic example from NLP that goes back to you know the early 90s so this idea of word clustering the employee you have a bunch of raw text lots of news articles and you put into this algorithm which I'm not going to describe but I'm gonna look at we're gonna look at the output so what is this output it returns a bunch of clusters where for each cluster it has a certain set of words associated with that cluster okay you look at the clusters the pretty coherent so this is roughly the first cluster is days of a week second cluster is months third cluster is some sort of you know materials fourth cluster is synonyms of like no big and so on and you know one thing one thing the critical thing to note is that the input was just raw text nowhere did someone say hey this these are days of the month learned them and now go test you later it's all unsupervised so this is actually you know on a personal note the kind of the example when I was doing a master's that got me into doing an LP research because I was looking at that sounds like well you can actually take unlabeled data and actually mine really interesting kind of signals out of it more recently there's these things called war vectors which do something very similar instead of clustering words they embed words in into a vector space so if you zoom in here each word is associated with a particular position and words which are similar actually happen to be close by in vector space so for example these are country names these are pronouns these are you know years months and so on okay so this is kind of operating on a very similar principle there's also contextualized word vectors like Elmo and Bert if you've heard of those things which have been really taking the NLP community by storm more recently on the vision side you also have the ability to do unsupervised learning so this is an example from 2015 where you run a clustering algorithm which is also jointly learning the features during this kind of deep neural network and it can identify different types of digits zeros and nines and fours that look like nines threes and/or 5sy look like threes and so on so remember this is not doing classification right you're not telling that algorithm here's our five series our twos it's just looking at examples and finding the structure that oh these are kind of the same thing and these are also the same thing and sometimes but not always these clusters actually correspond to labels so here's another example of ships planes and birds that look like planes so you can see kind of this is not doing classification it's just kind of looking at visual similarity okay alright so the general idea behind supervised learning is that you know data has a lot of rich latent structure in it and that by that mean I mean there's there's kind of patterns in there and we want to develop methods that can discover this structure you know automatically so there's multiple types of unsupervised learning there's clustering dimensionality reduction but we're gonna focus on you know clustering in particular k-means clustering for this lecture okay so let's get into it more formally so the definition of clustering is as follows I give you a set of points so x1 through xn and you want to output an assignment of each point to a cluster and the assignment variables are going to be z1 through Zn so for every data point I'm gonna have a zi that tells me which of the K clusters I'm in 1 through K ok so pictorially this looks like this board here where I have let's say let's say I have 7 points okay and if it gave you only these 7 points and I tell you hey I want you to pluster them into two clusters you know intuitively you can kind of see maybe there's a left cluster over here and the right cluster over here ok but how do we formulate that kind of mathematically so here's the k-means objective function so this is the principle by which we're going to derive clusterings okay so k-means says that every cluster there's gonna be two cluster is going to be associated with a central okay so I'm gonna draw a centroid in a red square here and the centroid is a point in this space along with you know the data points and I'm gonna this is kind of representing where the cluster is and then I'm going to associate each of the points with a particular centroid so I'm going to denote this by a blue arrow pointing from the point into the centroid and you know these two quantities are going to kind of represent the clustering I have the locations of the clusterings in red and also the assignments of the points into the clusters in in blue okay so of course neither the red or the blue are known and that's something we're gonna have to optimize okay so but first we have to define what the optimization objective function is so intuitively what do we want we want each point of V of X I to be close to the centroid right for the centroid to be really representative like of the points in that cluster that centroid should be close to all the points in that cluster okay so this is captured by this objective function where I look at all the points for every point I measure the distance between that point and the centroid that that point is associated with it so we remember Zi is a number between 1 and K so that indexes which of the mu mu 1 or mu 2 I'm talking about I'm looking at the squared distance between those two the centroid on the point yeah yeah yeah how does each point get to assign to a centroid so that's going to be specified by the Z's which is going to be optimized over paper you don't know yeah question is do we know how many clusters there are in general no so there are ways to select it's another hyper parameter so it's a something that you have to set before you run the k-means object function so when you're tuning you try different number of clusters and see which one kind of works better okay so we need to choose the centroids and assignments jointly so go this this hopefully is clear you just want to find the Simon's e and the centroids mu to make this number as small as possible so how do you do this well let's let's a look at a simple one-dimensional example and let's build up some so we're going 1d now and we have four points and the points are at they're going to be at zero to ten and twelve okay so I have a points for points at these locations okay I want a cluster and intuitively you think I want two clusters here there's going to be two centroids and suppose I know the centroids okay so I just someone told you magically that the central this example is are going to be like at one and eleven okay so someone told you that and now you have to figure out the assignments you know how would you do this [Music] well let's assign this point where should it go you look at this distance which is one you look at this distance which is 11 and which are smaller one is smaller so you say okay that's where I should go same with this point one is smaller for these eleven and smaller and that's okay so mathematically you can see it's comparing the distance from each point to each of the center's and choosing the center which is closest okay and you can convince yourselves that that's the way to if the cluster centers were centroids were fixed how you would minimize the objective function because if you choose a center which is farther away then you get just you know a larger value and you want the value to be as small as possible okay I don't know why this is true I think this should be one right okay near alpha okay so let's do it the other way now suppose I now have the assignments so I know that these two should be in some cluster these two should be in a different cluster cluster two and now I have to place the the center's you know where where should I paste that should I place it here should I place it here Shea place it here you know we're shy placement and if you look at the despite here what you're doing is you're saying okay for the first cluster I know two and zero are assigned to that cluster and I know that the the sum of the distances to this the centroid amou is this and I want this number to be as small as you know possible okay and if you do the first homework you know that you know whenever you have one of these kind of squared up some objectives you should it be averaging the points here so you can actually solve that in closed form and you given the assignments here you know the center should be there which is average of 0 & 2 and for these this cluster you should average the two points here and that should be at 11 yeah okay so what's the difference between centroid and assignment so when you're clustering you have K clusters so there's K centroids so in this case there's two centroids there's no those are the red the assignments are association between the points and the centroids so you have n assignments we then these are the things [Music] yes okay here is a hyper parameter which is a number of clusters which you can turn okay so here's a chicken-and-egg problem right if I knew the centroids I could pretty easily come up with assignments and if I knew the assignments I could come up with the centroids but I don't know either one so how do I get started so the key idea here is alternate minimization which is this general idea in optimization which is usually you know not a bad idea and the principle is well you have a hard problem maybe you can solve it by tackling kind of two easy problems here so here's a k-means algorithm so step one is you're going to you're given the centroids now kind of go into more general notation mu 1 to MU K and I want to figure out the assignments so for every data point I'm going to assign that data point to the cluster with a closest centroid so here I'm looking at all the clusters 1 through K and I'm going to test how far is that point from that centroid and I'm just going to take the smallest value and that's going to be where I assign that point okay step two flip it around you're given the cluster assignments now is you 1 through Zn and now we're trying to find the best centroids so what centroid should I pick so now you go through each cluster one two okay and you're going to set the centroid of the case cluster to the average of the points assigned to cluster right so mathematically this looks like that you're just sum over all the points I which have been assigned to cluster K and you add you basically add up all the the feature vectors and then you just divide by the number of things you summed over okay so putting it together if you want to optimize this objective function the k-means reconstruction loss first you initialize mu1 from UK randomly there's many ways to do this and then you just iterate set the assignments given the clusters centroids and then set the centroids given the cluster assignments there's alternate yeah see how this makes sense for like coordinates for like images where like if you read in a similar image by bytes it looks the same like words where words that are spelled totally differently can have like these same like semantic meanings how do you accurately map them to like a same location to cluster essentially around yeah so the question is like maybe four images distances in your pixel space makes kind of more more sense but if you have words than you know two words which you shouldn't be looking at like the Edit distance between you know though are the words and two synonyms like a big and large look very different but they're somehow similar so this is something that word vectors you know address which we're not going to you know talk about basically you want to capture the representation of a word by its context so the context in which big and large occur is going to be kind of similar and you can construct these context vectors that give you a better representation we can talk more offline yeah yeah you can get stuck and I'll show you an example okay any other questions about the general algorithm yeah yeah yeah maybe I'll answer that I'll sure example yeah so this is going up to a fixed number of iterations T typically you would have some sort of you board monitor this objective function and once it gets below it stops changing very much then you just stop actually this so the k-means algorithm is guaranteed to always converge to a local minimum so I just should be this demo and I think it'll be make some things clear okay so here I have a bunch of points so this is a JavaScript demo you can go in and play around and change the points if you want it's linked off the course website and then I'm gonna run k-means okay so I initialize with these three centroids and these regions are basically the points that would be a sign to that centroid so this is a Voronoi diagram of these centroids okay and this is the loss function which hopefully should be going down okay so now I iterate so iteration one I'm gonna assign the points to the clusters so these get assigned to blue this one gets assigned red these gets assigned to green and then the step two is going to be optimizing the centroids so given all the blue I put the center in the smack in the middle of these blue points and then same with green and red notice that now these points are in the red region so if I reassign then these become red and then I can iterate and then you know keep on going and you can see that the algorithm you know eventually you know converges to clustering where these points are blue these points are red and these are green and if you keep on running it you're not going to make any progress because if the assignments don't change then the cluster centroids are going to change either okay so let me actually you know skip this since I'm it's just gonna do it on the board but I think you kind of get the idea so let's talk about this a local minimum problem so k-means is not guarantee is is guaranteed to converge to a local minimum but it's not guaranteed to come find a global minimum so if you think about this as a toy visualization of objective function you know by going down here you can get stuck here but it won't get to that point so it's the example for different random seeds you can let's say you initialize here okay so now all the three centers are here and if I run this and I run this now I get this other solution which is actually a lot worse remember the other one was 44 and this is 114 and that's where the algorithm converge and you're just stuck so in practice people are typically try different initializations run it from different random points and then just take the best there's also a particular way of initialisation called k-means plus plus where you put down a point and you put down a point which is as far this way as possible and then as far as where as possible and then that kind of spreads out the center's so they don't kind of in interfere with each other and that generator works pretty well but still there's no necessary guarantee of converging to the goalball optimum ok any questions about k-means yeah how do you choose k you guys love these hyper parameter tuning questions so one thing you can kind of bra is the picture so k and then give a loss that you get from k and usually if you have one cluster the loss is going to be very high and that at some point it's you know going to go down and you generally you know lop it off when it's you know not going down by very much so you can monitor that curve another thing you can do is you have a validation set and you can measure reconstruction error on that you know validation set and choose the minimum based on that which is just another hyper primer that you can do yeah how is the training loss calculated so the training loss is this quantity so you sum over all your you know points and then you look at the distance between that point and the assigned centroid and you square that and you just add all those numbers up okay so to wrap up oh actually I haven't you have more slides here uh-huh [Music] unsupervised learning you're trying to leverage a lot of data and we can kind of get around this difficult optimization problem by you know doing this alternate minimization so these will be quick so just to kind of summarize the learning section we've talked about feature extraction and I want you to think about the hypothesis class that's defined by a set of features prediction which boils down to kind of what kind of model you're looking at for classification and regression supervised learning you have linear models or neural networks and for clustering you have you know with a k-means object objective you have loss functions which you know in many cases all you need to do is compute the gradient and then there's generalization which is what we talked about for the first half of this lecture which is really important to think about you know the test said remember is kind of only a surrogate for unseen future examples so a lot of these ideas that we've presented are actually quite old so the idea of least squares you know the for regression goes back to you know Gauss when he was you know solve trying to solve some astronomy problem logistic regression was you know from statistics in AI there was actually some learning that was done even in the you know in the 50s for playing checkers as I mentioned the first day of class there was a period where learning kind of fell out of favor but it came back with back propagation and then much of the 90s actually a lot more kind of rigorous treatment of optimization you know formalization of when algorithms are guaranteed you know converge that that happened in 90s and then in a 2000's you know we know that people looked at kind of structure prediction and there was a revival of neural networks some things that we haven't covered here are you know feedback loops right so learning assumes kind of the static view where you take data you train a model and then you go in generate predictions but if you deploy the system in the real world those predictions are actually going to come around and be data and those feedback loops can also cause your problems that you might not be aware of if you're only thinking about ah here's I'm doing my machine learn anything you know how can you build classifiers that don't discriminate so we often have classifiers you're minimizing the training set average of a training set so but by a kind of construction you're trying to drive down the losses of you know kind of common examples but often you get these situations where minority groups actually get you know pretty high loss because they look different and almost look like outliers but you're not really able to fit them but the training loss doesn't kind of know care so there's other ways there's techniques like distribution robust optimization that tries to get around some of these issues there's also privacy concerns how can you learn actually if you don't have access to entire data set so there's some techniques based on randomization that can help you and then Inter bility how can you understand what you know the algorithms are doing especially if you have a deep neural network you've learn a model and there's no work which I am happy to discuss with you offline so the general so we've concluded three lectures on machine learning but I want you kind of to think about learning in the most general way possible which is that you know programs should improve with you know experience right so I think we've talked about you know linear classifiers and all these kind of nuts and bolts of basically reflex models but in the next lectures we're going to see how learning can be used in state-based models and also you know variable based models okay with that so that concludes next week dorso will be giving the lecture on state-based models