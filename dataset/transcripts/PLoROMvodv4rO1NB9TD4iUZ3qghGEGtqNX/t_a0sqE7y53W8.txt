welcome to the last lecture it's a smaller group of people today so yeah so just a quick announcement the project reports are due next Friday so just make sure that you return those and yeah poster session was awesome so I showed up for a little bit of it but the the posters that I saw was really amazing and I really enjoyed like talking to the groups I talked to and good job on all the projects it was great we were going to have best poster or word thing - and you're going to announce that on Piazza later so just an announcement on that okay all right cool so let's must conclude the lecture so CS 221 so the plan for today is we're going to do a quick summary of what we have talked about so general summary of the class like all the things that we have learned and then I want to talk a little bit about some of the next courses that you might want to take after taking 221 if you were in 229 I know in the morning they they went over a bunch of next courses from the perspective of 229 all the AI courses one would want to take we're kind of doing a similar thing here but from the view of 221 what would be some of the next courses that would be good to take and then after that I want to talk a little bit about the history of AI we did this in the first lecture so it would be a little bit of a review of that but then I want to talk a little bit also about some of the next directions that might be interesting to think about and some of the research that is done currently in various topics of subtopics of AI and what are some of the problem is that people are struggling with so it would be fun to think about that and if you're interested in any of that you can go do research in that area or take classes in those particular areas so that's kind of the plan for for today's lecture it's gonna be shorter than usual so I think it's gonna be probably an hourish okay all right so so let's talk about the summary of the class so we started the class talking about this paradigm of modeling inference and learning right so we started thinking about how there exists a real-world problem you're going to pick up a real-world problem and we're going to do modeling so modeling would be an abstraction of that real-world problem and in general we are in and reasoning about that real-world problem like finding the shortest path or solving some sort of optimization about that problem and we call that in France right so we had a model of the world and then you would do inference reasoning on that model and the idea of the learning the learning part of the lectures was that well our models are not going to be perfect right you're not going to be perfectly model everything in the world instead we might have a partial model and in addition to that you might have some data around the world and around the things that are happening in the world so you would like to use that data to to learn about the model and kind of complete our model so so this was kind of the common paradigm through the class and and this was the topics that we covered we started with machine learning and retreated machine learning as a tool that allows us to better learn these models parameters of these models and then we talked about various levels of intelligence in the course right so so we started with reflex based models then we increase the level of intelligence a little bit talked about state based models variable based models and and finally logic so let me briefly just remind you of some of these some of these topics so in machine learning kind of the common thing that you started looking at was this idea of loss minimization so we have some data we have some training data inputs and outputs X and Y's and then we define some sort of loss we looked at different types of loss functions and properties of these loss functions and then the idea was we would want to minimize this training loss for the hope of generalizing to a new scenario right for the hope of if I get some sort of new input I would be able to give the best output possible with respect to that so so I would in general I would like to minimize my test tester but one way to go about that is to minimize this training loss based on some set of variables that we have in our model and the approach that we followed for that was using techniques like a stochastic gradient descent so this would be the most common thing one would want to do so we have this loss function how do I go about optimizing it I take the gradient and move in the negative direction of the gradient so so stochastic gradient descent was commonly used when we were doing a lhasa minimization so so these two things like like writing out the loss minimizing and doing something similar to stochastic gradient descent this was kind of common across a lot of different machine learning algorithms that we use throughout the course and we applied that to a wide range of models so we would like apply this to all sorts of models that we had and reflex based models or state based models it was kind of the same framework throughout so and in the first set of lectures that we had we started talking about reflex based models the simplest form of which was these linear models if you remember regression like we would have this linear class regression classification we had these linear models linear classifiers and we just wanted to learn the parameters of that and a more complex version of that or things like neural networks or deep neural networks or even like nearest neighbors would be an example of these reflex based models and what was in France well inference was pretty easy when it came to these things right inference was just a feed-forward run of your model and that would give you the output so we weren't doing that much hard work ready when it would come to inference and in terms of learning well we looked at stochastic gradient descent we looked at other things like alternating minimization as a way of learning these models okay so that was reflex based models then increasing the level of complexity they started talking about state based models and and the key idea that I want you guys to remember from from the state based models is it's the idea of like what is a state a state is a summary of all past actions that is sufficient to choose the future actions optimally and then we spent a good amount of time thinking about how to define a state like how to pick a good state and how to how to do this this modeling when it comes to state based models and and we looked at things like search problems or we have deterministic systems we looked at things like MVPs when when we are playing against nature we have a little bit of a stochastic city and then we looked at things like games where you're playing against some other intelligent agents there's some other intelligent agent that's coming in and playing against us and in terms of inference we looked at a couple of really cool algorithms we talked about uniform cost search and a-star we talked about dynamic programming value iteration and minimax that we covered a set of number of different different ways of intelligently looking these models and doing inference and when it came to learning he looked at structured perceptron Q learning TD learning reinforcement learning in general but those were some of the learning algorithms that we applied when it came to state based models so so that was state based models then then we moved the level of complexity and intelligence a little bit higher and we looked at things like variable based models and the idea of variable based models was that the ordering of these states doesn't really matter it's just the relationship between them that matters and may find things like factor graph so if you remember the map coloring example we defined a factor graph around it and and the idea was there's a scrap that graph structure Adhir captures the conditional independence between different variables that we have so the different variables in this case was these different provinces and then you would want to color them differently and the relationship between them is going to be represented by a factor the two types of models we discussed in this setting were constraint satisfaction problems and Bayesian networks Bayesian networks but in the case where we have probabilistic relationships and and we talked about inference and specifically backtracking forward backward beam search Gibbs sampling as various ways of doing inference on these algorithms and I came to learning we looked at things like maximum likelihood and em2 try to do learning on these done these types of systems and then finally the last few lectures we've been talking about logic so pushing the level of complexity just a little bit higher and thinking about formulas like actual like logical formulas that represent intelligent things about your system so so the key idea of logic is we're gonna have these powerful formulas that are going to represent powerful like meaningful things about your system and we talked about things like propositional logic or first and first order logic and we talked about model checking which is commonly used in satisfiability we talked about modus ponens resolution as various types of inference algorithms that could be used when you have logic and we didn't really talk about learning when it comes to logic and I would say this is kind of like an open question still like how do you combine ideas from learning and ideas from logic and get the best of the both worlds like there are ways of combining them but how do you ensure that they're getting best of both worlds like from data-driven ways of looking at things and model-based looking at way of looking at things so so what did CS 221 gave us really the CS to Tony one is this is this class where it gives us a set of tools to look at the world and and and think about difficult problems in the world and pick the right models P computer right like way of formulating that problem and the right inference algorithm to go about solving them so I would say it's pretty much like we've covered a lot of material so so we covered breadth like pretty broad set a set of topics and the idea of it is to know that you have all these tools and you can pull out these tools and you can go deep in any of them but but but the goal of CS 221 was to just give a broad view of what is artificial intelligence and what are some of these tools that that we would have so but if you're interested in going a little bit deeper in any of these topics that we've discussed in the class are a good number of classes that that you can take any one just briefly mention some of them an overview of some of these courses so I would categorize the classes the next classes that you can take into into two main categories you can take foundational classes that you go deeper in some of the foundational things you are talking about or you can take application classes where you go deeper in like the specific applications like natural language processing vision robotics the specific applications that we kind of briefly covered in this class but we didn't go that deep in so so if you're interested in foundational classes some of these other AI base classes are things like probabilistic graphical models so 228 if you're interested machine learning there's 229 and 220 90 and there is the deep learning class if you're more interested in optimization side of things there is convex optimization decision-making under uncertainty and also if you're interested in logic side of things then there's this logic and AI course and also there's a there's a big data class to that if you're interested in that so I'm going to go a little bit deeper in some of these courses but this is just more of an overview of some of the foundations and if you're an X courses that you might be interested in taking and all of these are also posted on the AI website so I thought Stanford that they do you slash courses so so that's foundations but if you're interested more on the applications of things there's a good number of courses around natural language processing I'm gonna go again a little bit deeper in some of these courses and then there's a good number of courses around vision good number of courses around robotics there's also this other course around general game playing which would be fun to take if you're interested in that side of Thanks all right so so let me just briefly mention like one slide on some of these courses that I think would be good courses to take after this class so so one of these is CS 228 so CS 228 is a probabilistic graphical models course if you remember the variable variable based models part of part of the lecture this would be kind of the next course that goes deeper in that so so we talked about algorithms like forward backward variable elimination but if you take 228 you'll be talking about more general type algorithms like belief propagation variational inference Markov chain Monte Carlo and so on and another thing that 228 is going to cover is in variable based models like the way we treated things was the model was given the structure was given to us right you would say well this is an hmm and given that it's an hmm I'm gonna do these extra things on it but in 228 you'd actually be thinking about learning the structure learning the right structure to put in and and how to think about these different variables and the relationships between them so if you want to go deeper in that that would be the corseting another interesting course to take and some of you might have already taken it is the machine learning course so in this class the way we treated machine learning was just as a tool right like we we had a few lectures on it and we just learned about machine learning just enough for us to do some of the things we want to do in this AI course but but it's definitely broader than what we have discussed in the class and some of the ways that it is it is broader and more general than what we have discussed in the class is first off in this class we talked about discrete actions and and discrete and discrete time discrete action and state type systems 229 is going to cover a more broader set of set a set of models were you actually thinking about continuty a little bit we talked about linear models 221 we'll talk about kernel methods decision trees boosting bagging feature selection like all these sorts of different types of algorithms and models that are go that are gonna go beyond we have discussed in this class we talked about k-means they were going to talk about more broader set of clustering algorithms like mixture of gaussians pca ica all these sorts of things so a really useful a good class to take if you if you want to just learn more on the machine learning side of things more from a practical perspective if you're more interested in a theoretical approach a theoretical side of machine learning there is this other course called 229 T so this is statistical learning theory and this is going to actually think about the mathematical principles behind learning so so it doesn't necessarily cover the particular algorithm but it's going to cover like properties mathematical properties around that algorithm saying things like uniform convergence let's say you have a predictor and you want to make sure that that predictor with high probability is going to have some bounded error how are you gonna bound the error of that how are you going to bound the tester with respect to the training error and some properties of of your predictor or how would you formalize things like regret of your learning algorithm so so thinking about complexity thinking about putting bounds convergence regret these are going to be the topics that will be discussed in the statistical learning theories yes to 2019 so if you are more theoretically minded I think this would be a good course to take so now thinking more on the application side of things so a couple of good applications of AI after this course are things around vision NLP and robotics I would say those would be kind of a three main applications that you might want to consider going deeper and if you're interested in any of these areas if you're interested in visioned or a good number of classes around vision there are a lot of interesting tasks around vision some of them are more solved and some of them are more researching things around like object recognition detection segmentation but also things like activity recognition right like if I if you have frames of different frames of a video of say a soccer game how would you predict where the ball goes or how would you predict where the person goes in the next few frames like that's actually a pretty difficult problem like doing activity recognition from just frames of images so if you're interested in some of these problems from the vision perspective I would recommend taking 231 type classes so robotics would be another interesting application to look at so in robotics in general we are interested in problems around manipulation and navigation and grasping so the main applications that you might think about are things around self-driving cars medical robotics assistive robotics and the interesting thing that robotic Springs is that there's a physical system sitting there so you're putting your AI algorithm the things you have developed in this class and some stuff beyond it on this physical system physical robot and you need to deal with things like continuity you need to deal with things like uncertainty and you need to deal with physical models that could come from kinematics and control so there are a lot of interesting robotics classes if you're so so I think intro to robotics is offered next corridor sama kitv's teaching it but there is also a new robotics series course that just came out so this is the robot autonomy 1 & 2 so advertisement for myself I'm teaching this next quarter at markup Ivana and Jeanette spoke so this is a robot autonomy - robot autonomy one was already offered in the fall and and the idea of robot autonomy one is to just cover the different layers of the robotics stack and at the end of the day they actually have this project it's really cool project where you have you have a robot platform and you have a lighter on top of it and you want this robot to just move around in a fake city so if you're interested in I think the project the project presentations is not already done so so if you're interested it can show up to do and and see how how these robots are moving around but they basically have this fake city where this robot just navigates around in this fake city and does autonomous driving so you can see a picture of a bicycle in the back for the robot it's detected and the bicycle is actually like moving so detective and do obstacle avoidance the coordination and with other agents around it in this particular environment so that next robot autonomy one in robot autonomy - which is which is offered in winter what we want to do is you want to put a manipulator on top of the robot so we're looking at mobile manipulation where we actually have an arm and we have this arm pick up objects and blocks and put them on top of each other and do interaction so the class is - - big chunks of the class four is focused on interaction with the world with the physical world and interaction with other agents so there are interesting multi agent like game theoretic questions that could come up and you have multiple robots trying to interact with each other so ideas from games could come up there like naturally all right so so that was robotics and then another of interesting application is natural language processing and natural language processing games particularly very interesting because if you think about it the role this continuous but the words that we are using are discrete and these discrete words have continuous meanings so there's a lot of mismatch between the fact that we have discrete words and continuous world and and and we need to use these words to describe the discontinuous world and there are very interesting questions and challenges that arise in NLP around like compositionality and and grounding and if you're interested in these types of tasks I think there are a lot of again interesting tasks that are more soft versus less solved and more research II around NLP so if you're interested in any of them I would recommend taking classes like 224 so some of these tasks are around like machine translation let's say text summarization dialogue some of them are much harder like text summarization dialogue those sort of things so and we had a couple of homeworks around this so if you're interested in going deeper in that I do recommend taking an LP classes so those are some of the foundation and more application II courses that I would recommend taking I want to briefly mention two other courses - that are that are not necessarily directly in AI but during a neighboring field that that would be still interesting to look at one is looking at cognitive science so so cognitive science in general looks at how human mind works and it's one of those fields that that kind of grew together with AI right like the cognitive science and AI they kind of started together and they went their ways but they still tend to inform each other and there's a group of cognitive scientists who are looking at computational cognitive science and they use ideas like Bayesian modeling and probabilistic programs when they look at cognitive science so there's this course psych 204 which is cross-listed ICS 428 I think Noah Goodman teaches this usual and and I think this would be a great course to take if you're interested in the cognitive science side of things and you would have ideas and topics from other other cognitive scientists like Josh Tenenbaum and Tom Griffith and Noah we're working in this particular area of computational cognitive science so that's one but if you think about cognitive science as kind of the software here and neuroscience on the other hand is the hardware of the problem so so another neighboring field that you might be interested in looking in a little bit deeper is neuroscience and if you think about neural networks like back in the day when they first start like when people first started looking at neural networks they were kind of thinking of them as computational models of brain but today's neural networks modern neural networks aren't really like biologically plausible in any ways right so they're not really models of of the brain but still they're they're interesting they're interesting connections and insights that could be used in neuroscience from the perspective of AI or from AI to neuroscience so I do recommend taking kind of cross section neuroscience courses with AI think Danny means would be someone who works in this area if you're interested in looking deeper in courses around neuroscience all right so that was kind of a quick summary of what we have discussed so far in the class what are some next courses you are interested in taking sink deeper around them if you're interested in learning more around and just come come chat with me I'm around like if you want to talk about them but for the rest of the class what I want to do is I want to just do it quick like history of AI again and then after that let's just talk about what are some of the problems like what is left like we've talked about all these cool algorithms tool sets but what is difficult what is not solved so so I want to spend a little bit of time on that so let's talk about history of AI all right so birth of AI so we talked about the searing the overview lecture this was the first lecture we couldn't came in and were like okay how did I happen so the press of AI really like people refer refer to it as this workshop that happened in 1956 this was a summer school in Dartmouth and basically everyone famous in the field showed up to this workshop including people like John McCarthy Minsky Chanin all of these people showed up to this to the summer school and the reason for the summer school was to kind of understand the general principles of intelligence so what they really wanted to do was they wanted to figure out all the features of intelligence and if they could formalize that they could go ahead and like simulate that and have similarly intelligence that is what they really wanted to do and the workshop was really useful because after that these people went back their ways and then started doing really cool stuff and this was the first rise of AI and we started seeing things like problem-solving type type type systems things like Samuels checkers program which was able to kind of beat a strong amateur level player we started seeing other types of problem-solving programs like theorem provers and then people like started really using logic as a way of thinking about AI and thinking about problem-solving so that was really exciting because people like at a time were thinking they have just solved everything like it was exciting logic was there like people had all these cool programs they were super excited about the the potential that that these systems can bring here are some of the quotes from people are on bad time things like machines will be capable within 20 years of doing any work a man can do so this is what Herbert Simon said Marvin Minsky said within 10 years the problems of artificial intelligence will be substantially solved cloud Shannon said I visualized a time when we will be two robots what dogs are to humans and I'm rooting for the machines so none of these really happened but one thing to notice is these are not random people on the street like these are like fathers of the fields like these are people who were like in in it's like they were like looking yet there they had a lot of insights like in terms of what is chaotic what we can do and what we can't do and it's kind of interesting that even like them like they had all this like overwhelming optimism and this did not pan out right so so there was a lot of optimism here and and we started saying really underwhelming results so lots of optimism government came in government was like here's my money take my money go do stuff and basically the problem that government was excited about was machine translation right so they wanted to a crushing takes and translate that and and the outcome of that was something like this so the translation is the vodka is good but the meat is rather so that's not really like a good translation of that text and people started feeling like these types of problem solving algorithms are not gonna do it so so and at that point then the government cut funding and this was the first winter of AI so we had the rise of AI with problem solving with that summer school lots of excitement and then it didn't really work when it came to machine translation and then we had like the first winter of AI one thing that I want to kind of like point out is is you're at a good place for AI right now right like I would say like AI right now it's also pretty overhyped right and then I wanted to put this code here so this is from Roya Mara who says we tend to overestimate the effect of a technology in the short run but actually underestimate the effects of it in a long run like if you don't think about any system any technology that we have developed it's always like oh within two years it's gonna solve everything and it's not gonna solve everything within two years but if you look and what it has achieved in twenty years it's actually achieved a lot of things and we usually underestimate that and I think it's the same thing with AI like we are going to think oh we're gonna have a ton of ass cars tomorrow or by 2020 actually Thomas car companies were saying you're gonna have autonomous cars by 2020 when I first started working on that that's next month I don't think you're gonna have autonomous cars Oh a 2020 but you're going to see a lot of advances like we are seeing a lot of improvements in terms of the algorithms in the systems that we are developing so so I think in general we should be aware of that and we should be we should be smart about it it's like like sure AI is overhyped but what can we do to actually address some of these problems and I'm going back to this first era of AI just problem solving era but why didn't it work well the reason it didn't work was we had limited computation we had limited information this is the thing that we actually like started this class off we said well a lot of AI algorithms they haven't changed that much right but but the thing that has changed over the years is we have lots and lots of computation we have lots of lots of data and that's the thing that has really made the bigger difference here and that's kind of like one of the that it didn't work but even though like we had these problems and we had this first winter of AI there were lots of interesting contributions from that era the Lisp programming language came out around that time garbage collection came out that time time sharing like really interesting foundational ideas of computer science emerged during this period and also this key paradigm that we are using in this class thinking about separating modeling and inference that actually came out around the same era to like the fact that we should have this declarative model thing and at the same time this procedural inference algorithm kind of separated out from each other and think of them as separate things like is a huge contribution that came out around that time alright so that was kind of rice up and down of first up and down away the second rice of a I was around 70s and 80s this is when the knowledge based systems came out the expert systems and I would argue that the reason that we had the second rice was people people started thinking about AI differently like originally people wanted to build artificial intelligence because they were interested in intelligence they were interested in understanding intelligence that that's kind of what the summer school was about but at this point people were not interested in in intelligence what they were interested in was just building really useful systems that can do things like they didn't care about intelligence at this point and and that's why we had this rise of expert systems or we think about logic and we think about using domain knowledge to have things like if-then-else type statements like if we have a premise then we have some sort of conclusion and and building these experts expert systems allows us to do a lot of cool tasks in in the real raw so like we had we had actual impact around this time on things like inferring molecular structures like diagnosis diagnosis of blood infections things like converting customer orders in two parts and specifications so actual applications in the world people started taking each of them and thinking about the expert knowledge that you have in that particular application and formulating that in these expert based systems and I'm putting an AI on top of it that does actual work so that was really cool so the contributions of this era is that first off we had real applications that actually impacted industry and and this domain knowledge the idea of I'm gonna pick the domain knowledge and this knowledge is actually going to help me make exponential growth was the thing that that was really powerful at this time but well why did it work it didn't work right this was the second rise and we had another winter the second winter of AI so the second winter of AI came because there was a bunch of problems one of the problems was knowledge in general it's not deterministic right like we have a lot of uncertainty when we think about knowledge and and these systems were not able to like encode uncertainty the way we want it to be and in addition to that there was a lot of information right like if you think about any of these expert based systems that requires a lot of many manual efforts to write down these rules and all these relationships between all these different sub parts of the system so an example of that is shred loose so shred Lu is one of the first natural language understanding programs computer programs this was written by Terry Winograd was at Stanford now I think this was when he was at MIT and and he created this system this computer program where you have this block world environment in it and you can actually have a person that interacts with this computer program and maybe the person says pick up a big red block and the computer says okay because the computer understands what like the relationships between big and small and red and different colors and where the blocks are placed and what can be picked and what cannot be picked right so so this had a whole bunch of relationships and and rules around around it and you could actually converse something with this computer program and that was really powerful but even even Terry himself like a couple years after had the statement talking about his worries about how how Shred li-like programs extra blue are not going to solve the problem they're not gonna go all the way like he was saying this is kind of a dead end in AI and and thinking about these complex interactions there's just a lot of them and it just doesn't seem feasible to write down all these rules that you would have between each one of your sub parts with with other parts are parts and there is no easy footholds so so at this point people were thinking this complexity barrier is not gonna really allow these AI systems to do cool big things so then we had the second winter of AI and then finally this is third rise of AI that you're so on and god knows where it's gonna come pound again is this modern modern view of AI that started around 1990s and then I I would argue that this this morning I the reason that we had this this new rice is due to two main things one is the idea of using probability in AI this was not a thing that existed from early on this is actually due to efforts of Jude a pearl who was very adamant about using Bayesian networks in AI to model uncertainty so so finally people were able to to use probability to bring ideas from probability to model uncertainty because if you remember like expert based systems they were super deterministic like we didn't really have a way of talking about uncertainty but dude a pearl pearls idea was let's bring probability in this let's actually talk about uncertainty and let's have our models and make predictions and then the second reason is machine learning right so so people start inventing support vector machines to tune parameters and then from that point on we started seeing the rise of neural networks and the fact that we have lots of lots of data now and that can actually help us build better models so so given that we have these two two big new viewpoints and in AI we have started seeing all these new advances and at one point that I just want to make at the end of this is is that AI is really a melting pot of a bunch of ideas from different fields right like not all of these are from pure AI right like if you think about Bayes rules it comes from probabilities these squares come from an astronomy first-order logic well from logic maximum likelihood from statistics we have ideas from neuroscience econ optimization and algorithmic algorithms theory control theory like we can see my value iteration that came from Bowman from the field of control theory so it's really like if you think about artificial intelligence it just brings all these different ideas from these different fields together to solve this AI problem and in general I think it's a good idea to be mindful of that and to be open to that because the new insights ideas really come from like having this broader view of things and kind of the boundaries you put between different fields are really superficial and don't really need to be there alright so so that was a history of AI right like rise of AI downfall arise downfall and then we're on this this last rise now so so let's just think a little bit about what have you chief what are the cool things you've had in the past couple of years and then what can go wrong and what should be what should we worry about okay alright so so I think I've argued enough that AI is everywhere right like AI is being used in consumer service and advertising in healthcare transportation manufacturing and and AI is going to make decisions now right because like it has shown all these advances and because of that like we're using AI DS days to make decisions for us to make decisions for our education to make decisions for credit employment advertising health care all of these different applications so so if a is making decisions then we should actually be really careful about how I is making decisions and the fact that we should we should think about like all the possible things that could go wrong or could not go wrong and understand the system fully before making it make decisions for us so so what are some of the advances so what a huge advance that we have seen in recent years is this Google neural machine translation so the idea is this was kind of a huge advance when it comes to machine translation the idea is you could have a bunch of different languages and you can have a way of translating let's say from Korean to English and English to Korean you can have a lot of data on that and that would be great and then maybe you can have Japanese to English and English to Japanese and trained on that and that's a lot of data and that's all great but then even but if you put all of that data in the same like a melting pot then whether you can do is you can actually go from Korean to to Japanese without having any data that just goes directly from Korean to Japanese and that's kind of exciting right like because you have you had like new data for that and if you're putting all of these together then you can actually make a lot of advances in terms of language translation so this came out around 2016 lots of excitement language translation just became so much better after that there still problems here is one of them so here is the problem of bias okay so let's say you're starting from language like Hungarian that doesn't have gender and then you start from this language and then you go to English language that has gender and this is a translation that you're gonna get you're gonna get she's a nurse and he's a scientist and and and she's a baker and of course he's a CEO alright so so you're gonna get like all these like gendered furnace here that there's no reason for well just just by looking at it and assuming that the algorithm is neutral there is no reason for it to pick up like these particular genders but the reason that it's picking up these genders is this algorithm is trained on data our data is biased if our data is biased the algorithm has learned to pick up pick up patterns so it's gonna pick up this bias and sometimes even reinforce it so so we're gonna see all sorts of these weird behaviors I'm gonna say it's weird it's biased behavior but we should actually be aware of this if you're building these types of systems and even in addition to bias bias I can explain it you might get weird behaviors that are even harder to explain so you might have a text that looks like this like dog dog dog dog dog and then that could be just just translated to like something else that is kind of crazy so so like under like understanding what goes on and a lot of times with these kind of closed form black box systems are a little challenging and I think there's a lot of research around trying to better understand and give transparency to some of these systems and understand what goes on so that was that was language right that was translation another example is image classification so image classification has just become so much better over the years around 2015 it just hit human performance so we have image classifiers there's just much better than humans and and that is amazing right like that is really exciting because perception is a difficult problem if I can do image classification then I can use these systems of real real worlds like systems like my phone or my autonomous car and that's really exciting right right but there are again a lot of issues around this one of the issues we actually discuss this in the lecture is the idea of having adversarial examples right so I can have accent a system that does image classification and an alux net is going to classify these images on the left perfectly fine that's a school bus that's a temple like it's going to classify them correctly but then what I can do is I can add some sort of noise to them and when you add this noise to this picture you're gonna get a third picture and that kind of looks like a school bus to me like I don't know I can't tell the difference between the first and third picture but what's gonna happen is that Alex that is going to predict ostrich for all of the pictures on the on that side on the right so so that's not great right that having these adversarial examples it's not that great because the system is not really robust right you're you're AI based image classifier is not very robust when it comes to just adding these sort of adversaries another and then after this work came out basically people started writing all sorts of papers about how to create adversarial examples and how to be robust towards that particular adversarial example and in breaking that again and creating more robustness and a lot of back-and-forth one of my favorite papers actually around this area came out this year so this is from Samir and others and what they have shown is for a specific type of a neural network when you have Ray loose what they have shown is what you can do is you can always make the system classify the classify the numbers in this case as something else so so let me give a concrete example so this is an amnesty data set I have numbers in it from 0 1 2 3 4 I have 10 numbers here right and and what Shamir and others have shown is you can pick this 7 and you have 10 classes so you need at most 11 pixels so pick 11 pixels that they pick the 11 pixels carefully so it's not any 11 pixels but peeking 11 pixels and change it as much as their algorithm totally tells you and then the 7 is going to be classified as 0 so you can make this 7 be classified as any of these numbers 0 1 2 3 4 or 5 & 6 & 7 & 8 & 9 by just picking the right 11 pixels to modify and they tell you like how much you modified it's just like crazy because like give me anything I'll create this adversarial example for you two to just misclassify that's something else and you only meet eleven pixels because there are ten classes here so there are a bunch of assumptions that I haven't really discussed here like one of the assumptions is the way you're modifying these pixels is unbounded in this picture so the the greens and reds are just very high and very low so it is not actually between 0 to 255 it's it's numbers greater than 255 and less than 0 but they've actually shown that if you're allowed to have more than 11 pixels let's say you have 20 pixels you could actually fit it between 0 to 255 to make it like a realistic realistic figure anyway so lots of work around this lots of exciting theory work and practical work thinking about adversarial examples when it comes to images but yet what are the implications of this why are we so scared of this because well these systems are going to run on our phones doing image recognition in our cars doing recognition of other vehicles and they can easily be like they can easily be attacked a group at Berkeley dongsaeng's group what they did was they had these stop signs and they put stickers and stop signs again the stickers are at the right place like the place they wanted it to be but then the stop signs are are now classified as like a speed limit sign which is not what you want your autonomous car to detect or here's another example in other work where you have these pictures and you put these funny glasses on them and when you put the glasses on the pictures and they are classified as the celebrities pictures so you can easily attack these systems not easily but you can attack to systematically attack these systems and that can actually affect the security of your your vehicle or your image recognition system all right another example that's pretty challenging is around reading comprehension so so what is reading comprehension so if you remember your SAT or GRE liked I type type exams you have a text you have a lot of text and you have to read that text and you have to answer questions so you'd have a question like this so the number of new Huguenard colonists declined after one year so this is the thing you got to answer so so Google put out the system burped which actually really amazingly you can can do this this reading comprehension and birds can answer this question perfectly it's gonna say 1700 and that's great but but what people have shown is you could actually just add an extra sentence at the end of this text that has nothing to do with the rest of the text like that's something to do it has a word year in it but but it doesn't have to do anything with this particular question that's asked and now Bert is going to respond 1675 so you can again easily trick the systems and the way they're tricked is just not the same way that humans are tricked and and that is I guess weird to people and that's kind of expected but but that is something that we are dealing with these days all right so another example so I'm basically going to talk about a bunch of examples throughout the lecture the rest of the lecture so another example I wanted to briefly talk about is this idea of optimizing for clicks so so is that a good thing is that the thing we should be doing right so sometimes the objectives that we have the rewards function we are writing for our system we know what it is we want to do machine translation we know exactly what we want to do and it's very clear but sometimes it's actually not clear what we should be optimizing right like your Facebook or say wants to make money like should they optimize for clicks is that an ethical like rewards function to put in and what could be some of the effects of optimizing for clicks let's say that I have a reinforcement learning algorithm I'm making yourself let's say I have a reinforcement learning algorithm that wants to optimize for clicks and after and I have my own Facebook account and it's optimizing clicks from dorsa right so so this reinforcement learning algorithm what it can do is it can learn that well maybe if I show outrageous articles to the Ursa norsu is more likely to click on these outrageous articles and then I'm gonna get more rewards because I'm optimizing for a quick so that's all good right that's expected but another thing that the reinforcement learning algorithm by itself can figure out is that if I shall outrageous articles to dorsa diversa is going to become more and more outrageous and then I'm gonna get more quicks because then I'm gonna show more articles and it'll be great and that's kind of amazing because these systems are not interacting in a closed-loop world like during our thing with other systems like humans who are also changing or also adapting and and the system through this our algorithm by itself could figure out how to change me to like more outrageous things and then we would end up in a situation where we are right now with very bipolar views right because because you're optimizing for cliques so so it's quite interesting to think about what are the objects is we should be optimizing and what world are we dealing with you're not always like in a pac-man world where we can control everything right like usually these systems are running in a real society where there are people being affected by them and their responses are going to change and there is the changes in their responses is going to affect things even more so so it's interesting to think about these feedback loops and speaking of humans I think another thing and not another question that comes up usually when it comes to robotics orange comes today is well what is it that humans want like in general even even in the case of robotics is a big problem I have a robot arm and I want my robot arm to pick up pick up this object that's all I want right this is the thing that me as a human wants right I want the robot up to pick up and the robot arm to pick up this mobile phone so back in the day this was called good engineering right good engineering was good engineers would write down the correct reward function and the correct objective and the robot arm would go and pick up the object and everything would be great the problem is that doesn't always work right it's really hard to write the correct reward function and get the robot do that and because of that people these days are more interested in trying to do things around imitation learning or things around preference based learning where you just try to learn from how humans do it like how a human would do this as opposed to like just a human sitting down and saying well this is the object if I want you to become pickup the robot arm because because the robot might end up doing very weird things like an example of that that commonly comes up is this vacuum cleaner example let's say let's say you have a vacuum cleaner you have a robot vacuum cleaner that wants to clean your house and your objective for the vacuum cleaner is to suck up dirt that that's all it needs to do okay so you write your objective everything is great and one way that the vacuum cleaner could suck up to it is it could just go to a place suck up dirt put it out suck up dirt put it out so put it out and just keep doing that right obviously you didn't want your vacuum you do we don't want that vacuum cleaner because you didn't want your vacuum cleaner to do that or that wasn't the thing you were thinking but the objective of go suck up dirt could end up in that behavior another behavior it could end up with as you could have your vacuum cleaner and your vacuum cleaner by itself could just break its own like sensors so now it doesn't sense dirt now you're good because there are no dirts around us because you can't see them I'm gonna close my eyes so I can't see the dirt so I'm not gonna suck up anything so so all of these are things around reward hacking like if you if you just write the reward function that you think the robot should optimize it's not necessarily going to work and thinking about what are some good objectives that you should optimize is actually a really difficult problem and this is something that I'm very interested in in my group we focus on that a lot actually another work that has recently came out on this is this work by this this new book by Stuart Russell on human compatible and and basically what Stuart is kind of arguing is is the fact that there's a mismatch between what humans actually want what is the reward function that's in their head and then what is it that that the AI system or the robot thinks the human wants and then those two are not always the same thing and that could cause a lot of issues around it so interesting we'll take a look all right what else can go wrong so generating fake content that was the thing that came out a couple years back so so you could create like videos that just or images that look exactly like like Obama in this case and and you can just put fake content on that and and that that again raises an ethical question right just because you can build it should you build it or not like we can build that right like we have the we have the system to create a fake content it sounds fine but but should we do it just because we can do it another place that this question comes up and and I do encourage you guys in general to think about that in your future like when you can build something but but you do build it and and yeah another place this comes up is in autonomous weapons systems so having like thinking about military and then thinking about having autonomous weapons right like we would have we could but you can have autonomous weapons these days we can have systems that automatically detect an enemy and and automatically just stupid like just do the job right yeah you do the task so um should we do it should we have autonomous weapon systems or does there need to be a person in the loop and if so so it was just like thinking about it like let's say that yeah we do not we never want to have autonomous weapon systems and we always want to have a person in the loop well why like look what is it about the person that we want to be in the loop like that kind of tells us that there is something about the person maybe it's empathy maybe this something about what what people know or what people have that that the autonomous system doesn't have yet and just like understanding that I think by itself is a very interesting problem and there's a whole debate around this like of autonomous servant systems should we have them if we don't have them what if other countries have them like how do we go about it should you put a moratorium on it and lots of debates around these types of systems so so in general I do encourage you to think about some of these ethical aspects of building AI systems all right next stop fairness so so far this is a big problem I think a lot of you know this already right so so we might have a classifier that purr like on your majority of data set perfectly separates your majority of data set such as the the picture on the left and then you might have some data points from minority group and the classifier just does exactly the opposite thing for the minority group so so if you if you put all these data together then you're probably going to get data a classifier that looks like the first one and it's just not gonna work on the minority data set and and that is kind of that's a big problem especially like when it comes to applications like let's say health care like you might have different populations and a drug might just act very differently on different populations and the question is how should we address these fairness questions and one way to go about it is to think about our errors so so you might have two classifiers and both of them might give you five person error but one of them could give you five percent random error and could give you 5% systematic error and and I think it's pretty important to think about if you're getting systemic error random error and what type of error on what population are you getting and and that could address some of these questions around fairness there's a lot of work actually around fairness these days there's a there's a conference around it around furnace accountability and transparency this is work by more its heart so if you're interested in these take a look at some of the some of the work from where it's another example of fairness I think we did talk about this and the overview lecture is around this criminal risk assessment so it's a North Point is a company that put out the system called comm-pass and what comm-pass does is is it predicts if a criminal is going to reoffended not the risk of a criminal reoffending or not and it's going to give the score of 1 through 10 so that's what the system does and and they put out the system this system was actually being used and what happened was probably which is a non-profit came out and did a study and ProPublica showed that given that an individual did not reoffending americans were twice likely to be wrongly classified 5 or above okay so that just seemed not fair so so republic up with us puts out this article being like whoa the system is not fair why are we using this like it doesn't satisfy this fairness criteria and then Northpoint actually did further studies north when did further studies and they showed that well they said well now our system is fair because we're looking at this definition of fairness our definition of fairness is that given a risk score of seven sixty percent of whites reoffended and sixty percent of blacks reoffended so we want to make sure that we get the same percentage to be fair and that's our furnace furnace property we do satisfy that and this kind of this these two furnace definitions kind of made a group of researchers from actually Stanford Cornell a bunch of different places to start thinking about definitions of fairness and what they've actually shown is that these two definitions of fairness they are not going to be satisfied at the same time they're always going to go against each other we can't have both of them at the same time so so then if that is the case then what is the right definition of fairness that we should use right and have both of those at the same time then then how do we make sure that we can use these or should we ever use these systems again so um lots of interesting questions about formalizing furnace or moraine gold here in the cs department works a lot around the ideas of fairness from the algorithmic side of things so if you're interested in that you can take homers classes learn learn about that and I'm kind of going back to this idea of our algorithms neutral like when you talk to people who haven't taken necessarily algorithm classes or AI classes do you usually think well yeah all right algorithms gotta be neutral like they're doing math they gotta be neutral but as we've seen already they're not really neutral because by design you really want our algorithms to pick up patterns that's what they're good at they're good at picking up patterns and and biases and all sorts of weird things that we see in our data they're just in our data their patterns in our data and these algorithms just will pick them up and even reinforce them at times and that's why we see bias in our algorithms and all these issues around fairness and security and all these other things in our data and an another problem that comes up is this feedback loop that I was talking about earlier right so so if algorithms are picking up patterns well they're putting out in those patterns if they have biases they're putting out those biases in a world where there are humans and those humans are observing those biases and can get even more biased and give more biased data and and this could be like this negative feedback loop that could go forever so again we got to be really careful about what we were putting at and and what it is like how it is affecting the bigger society next stop is privacy I guess I have like a couple of more things around these and after that I'll wrap up another another thing another issue in general is privacy right so if you're using a lot of data and and in a lot of our algorithms and and in general some of them could be could be sensitive data and we don't want to we don't want to actually reveal that sensitive data so so big to address that one way to address that is instead of putting out the actual data putting out the right statistics that gives us the right information so for example you might want to compute the average statistics and like if you're asking if someone cancer or not instead of getting the yes/no answer you could just you might just need the average statistics and that would just be enough for you so in general when you're collecting data you should shoot you could randomize your data or you could change your data so you can get the average statistics as one way of protecting privacy another way of protecting privacy is in general randomize responses so so you might have a question of like do you have a sibling so that is a question you can ask a user and the user might not want to reveal exactly if they have a sibling or not so so one way of responding that is the user could flip two coins and then if both of them can come heads then they can say answer yes no randomly otherwise they can answer yes no truthfully so so based on the answer that you get from a particular user you wouldn't be able to tell if that particular user is going to have it has a sibling or not but you could actually compute the true probability of that because now you have observed this probability 3/4 of the time there by true probabilities are telling you the truth 1/4 of the time they're telling you randomly so so then you have this observed probability and then from that you can recover the true probability and that is probably enough for like the type of data that you need to deal with so so randomized responses in general could be one way of going about some of these privacy issues another issue that comes up is causality so this is a little bit and in the variable based models right so so you might want to look at the effects of something let's say you want to look at the effects of a treatment on survival and this is your data so you have 4 untreated patients 80% of them survived and 4 treated patients 30% survived this is your data so the question is does treatment help or not how many of you think treatment helps so so the answer is actually who knows because well if you think about it they're sick people are probably more likely to take treatment right like if sick people are more likely to undergo treatment then then you can't really like Texas theta like as it at its face and say well treatment helped or didn't help because your your data actually there's this extra causality that you didn't really consider the fact that well those people who took trip and they were sick so you have to actually consider that how the sickness is going to affect and the the rate of survival or not and finally the last I think this was the last thing I wanted to briefly talk about is is this idea of interpretability versus accuracy right so you've seen kind of just rise of neural networks and a lot of applications and most of them are not safety-critical applications we haven't really seen like things like neural networks and safety-critical applications I guess you've seen it in cars so we have started saying in autonomous cars but let's say airplanes or or like other types of safety critical systems healthcare systems and and one question that always comes up is should we use these systems in safety critical settings because as we were using them you're going to lose interpretability so so there's this work by Michael Cooke in their first group where they were basically looking at air traffic control and and and they're looking at the system that runs on aircraft and then previously it was basically a bunch of rules that the system it needed to follow but it was interpretable like like they could actually interpret it and understand what it does and then the system's the aircraft systems would use that but michael has been working with this new system called a Cass and a Cass Xu where they are basically trying to replace that with just let's say upon VP a partially observable Markov decision process that that does the same job but it is not necessarily this it doesn't have necessarily the same level interpretability but it's pretty accurate you can prove that it's even accurate it's not even a neural network right it is a thing that you can actually like enumerate and and the question is what are we going to put in on our safety critical systems if you lose transparency if you lose interpretability are we still willing to put these systems that we think there are statistically more accurate and in general how can we increase interpret ability and transparency of some of these systems that we are building because that is useful and we can really think about these systems so sorryi is important I think I think I've convinced you guys that AI is important and then all these different governments also think that AI is important in 2016 the White House put out an article about some of the directions that we should invest money in and a lot of them were just around the eye so making long-term investments in AI research thinking about human eye collaboration thinking about ethical and legal and societal implications of AI safety and security of AI systems so all of these things that we have been discussing so far are really challenging problems and then everyone's excited about them and everyone wants to put in putting a lot of energy and time and money in it and and in this document well the sacraments that big data analytics have the potential to eclipse long-standing civil rights protections in how personal information is used in all sorts of applications like housing credit employment and so on and Americans relationships with data should expand not diminish their opportunities and some of the things that we have discussed so far right like biases fairness safety all of these issues are not necessarily satisfying this last sentence right like if you're building these these systems we should actually be really careful about some of these implications and and as I was saying earlier like around this there's a new conference there's this fact ml conference around fairness accountability and transparency and kind of the guidelines of this new community that that's being built around AI is that we got to think about the fact that there's always a human that's behind these algorithms so there's always a human ultimately responsible behind what is going to happen and then we can't just say well the algorithm did it like in general like that's just like the wrong way of going this because there's a human designer one of you guys one of us right that's going to write these algorithms and and I do really want you guys to think about some of these principles as you go further in your in your career and you think about building this sort of AI are and just to end on a more positive note there's an enormous potential for actually positive impact for AI systems and then please just use it responsibly that I want to thank you guys for this exciting quarter and please fill out the surveys on axis thanks [Applause]