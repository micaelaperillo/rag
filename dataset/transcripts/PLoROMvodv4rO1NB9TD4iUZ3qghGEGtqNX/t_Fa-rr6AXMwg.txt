okay let's get started again so before I proceed to Bayesian networks I'm gonna talk about a few announcements so as you probably know car is due tomorrow P progress is due Thursday and then finally the kind of big one is the exam which is next Tuesday so everyone if you're in the room or if you're watching from home please remember to go to the exam the exam will cover all the material from the beginning of the class up until and including today's lecture so all Bayesian networks no logic doesn't mean you shouldn't use logic on the exam reviews are going to be in Section this Thursday and Saturday so you get not one but two review sessions the first one we'll talk about reflex and state based models and the second mode talk about variable widths models at this point all the alternative exams have been scheduled if you can't make the exam for some reason then really please come talk to us right now and just a final note is that the exam is the problems are a little bit different than the homework problems they require more kind of problem-solving and the best way to prepare for the exam is to really go through the old exams and get a sense for what kind of problems and questions you're going to be asked question where is the Saturday session does anyone know we don't know yet it will be posted on Piazza any other questions about anything I know this is a lot of stuff but this is probably going to be the busiest week and then you get Thanksgiving break so they'll be great okay so let's jump in so two lectures ago we started introducing a Bayesian networks so Bayesian networks is a modeling paradigm where you define a set of variables which capture the state of the world and you specify dependencies depicted as directed edges between these variables and given the graph structure then you proceed to define distribution so first you define a local conditional distribution for every variable given the parents so for example H given C and a and I given a and then you slam all these local conditional distributions aka factors together and then find a glorious joint distribution over all the random variables okay and you think about the Joint Distribution as the source of all truth it is like a probabilistic database a guru or Oracle that can be used to answer queries so this in the last lecture we talked about algorithms for doing probablistic inference where you're given a Bayesian network which defines this glorious joint distribution and your ass a number of questions and questions look like this condition on some evidence which is a subset of the variables that you have observed what is the distribution over some other subset of the variables which you didn't observe then we looked at a number of different algorithms in the last lecture there's a forward backward algorithm which was useful for doing filtering and smoothing queries in hmms this was an exact algorithm then we looked at particle filtering which happens to be useful in the case where your state space the number very odd values that a variable can take on it can be very large and this is an approximate but in practice it tends to be a good approximation and then finally particle filtering which sorry and Gibbs sampling which is this much more general framework for doing probabilistic inference in arbitrary factor graphs and this was again approximate okay so so far we've bit off two pieces of this modeling inference learning triangle we've talked about models we've talked about inference algorithms and finally today we're gonna talk about learning and so the question in learning as we've seen repeatedly throughout this course is where did all the parameters come from so so far we have assumed that someone just hands you this these local conditional distributions which are have numbers filled out but in the real world where do you get these and we're gonna consider a setting where all of these parameters are unknown and we have to figure out what they are from data okay so the roadmap we're gonna start first start with supervised learning which is going to be the kind of the easiest easiest case and then we're going to move to unsupervised learning where some of the variables are going to be unobserved okay so any questions before we dive in okay let's do it so the problem of learning is as follows we're given training data and we want to turn this into parameters okay so for the specific instance of bayesian networks training data looks like an example is an assignment to all the variables okay this will become clear in the example and the parameters are all those local conditional probabilities that we now assume we don't know okay so here's a question which is more computationally more expensive for Bayesian networks is it probabilistic inference given the parameters or learning the parameters given data so how many of you just use your intuition how many of you think it's the former problem inference is more expensive okay one maybe how about how many of you think learning is more expensive yeah that's probably what you would think right because learning seems like there's just more unknowns and a camp how can possibly be easier it turns out that it's actually the opposite yeah so good job and this will hopefully be a relief to many of you because I know probably the inference gets a little bit quite intense sometimes and learning will actually be maybe not quite a walk in the park but it will be a brisk stroll in the park and then when we come back to unsupervised learning it's gonna get hard again so at least in the fully supervised setting it should be easy and intuitive so what I'm gonna do now is going to build up to the general algorithm by a series of examples of increasingly complex Bayesian networks so here's the world's simplest Bayesian network has one variable in it let's assume that variable represents the rating of a movie so it takes on values 1 through 5 and to specify the local conditional distribution you just have to specify the probability of one the probability of 2 the probability of three probably four improbability five okay so these five numbers represents the parameters of the Bayesian network okay by fiddling these I can get different distributions okay so suppose someone hands you this training data so it's fully observed which means I observe one time I preserve the value of R to be one the second day I observe the value to be three third day of observe it to be four and so on okay so this is your training data okay so now the question is how do you go from the training data here to the parameters okay any ideas does use your cut yeah count and then divide or normalize okay so this is seems a very natural intuition later I'll justify why this is a sensible thing to do but for now let's just use your gut and see how far we can get okay so the intuition is that the probability of some value of ours should be proportional to the number of times it occurs in the training data so in this particular example I look at all the possible values of R 1 through 5 and I just see one shows up once two shows up zero times four shows up five times and so on so these are the counts of the particular values of R and then I simply normalize okay so normalization means adding the counts together which gives you 10 and dividing by 10 which gives you actual probabilities okay it's pretty easy huh yeah good okay so let's go to two variables so now you improve your model of ratings so now you take into account the genre so as John Rizzo variable G which can take on two values drama or comedy and the rating is the same variable as before and now let's draw the simple Bayesian network which has two variables and the local conditional distributions are P of G and P of R given G okay so again I'm going to give you some training data which specify each training example specifies a full assignment to all the variables so in this case it's D for D and this one it's D for again this one's c1 and so on and the parameters here are the local conditional distributions which is P of G M P of our given G okay okay so let's proceed to do this and here follow me our nose again we're going to estimate each local conditional distribution separately okay so this is may not be obvious why separate doing it separately is the right thing to do but trust me it is the right thing to do and certainly is the easy thing to do so let's just do that for now okay so again look if we look at P of G so we have to look at only this data restricted to G and we see that D shows up three times C shows up twice so I get these counts and I normalize and that's my estimate of the probability of G okay and now let's look at the conditional distribution our given G again I'm going to count up the number of times that these variables G and are show up so d4 shows up twice d5 shows up once and so on count and normalize question and probability are our would that cause any differences in the results like if the order worked yeah it's a good question so the question is what happens if we define our model in the opposite direction where we appeal R and P of G given R so then you would be estimating different probabilities UPS many P of R which contains one through five and then P of G given R which would be a different table so the question is if you do inference do you get the same results in this case you will get the same results in general you're not you're depending on the the model you define you're actually going to get a different distribution which will lead to different inference results yeah this happens when you have two variables and you couple them together and you do things like this way you're effectively estimating from the space of all joint distributions G n R but we if you have conditional independence and you have for example hmm to order how you write them all definitely affects what inference is going to return okay all right so so far so good so now let's add another variable okay so so now we have a variable a which represents whether a movie one own award or not we're going to draw this Bayesian network and the joint distribution is P of G P of a P of our given a and G okay so this type of structure is called a V structure for obvious reasons and this remember was the thing that was tricky this is the thing that leads to explaining away and all sorts of tricky things in Bayesian networks it turns out that when you're doing learning in them it's actually the all the trickery goes away okay so let's just do the same thing as before so we get this data each data point full assignment to all the variable so this is d 0 3 corresponds to g equals d a equals 0 and R equals 3 and the parameters are all again all the local conditional distributions and now I'm gonna count and normalize so this part was the same as before counts and genres D shows up three times seizures up twice normalized a is treated very much the same way so three movies have won no Awards two movies have won one award so the probability of movie Winnie Award is two out of five and then finally this is the local conditional distribution of our given G and a and here we have to specify for all combinations of all the variables mentioned in that local conditioner distribution I'm gonna count the number of times that configuration shows up so d01 shows up once right here d03 shows up once right here and so on okay and now when you normalize you just have to be careful that you're normalizing only over the variable that you're the local distribution is on in this case R so for every possible unique setting of GNA I have a different distribution okay so D 0 that's a distribution over 1 & 3 and if I normalize I get half and half and each of these other ones are completely separate because they have different values of GNA okay any questions oh good alright so that wasn't too bad so now let's invert the V and look at this structure where now we have genre there's no award variable but instead we have two people radio movie and the Bayesian network looks like this the genre and we have our one given G and our two given G and for now I'm going to assume that these are different local conditional distributions so I have P R 1 and P of R 2 so notice that in this lecture I'm being very explicit about the local conditional distribution here I'm instead of just writing P of G which can be ambiguous I'm writing PG to refer to the fact that this is the local at the conditional distribution for a variable G this one's the one for R 1 this one for R 2 and those are different and you'll see later why this this matters okay so for now we're gonna go through the same motions hopefully this should be fairly intuitive by now you simply count normalize for the P of G and then for R 1 I'm just going to look at the first or the second element here which corresponds to R 1 and ignore our two right so gr 1 so d 4 shows up twice so i have d 4 and D for that shows up twice d 5 shows up once that's a d5c one shows up once and c 5 shows up once okay and then normalize get my distribution and then same for r2 now ignoring R 1 I'm gonna look at how many times did G equals D and R 2 equals 3 okay so you can think about each of these is a kind of a pattern that you sweep over the training set so you have D 5 so that's a 1 here and d 4 that's a 1 here and D 3 that's a 1 here and so on and so forth ok and then you normalize okay how many of you are following this okay cool alright so now things I'm gonna make things a little bit more interesting so here I've defined different local conditional distributions for each rating variable r1 and r2 rights but in general maybe I have r3 and r4 and r5 and have maybe a thousand people who are rating this movie I don't really want to have a separate very distribution for each person so in this model I'm going to define a single distribution over rating conditioned on genre called PR and this is where subscribing with the actual identity of the local conditional distribution becomes useful and this allows us to distinguish PR from this case which is PR one and PR 2 okay notice that the the structure of the graph remains the same so you can't tell from just looking at the Bayesian network you have to look at carefully at the parameterization okay so if I just have one PR what I'm going to do is what do you think the right things should to do is if you were just following your nose I'm sorry yeah so count both of them I think is what you're saying so you combine them right so so P of G is the same and now I'm going only have one distribution I need to estimate here are given G and I'm going to count the number of times in the data where I'm using I have a particular value of G and a particular value of R okay and I'm going to look at both R 1 and R 2 now okay so D 3 shows up once here so that's our to d4 shows up three times so once here with r1 once here with r1 and once here with our two and so on I'm not gonna go through all the details and then you counted normalize another way you can see this is that if I take the counts from these two tables I'm just kind of merging them adding them together and then normalizing question the ours are so is this assuming something about independence here so here when I'm doing I am assuming the data points are independent first of all and moreover I'm also assuming the conditional independence structure of the the Bayesian network is true so conditioned on g r1 and r2 are independent yes so here I'm also assuming that r1 given G has the same distribution as r2 given G and this is part of the when I define the model that way I'm no making that assumption okay so this is a general idea which I want to call out which is called parameter sharing and parameter sharing is when the local conditional distribution of different variables use the same parameters so the way to think about parameter sharing is in terms of powering so you have this Bayesian network it's hanging out here and behind the scenes the parameters are kind of driving it right and you can think about the the parameters as these little tables sitting out there and you connect a table up to a variable if you say this table is going to power on this node okay so what parameter sharing in this particular example is saying this distribution of G powers this node and these two variables r1 and r2 are going to be powered by the same local conditional distribution okay so okay I'm gonna go to two more examples maybe I'll draw them on the board to make this a little bit more clear so remember the naive Bayes model from two lectures ago so this is a model that has a variable that represents adapted to our movie genre setting we have John reviewable which takes on variable values comedy and drama and then I have a movie review which represents a document with L words in it and each word is drawn independently given Y so I have so in the joint probability is therefore going to be probability of genre Y times the probability of WJ given Y for all words Jade from 1 to L okay so so the way to think about this graph the Bayesian networks is you have a variable Y here and so I'm just going to draw with w1w - w3 and I'm going to have a local conditional distribution here which is why P genre of Y so that's some table that's powering this node and then I have a separate one single other variable sorry local conditional distribution of doublets y Y and W and probability of what I call it word word W given Y and this distribution powers these variables okay so notice that here there's two local conditional distributions we have genre and we have word even though there are L plus one variables in the Bayesian network yeah yeah so the input water so the question is what is the input to P word W given Y so it when you apply this to a particular variable in some sense you bind why - why and you bind W to the particular WI at hand when W and you can see this kind of mathematically where you here we have you pass into the P word W J given Y and J ranges from 1 to L just to kind of solidify understanding let me ask the following question if Y can take on two values as in here and each word can take on D values how many parameters are there in other words how many numbers are in these two tables on the board so shout out the answer okay so 2d plus news so there's two here right so they're CG there's two numbers and then they're for every C there's D possible values for D there's D possible values so there's two plus so there's two here and then there's D plus D here okay now if you really want to be fancy and count the number of rama's you really need it should be less than this because if I specify the probability of C then you can 1 minus that probability is probably D but let's not worry about that okay let's do the same thing for hmm s just to make sure we're on the same page here since we all love hmm okay so in hmm remember there's a sequence of hidden variables H 1 H 2 H 3 and for each hidden variable we have e1 e2 and e3 which are the observed variables and there are 3 types of distributions for each amounts there is the probability of I'm going to call P start of H which is going to specify the initial distribution over h1 I'm going to have the transition probabilities which I'm going to denote age it's called eighth prime probability of H prime should write down P trans here H prime given H so this is going to be another distribution which powers each transition each non initial hidden variable okay remember these are pointing two variables not edges or anything else and finally I'm going to have a distribution the mission distribution of e given each that table is going to power each of observed variable Z okay and here again there are three types of distributions start transition in emission even though there could be any number of variables in actual Bayesian network okay and just to be very clear about this when I apply this table to this node H binds to H 1 and H 2 H prime binds to H 2 and then when I apply it to this node H 2 binds to H and H prime binds to H 3 and again you can see this from the formulas where I'm passing in to P trans H I given H I minus 1 as I sweeps from 2 to n ok so you can think about this is like a little function with local variables the argument to that that function are H and H prime but when I actually called this function so to speak when defining a Joint Distribution I'm passing in the actual variables of the Bayesian network okay any questions about this okay maybe just summarize some concepts first okay so we talked about learning as the generically the problem of how we go from data to parameters and data in this case is full assignments to all the random variables in the hmm case it's a data point is a assignment to all the hidden variables and the observed variables and parameters usually denoted theta is all the local conditional distributions which are these three tables in the case of hmm okay the key intuition is count and normalize which is intuitive and later I'll justify why this is an appropriate way to do parameter estimation and then finally parameter sharing is this idea which allows you to define huge Bayesian networks but not have to blow up the number of parameters because you can share the parameters between the different variables okay so now let's talk about the general case so hopefully you kind of understand the basic intuition so this is just going to be some abstract notation that's gonna kind of sum it up so in a general Bayesian network we have variables X 1 through X and and we're gonna say that parameters are a collection of distributions so theta equals P a sub D ok and so Big D is going to be the set of types of distributions so in the hmm case it's going to be three types start trans and omit so basically that's the number of these little boxes that I have on the board are and the Joint Distribution when you define a Bayesian network is going to be the product of P of X i given X parents of I so this is the same as we had before but now notice the crucial difference which I've outlined in red here is that I'm subscribing this p with a di okay so what di for the ithe variable says is which of these distributions is powering that variable okay so D of this variable is omit D of this variable is transition and D of this variable is start okay so this is looks maybe a little bit abstract and notationally but the idea is just to multiply in the probability is that was used to generate that a variable or power that variable okay and parameter measuring just means that di could be the same for multiple eyes yep model case wish the emission probabilities are all the same like why do we need multiple missions it's like we're going to be the same it's just drawing from the same distribution three times yes so the question is if we only have one emission distribution why do we need so many of these replica copies and the reason is that these variables represent the objects locations at a particular time so the value of this is going to be different based on what time step you're at but the mechanism for generating that variable is the same just like if I flipped a coin you know ten times I only have one distribution that represents the probability of heads but I have ten realizations of that random variable another analogy might be helpful is think about really well these as like a primitive like functions in your program that you can call right this is like a sorting function and sorting is just used in a whole bunch of different places but it's kind of the same kind of local function that powers a bunch of different use cases which are specific to the context yeah okay so in this general notation what is the learning look like so the training data is a set of four assignments and I want to output the parameters so here's the the basic form its Celt and normalized so in counting there's just a bunch of for loops so for each training example which is X is a full assignment and I'm going to look at every variable I'm going to just increment a counter which of the DI distribution of this particular configuration X parents I and X I okay and then in the normalization step I'm going to consider all the different types of distributions and then I'm going to consider all the possible local assignments to the parents and I'm going to normalize that distribution yeah yes so the DI refers for the I variable which read table I'm looking at okay so I've given you already a bunch of examples so hopefully this the notation might be a little bit loose but hopefully you guys already have the you know intuition any questions moving on the main point of this slide is just to say that this is actually very general and I just didn't do it for hidden Markov models now you phase-in it doesn't work for anything else okay so now let me come back to the question of you know why does count and normalize make sense right so this is kind of normalize this is like you know some made-up procedure so why is it a reasonable thing to do and it turns out this is actually based on very firm kind of foundational principles this idea of maximum likelihood which is an idea from statistics that says if I see some data and I have a model over that data I want to tweak the parameters to make the probability of that data as high as possible okay so in symbols what this looks like is I want to find the parameters theta so remember parameters are probabilities in the red tables on the board and then I'm going to look at make the product of all over all the training examples the probability of that assignment so for every possible setting of the parameters I can a stat assigns particular probabilities to my training examples and I want to make that number as high as possible okay and the point is that the algorithm on previous lie exactly computes this maximum likely so the parameters in closed form so which is really nice if you think about when we talk about machine learning we define a loss function and you can't compute anything in closed form except for maybe linear regression and you have to use stochastic gradient descent to optimize it here it's actually much simpler because of the way that the model is set up you just counted normalize and that is actually the optimal answer so just because you write down a max doesn't always mean you have to use gradient descent is a lesson here okay so let me I'm not gonna prove this in generality but I want to give you some intuition why this is true and hopefully connect the kind of the abstract principle with on the ground algorithm that you of counter normalizing so suppose we have a two variable Bayesian network with genre and rating so I have three data points here and I have this maximum likelihood principle which I'm no going to follow and let's do some algebra here so I'm going to expand the Joint Distribution and remember Joint Distribution is just the product of all the local conditional distributions and I've also expanded this you know the product over these three instances so I have the probability of D probably a for given D probability of D here probably a v given D probably of C I'm probably a v given C okay and what I'm maximizing over here right now is a distribution of Roshanara and I have a distribution of rating conditioned on genre as C and the distribution of rating condition genre equals D okay so I've color-coded these in a certain way to emphasize that that all the red touches is is the local conditional distribution corresponding genre the blue is corresponds to probability of rating given genre codes D and green is probably of rating given genre Cosi okay so now I can shuffle things around okay and I notice that these factors don't actually depend on probability of G at all so they can just hang out over here and I've essentially and and this likewise on if I'm thinking about the maximum over you know argument see these other factors are just constant so don't really matter either so I basically reduce this as a problem of three independent maximization problems okay and this is why I could take each local conditional distribution in turn and do count and normalize on each one separately okay so and then the rest is actually you know to do it actually properly you have to use Lagrange multipliers to to solve it but intuitively hopefully you can either do that or just believe that if I ask you what is the probability best way to set probabilities so that this quantity is maximized I'm going to set probability of D equals 2/3 and probably if C equals 1/3 this is similar to one of the questions on the Foundation's homework if you not remember oh but only for a probability of a coin flip it essentially okay so hopefully some of you can now rest you know sleep at night thinking that if you do count and normalize you're actually obeying some high-minded principle of maximum likelihood okay so let's talk about Allah plus smoothing so here's a scenario if I have give you a coin and I said I don't know its probability of heads but you flip it a hundred times and 23 times its heads and seventy-seven times its Hale's what is the maximum likelihood estimate going to be yeah so it's going to be probably of heads is you know counting normalize its 23 over 100 which is 0.23 probably if tails is point seven seven okay so seems reasonable right so what about this so you flip a coin once and you get heads what's the probability of heads so the maximum likelihood says 1 & 0 so you know some of you are probably thinking you know smiling and probably thinking this is a very closed-minded thing to do right just because you saw one has it's like oh okay the probability of heads must be one tails is impossible because I never saw it so seems pretty you know foolish and intuitively you feel like well tails might happen you know sometimes so it shouldn't be as stark as 1 0 okay and this is an example of overfitting which we talked about in the machine learning lecture where maximum likelihood will tend to just maximize the probability and here it does maximize the probability because the probability of data is now 1 and you can't do better than that but this is definitely overfitting so we want a more reasonable estimate so this is where Laplace smoothing comes in and again I'm going to introduce the plus smoothing from kind of a fall your nose kind of framework and then I'll talk about why it might be a good idea okay so here's maximum likelihood just a number of times head occurs over a total number of trials you have so Laplace smoothing is just adding some numbers so look this is applause named after the famous French mathematician who did a lot more than add numbers like applause transform and well plus distribution but we're only going to use to talk about his adding numbers invention I guess so so here in red I'm shown that for this problem estimate no matter what the data is I'm just going out of one here I don't care what the data looks I'm just gonna add a one I'm gonna divide by the total number of values are which are possible which is two heads or tails and four tails I'm also gonna add a 1 I don't care what the data says and I'm gonna divide by two okay so now I get 2/3 and 1/3 which should be a more you know sensible intuitively sensible estimate if you're gonna come up with any sort of estimate it says well I saw heads so probably more than 50% it's going to be heads but it's probably not you know 100% ok so let's look at it in a slightly more complicated settings so here I have two variables and Laplace smoothing is driven by a parameter of lambda which by default is going to be one but it can be any number non-negative number and what the plus moving amounts to is saying start out with your tables and instead of filling them with zero fill them with lambda okay so here's my table before I even look at the data I'm just going to put ones down and then when I look at my data I'm just going to add to that counter so D shows up twice C shows up once and then accounting normalize okay same over here before any look at any data I'm just going to populate with ones which is lambda and then I get my three data points and I add the three counters which are shown here in a counter normalize so by construction no event should have probability 0 unless m to 0 because I already start with one and one divided by some positive number is not zero okay so the general idea is for every distribution impartial assignment I'm going to add lambda to that count you know and then I'm just going to normalize to get the probability estimates okay so the interpretation of Laplace smoothing is you're essentially you know pretending that you saw lambda occurrences of every local assignment even though you didn't see in your data you're hallucinating this this data sometimes it's called pseudo counts or virtual counts and the more higher lambda is the more hallucination or more smoothing you're doing and that will draw the probabilities closer to the uniform distribution and the other extreme lambda equals zero is simply maximum likelihood but in the end you know data wins out so if you had Laplace moving and you saw one head and now you're saying estimating the probability to two-thirds but suppose you keep on flipping this coin and it keeps on coming up heads 998 times then by doing the same Laplace smoothing you're going to eventually update your probability for 0.999 you're never going to reach 1 exactly because no probabilities are going to be ever 0 1 but increasingly you're going to be much more confident that this is a very very brave coin ok any questions about Laplace me so what plus smoothing is just add lambda to everything essentially and oh so I forgot the principle behind Laplace smoothie is if you think in terms of this is kind of beyond the scope of this course but you can think about a prior distribution over your parameters which is some uniform distribution and instead of doing maximum likelihood you're doing map or Mac maximum a pastori estimation so there is another principle but you don't have to worry about it for this class yeah the question is how big do you make lambda it's a good question so one is lambda should be small it probably shouldn't be like 100 probably should be more like 1 or even you can make it less than 1 maybe it's trivial like point 0 1 or something depends on you know how I guess uncertain you know where you are so in general that these priors are meant to capture what you know about the kind of the distribution okay all right so now we get to the fun part on this is going to in some sense combine learning which we've done with probabilistic inference which we did before and the motivation here is that what happens if you don't observe some of the variables so far learning has assumed that we observe complete assignments to all the random variables but in practice this is just not true right the whole reason people call them kidding Markov model is is that you don't actually observe the hidden States so what happens if we only get observations or in this simpler example what happens if the data looks like this where observe the ratings but we don't observe their genres so what can you do so obviously the count and normalized thing doesn't work anymore because you can't count with question marks so there is again two ways to think about what to do here the high-minded principle is appeal to maximum likelihood and make it work for unobserved variables and the other way to think about it which I'll come to later is simply guess what the latent variables are and then do count and normalize okay and those these two are going to be equivalent so let's be high-minded for now and think about what maximum marginal likelihood okay so in general we're going to think about H as the hidden variables and E is observed variables in this case G is hidden and r1 and r2 are observed and suppose I see some evidence e so I see r1 equals 1 r2 equals 2 but I don't see the value of G and again the parameter is all the same as same as before I just have less data or information to estimate the parameters okay so if you're following maximum likelihood what is maximum likelihood say it says tweak the parameters so that the likelihood of your data is as large as possible so what is the likelihood of the data here it's simply instead of H and E I have probability of E okay so this seems like a kind of a very natural sound extension of maximum likelihood and it's called maximum marginal likelihood because this quantity is a marginal likelihood right it's not a joint likelihood or joint distributions as a marginal distribution over only a subset of the variables okay so now to unpack this a bit what is this marginal distribution it's actually by axioms of probability it's the summation over all possible values of the hidden variables of the Joint Distribution so in other words you can think about maximum marginal likelihood is saying I want to change the parameters in such a way that such that the probability of what I see as high as possible but what that really requires me to do is think about all the possible values that H could take on I don't see it so I have to consider what ifs what if it were see what if you were D and so on okay so in other words fundamentally if you don't observe a variable you have to consider possible values of them all right so now let's skip to the other side the kind of a scrappy way and think about what is a reasonable algorithm that makes sense and I'm not gonna have time in this course to kind of show the formal connection but if you take you know CS 228 or a graphical modest class it will go into this in much more detail so the intuition here is the same as what we had for k-means so remember in k-means you tried to do clustering you don't know the the centroids and you also don't know the assignments so you it's a chicken-and-egg problem so you go back and forth between figuring out what the centroids are and figuring out what the assignments are and the centroids are going to be an analog of parameters here and assignments are going to be the analog of the hidden variables okay so here's here's algorithm it's called expectation maximization it was no formalized in its generality and in the 70s and you start out with some parameters maybe initializing to uniform or uniform plus a little bit noise and then it's going to alt we're going to alternate between two steps that each step and the M step and you it's useful to think about the k-means in your head while you're coming through this algorithm so in each step we're going to guess what the hidden variables are or what the values of the hidden variables take on so we're going to define this Q of H which is going to be or represent our guests and since we're in probability land we're going to consider the distribution over a possible values of H and this guess is going to be given by our current setting of parameters and the evidence that we've observed in our fixing so we're asking the question what is the probability of the hidden variables taking on particular values of H given my data and given my parameters so this should this should look kind of familiar to you right what is this there's just probabilistic inference right which we've been doing for the last lecture which means that you can use any problem sick inference algorithm here you can do forward backward you can do gibbs sampling and that's kind of a sub-module that you need to do i'm am ok ok so now what happens if we have our setting of or guess over the possible values H now we make up data so we create weighted points of with particular values of H and E and each of them gets some weight q of H and then finally once we're given these weigh two points now we're just going to use do count and normalize and do maximum likelihood ok so I'm going to walk through an example to make this a little bit more grounded out so I'm going to do this on a board because it's gonna get a little bit maybe a little bit hairy okay let's let's do this so here we have a three variable [Music] Bayesian so we have let's draw it over here actually might need a space so we have G we have r1 and r2 okay and our data that we see is we get data which is two two and one two okay so I observe two two that's one data point and one two that's another data point okay so initially what I'm going to do is start with some setting of parameters okay so I'm going to start with my parameters theta which specify a distribution over G and for lack of information I'm going to consider just half and half let me write point five to be consistent so I'm initializing unit with a uniform distribution and my other table here is going to be G and R so probability of our given G and here I have seen the possible values of G and the possible values R which for simplicity I'm going to assume to just be one and two and then for this I have for these numbers 0.4 0.6 and 0.6 0.4 so I'm I'm not sending them to uniform but adding a little bit of noise so hopefully people can check that I'm the numbers on the board are the same as the numbers on the slides okay so so that's my initial parameter setting so now in the e step I'm gonna use these parameters and to guess what G is okay so what does this look like so for each possible data point so for every example so I have two two that's one data point I'm gonna try to guess what the value of G is so it could be C it could be D and for the other data point one two it's also could be C or could be D okay and now I'm going to try to compute a weight for each of these data points based on the model okay so the weight is not now look look at this this is cool right because I have a complete assignment and you know what to do with complete assignments I can just evaluate the probability of that assignment okay so now just just to make things concrete I have positive G times probability of r1 given G probability of our - given G this is by definition the probability of G so this is Joint Distribution by definition of this evasion Network it's this okay so how do i compute the probability of this configuration well it's a probability of G equals C so I look at this table that can be 0.5 times the probability of r1 given G that's 2 and C so if you look over here that's a 0.6 and then another r2 given G that's also a 2 and a C so that's 0.6 which has a parameter sharing and that's a point 1 age right let me to slide so you guys can check my work ok so now I look at this table so probably ability of G equals D is 0.5 and then the probability of r1 equals to given G equals D well if look in this table that's 0.4 and then I have another point for from another two and that is 0.08 and then I can normalize this question DQ versus weight six for c2 so why is this point four versus point six here this is because I initialize the parameters so that the probability of R equals two Givens G equals D is point four why did I use this internationalization just for fun I just made it up just so that if I had put point fives here then you couldn't really tell what was going on and also as a side market wouldn't you know work initialization is generally going to be random but as close to uniform as as possible yeah okay so this is this is a column is going to be the probability of basically G equals G R r1 equals r1 r2 equals r2 okay and then the Q is going to just be is just going to be the weight of this and normalizing these two which means you add them up and you divide and then you get was a point six nine and three one sorry the numbers are a little bit kind of awkward but that's what you get there I'm not going to do the second roll but it's it's basically the same type of calculation question Asian kind of like particle filtering in that like because step you do like the proposal and then waiting and then that is like the recent resampling like finding them accident the question is is each and expectation-maximization because you have this proposal when you're waiting structurally it's quite different I mean there is a sense like if there are some they're proposing different options but certainly the two algorithms are meant to solve very different tasks one is for learning or one is for probabilistic inference yeah okay so let's look at the m-step now okay so the m-step I'll let you guys fill this end so the m-step I actually want to keep this up let me do that I'm step over here is going to now just take these examples so these weights are point five so it's it's like someone handed you fully labeled data complete observations for points but each of observation has a weight so now the only difference to encounter normalized instead of just adding one whenever you see a particular completists configuration you're gonna add the weight okay so for each of the parameters I'm going to look at so this is going to be G probability of G so this can be either C or D and to get this I have let me first do the count yes well okay let me just add it here so I look at my data so let me mark this so this is now my kind of I'm gonna put data in quotes because I didn't actually observe it I just hallucinating it and these are the weights which are associated with the data points so now I'm going to look at yes look at G equals C so I see see here see here and I have the weights point six nine six nine plus point five probability of so you just count and then for D I see a D here and I see a D here that's point three one plus 0.5 and then I normalize this and I get my estimate okay so on the slide this is exactly what I did here I count and normalize and I'm gonna and I'm gonna do this table by you hopefully get the idea yep yes so in the each step you have to estimate for all possible assignments to H so in this case H is only one variable in the next example H is going to be in a hidden Markov model all the variables and we're gonna see how we can deal with that yeah playing briefly how we get the values in the second table the second table which this one this one yeah yeah sure just briefly so you look at these data points and you see c1 okay so where does c1 show up so G equals C here R 1 equals 1 here so that's a point 5 here and then what about C 2 where does C to show up C 2 shows up twice here with R 1 and R 2 so that's why there's two of point six 9s here and C two shows up once here with a weight of 0.5 yeah everything here these counts are based on the table from okay all right so let's do something a little bit fun now so the copula cipher is this 105 page encrypted volume that was discovered and people dated back from the 1730's so it looks like this so it's unreadable because it's actually a cipher it's not meant to be it just read and for decades people are trying to figure out you know what is the actual message that was inside this text it's a lot of text and and finally in 2011 some researchers actually crack this code and they actually use e/m to help do this so I'm going to give you a kind of a toy version of using a.m. to do decipherment and it turns out that this code is basically some book from a secret society which you can go read about on Wikipedia if you want so substitution ciphers a substitution cipher consists of a substitution table which specifies for every letter assume we have only 26 right now a cipher letter okay and the way you apply a substitution table is you take a plain text which generally is unknown if you're trying to decipher like let's say hello world and you look up each letter in the substitution table and you write down the corresponding thing so H maps to n so you write n e maps to M so you write down to M and so on and so someone did this and then they obviously didn't give you the plaintext they give you the ciphertext and so now all you have is a ciphertext and obviously didn't give you the substitution table either so you all you have is the ciphertext and you're trying to figure out both the cipher a substitution table and also the plaintext okay so hopefully this pattern matches something and let's try to model this as a hidden Markov model okay so here the hidden variables are going to be the characters in the plaintext this is actual sequence that the hello world and then observations are the characters of the ciphertext so familiar equation the Joint Distribution of the hidden Markov model is given by this equation and we want to estimate the parameters which include P star P trans and p.m. it okay so how do we go about doing this so we're going to approach this in a in a slightly different way I guess then simply just running the algorithm because we have additional structure here right so the probability of start I have no idea which is just set it to uniform the the probability of transition so this is interesting so normally if you're doing M you don't know the problem of transition but because we know let's say we know that the underlying text was English we can actually estimate the probability of a war character given a previous English character from just English text lying around so this is cool because it allows us to hopefully simplify the problem I should maybe comment that in general and supervised learning is really really hard and just because you can write down a bunch of equations doesn't mean that'll work and it's very hard to get it to work so you want to get as much kind of supervision or information as you can ok so finally e p.m. it is the substitution table which we're going to derive from yeah ok so now this might not type check in your head because a substitution specifies for every letter one other letter the cipher letter but in order to fit it into this framework we're going to think about a distribution over possible cipher letters given a plaintext letter okay with the intention that well you know if it's doing its job it can always put a probability 1 on the actual cipher letter okay so just you know set me back away from the formulism why might you think this could work and in general this is not obvious that it should work but but what what you have here is a language model P transition which tells you kind of what plaintext looks like right if you generated a cipher and you say yeah this is my cipher and you get some garbage that it's probably not right so we have some information about what we're looking for like so like when you solve a puzzle you know when you kind of solved it and then finally we also have this omission that tells us that each letter that E has to go to kind of be substituted in the same way so you can't have you going to you know different completely different things at different points at the time okay so so for doing estimation hmm we're going to use the EMF Witham and remember in the estep we're just doing probabilistic inference and we saw that forward-backward it gives us probably stick inference in particular it gives you for every position I'm going to give you my guess which is a distribution of our possible hidden variables so remember I each position eyes observe the cipher letter and I'm going to guess a distribution over the plaintext letter okay and then the m-step I'm just going to count and normalize as we did before so once I've guessed the cipher letters I can just compute the probability of some cipher letter given a plaintext letter okay so I'm actually going to code this up and see it so you can see it in action okay I only have five minutes here so to make this quick okay so here we have cipher text okay looks pretty good we're gonna decrypt this or decipher it and then we also have our text which is just some English text that we found and then I'm going to I want to decipher an island cipher okay so there's some utilities which are going to be useful so things for reading texts converting them things into integers normalization of weight and then most importantly this implementation of a forward backward algorithm which we're going to use okay because I'm not going to try to do that in five minutes okay so let's initialize the hmm and then later we're gonna run them on it okay so the hmm parameters are there's going to be start probability which is P in our notation probability of start and this is going to be 1 over the number of possible letters here for this is just a uniform distribution 1 over K and I should say K is 26 plus 1 so this is lowercase letters plus space okay so now we have our transition probabilities and I'm going to this is going to define a distribution over hidden variable an X hidden variable given previous hidden variable and notice that the order is reversed here because I want to first condition on h1 and then this thing is going to be a distribution so to do this I'm going to first remember my strategies I'm gonna use the language model data to count so I'm going to have again set things to 0 for H 2 inch range K for h1 and range KITT so this basically gives me a matrix of K by K matrix of all zeros and now I'm going to get my raw text and I'm going to read it from train which remember looks like this and I'm gonna convert this into a sequence of integers where they're going to be 0 to K minus 1 and then I'm going to how do i estimate the transition again Calton normalize so I'm going to go through this sequence and and I'm going to count every successive transition from one character to another and I'm going to so I'm going to add position I I have a character which is brought X of I at position I plus 1 I have another character h2 and I'm just going to increment this count okay and finally I'm going to normalize this distribution so transition probably calls so for every h1 I have I have a transition counts of h1 and I can call the helpful normalize function which takes this distribution and normalizes it ok so this is just doing fully observed to maximum likelihood count normalize on just the plain text okay all right so now emission probs this is going to be probability of emit of a given H and I'm going to just initialize these to uniform because I don't know any better so 1 over K for ian's range K for range K so just it happens that both that hidden variables and observed variables have the same domain that's not generally the case okay so now I'm ready to run yeah so can it do two hundred iterations just just put in a number so have the e step in the m-step so in the east step remember I'm going to do probabilistic inference I'm going to call forward backward and remember this is this is going to be basically in our notation at the position I what is my distribution over hidden States so this is actually forward backward oh I need to read some observations I read my ciphertext ciphertext and I'm going to convert this again into an integer array and then I have observations and then I pass in the parameters of my hidden Markov model and then I have my gas so let me print out what that guess is okay so what I'm going to do here is for every position I'm going to get the most likely outcome of each so util of arc max of Q I for I in range length of observations okay so that gives me an array of guesses for each position and then I'm gonna convert that into a string so it's easier to look at and put in your line so this is printing the best guess okay so finally for the m-step I'm going I have my Q which gives me counts so now I pretend I have a lot of you know data which are weighted by Q so I'm going to have emission counts equal as same as before I'm just going to get a matrix of zeros even range k4h and ring okay and then I'm going to go through the sequence from I equals range of see the length of the observation sequence and for each position in the sequence I'm going to consider all the possible possible plaintext hidden values and I'm going to increment H observations of I so this is the observation that I actually saw and this should be qi h which is the weight of 8 h at position i okay and then finally I'm just going to normalize so like what I did well okay I've just normalize that here so emission probabilities is util dot normalize of the counts for H and range okay okay and I think that's pretty much it let me see if this okay this should be for for what right okay okay good okay so let's run this and then I'll go back to there let me just look over the code just one more time okay so so we initialize the the probabilities to uniform for transition probabilities we can use the observations or the just a plain text we just count and normalize in emissions we initialize with uniform and then what running am we read the cipher text and then in the east step we're going to use the current problem parameters to guess where the cipher text is and then we're going to update our estimate of what the parameters are given our guess of what the cipher text is okay so here's the final truth the ciphertext and so remember each iteration it's going to print out the best guess so it will look like gibberish for a little bit it's not gonna be perfect but this is starts to look somewhat like English there's a hand and in my in there anyone can read this alone without okay that looks like English maybe this is like anyone that I could you want to guess what this text is so here's the plain text so I've lived my life alone without anyone that I could really talk to until I had I'm accident with my plane and the does with my whatever something but but so again unsupervised learning doesn't it's not magic it doesn't always work but here you at least see some signal and in the actual application they got partway there and then you can iterate on this man kind of in a man your way okay all right so that's for it for Bayesian networks on Wednesday with your logic