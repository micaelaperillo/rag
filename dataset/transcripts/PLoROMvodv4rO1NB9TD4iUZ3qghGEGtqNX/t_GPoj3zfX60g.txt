alright let's get started so we're gonna continue talking about Bayesian networks which we started on Monday and just a kind of quick recap we've been talking about Bayesian networks which is a new paradigm for defining models and what is a Bayesian network you have a set of variables which are nodes in a graph for example whether you have a cold way they have allergies where you're coughing whether you have itchy eyes these nodes are related by a set of directed edges which capture various dependencies for example itchy eyes is caused by allergies but not cold or bike off and then formally for every variable in the Bayesian network you have a local conditional distribution which specifies the distribution over that variable given the parents so the parents of cough are cold and allergies so it you would have a local conditional distribution of P of H given CNA you do that for all the variables and finally you take all the factors or a local conditional distribution and you multiply them together and you can one glorious joint distribution over all the possible variables in your distribution okay so in other words to sum it up you can think about Bayesian networks as factor crafts plus probability they allow you to define ginormous joint distributions over lots of random variables using factor graphs which allow you to specify things very compactly and moreover we saw glimpses of how we can use the structure of factor graphs to permit efficient inference so probabilistic inference in bayesian networks is the task of given a Bayesian network which is you know this Oracle about the what you know about or the world and you look at some evidence that you've found so it's you know it's raining or not raining or or you have itchy eyes or so on and you condition on that evidence and you also have set of query variables that you're interested in asking about and the goal is to compute the probability of the query variables condition on the evidence that you see big e equals literally remember Laura uppercase is random variables lower case is is actual values and so for example in the coughing case there's the probability of a cold given the fact that you're coughing but don't have a chi is okay and this problem this probability is defined by just the laws of probability which we went over the first slide of last lecture and the challenge is how to do this efficiently okay and that's going to be the topic of this so any questions about the basic setup of what Bayesian networks are and how do you what is mean to do for all the skin friends okay one maybe kind of a high-level note about Bayesian networks is that I think they're really powerful as a way to describe time knowledge I think a lot of AI today is focused on particular tasks where you define some output inputs and you define some outputs and you train a classifier and the classifier that you train can only do this one thing input output but the paradigm behind Bayesian networks and databases in general is that you have developed a kind of knowledge source which can be probabilistic it's captured by this Joint Distribution and once you have it you use those tools of probability to allow you answer an arbitrary questions about it so you can give me any pieces of evidence and any query and it's clear what I meant to do I'm supposed to compute these values so this it's kind of a more flexible and powerful paradigm than just you know converting inputs outputs and so that's why I think it's so interesting okay so today we're going to focus on how to compute these arbitrary inference query efficiently I'm gonna start with forward-backward and particle filtering these are going to specialize to the specific Bayesian networks called HMMs hidden Markov models and then we're going to look at Gibbs sampling which is a much you know more general way of doing things okay so a hidden Markov model which I talked about last time which we're going to go in more detail at this time is a Bayesian network where there exists a sequence of hidden variables and a corresponding sequence of observed variables so as a kind of motivating example imagine you're tracking some sort of object in your homework you'll be tracking cars so H is going to be the location of the object or car at a particular time step I and II I is going to be some sort of sensor reading that you get at that particular time step it could be the location plus some noise or some sort of distance to that the true object and so on okay so these are the variables and you know it goes well it's saying that the hidden variables are hidden and the observed variables are observed so the distributions over this all the variables is specified by three types of local conditional distributions the first one is just the starting distribution what is the probability of h1 this could be a uniform over all possible locations just as an example and then we have the transition distributions which specify what is the distribution over a particular hidden variable the location of the true location of object h i given h i minus 1 so this captures dynamics of how this object or car might move over time for example it could just be a uniform over adjacent locations right so cars can't teleport they can only move to adjacent locations over a one time step and finally we have emission distributions which govern how the sensor reading is computed as a function of the location okay so this again could be something as simple as uniform over adjacent locations if you expect to see some noise in your sensor the sensor doesn't tell you where exactly the car is but tells you approximately where the car is and the Joint Distribution over all these random variables is going to be given by simply the product of everything you see on the board I'm just gonna write this up on the board just for reference so we have the probability of H equals H so this when I write H equals H that means that h1 through hm so all the random variables all the hidden random variables all the observed random variables and this is my definition equal to the the start distribution okay so start distribution over h1 and then I have the transitions I equals 1 I guess to 2n okay so this is the probability of H I given H I minus 1 and then finally I have for every time step I through one through n I have the probability of observation given H I so multiply all these factors together that gives me a single number that is the probability of all the observed and all the hidden variables any questions about the definition of a hidden Markov model okay so given we have one of these models remember with a Bayesian network I can answer any sort of queries I can ask what is the probability of H 3 given H 2 and E 5 and I can do so all sorts of crazy things and all these things are possible and efficient to your compute but we're gonna focus on two main types of questions I'm motivated by the let's say object tracking or example the first question is filtering filtering says you're in a particular time step let's say time step 3 what what do I know about the true object location H 3 given all the evidence I've seen up until now so this is kind of a real time object tracking at each point in time you look at all the evidence and you want to know where the object is a similar question is smoothing and you're still looking at a particular time step 3 let's say but you're conditioning on all the evidence so you're looking at all the observations and you're looking at you're kind of thinking about this more retrospectively where was object where was object at time step 3 so think about if you're trying to reconstruct the trajectory or something ok so this is called filtering and smoothing so let's now try to develop an algorithm for answering these type of queries and without loss of generality I'm going to focus on answering smoothing queries so my why is it the case that if I tell you I can solve all smoothing questions I can also solve all filtering questions so it is true so the this in filtering this is us the evidence is a subset but the answers are going to be different depending on what evidence you computer on so you can't literally just use one as answer for the other yeah the lines over the things that you eat for tea yeah yeah so you marginalize that so that's a key idea is that suppose I had a smoother and I wanted to answer this filtering query right so this is H 3 given E 1 to e 2 e 3 right remember last time we talked about how you can take leaves of bayesian networks which are not observed and just essentially wipe them away so if you don't observe a 45 h4 h5 you can just pretend those things you know don't exist right and now you're back to a smoothing query where you're conditioning on all the evidence ok so we're gonna focus on smoothing and to make a progress on this problem I'm going to introduce a representation that's going to help us think about the possible assignments right and just to be be clear right there's the reason why this is not completely trivial is that there are four if you have end hidden variables there's to the N or x-men join and number of possible assignments and you can't just enumerate all of them so you're going to have to come up with some algorithm that can compute it more efficiently ok so what we're gonna do is introduce this lattice representation which is going to give us a compact way of representing those assignments and then we can see how we can operate on that representation ok so it's gonna smell a lot like state based models so we're kind of going backwards but hopefully it'll make sense so the idea behind a lattice representation is that I'm going to have a set of rows and columns so each column is going to correspond to a particular variable so the first column is going to correspond to h1 and each row is going to correspond to some setting of that variable so there's two possible things I can do I can either set h1 equals 1 or I can set each one equals 2 I'm the version I'm drawing on the board is going to be a simplification of what I have on the slides just such in the interest of space and the second column is going to be either h2 equals 1 or h2 equals 2 so by going through these all lattice nodes which are drawn as boxes I'm kind of assigning random variables to a particular value okay so I'm gonna connect these up so from this data I can either set h2 equals 1 or 2 here I can also go from 2 1 or 2 and finally let's just do H 3 equals 1 H 3 equals 2 and similarly I can choose either one of them from no matter where I am and finally I have an end State okay so first notice that the size of the lattice is reasonably well controlled it's simply the number of time steps times the number of values that a variable can take on so let's suppose that there's n time steps and K possible let's say locations so values of H I so how many nodes are here a times n right ok so that means we can essentially this doesn't blow up exponentially ok so now let us interpret a path from start to end what is a path from start to end tell us so let's take let's take this path now what are the what does this tell us yeah yeah it's a particular assignment of the variable so this one says set H 1 2 1 H 2 2 1 H 3 2 1 this path says set H 1 2 2 h 1 2 1 and H 3 2 2 and so on ok so every path from start to end is an assignment to all the unobserved or hidden variables ok so now remember each assignment comes with some sort of probability so we're gonna try to represent those probabilities I'm Chuck suppose on this graph ok so I'm gonna go through each of these edges so remember these are the probabilities right so every assignment has is some product of the factors and I'm going to basically take these factors and just sprinkle them on the edges at the points where I can compute the factors and I'll explain more what I mean by this ok so so maybe one one kind of preliminary thing I should mention is suppose for this example we have we are conditioning on e 1 equals 1 e 2 equals 2 let's say e 1 equals sorry 3 equals 1 okay so I'm conditioning on these things notice I'm not drawing them in here because these are observed variables I don't have to reason about what values they take on I'm only going to consider the hidden variables which I don't know but this is just going to be some sort of reference ok so let's start with start to H 1 equals 1 right so if you remember back tracking and CSPs right we basically took factors and then whenever we could evaluate the factor we just put it down on that edge in the backtracking tree so here what what can we do we have the probability of H 1 equals 1 ok and then we also have the probability of the first emission probability I can compute right so that's a probability of e 1 equals evidence I saw which is 1 given H 1 equals 1 which is the value that I've committed to here so this is a number that is essentially the weight or cost or score or whatever you want to call it that a important to incur when I traverse that edge ok so what about this one so I have the transition from let's say H 2 equals 1 given H 1 equals 1 and times the probability of e 2 equals whatever I observe which is 2 given H 2 equals 1 and similarly over here I have probability of H 3 equals 1 given H 2 equals 1 times the probability of e 3 equals 1 which is whatever observed given H 3 equals 1 and then over here there's no more factors left so I'm just going to put one there okay and you can check that when I traverse this path and I multiply all of these probabilities together that's exactly this expression for H 1 equals 1 H 2 equals 1 H 3 equals 1 e 1 equals 1 e 2 equals 2 e 3 and for each of these edges I have an analogous quantity depending on the values that I'm dealing with ok any questions about this basic idea so in the slides this is basically what I just said ok so so now now what I'm trying to do now is to let's say I'm interested in you know smoothing so I'm interested in what is the probability of H 3 equals 2 right actually let's let's do the example on the board just just because that's one L actually do so suppose I'm interested in probability of h2 equals a lot say to given the evidence e 2 equals to e 3 ok so this is the query I'm interested in you know computing okay so how can I interpret this quantity in terms of this lattice right so this is there is this H 2 equals 2 here right that's somehow privileged and then asking you know what is the probability of this given the evidence yeah yeah so sum over all the probabilities of the paths right so remember every path through from start to end is an assignment some of those paths go through this node which means that each q goes to and some of them don't which that means that it's not true right so if you look at all those paths and you sum up their weights of this node and divide by the sum over all paths then you get the probability of H 2 equals 2 given the evidence ok so let me just write this this is going to be sum over colloquially sum over paths through H 2 equals 2 divided by sum over all paths ok so now the problem is to compute the sum over all paths going through H 2 or not going through H 2 okay again we don't want to sum over all the paths literally because that's going to be exponential time so how can we do this yeah so what do I mean by some I mean some of the weights so every path has a weight which is a product of the weights on the edges and you some of those ok so what's an idea that we can use to compute the sum efficiently programming okay so this we're gonna do this kind of recursively let me just show this slide so it's going to be a little bit different from the diamond programming that we saw before it's going to be more general because we're not computing and let's say one particular query but I'm going to compute a bunch of quantities that gonna allow us to compute all queries essentially okay so let's there's going to be three quantities I'm gonna look at and hopefully I can kind of out of colors but that's not you screen for this though so there is going to be you know forward messages F which I'll explain a bit there's going to be backward messages B okay so what I want to do is for every note I'm going to compute two numbers yeah no I think I'm okay what color is that great thanks okay so let's call this s okay okay so so for every note I'm going to compute two numbers one number called the forward number is or the for message it's going to be the sum over all the weights of the partial paths going into that node and the orange number is going to be the sum of all the partial paths from that node to the end okay so let's let's so these are all meant to be probabilities so the numbers should be less than one but just to keep things simple I'm gonna put actual numbers on these just two just two integers on them so they don't have to carry out decimal points okay so this is one let's say one two one one two one one two okay so remember every Edge has a weight associated with it and so now let me compute the forward probability so what is the sum of over all Pat's going into that note it's just one right okay so sorry forward is screen so one and this is two okay just copying this and now recursively what is the sum of all the weights going into this okay so I couldn't have come from here or it could have come from here all right so if I come from here it's one times whatever the sum was there so that's 1 times 1 that's 1 plus 2 times 2 okay so I'm gonna get a 5 1 plus 4 and here I'm going to have 1 times 1 so that's 1 1 times 2 that's 2 so that's going to be 3 hopefully you guys are checking my math here so what about this notes so now recursively this could have the paths going in here could have come from this node or that node so that's 2 times 5 so that's 10 1 times 3 and that's 3 so that's 13 and this is 1 times 5 plus 3 times 2 so that's 11 right okay so 13 represents the sum over all paths going into H TiVo's okay so I can do the backward direction so so this is going to be in orange the backward messages which are paths going to the end so this is going to be one it's here I'm going to have so 2 times 1 plus 1 times 1 so that's a 3 this is 1 times 1/2 times 1 so that's a 3 as well this is 1 times 3 1 times 3 so that's a 6 this is 2 times that's a 6 and a 3 so that's a 9 and then I'm done ok ok does that all make sense so these are kind of compact representations over the essentially the flow in and out of the lattice nodes okay so the they're kind of the kind of magic happens when I have these s's so now for every node I'm going to also just multiply them together ok so that's going to be 6 18 18 9 13 11 and that's it okay so what happens when I multiply them together let's take out the look at this node right so what is nine represent nine represents the sum over all the paths going through here right because I can take whatever pads I have coming in and I can take whatever has I have going out and any sort of combination of them will be a valid and and path okay and the so this total weight is yeah so why do we multiply instead of sum here because we're multiplying the weight of a path is the product okay mathematically what's going on is exactly factoring right so suppose I have numbers let's say a B and C and D and I could choose a and B and C and D so what are the possible so then I can do a plus B times C plus D right which is the sum over all possible paths and you can dust paths are either AC ad BC and okay so I'm basically doing this computing it in a factorized way rather than expand now that's mathematically what's going on when I multiply the four were in the backward messages and why are these called messages so the idea of messages comes from the fact that you can intuitively think about the former messages as being kind of sent across the graph right because the message here depends only on the neighbors here and once I get these messages i can compute the my messages the next time step based on this so it's kind of a summary of what's going on and I send the messages forward and same in the backward direction okay so now once I have these values how do I go and back and compute my query some of our paths through HTTP goes to what is that nine right and over the sum of our paths what's the sum over all paths sorry this should be 15 I was wondering this screw anything else up I think that's right I was checking because you know when if you sum these two numbers you get 24 which is all the sum of all Pat's going through here and that better be the same number here and also there would be the same number there right okay so this should have caught that okay um all right so these are all the paths going through nine and I'm sorry going through this note and if you look at all paths that's going to be 15 plus nine that's going to be 24 okay so final answer is the probability of H 2 equals 2 given these made-up weights is going to be 9 over 24 any questions about that okay so let me just quickly go over the slides which is going to be a more mathematical treatment of what I did on the board hopefully one of the ways will resonate with you so define the former messages for every node is going to be a sum over all the values at the previous time steps of the former message at a previous time steps times the weight on the edge from the previous value to the current value the backward is going to be defined similarly for every node sum over all the values assigned to then at the next time step all outgoing edges of the backward message at the next time step times the weight into that next time step and then define s as simply just the product of F and B and then finally if you normalize the sum at each point in time you can get the distribution over the hidden variable given all the evidence and to summarize the algorithm the forward backward algorithm is actually a very old algorithm develop actually first for speech recognition so while back I think it's probably in the 60s or so so you sweep forward you compute all the forward messages and then you sweep backwards and compute all the backward messages and then for every position you compute si for each eye and you normalize so the output of this algorithm is not just the answer to one query but all the smoothing queries you want because at every position you have the distribution over the hidden variable H I and the running time is n times K squared because there's n time steps and every time step you have to compute this sum so for K possible values here you look at K possible values there so that's a K squared and it's n times K squared interestingly if you ask okay what's the cost of computing a single query it would also be n times K squared so it's kind of cool that you compute all the queries in the same time that it takes to compute a single so question is does this only work for a hidden Markov models or is it more general there's certainly add app tations of this which work very naturally for other types of networks and one immediate generalization is if you have not just a chain structure but you have have tree structure then the idea of passing messages along that tree it's called belief propagation is just works pretty much out of the box for arbitrary Bayesian networks this won't work because you once you have cycles then you can't represent it as a lattice anymore any other questions okay so summarize the slightest representation allows us to think of paths as assignments which is a familiar idea if we're thinking about state-based models we can use the idea of dynamic programming to compute these sums efficiently but we're doing this extra thing we're computing all the sums for all the queries at once and the forward-backward algorithm allows you to share in a media computation across different queries so the output of this algorithm is basically the probability of H I given all the evidence for every I owe so how would you actually use this do you sample from it depends on what you want to do with it so the output of this you can think about it as a distribution at each time step so it's a n by K matrix of probabilities right from that you can sample if you want you can mean you might all be only interest in only a various points in time it's okay so let's move on to the second ogram which is called particle filtering so we're interested still in hidden Markov models all the particle filtering again is something actually much more general than that and we're going to only focus on query filtering questions so we are filtering we're at a particular time step we're only interested in the probability of the hidden variable at that time step ditchin on the past and why might we not be satisfied with hidden Markov or the forward backward so here's the motivating picture so imagine you're doing the car assignment let's say and you're trying so you're tracking cars okay so cars let's say live on a huge grid so at each position H I the value of H I is some point on this grid but you don't know where it is you want to track it okay so if this is like a hundred by hundred you know that's ten thousand if this were thing were continuously would be even you know worse so this this K squared where K is the number of values could be like ten thousand squared and that's a large number right so even though hidden Markov a lot of fat with backward forward backward is not exponential time even the quadratic can be pretty expensive and in the further the motivation is you know you into Utley shouldn't have to pay that much right because let's say your sensor tells you that oh the car is up here somewhere you know cars can't move all the way across here so then you know but the algorithm is going to consider each of these possibilities and most of these probabilities are going to have pretty much zero probabilities so that's really wasteful to consider all of them so can we somehow focus our energies on the region that have actually hi yeah it's like fruit the later time steps you can't have zero in one of those positions the question is can you go backwards like if you can't like if your continue to move one way and you say like it very unlikely that I'm going to go back to the starting position do each of those variables have the same domain I'm saying oh so each of these variables they don't have to be from the same domain for this presentation they're in the same domain just for simplicity but I think what you're asking is you know that maybe a car only moves let's say forward or something then there is some restriction on the domain it's not going to be that significant because you still don't know where the car is so it doesn't really cut out that many possibilities yeah maybe by a factor of two or something but that's not that significant okay so how do we go about making this a little bit more efficient so let's look at beam search so our Folio algorithm is not going to be beam search this can be particle filtering but beam search is going to give us some inspiration so remember in beam search we keep a set of K candidates of partial assignments and algorithm as follows you start with a single empty assignment and then for every position time step I'm going to consider all the candidates which are assignments to the first I minus 1 variables I'm going to extend it there's possible ways of extending our setting hi2 V from any V in the domain of I so now I'm going to amass this set of extended assignments now I have K times as may because each of previous I'm an gothics standing by K so I'm gonna prune down I'm just gonna take all of them sort them by weight and take the top K okay so visually remember from last time it looks like this so here is this object tracking where we have five variables and you start with a beam search which is assigning X 1 to 0 or 1 and then you extend it so you extend the assignments you prune down you extend the assignments you prune down you extend the assignments have cleared out and so on and at the end of the day you get K candidates each candidate is a full assignment to all the variables and it has a particular weight which is its actual weight and each intermediate time it's a partial assignment to only the prefix of I random variables okay and remember that beam search doesn't have any guarantees it's just a heuristic but it often it works while in practice and a picture a picture you should have in your head is that you have the exponentially sized tree of all possible assignments and beam search is a kind of this pruna breadth-first search along this tree which only looks at promising directions and continues so you don't have to keep track of all ok so at the end you can use beam search you get a set of candidates which are full assignments to all the random variables and you can compute any quantities you want so the problem with this is that it's slow for the same reasons as I described before it requires considering every possible value of H I so it's a little bit better than forward backward right so for forward backward you have to have the domain size times the domain size and now for beam search it's the size of the beam times the domain size now which is better but but still I think we can do a little bit better than that and finally there's this kind of more subtle point as that as we'll see you later greedily taking the best K might not be the best thing to do because you want some maintain some diversity right just so kind of a quick visual so suppose you you've beam consists of only cars over here it's kind of a little bit redundant but you might want a kind of a broader representation okay so the idea with particle filtering is to just tweak beam search a little bit and this is going to be expanded into three steps which I'll talk about okay so let me does anyone need this on the board and erase it okay we're good good look at the video if you don't remember all right so there's three steps here okay and we're going to try to do this pictorially over here so so the idea behind particle filtering is I'm going to maintain a set of particles that kind of represent where I think the object is so imagine you know the object starts over here somewhere so you have a set of particles and I'm going to iteratively go through these three steps so proposed weight and resample so this is meant to be kind of a replacement of the extent perón strategy for search okay so the first step is to propose so at any point in time particle filtering maintains a set of partial assignments known as particles that kind of tries to mimic a particular distribution so to kind of jump into the second we can think about this set of particles as representing the probability of H 1 and H 2 given the evidence yeah so far okay on the board I'm only going to draw the the particle representing the value H 2 because I it's hard to draw trajectories but you can think about really particle filtering maintains this lineage as well okay so the key idea of the proposal distribution is that okay we want to advance time now so we're interested in H 3 but we only have h 2 so how do we figure out why H 3 is so we propose possible values of H 3 based on H 2 so this is idea of proposal we just simply draw H 3 from this transition distribution so this remember this distribution is comes from the hmm this is your given hmm so you can do this and this gives us a set of new particles which are now extended by 1 and this represents the distribution H 1 H 2 H 3 given the same evidence okay so pictorially you should think about proposed as a proposed is kind of taking each of these particles and sampling according to the transition so think about the particle is as you know just moving in some direction it's almost like simulating where you know cars are going and this is done kind of sarcastically a randomly okay step 2 is to wait so so far the new locations really don't represent reality right because we also see III at time step 3 we get a new observation that hasn't been incorporated somehow into this we're just kind of simulating what might happen and so the idea here behind a real waiting is for each of those particles we're gonna assign a weight now which is equal to the mission distribution / III given you know H 3 again this is the mission distribution which is given by our hmm so we can just evaluate it whatever if you like it and the set of new particles are which are weighted can be thought of representing this distribution where now we have conditioned on III equals 1 okay so now each of these particles has some weight so on on this picture it kind of looks like this so maybe let's say the emission distribution is kind of let's say let's say a Gaussian distribution around the observation so suppose the observation tells you well it's over here somewhere which means that these particles are gonna get higher weight and these particles are gonna get lower weight and if they're far away enough then maybe they get like almost zero weight so I I'm going to kind of softly X these out so think about these as okay maybe maybe I'll do this so I'll up weighed these and kind of start down worried about so nothing really gets a zeroed out but you can think about these it's down waiting and these is up waiting provided you have let's say some evidence III that tells you you're going in that direction okay okay so the final step is it's really about a resource distribution question so now we have weighted particles we need to somehow get back to unweighted particles okay so which one's it choose so imagine you have this situation where you have particles which are kind of distributed the weights of the particles are fairly uniform then you could imagine let's take just the particles with a highest weight all right this is very similar to what beam search would do it would just take all particles with high weight and just keep those and nothing else so this is not a crazy thing to do but it might give you an impression that you're more confident than you actually are right because imagine the the weights are fairly uniform so maybe this one is like you know 0.5 and this one is like point four eight so you're kind of just breaking ties in a very biased way so the idea is that if you sample and set sample from this situation you're gonna get something a lot more representative rather than just taking the best okay so how do you sample from this distribution so the general idea is if you have if I this is just kind of useful module to have if I give you a distribution over n possible values then I'm gonna draw let's say K samples from this so if I have the distribution over four possible locations with these probabilities or you know weights then I might draw four samples and I might pick a 1 and then a 2 and then I may pick a 1 and a 1 so some of these things are not going to be chosen if they have sufficiently low probability okay so going back to the particle filtering setting we have these old particles remembering which are weighted and first I'm going to normalize these whites to get a distribution so add these numbers up and divide by that and then I'm going to sample according to this distribution given weights on the previous slide so I might draw in this case 0 1 1 months and I might draw it again and might not even keep this particle and the idea here is that suppose a particle has really really low weight as I point 0 users at 1 then I shouldn't kind of keep it around and because it needs to occupy memory and after keep track of it it's basically gone right so this resampling kind of regenerates the pool by focusing the efforts on the higher way particles a might even a very sample highway particle multiple times and then not sample the low way particles zero times okay so in this in this picture here so resampling might let's see how do i how do i draw this so maybe now i have maybe i sample this twice maybe i sample this twice and maybe these don't get sample maybe i sample this once this one's right so so the blue represent the particles after this one round of particle filtering where I've kind of moved the particles over here a little bit and less away from there so that's why it's kind of called particle tracking because you can think about the swarm of particle is representing where the object might be and over time as as I follow the transition dynamics and hit it with observations I can move this swarm over time so that's a picture you haven't you okay so let's go through the formal algorithm it's going to be very similar to beam search so you start with empty assignment and then you propose so where you take your partial assignments to the previous I minus 1 variables and then I'm going to consider for each one of them just sampling once from this transition distribution and augmenting that assignment so unlike beam search where the size of C prime was K times larger than C the size of C prime is equal to the size of C in this case second irie weight so looking at the evidence and applying the evidence probability of evidence given the particle h i gives me a weight for every particle and then i'm going to normalize this distribution sample que ailments independently from that distribution and that the redistributes the the particles to where I think they're more promising okay so let's go through this quick demo so the same problem as before I'm gonna set the number of particles so 100 so I start with all the particles assigning X 1 you know to 0 and there's a hundred copies of them I extend and notice that some of the particles go to 0 and some of the particles go to 1 with approximate probability proportional to whatever the transitions are and then going to redistribute which changes the balance a little bit and you're going to extend prune extend by prune I really mean Riis rewrite every sample and notice that the particle is kind of get more dive this is more diverse than well it's more diverse than beam search because I'm using K equals 100 rather than like three but but you can see that some of these particles might collect my like this one have zero weight so that when I resample they just go away yeah so just to show the brain branching pattern or is it actually relevant yeah so that's a good question so notice that all of these are all of these zero for the purposes of x4 are just three the same so if you only care about the marginals then you can collapse them you're absolutely right in this demo it's I'm making the entire histories so that yeah you can show the see the branch okay so this is that point if you only care about the last position and not about the possible trajectories then you action actually collapse all the particles with the same HHI into one and then furthermore if there's repeats then you can just keep track of the count right and this is actually what you would do in your assignment I'm giving you kind of the more general picture in case because particle filtering is more general than just looking at the last time step but most of the time your position so last time so okay so just a quick kind of quick visual illustration of this let's define this factor graph where you have transitions that are basically 1 or 0 depending on whether H Y inhi 1 minus 1 aren't close to each other and oh I is some sensor reading one thing I've been a little bit slightly under the rug is sometimes I've talked about local conditional distributions sometimes I've been talking about factors remember from the point of view of inference it really it doesn't matter they're all just you know factors right so in particular if I give you a factor graph which right remember which is not necessarily a Bayesian network I can nonetheless still define a distribution by simply just normalizing take all the weights a mult and normalize and divide by that and these objects are actually called Markov networks or Markov random fields which is another object of study that we're not going to talk about in this class but this is actually a more general way to think about the relationship between factor graphs and distributions we're only mostly focusing on Bayesian networks in this class but some examples will be more general than that ok so you have this distribution and so you can play with this demo in the slides if you click then this yellow dot shows you the observation at a particular point in time and the noise the observation is related to the true position of some particle by based on the noise that you define here so here I've defined Box noise which means that it's going to be a uniform distribution over a box of three by three by three or I guess a six by six box and so if I increase the number of particles to let's say 10,000 then what I'm going to show you is a red blob that looks like it's trying to eat the yellow dot and this red blob shows you the set of particles where the intensity is the count of that number of particles in a particular cell so this swarm I kind of corresponds to on the board it's the set of particles but since this is discretized you can kind of see the particles piling up on each other and just to see how well this is doing shofu position you can see the blue dot is an actual object position the yellow dot is the noisy observation and it's trying to do its best to track where the blue is it's not perfect because this is kind of approximate out without about it kind of gets ok any questions about particle filtering so to summarize you can do forward backward if you can swallow computing a number of domain values times number of domain values if you have large domains but you really think that none of most of them don't matter then particle filtering is it's a good tool because it allows you to focus your energies on the relevant part of the space okay so now let's revisit gibbs sampling from a probabilistic inference point of view so remember gibbs sampling we talked about last week as a way to compute the maximum weight assignment in a factor graph where their main purpose is to get out of local minimum so remember how Gibbs sampling works you have a weight which is defined for complete assignments so unlike particle filtering or beam search we're starting with complete assignments and trying to modify the complete assignments right they're trying to extend partial assignments so you loop and you compute you pick up a variable X I and then you consider all the possible possible values it could take on and you choose the value with probability proportional to its weight okay let me show you this example that we saw last week so so same graph here so we start with this complete assignment and then we're gonna examine X 1 X 1 can take on two possible values for each of these values I'm going to compute its weight and remember in Gibbs sampling I only need to consider the Markov blanket of that variable the factors here are Oh 1 and T 1 because that's the only thing that changes everything else is a constant and then I was normalized and sample from that ok so then I go on to an X variable and so on so I sweep across all the variables and eventually the weight hopefully goes up but not always up because sometimes I might sample value that has okay and at the same time I can do various things like computing the the marginal distribution over a particular variable so or if two variables so I can basically count the number of times I see particular patterns and I can normalize that to get a distribution over that particular pattern ok so now let's try to interpret Gibbs sampling from a probabilistic point so instead of just thinking about Oh a weight is just a function we can actually think about the probability distribution induced by that factor graph by again summing all the weights over all possible assignments normalizing in there you have a distribution so the way to think about Gibbs sampling is now more succinctly and more actually traditionally written as the following which is you loop through all the variables and for every variable you're going to look at the probability of that variable taking on a particular value conditioned on everything else so now we can give like write down this probability which is you know a nice way to think about what Gibbs sampling is actually and the there guarantee with Gibbs sampling under you know some conditions which I won't get into is that as you run this for a long enough you the sample that you get is actually a true sample from this distribution as if you had sampled from this distribution and if you did that multiple times now you can actually compute any sort of marginal distribution do you like so now while do is that guarantee sounds really nice there are situations where Gibbs sampling could take exponential time to get there so caveats okay so let's look at a possible application of Gibbs sampling image to you so suppose you have some sort of noisy image and you want to clean it up so how can this be helpful so we can model this image nude denoising problem as this factor graph where you have a grid of pixel values and they're connected in this kind of grid like way so every value of x I is where I is a location two numbers is going to be either 0 or 1 and we're going to assume that some subset of a pixel are observed and in case we observe it then we're actually just going to have a factor that it's actually a constraint that says that value of x I has to be whatever we observed and these we have these transition potentials that say neighboring pixels are more likely to be the same different so it assigns value to 2 pixels which are the same and 1/2 pixels which are different okay this is the motto of clear so now let's try to do Gibbs sampling in this in this model just to give you an concrete idea of what this looks like so I'm gonna look at I'm not gonna draw an entire the grid but I'm gonna draw center around a particular know that we're interested and sampling so there's more stuff away okay so and remember and get sampling at any point in time all the variables have some sort of preliminary assignment so this might be 1 1 1 and 0 and this might be 1 ok so now I sweep through and I'm gonna pick up this variable and kind of say hmm shall I try to change its value first of all you ignore the old value because it doesn't factor into the algorithm and now you're going to consider let's say this is X I so X I there's two possible values here so 0 and 1 right so and I'm gonna look at the weight so if it's zero then I'm going to evaluate each of these factors based on that so remember the transition potential is well I'm not going to write down but if let's consider zero here so these are different that means I'm gonna get a one these are different that's going to be a one these are different that's going to be a one these are the same and then going to get a two okay now I try one these are the same these are the same these are the same and these two are different so for every assignment have this wait so this is two this is eight and then I normalized so this is going to be 0.8 and this is going to be 0.2 and I draw flip a coin with heads 0.8 and whatever I get I put down so I might have with point a probability I put that one back that's okay and here's another example which I'm not going to go through okay so keep sampling it's gonna do that so now let's look at this concrete okay so okay so what you're looking at here is this grid of pixels white means that the pixel is unobserved black or red means that it's observed to be whatever color you see on the screen and so this is somewhat of a noisy image and the goal is to fill in the white pixels so the picture makes sense and visually you guys can probably look at this and see the hidden text no okay okay so you're the noisy system is pretty good okay so I'm gonna run Gibbs sampling click and what you're seeing here each iteration I'm gonna go through every pixel and apply exactly the algorithm I whatever I did on the board okay so you can see that these are just samples from the set of unobserved random variables okay so to turn this into something more useful you can instead of looking at the particular sample you look at the marginal which is for every location what is the average pixel value that I've seen so far and if you do that then you can see a little bit clearer picture of 220 and while and it's not going to be perfect because the model that we have is fairly simplistic it always says is similar pixels neighboring pixels hours are tend to have the same color it has no notion of letters or something okay but you can see kind of a simple example of get sampling at work there's a bunch of parameters you can play with you can here's a another picture of a cat you can play around with the coherence which is how sticky the transition constraints are you could try to use ICM which won't work at all okay any questions make we my action and earlier today okay so just to kind of sum up we've last week Monday's we define new models so we define a Bayesian network or a factor graph and today we're focusing on the question of how do we do probabilistic inference and for some set of models I've shown you how to do this there's a number of algorithms here in a forward backward which work for hmm and our exact particle filtering which works for hmm although they can be generalized which is approximate and Gibbs sampling which works for general factor graphs which is also approximate each of these algorithms we've seen kind these ideas in previous incarnations so forward backward is very similar to variable elimination because it variable animation also computes things exactly particle filtering is like beam search it computes things approximately and Gibbs sampling is like iterative conditional modes or Gibbs sampling which we saw from last week okay so next Monday we're gonna look at how we do learning so up until now the Bayesian network all the probabilities are fixed and now we're going to actually start to do learn it I should say that maybe this learning sounds a bit scary because there's already a lot of machinery behind inference and factories and all this stuff but you'll be pleasantly surprised that learning is actually much simpler than in France so stay tuned