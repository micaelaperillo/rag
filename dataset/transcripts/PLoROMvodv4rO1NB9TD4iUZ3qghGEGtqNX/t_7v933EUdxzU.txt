okay so let's begin first of all I want to say congratulations you all survived the exam well you don't have your grades back but you you completed it so yeah I just want to say so we're gonna have the grades back as soon as we can the CAS are all busy grading and we're actually going to cancel office hours today so we can focus on getting those grades back to you quickly after this the course definitely goes downhill so you guys can kind of like take a break so after the exam there's pretty much just two things left so there's the project so the final presentation the poster session for the project is going to be I believe that Tuesday after vacation it's like a big auditorium hall those give me a lot of people from industry and academia it's really exciting to have like you know so many smart people are showing off their hard work and then you have the last piece set which is logic yeah Oh Monday okay yeah so months of right after you back from vacation is that poster session and then on Thursday is the last piece that is due logic so logic is so this is not like my official opinion but I think logic when I took the class was easier than the others it doesn't take as much time so you guys are definitely past the hardest point is class yeah I think I think that's the general opinion yeah but that being said I wouldn't wait until the last minute so start really yeah ok so then yeah so Piazza and office hours will be your best friend yeah okay so but today though we're talking about this fun advanced ish topic which is deep learning I say ish because I think a lot of you are probably working on deep learning or I've heard of it already a lot of you have it in your projects and today we're just kind of do very very high-level broad pass different subjects within deep learning hopefully get you excited give you kind of like a shallow understanding of a lot of different topics so that if you want to take follow-up classes like to 24 N or 229 even then you'll be armed with some kind of background knowledge okay so first we're going to talk about the history so deep learning you've probably heard of it it's really big especially in the last 5-10 years but it's actually been around for a long time so even back to the 40's there's this era where people are trying to build more computational neuroscience models they noticed that they knew back then that you know there's neurons in the brain and they're arranged in these networks and I know that elegance arises from these small parts and so they really wanted to model that the first people to really do this where McCulloch and Pitt's so Pitts was actually logician and it was they were concerned with making these kind of like logical circuits out of a network like topology like what kind of logical expressions can we implement with a network back then this was this was all just a mathematical model like there was no back propagation there are no parameters there was no inference it was just trying to write about fruit I guess like theorems and proofs about what kind of problems these structures can solve and then head came along about ten years later and started moving things in the direction of I guess like training these networks he noticed that if two cells are firing a lot together then they should have some kind of connection that is strong and this is a was inspired by observation so there's actually no formal math theory backing this it was a lot of it was just very smart people making conjecture and then it wasn't until the 60s that so neural networks was I guess you could say maybe in the mainstream like a lot of people were thinking about it and excited about it until 1969 when McKenzie and pepper they released this very famous book called perceptrons which was this like big fat book of proofs and they were basically talking about the they proved a bunch of theorems that were about the limits of very shallow neural networks so for example early I think very very early in this class we talked about the XOR example where if you have two classes and they're arranged in this configuration then there's no there's no linear classification boundary that you can use to separate them and cross by them correctly and so there we can ski pepper in their brook perceptrons they came up with a lot of these I guess you could say like counter examples like that a lot of theorems that really proved that these thin neural networks couldn't really do a lot and at the time it was it was a little bit of a killing blow to neural network research so mainstream AI became much more logical and neural networks were pushed very much into I guess a minority group so there's always people thinking about and working on it but the mainstream a I went definitely towards kind of the symbolic logic based methods that Percy's been talking about the last couple of weeks but like I said there's still these people in the background working on it so for example in 1974 we're both C came up with this idea the back propagation that we learned about using the chain rule to automatically update weights in order to improve predictions and then later on so Hinton and rumelhart Williams they kind of I guess you could say they popularized this so they they definitely I guess you could say rediscovered war BOCES findings and they really said oh hey everybody you can use back propagation and it's a mathematical fail well kind of like well founded way of training these like deep neural networks and then in the 80s so today we're going to talk about two types of neural networks convolution networks and recurrent neural networks and the convolutional networks traced back to the 80s so there's this neo cognate Ron that was invented by a Japanese Fukushima and it kind of laid out the architecture for a CNN but there was no way of training it and in the actual paper they used hand-tuned weights they were like oh hey there's this architecture you can use and basically we just like buy travel and Eric came up with these numbers to plug in it and look at how it works now it just seems like insane but back then that was this you know there were no ways of training these things until McCune came about ten years later and so he applied those ideas of backpropagation to CNN's and laocoön actually came out with a so there's the laocoön net which was a very famous check reading system and it was one of the first like industrial large-scale applications of deep learning so whenever you write a check in and have your bank read it almost all the time there's a machine learning model that reads that check for you and those check reading systems are some of the oldest machine learning models that have been like use at scale and then later so we're kernel networks came in the 90s so Elliman kind of proposed it and then there's this problem with training it that we'll talk about later called expect exploding or vanish ingredients and then grater and zoom breeder about ten years later came out with I guess you could say maybe a it solved to some extent those issues with a long short-term memory Network lsdm and we'll talk about that later and then but I guess you could still say that no networks were kind of in the minority so in the 80s you used a lot of rule-based AI in the 90s people were all about support vector machines and inventing new kernels if you remember support vector machine is basically just like it's it's a linear classifier with the hinge loss and a kernel is a way of projecting data into it kind of like a nonlinear subspace but it was in the 2000s people finally started making progress and so Hinton had this cool idea of hey we can train these deep let works one layer at a time so we'll pre train one layer and then we'll pre train a second layer and stack that on third layer stack that on and you can build up these sexist representations and then deep learning kind of became a thing so this looks like maybe three four years ago really started taking off and ever since then it's really been in the mainstream and you can kind of proof evidence towards its mainstream this you can look at all of these applications so speech recognition for about almost a decade this performance in speech recognition severe recognizers were using hidden Markov model based like that was in that was the heart of these algorithms and for ten years performance just stagnated and then all of a sudden their own that was him around drop that formats and what's most surprising is that all of the big comes so IBM Google Microsoft they all switched over from these classical speech recognizers into fully end-to-end neural network based recognizers very quickly in a matter of years and when these these large companies are operating at scale and they've you know dozens maybe hundreds of people have tuned these systems very intricately and for them to so quickly and so radically shift the core technology behind this product really speaks to its power same thing with object recognition so there's this image net competition which goes on every year that says basically like how well can you say what's in a picture and the first and so for years people use these handcrafted features and all of a sudden Alix net was proposed and it almost got half the error of the next-best submission for this competition and then ever since then people have been using neural networks and now if you want to do computer vision you kind of have to use these CNN's it's just a default if you walk into a conference every single poster is gonna have a CNN in it same thing with go so google deepmind had I had a CNN based algorithm they trained with reinforcement learning and it beat the world champion in this very difficult game and then in 2017 it did even better didn't even need a like real data it just did self play and machine translation so Google Translate for almost a decade had been working on building a very very advanced and very well performing classical machine translation system and then all of a sudden the first machine translation system was proposed in like 2014-2015 and then about a year later they threw away ten you know almost a decade of of work on this system and transferred entirely to a completely new algorithm which again speaks to its power so but what is I guess deep learning like what you know why is this thing so powerful why is it so good and I think so broadly speaking it's a way of learning of taking data you can slurp up any kind of data you want like a sequence a picture even vectors or even like a game like go and you can turn it into a vector and this vector is gonna be a dense representation of whatever information is captured by that data and this is very powerful because these vectors are compositional and you can use these components these modules of your deep learning system kind of like Lego blocks you can you know concatenate vectors and add them together and use this to modify you and just the compositionality makes it very flexible ok so today we're going to talk about feed for electronics convolutional metrics which work on images or I guess just anything with repeated complex structural information in it we're currently networks which operate for sequences and then if we have time we'll get to some unsupervised learning topics okay so first for feed-forward networks so in the very beginning this class we talked about linear predictors linear predictor if you remember is basically you define like a vector W that's your weights and then you hit it with some input and you dot them together and that just gives your output and neural networks we define very similarly so you can think of each of these hidden units as the result of a linear predictor in a way so working backwards you so you have that you define a vector W and you hit it with some activation function with some activation like inputs some hidden inputs and you dot that with your hidden and you get your output and then you arrive with either your hidden by defining a vector and hitting it with inputs so you use your inputs to compute hidden numbers and then you use your hidden numbers to compete your final output so in a way you're kind of like I guess stacking linear predictors like each each number so H 1 H 2 and F of theta are all the product I guess you could say they're all the result of like a little mini linear predictor they're all kind of like roped together so just to visualize this if we want to go deeper you just rinse and repeat so this is you could say this is a one layer neural network it's what we were talking about before with linear predictor you just you have your vector weights and you apply it to your inputs for a two-layer you apply instead of a vector to your inputs you apply a matrix here inputs which gives you a new vector and then you dot this intermediate vector this hidden vector with another set of weights and that gives you your final output and then you can just rinse the repeat so you pass through a vector you pass through a matrix to get a new vector you pass that through another matrix to get a new vector and then you finally at the very end dot it with a vector to get a single number so just a word about depth that's one of the reasons why these things are really powerful so there's a lot of interpretations for why depth is helpful and why kind of like stacking these matrices works well one way to think about it is that it learns representations of the input which are hierarchical so H is going to be some kind of representation of X H prime is going to be a slightly higher level representation of X so for example in a lot of image processing systems H maybe represents H could represent like the edges in a picture H prime would represent like corners H double prime could represent like small like fingers or something H triple prime would be the whole hand so it's successfully I guess you could say like just higher level representations of what's in the data you're giving it another way to think about it is each layer is kind of like a step in processing and you could think of it maybe like a for loop where it sits like the more the more the more iterations you have the more steps you have the more depth you have the more processing you're able to perform on the input and then last the deeper the network is the more kinds of functions it can represent and so the yeah so there's flexibility in that as well but in general there isn't really a good formal understanding of why depth is helpful and I think a lot of deep learning is there's definitely a gap between the etherion practice so yeah so this I guess just goes to show why depth is helpful so if you input pixels maybe your first layer is giving you edge detection and your second layer is giving you little eyes or noses or ears and then your third layer and above is giving you whole objects yeah so just to summarize so we have these deep neural networks and they learn hierarchical representation of the data especially I guess you could say it's like gaining altitude in its perspective you can train them the same way that we learned you can train them the same way that we learned how to train our linear classifiers just with gradient descent so you have your loss function you take the derivative with respect to your loss and then you propagate the gradients to step in a direction that you think would be helpful and this optimization problem is difficult so it it's nonlinear and non convex but in general if we found that if you throw like a lot of data at it a lot of compute at it and somehow you managed ok so it seems like the slides are a little out of order but basically just to review how you train these things in general it's the same as a linear predictor you define a loss function so for example this is squared loss where you'd say I'm going to take the difference between my true output and my predicted output and square that and then the idea is to minimize this and the way you do that is you sample data points from your training data and you take the derivative of your parameters with respect to this with respect to your loss function and then you move in the opposite direction of that gradient which would hopefully move you down on the error surface so the problem is a non convex optimization problem so for example linear classifier because it's linear we'll have a cut it'll just look like a bowl whereas these things you have these nonlinear activation functions and you end up with a very messy looking error surface and before the 2000s that was the big that was the number one thing that was holding back neural networks is that they were difficult to get working hard to train and so basically the thing that's changed is one way faster computers we have GPUs which can paralyze operations especially those big matrix multiplications and then there's a lot more data that's not entirely true so there's also a lot of other tricks that we found out recently so for example if you have lots of hidden units then that can be helpful because it gives more it gives more flexibility you could say in the optimization like if you have if you over-provision if your model has two more capacity than it needs then you can be more flexible with the kind of functions that you can learn so we have better optimizers so whereas SGD will make it'll take it'll step in the same direction by the same amount every time we have these newer optimizers like auto granite atom that decide how far to move in a direction once you've decided the direction we have drop out which is where you noise the outputs of each hidden unit and that makes the model more robust to its own errors and it guards against overfitting there's better initialization strategies so there's things like save your initialization and there's things like pre training the model on a related data set before moving on to the data you actually care about and then there's tricks like back to norm which is where you ensure that the inputs to your neural network units have are normally distributed they have mean 0 standard deviation 1 and what that does is allows you to basically take bigger steps sizes yeah and the takeaway here is that but in general the optimization problem and the model architecture you define are very tightly coupled and it's kind of a black magic to get that right balance that you need and we're still not very good at it okay so we're going to talk about convolutional neural networks now and so these operate over images the motivation is that okay so we have a picture here right and we want to do some kind of machine learning processing on it we have all the tools that we need to do that you could say okay each picture each pixel is an element in a big long vector and then I'm just gonna throw that out of matrix but the thing is is that that doesn't really take advantage of the fact that there's spatial structure in this picture so this pixel is gonna be more similar to this pixel than this pixel down here but if you pass this entire thing through a matrix then every pixel is gonna be treated uniquely differently and so we want to leverage that spatial structure and the idea to the core idea is with convolutions so convolutions you have this thing called a filter which is some collection parameters and what you do is you run your filter over the input in order to produce each output element so for example this filter when applied to this upper left corner produces this upper left corner of the output and an application of a filter works kind of like a dot product where you multiply you multiply all the numbers and then you add up them all up and so how you produce these outputs as you take your filter and you basically just slide it around in the input in order to get your output at the next layer so yeah so this this example zone one concrete so here so whereas this was a two dimensional convolution because we had a two dimensional filter and we were sliding around in both dimensions this is one dimensional we have a one dimensional filter and we slide it horizontally across so for example at the very left we apply it so 1 times 0 is 0 0 times 1 is 1 and negative 1 times 2 is negative 2 so negative 2 goes in the output and then we do the similar thing here so we would dot product this filter with these three numbers in order to arrive at to one of the advantages of this is that whereas a so if you had so if you had let's say you had like 4 inputs you so this is your hidden layer so Y h1 h2 h3 and h4 and then you had 4 inputs x1 x2 x3 and x4 if you did a regular fully connected matrix layer then every one of these is going to be connected to every one of these and your parameters you're gonna you're going to end up with a 4 by 4 matrix 1 1 W 1 2 W 1 3 w 1 4 and W this is what your matrix is going to look like if this is your W because you need that new you need a weight for every one of those connections whereas if you're doing convolutions it's much more efficient because there's this idea of local connectivity so you have your 1 H 2 H 3 H 4 X 1 X 2 each hidden layer is only connected to what's called its receptive field which is the inputs that the filter would be applied to and in this case we would only have three weights because we just have this sliding window and you apply it at each step so a gives you local connectivity B it's much more efficient in terms of parameters you you're sharing the same parameters at different places in the input and it gives you this cool intuition of sliding around in the input so it's like I have my filter of three things and it gives you this good intuition of if if a let's say this is let's say this is negative one this is a hundred this is 1 and this is 3 then what this you could you can interpret this as my filter really likes whatever pattern is going on in these three inputs and it doesn't like so much all the other patterns that it's picking up on and so yeah you have this nice interpretation for the filters in general what this looks like is so in practice instead of one dimensional two dimensional they're very high dimensional volumes and so your filter is going to be a cube in the input space and you're sliding it around and applying it at every place it can fit in this input and then the reason why the output is also a volume is because you have multiple filters so over here for example this blue filter is when you slide it around in the input it's going to give you this like plane of outputs but then you have a second filter this green filter that you can also slider on the input and that's going to give you a second dimension to your hidden states so Andre Karthi has this nice demo where basically we have so we have a three-dimensional input and we have two filters which are like you can think of as little cubes and it's sliding these cubes around the input and every application gives you one output in this like three dimensional output volume so this is the same picture as before where you sliding around cubes in order to fill in you could say like layers of the out put solar on cubes to fill in these layers of the output another thing people do is max pooling so remember that interpretation of a filter as a as like a pattern detector what this is saying is you take a region in your input so you you run your filters of the input get your like preliminary output and then you look at regions in the output and take the maximum activation and carry that on two successive layers and the intuition there is that instead of that you're looking you're searching for a pattern in a region of the input and it's also helpful because remember at the end of the day we want to do classification or regression or something we want to get this thing down to like a very small number of numbers and if we have this huge high dimensional volume then anyway we can reduce its size is good so this is an example of how these things work is is it's they're pretty straightforward basically so you you just have your convolutional layers you stack them all up everyone somebody have some pooling and you go down and down in dimensionality until you eventually get down to a like a distribution over possible labels and this ties saying before about that Lego block analogy because this this entire network is built up of one two three four different Lego blocks in a way and it's basically just stacking them on top of each other and composing them up in order to get a image classifier so I'm going to talk about three case studies of CNN architectures so the first one is Alex net so this was that one that did really well in the image net competition and really brought CNN's to the mainstream for computer vision basically it was just a really big neural network one trick they did was they use Braille use instead of so the sigmoid that we've learned about I think we've the sigmoid that we've learned about this is an activation function and it's going to look something like this and what they did was instead they used the review which looks a little more like that and in practice it turns out to be a little easier to train and use the next one is Vida Gina which did them imagenet a couple years later basically it's very similar it's just a CNN I think the thing to note about this one is that it's very uniform so it was 16 layers and there's nothing fancy in it it was just a bunch of these Lego blocks stacked up the entire network is pretty much like you just by looking at this picture you could probably reimplemented out it is it started this trend of deeper of kind of like tall and skinny networks so you'll notice that there's a lot of layers but each layer is very thin and residual networks or rez Nets kind of takes that to the nth degree so the idea with ResNet is so most of the time you take your input you pass it through a matrix state an output if you add in your input again then that is very helpful because it makes it easy for the model to learn the identity function and so you can give the model the capacity for like a hundred layers but if you add in these residual connections which is what you call it when you basically just like added and I'd add an X then it allows the model to skip a layer if it decides that that's what's best for itself you just set W to zero that's what you do so it also helps with training back propagation if you take the derivative of the loss with respect to your input that derivative is just gonna be one for this part of the sum and so it gives in a way you could think of it as its it gives the it gives the error signal kind of like a highway through the network and it allows the gradients to propagate much deeper into these large networks and so ResNet got three point six percent error on image net if you remember the alux net way it blew everyone all the water and i got like fifteen percent i think this is much better than human performance and it will come up later when we talk about this idea of like residual connections will come up later when we talk about we're current mental networks so just to summarize convolutional neural networks are often applied in image classification the key idea is that there is you have these filters which you are sliding around the input and that lets you one have it does kind of this like I give local connectivity so as a space in the app in the output only depends on a small patch at the input instead of the entire input and then second it's the perimeters are shared depth has turned out to really matter for these networks and I think to this day it's like people it's like every day there's just a deeper network that's coming out and people haven't really found a bound to death I guess yeah design intuition that said how many layers what the layers should be and so forth yeah so the question was how to design these things since this it seems so arbitrary and yeah it is it is really arbitrary I think I think so there's a few different ways so first you start with something okay so first you would start with something that sounds reasonable and then you would do some kind of like a grid search or you would do now there is a literature on medal earning which is where like you have a model decide what your model looks like or but in most cases you just kind of like and tune it you're like oh if I add a layer does it go up or down second you look at the literature and say okay someone else solved a similar problem to me and they used network XYZ and so I'm gonna start with that and then start fiddling from there and then third is to literally take that network that's been pre trained on a task and then apply it to yours we'll talk about it later but pre-training networks and applying that to your task has shown to be very helpful okay so now we're going to talk about recurrent neural networks the idea here is that you're modeling sequences of input this could be things like text or sentences it could also be things like time series data or financial data and a recurrent Lamarque is something where the input feeds its past inputs into itself so it has time dependencies so for example we have this very simple recurrent neural network here it is a function with one matrix and it takes as arguments a past hidden state and a current input and then it predicts the next hidden state so this this is what it looks like if you were to write it in code or something this is what the actual network looks like so there's an input and you feed that into your function as well as your current state and it just kind of loops on itself and most of the time people talk about this third perspective which is taking kind of this network and I and kind of like unraveling it across time I guess you can say unfolding it across time where every time step you have an input and you have a state and then you have your function which carries you to your next state yeah excuse me colleges defer from having your original oh I see so the question was what's the difference between this and the setting before when we had when we were like the cost of gradient descent where we were updating our weights sequentially yeah so that is that is an interesting question so the in the difference is that before for SGD that was for it was sequential in the training whereas this is sequential in the inference so each so you do you feed in 10 inputs let's say 10 times such those are inputs and then after all that time then you back propagate once for all those time steps so to make that more clear so for SGD it's like you have x1 y1 x2 y2 and you use this to update W right so you update W and then you update W and yeah that's that's an interesting observation that there's this kind of like time dependency on but there's no time within the data itself for for the recurrent setting it's it's more like this it's like if I'm which marker is better so it's more like this it's more like you have X 1 1 X 1 2 X 1 3 and Y and then X 2 1 X 2 2 and X 2 3 and Y and then you use this to update W and in this setting when we talk about time or temporal and like when we talk about a sequence we're talking about a sequence here in the data not necessarily in the learning yeah ok so to me this is a more concrete example we're going to talk about a neural network language model so this is a model that is in charge of sucking in a sentence and predicting what is the most likely word that would come next in the sentence so we're so each input we call X and our hidden States we call H's and the way this works is we have some function that takes X 1 and it codes it in to our hidden state and then we have a second function that takes the hidden state and decodes it into the next input we continue by taking both the next both x2 our next have input and h1 our previous hidden state and then we use that to and create a new hidden encoding and then we take that new hidden encoding and decoding to our next input and we just rinse and repeat each time we take the current input and the previous hidden state to first create an encoding and then second predict our next input you so there's those two steps the cool thing about this to note is that this now were building up vectors these H ice and that's exactly what we're looking for it's a vector that in some way captures the meaning or a summary of all the X all the inputs that we've fed up until that time step so now we have a vector which compresses all those inputs into one vector so to make this very concrete one way you could build this thing is by basically each of these arrows you stick a matrix in there so our encode function would take the input the XT and multiply it by a matrix in order to get a vector and then it would take the previous hidden state HT minus one and multiply that by different matrix to get a new vector then you add those vectors and that gives you a vector which is your new hidden state and your decode is the same thing you take your hidden state pass it through a matrix to get a vector and then send that through a softmax to turn the vector of logits into a distribution of probabilities in general though there's this problem with kernel networks so if there is a short dependency between the input and the output if an output depends on a recent input then the path through this network is very short right so it's easy for the gradients to reach where it needs to reach in order to train the network properly but if there was a very long dependency then the gradients have a had they have difficulty getting all the way through you so if you remember we talked about gradient descent as a credit assignment problem where the gradient is in some ways like saying how much okay so if I change the input if I change this input by a small if I perturb by a small amount how much would the output change that's in what sense that what the gradient is saying but if the input and output are super far away from each other then it's very difficult to compute how small perturbations the input would affect your output and the reason for that we won't get into it so much but basically if you want to compute the gradient then what you have to do is you have to trace the entire path of that dependency and you look at all the partial derivatives along that path and you multiply them all up and so the problem is is that if the path is very long then you're multiplying a lot of numbers and so if your numbers are less than one then the then the product the over product is gonna get really small really fast right and if the numbers are bigger than one then that product is going to blow up really quickly and so that is a problem because it means your gradients are going to be tiny and you know learning signal or they're going to be way too big and you're going to just like shoot into some crazy direction that in practice will like blow up your experiments and nothing will work so it's problem so for the good thing is that for the exploding braiding problem that's not so bad there's a quick fix what people do is what they do is what's called clipping gradients so you'll specify some norm you'll be like any gradient with a norm bigger than two I'm going to clamp off at two so if your greens explode and they go to ten million you're gonna say okay that's bigger than two so it was it wasn't ten million it was actually two but for the finishing grading problem there's this cool idea the long short-term memory cell which is similar to a recurrent law at work but it has two hidden states and so this is kind of a wall of equations but I think the important thing to note is that you're doing this this is basically like your input in a way and this is kind of like your previous hidden state and so what's going on here is you're doing an additive combination you're taking your input and you're adding in your previous hidden state very similarly to those residual connections in the resin ette and so this because you're adding in your previous state it's kind of like adding in your proofs input I guess you could say and it allow it gives the gradients kind of like a highway to very easily go back in time there's another perspective on this so this picture that in the notation is different but I think the thing to note here is that this so those those are are you could say our hidden states in this network or I guess so in an LTM cell you could I guess you could say that there's two hidden states that's what people say so you have one hidden state which is your HT and that's the state that you exposed to the world and if you said my else team is gonna have the same API as my RNN then this would be like the equivalent of that hidden state that we had for the Arnett but then you also have this in ternal hidden state the sea state that you never exposed to the world and so in this picture the sorry the notation is a little confusing but the Oh a Mis picture corresponds to the H in the previous perfect picture so this is the hidden state that you're exposing to the world and then the S corresponds to C so this is your internal hidden state and the thing to note about this picture is that s is just zipping around on what's called the constant air cárcel and it's always internal and it's zipping around this thing in a loop and so what it ends up doing in practice is it ends up work like that it's a vector and it contains very long term information that's useful for the network over many many time steps so if you if you poke around individual dimensions of that state then you and then you can find these long-term things being learned so for example Andre Karthi has a great blog post you find networks that you find units that track the length of the sentence you find units that track syntactic cues like quotes or brackets but in general you find a lot of things that are just not easily interpretable so one last cool I guess idea that people have used with these were criminal networks is sequence of sequence models like machine translation which is where you have two sequences you have an input sequence in an output sequence and you want to suck in your input sequence and then spit out your output sequence and you do this with what's called a encoder decoder paradigm you encode your sequence by giving it to your RNN and that gives you a 1 vector which is encoding or a compression of that input and then you decode your sequence by spitting out your outputs just like we were talking about before with the language model and more recently there's these attention based models which are very helpful in the case where there's long sequences so if you look back here X 1 X 1 X 2 X 3 are all getting compressed into a single vector well if you have a really long paragraph maybe it's hard to shove that into your you know 200 dimensional vector it's hard to capture the depth of all that language with just a bunch of numbers and so the idea behind attention is to is to look back so the way attention works is at a very high level is so if you have your inputs we have x1 x2 and x3 and we've run our RNN over these inputs and so we have three hidden state vectors h1 h2 and h3 and now we're decoding and so we have we have our nand decoder that has some hidden state we'll call it s1 what happens is you compare your current hand state with all of the states in your encoder and you compute a number that says how much do I like this state so maybe maybe it like really really likes this number and it's not too happy about this number this vector and it doesn't like this vector what it does is it is it uses these scores to turn them into a distribution a probability distribution that again says how much how much do I as s1 like each of these vectors and then what you do is you compute a weighted average of these hidden vectors where the weights come from this distribution and what this does is it serves two purposes so first and there's another way of kind of writing this down on the slides but I think this serves two purposes so first of all it gives you some interpretation so every time step you can see what parts of the input is it focusing on what parts the inputs have a lot of probability mass on them and then second what it lets you do is it lets you it kind of releases the model from the pressure of having to put the entire in put sequins into a single vector now what it can do is it can dynamically go back and retrieve the information it needs and then more recently there's what's called transformer models which are which do away entirely with the RNN aspect it's just attention and with a transformer you have your hidden states and instead of instead of having some kind of decoder hidden state that you're comparing to the others what you're doing is you just you select each hidden state and you compare h1 to all the other H's including itself to get a number of how much h1 likes those other H's and then you compute your weighted average of all of these hidden states and that becomes your next layer so I would recommend taking 224 n if you're interested in this topic transformers are very cool and they've recently become I guess you could say like the new LS TM so from these attention distributions you get cool interpretable maps like in translations so this is an attention distribution and it points at words that are correspondent so like economic corresponds to economic and and you can see that in the distribution they also do this in computer vision you can highlight areas of a picture yeah so just to summarize so we're currently rx you can throw a sequence at them and they'll give you a vector there's this intuition that they are processing input sequentially kind of like a for loop but they have a problem with training where the gradients either blow up or they shrink very small so LS TMS are one way of mitigating this problem but they're not perfect they still have to shove information into one vector and so the way people get around this is with attention based models where you dynamically go back into your input and retrieve the information you need as you need it so now we're going to talk about unsupervised learning so like I said before neural networks we got them to work well recently and a lot of that is just because they need a lot of data but if you're a smaller lab or if you don't have enough money to basically pay for a data set or even if it's a hard problem that just there isn't a lot of data forward there's a lot of cases where there isn't enough data to train these very very large models with millions billions of parameters but on the other hand there's tons of unlabeled data laying around now you can download the whole internet if you want and there's kind of this real inspiration from us as human beings like we are never given labelled data sets of of what foods are edible and what foods are not edible right you just kind of you absorb experiences from the world and you use that to inform your future experiences and you're able to kind of like reason about it and make decisions so yeah so the first I guess thing that we're gonna get into is Auto cutters so the idea behind autoencoders is that if you have some information and you try to learn a compressed representation that information that allows you to reconstruct it then presumably you've done something useful so in neural network speak the way that works is you give it some kind of effector and you pass that through an encoder which gives you a hidden vector and then you pass that hidden vector through a decoder which you use to reconstruct your input and the implicit loss in most cases is you want to take the difference basically you want your reconstructed input and the original input to be very similar so just to motivate this this isn't deep learning but principal component analysis is could be viewed as one of these encoder decoders the idea behind principal analysis is you want to come up with a matrix U which can be used to both encode and decode a vector so you multiply X by u to give you a hidden vector or a new representation of your data but then if you transpose u and multiply it by your hidden vector then that should give you something as close as possible to your original data so but there's a problem so as we if we have a hidden vector with a bunch of units then it's not gonna learn anything right it's just gonna learn how to copy inputs into outputs and so a lot of research on Auto encoding and this kind of unsupervised learning is about how to control the complexity and and make the model robust enough to generate useful representations instead of just copying so the first pass at that can be using lawnmower transformations so you would do something like little logistic or the sigmoid loss and that means that the problem can't be solved any more by just copying into the output and so you're gonna have to actually learn something useful another way of doing it is by corrupting the input so you have your input and you noise it may be you drop out some numbers from it maybe you perturb some numbers of it maybe you add in maybe draw from like a Gaussian and add that to your input and then you pass that through so yeah so you could drop so if your vector is 1 2 3 4 you could drop out 1 in 4 and just set them to 0 or you could slightly perturb these numbers so that they're close to their original but different not the exact same and then the idea is that after you pass this encoded input through both your if you pass this corrupted input through both your encoder and decoder then the output the eventual x-hat should be very close to your original uncorrupted input yeah so another is a variational encoder which has a cool comp probabilistic interpretation so you can think of it as kind of a Bayesian network I think maybe this is more useful to look at so you have an encoder and a decoder and they are both modeling I guess probability distributions so what this is saying is I want to encode X into a distribution over HS and you learn a function which is in charge of doing that and then you want to specify some conditions first you say okay I want to make X recoverable from my each distribution and then second this is a term that kind of it prevents h from being degenerate so maybe a good way of thinking about this is instead of so a traditional auto encoder would take my input and it would map it it would send it through some kind of encoder and map it into a hidden vector send that through a decoder and that would reconstruct my input whereas a variational auto encoder is going to take my input and it's going to map that into a distribution over possible HS and then what I'm going to do is I'm going to sample from this distribution pass that through my decoder and produce my reconstructed input and the nice thing about this is that since this is a distribution instead of a vector you've imposed some structure on the space so points that are close together in this space should map to similar X hats and then similarly as you move through this space you should be able to gradually transition from one I guess reconstructed input to a second so for example there's these cool experiments and computer vision where they'll say up here this is going to give me like a chair or something and then down here is going to give me like a table and then if I move from one to the other then and you constantly decode then it'll like gradually morph into the table it's really cool okay and then so then the last method of unsupervised learning that we're going to talk about is motivated by this task so there's this data set called squad which is about a hundred thousand examples where each example consists of a paragraph and then a bunch of questions like multiple choice questions based on that paragraph so the problem here is that there's only a hundred thousand examples and really the intelligence that this task is trying to get at is just can you read a text and understand it which is more general and this game is captured by more data than just these hundred thousand in particular it's captured by just all the text that you could possibly read so there's billions of words on Wikipedia and Google you could just crawl the web and download it and if somehow you could leverage that maybe that would be helpful for your reading comprehension and that is just a perfect case of this of this setting where we have tons of unlabeled data very small amount of labeled data so recently the NLP community has come to its idea called Bert well there's actually not just Bert but there's a lot of people who are doing similar things but Bert is the example were talking about with Bert what you do is you take a sentence and then you mask out some of the tokens in the input and then you train a model to fill in those tokens and they actually train the model on a bunch of things so they trained on token filling and then they also would glue two sentences together and and ask the model are these sentences like would they be adjacent in the text or not like do they make sense together or not but the idea is basically like have give a bunch of unlabeled text to a model which is just going to kind of like manipulate that data in order to learn structure from it without any explicit purpose other than just learning the structure and they trained it on a bunch of data for a long time and then what you can do is once so Bert is actually a big so we talked about transformers before and birds like a big transformer so they trained this thing on a ton of unstructured text on just this word filling task for a long time and then what they did was they took their pre trained Bert and they took the and they started feeding it questions from squad so they took questions and then they would glue on to it the paragraph the context that they used to answer the question and then they would take whatever vectors are coming out of Bert and they would pass that through a single matrix which basically predicts the answer for that squad question and it did really well so this picture is right when Bert was released and these are all of these state-of-the-art models for squad and Bert not only beat all the other models by a large margin but also be human performance and I guess the in the intuition the intuition behind Burt was that by doing these seemingly trivial tasks like word filling and next sentence prediction what you end up learning is you end up learning the vectors are coming out of this our vectors that say like what is the meaning of a word what is the meaning of a word in this context what is the meaning of this sentence and that meaning isn't like operationalized towards solving any task in particular it's just like a in a very general sense like what is I'm gonna imbue this model with an understanding of language and then once I have an understanding language I'm gonna then apply it to my very targeted downstream task and that is that is kind of the principle behind unsupervised learning so you kind of like make up these almost trivial prediction tasks just to manipulate data and learn structure from it understand language understand what a picture is and then what you do is then you fine-tune it on the very small amount of label data that you have and that's kind of what the currents say the art is in a lot of fields is basically just doing more and more unsupervised pre training with bigger bigger models and bigger bigger data and the field really hasn't found like a limit to this yet and it'll be interesting to see how far it goes so I'm gonna skip those slides but just to kind of wrap things up recently I guess the biggest things that people have gotten to get neural hours working is one better optimization algorithms so we have these adaptive algorithms that are not as I would say like obtuse as SGD doesn't have to move in this by the same amount every time you have a lot of tricks like you know fine-tuning on supervised learning clipping the gradients batch norm we have better hardware we have better data and that allows us to experiment more and train larger models faster yeah we wait a long time but I think I think one of maybe one of the problems with the field is that the theory is is a lot of ways lacking and we don't know exactly why neural networks work well and why they're able to learn good functions despite having a very difficult like optimization surface yeah so just to summarize we we talked about a lot of different building blocks we talked about how to leverage spatial structure with convolutional neural networks we talked about how to feed sequences into recurrent neural networks and transformers and LSD m's we talked about you know the sequence the sequence paradigm for machine translation and unsupervised learning methods that helped you kind of like jumpstart your downstream applications and I think the big takeaway here is that and in some ways the big advantage of a neural network is that they are compositional so you it's like a it's like Legos you take they take an input and they turn it into a vector and then once you have a vector you can start combining these things in very flexible ways and so in a lot of ways designing these things is a lot like putting together Lego set you have your building blocks in LS TM attention and coding and you can decide how to it's like oh I want to run this LS TM here and this else I'm here and then I want this one to attend over this one and then I'm going to concatenate the result with the output of this CNN and because of because of I guess you could say like the magic about propagation you can combine these things and I think even more generally it allows you as a programmer to instead of make a program for solving a problem it allows you to make this scaffolding that allows a computer to teach itself how to solve the problem so instead of defining the function you want the software to learn you define a very broad family of functions that the software is allowed to learn and then you let it go and run off and find the best match within that yeah so those are all things talking about today but I hope you all have a good Thanksgiving break