okay so hi everyone so today's to continue talking about search so so that's what we were going to start doing finish off some of this stuff we've started talking about last time and then after that switch to some of the more interesting topics like learning so a few announcements so the solutions to the old exams are online now so if you guys want to start studying for the exam you can do that so so start looking at some of those problems I think that would be useful actually let me start with the search to lecture because I think that might be like that has a review of some of the topics we've talked about so it might be easier to do that also I'm not connected to the network so we're not going to do the questions or show the videos because I have I have a hard time connecting a network in this room ok all right so so let's start continue talking about search so if you guys remember we had this the city block problem so let's go back to that problem and let's just try to do a review of some of the some of the search search algorithms we talked about last time so so suppose you want to travel from city 1 to city n only going forward and then from city n you want to go backwards so and back to city 1 going only backwards okay so you so the problem statement is kind of like this you're starting in city 1 you're going you're going forward and you're getting to some city n so maybe you were doing out on this and then after that you want to go backwards and get to get to city one again so you're going to some of these cities so that's the goal and then the cost of going from any city I to city J is equal to CIJ ok so so that sells so the question is what which one of these following algorithms could you use to solve this problem and it could be multiple of them so we have depth first search breadth first search dynamic programming and uniform cost search and these were the algorithms we talked about last time so maybe just talk to your neighbors for a minute and then we can do votes on each one of these yes listen pasa pasa okay let me take that again thank you thank you for so let's maybe start talking about this so how about depth-first search like how many will saying we can use their search how many people think we can't use that search it's very like good split so so some of the people think we can't use that their search what are some reasons maybe yeah so here we are basically going from city one to see the end each one of these edges have a cost of CIJ I'm just saying CIJ is greater than or equal to zero that's the only thing I'm saying about CIJ but if remember that first search you really wanted the cost to just be equal to zero because if you remember that whole tree like the whole point of depth-first search was I could just stop whenever I could find a solution and we were assuming that the cost of all the edges is just equal to zero so so we can't really use that search here because because our cost is not zero so assuming that like not again you know that reasoning how about breadth-first search can be used for search so so that's a good way so what suggesting is can we think about the problem as going from city one to city in and then after that like introduce like a whole new problem that continues that and starts from CDN and goes to city one let me get back to that points like in a second because like you could potentially think about that actually like that might be an interesting way of thinking about it but but irrespective of that I can't use depth or search so I'm so far I'm just talking about depth or search irrespective of how I'm looking at the problem the costs are going to be nonzero so because the costs are going to be nonzero I can't use that first search so so let's talk about matters so how about birth research can I use breadth-first search you cannot use fresh first search here because for breadth-first search if you remember you really wanted all the costs to be the same they didn't need to be 0 date but they needed to be the same thing because then you could just go over the levels and then here I'm not like I'm not saying I'm not putting any restrictions on CIJ being the same thing ok so now let's talk about dynamic programming how about dynamic programming can be used sine amock programming alright so that looks right right like we could use dynamic programming here everything looks ok CI J's are positive looks fine how about actually one question so so don't I have cycles here we kind of briefly talked about this already so don't I have like this cycle here so we could actually use dynamic programming here even if it kind of looks like we have a cycle and the reasons we can kind of use this trick for we can basically draw this out again and for going forward basically go all the way here and then after that we're going backwards kind of include the directionality too so all I am doing is I'm extending the state this space to not just be the city but be the city in addition to that be the direction that we're going so if I'm in city four here it's City for going forward and if at some point in the future I'm in city and on a four again it's City four going backwards so I'll keep track of both the city and the directionality and when I do that then I'm kind of breaking the cycle like I'm not putting any cycles here and I can actually use dynamic programming that makes it and then uniform cost search that dad also sounds good - right like uniform cost search you could actually use that doesn't matter if you have cycles or not and then we have positive positive non negative costs so we could use uniform cost search yeah all right so this was just a quick review of some of the things we talked about last time and another thing we talked about last time was this notion of state okay so so we started talking about three search algorithms and at some point we switch to dynamic programming and uniform cost search where we are where we don't need to like we don't need to have this exponential blow-up and the reason behind that was you're we have memoization and in addition to that we have this notion of state okay and so what is a state a state is a summary of all past actions that are sufficient for us to choose the future optimally so so we need to be really careful about choosing our state so in this previous question we looked at past actions so if you look at like all cities that you go over it can be in City one then three and four five six and I see a three again so in terms of state the things that you want to keep track of is what city you are in but in addition to that you want to have the directionality because you need to know like where you are and how you're getting back okay so and we did a couple of examples around that trying to figure out what is what is like a specific notion of state for various problems all right so so we started last time talking about search problems and and we sort of formalizing it so if you remember our paradigm of modeling and inference and learning we started kind of modeling search problems using this formalism where we defined a starting state that's a start and then we talked about the actions of which is a function over States which returns all possible actions and then we talked about the cost function so the cost function can take a state an action and tell us what is the cost of that that that that edge and then we talked about the successor function which takes the state in action and tells us where we end up at and at the end we had this is end function that was just checking if you're in an end state or not so these were all the things that we needed to define a search problem and we kind of tried that in a couple of examples to trim' example the city example of that and then after talking about these these different ways of thinking about search problems we started talking about various types of inference algorithm so we talked about tree search so depth first search breadth first search depth research with iterative deepening backtracking search and then after that we talked about some of these graph search type algorithms like uniform cost search and and dynamic programming so last time we did an example of uniform cost search but we didn't get to prove the correctness of it so I want to switch to some of the last last last times slides to just go over this quick theorem and then after that just switch back to to this lecture ok so uniform cost search like if you remember what we were doing in uniform cost search we had three different sets we had an export set which was basically the set of states that we have visited and we are sure how to get to them and we know the optimal path and we know everything about them we had this frontier set which was a set with a set of states that we have got to them but we are not sure if the cost that we have is the best cost cost there might be a better way of getting to them and we don't know like we are not sure yet and then we have the uh neck Sports set of states which are basically states that we haven't seen yet so we did this example where we started with all the states in the unexplored set and then we moved them to the frontier and then from the frontier we moved them to the export set so so this was the example that we did under board okay and then we realized that like even if you have cyclones we can actually do this algorithm and then we ended up finding the best path being from A to B to C to D and that cost three so let's actually implement uniform cost search and so I think we didn't do this last time so going back to our set of so we started writing up these algorithms for search problems so we have we have written dynamic programming already and backtracking search so now we can we can try to kind of implement uniform cost search and for doing so we need to have this priority queue data structure so this is in a util file I'm just showing you what it like what functions it has it has an update function and it has a remove min function so it's just a data structure that I'm gonna use for my frontier because like my frontier I'm popping off things off my frontier so I'm going to use this data structure alright so let's go back to uniform cost search so we're going to define this frontier where you're adding states to it from unexplored set we're adding states to the frontier okay and it's going to be a priority queue so so we have that data structure because we just imported util and you're going to basically add this start state with a cost of 0 to the frontier so that's the first thing we do and then after that like while the frontier is not empty so while true what we were going to do is we're going to remove the minimum past cough an element from the frontier so so basically just pop off the frontier the best thing that exists there and just move that to the explored set okay so when I pop off the thing from the frontier basically I get this past cost and I get the state ok all right so so if we are in an end state then you're just going to return that pass cost with the history I'm not putting the history here for now I'm just returning the cost okay so after popping off a state from the frontier the thing we were doing was we were adding the children of that so the way we do that is we're going to use this successor and cost function that we defined last time so we can basically iterate over action new state and costs in the successor and cost function and basically update our frontier by adding these new States to it ok and then the cost that you're going to it's cost post past cost if that is better so so that's what the update function of the frontier does and that's pretty much it like that is uniform cost search you add stuff to the frontier you pop up stuff from the frontier and and that way you explore them you move things from now on exports it to the export set so let's just try that out it looks like it is doing the right thing so it got the same value as dynamic programming so looks like it kind of works okay so this code is also online so if you want to take a look at it later actually this is not what I want to do yeah okay all right so so that was and and here's also the pseudo code of uniform cost search okay okay so we have between there's a question right so what sort of the question is what's the runtime of uniform cost search so the wrong time of uniform cost search is order of n log n where the log n is because of like the bookkeeping of the priority queue and you're going over all the edges so so he can think of n here as the edges and worst case scenario if you have a fully connected graph it's technically and squirt log n but in practice we have sparse or graph so people usually refer to B that's just n log n where n is the number of states that you have explored and it's actually not all of the states it's the states that you have explored okay and dynamic programming is order of n so technically like dynamic programming is slightly better but really events actually go first give it is the only difference between this and I stress that you just don't have them all yeah so what's the questions was the difference between this and the Dijkstra's algorithm they're very similar the only difference is this is trying to solve a search problem so you're not I like exploring all the states when you get to the solution you get to the solution and then you just return that Dijkstra you're going from you're basically exploring all of all of the states in there in your graph all right sounds good okay so I just want to quickly talk about this correctness theorem so so for uniform cost search we actually have a correctness theorem which basically says uniform cost search does the right thing so what basically the theorem says is if we have a state that we are popping off the frontier and we're moving it from the frontier to the export then it's priority value which is equal to past cost of s is actually the minimum cost of getting to to the state s so what this is saying is let's say that this is my export set so this is my exports and then right here is my frontier and I have a start state okay and then I have some state s that right now I've decided that I am popping off s from the frontier to export because that is the best thing that has the best pass cost so what the theorem says is this this path that I have from and start to s is the shortest path possible to get to get to the state s ok so the way to prove that is to show that the cost of this path is lower than any other path paths that go from s start to s so let's say there is some other path this green one that goes from s star to s some other way and the way that it goes to s is it should probably leave the export set of states from some state called t maybe to some ghost go to some other state nu and then from you go to us UN s can be the same thing but the point of it is if I have this other path that goes through s it needs to leave the export set from some state so what I want to show is I want to show that the cost of the green line I want to show that that is greater than the cost of the black line okay all right so the cost of the Green Line what is the cost of the Green Line it's going to be the cost to here and then cost of T to you and the cost of u to s so I can say well this cost is actually greater than or equal to priority of T because that is the cost of getting to T plus cost of C to you and I'm just dropping that's this last part the u to s I'm just dropping that okay so cost of green is like at least equal to priority of t plus cost of T TT to you okay well what is that equal to priority is just a number right it's just a number that you're getting off the priority queue so that is actually equal to past cost of t plus cost of t to you and and this value is going to actually be greater than or equal to priority do you well why is that because if you is in my frontier I visited you so I already have some priority value for you and the value that I've assigned for the priority of you is either equal to this path cost of T plus cos of T to you because I've like seen that to use in my export use in my frontier so I've definitely seen this or it is something better that I don't know what it is right so so priority of U is going to be less than or equal to this path cost of T plus cost of T to you okay and well what do I know in terms of priority of you and priority of s well I know priority of U is going to be greater than or equal to priority of this well why is that because I already know I'm popping off s next I'm not topping off you like like I I know I'm popping off the the thing that has the least amount of priority and the least value here and that's s and and well that is equal to cost of the black line right so that was just a quick like proof of why uniform cost search always returns kind of the best minimum cost path all right so let's go to the slides again so just the comparison quick comparison between dynamic programming of uniform cost search so we talked about dynamic programming we know it doesn't allow cycles but in terms of action cost it can be anything like it look you can have negative cost you can have positive cost and in terms of complexity is order N and then uniform cost search you can have cycles so that is cool but the problem is the costs need to be non-negative and it's order n log N and if you have if you end up in a situation where you have cycles and your costs are actually negative there's this other algorithm called bellman-ford that we are not talking about in this class but you could actually like have a different algorithm that addresses those sort of the settings okay all right okay so that was that was this idea of in France right now we have like a good series of ways of going about doing inference for search problems you have formalize them and now the plan for this lecture is is to think about learning so how are we going to go about learning when we have search problems and when our search problem is not fully specified and there are things in the search problem that are not specified and you want to learn what they are like the cost okay so so that's going to be the first part of the lecture and then towards the end of the lecture we are going to talk about a few other algorithms that make things faster so so smarter ways of making things faster we're going to talk about a star and some sort of relaxation type strategies all right so so let's go back to our transportation problem so so this was our transportation problem where we had a start state and we can either walk and by walking we can go from state s to state s plus 1 and that costs 1 or we can take a tram a magic tram that takes us from state s to state to s and that costs - ok want to get to state so we can formalize that a search problem we can like we saw it we saw this last time we can actually try to find what is the best path to get from state 1 to any state and like we saw like past like walk walk tram tram tram walked round tram this is one potential like optimal task that one can get ok but the thing is the world is not perfect right like modeling is actually really hard like it's not that we always have this nice model with everything and we could end up in scenarios where we have a search problem and we don't actually know what the costs are our actions are so we don't actually know what the cost of walking is or what the cost of traumas but maybe we actually have access to to this optimal path like maybe I know the optimal path is walk walk tram tram tram walk tram tram but I don't know what the costs are so the point of learning is is to go about learning what these cost values are based on these these optimal paths that be half so so I want to actually learn the cost of walking is one and the cost of travesty and this is actually a common problem that we have like in machine learning in general so like for example you might have data from how a person does something or like how a person let's say like grasp an object and and I have no idea what was the cost that the person was optimizing to grasp an object right but I have like the trajectory I know like what the path they took when they picked up an object so what I can do is if I have access to that path of how they picked up an object then from that I can actually learn what was the cost function that they were optimizing because then I can put that cost function maybe on a on a robot that does the same thing that's a good question so the question is is it possible to have multiple solutions yes so we are gonna actually see that like later like what sort of like solutions are we going to get or they're there there could be cases where we have multiple solutions the ratio of it is the thing that matters so if you have like walk is one tram is for if you get to an eight you kind of get the same sort of behavior and then it also depends on what sort of data you have like if your data allowed you to actually recover that that's true solution so so we're gonna actually talk about all this cases okay all right okay so if you think about it when the search problem we were trying to solve this was the inference problem was when we were given kind of a search formulation and we are given a cost and our goal was to find a sequence of actions this optimal sequence of actions that was the shortest path or the best path and and some thought some way and this is a forwards problem so search is this forward problem where you're given a cost and you want to find the sequence of actions okay so it's interesting because learning in some sense is an inverse problem it's the inverse of search so the inverse of search is if you give me that sequence of actions that's the best sequence of actions that you've got then can you figure out what the cost this so so in some sense you can think of learning as this inverse problem of search N and we are going to kind of address that so I'm going to go over one example to talk about learning and I'm actually going to use the notation of the machine learning lectures that we had at the beginning of like last week basically so let's say that we have maybe I can draw this so let's say that we have a search problem without costs and that's our input so if so so we are kind of framing this problem of learning as a prediction problem and if you remember prediction problems and prediction problems we had an input so our input was X okay and in this case you're saying our input is a search problem search problem without costs okay so that is my input and then we have outputs and in this case my my output Y is this optimal sequence of actions that one could get yet so it's a solution path so it's a solution and what I want to do is I want to look like if you remember machine learning the idea was I would want to find this predictor this F function f that would take an input f of X and then it would basically return the solution path and in other settings that it would generalize so so that was kind of the idea that we explored in machine learning and you kind of want to do the same thing in here so let's start with I'm gonna draw that here so let's start with an example where we are in city one and then maybe we walk to City 2 so we can walk to city 2 and then from there maybe I have two options I can keep walking to get to City four so I can do walk walk walk or maybe I can take the tram and end up in City four and and the thing is I don't actually know what the costs of these these actions are I don't know what the cost of don't walk is what the cost of tram is but one thing I know is that my my solution path my Y is equal to walk walk walk so so one way to go about this is to actually start with some initialization of these costs so the way we are defining these costs are going to be I'm going to use the word I'm gonna write here maybe oh just right up here I'm going to use W like because I want to use the same notation as the learning lectures so W is going to be the weight that each one of my actions I have two actions in this case I can either walk or I can take the tram so I'm going to call them action one so W of action one is W of walking and then W of action is w of taking the tram so action to is taking the track so I'm defining these W values and the way I'm defining these weights is just as a function of actions this could technically be a function of state and actions but right now I'm just simplifying this and I'm saying the W's is this values the cost of walking just depend did the cost of going from 1 to 2 it just depends on my action it doesn't depend on what state I am in you could imagine settings where it actually depends on like what city you are in to okay so so then under that scenario what is the cost of cost of y it is going to be double your walk plus double your walk plus double your walk okay so what I'm suggesting is let's just start with something let's just start with yeah let's just start with these weights so I'm gonna say walking costs three and it's always going to cost three again the reason it's always going to cost three is I'm basically saying my weights only depend on the action they don't depend on the state so it's always going to cost three and I'm gonna say well why not let's just say the tram takes the cost of two okay so this doesn't like look right but like let's just say I assume this is the right solution okay so now what I want to do is I want to be able to update these weights update these values in a way that I can get this optimal path that I have this this walk walk walk so how can I do that so I started with this random initializations of what the weights are okay so now that I have done that I can I can try to figure out what is the optimal optimal path fear based on these weights so what is my prediction so that is y prime that is my prediction based on these weights that I've just set up in terms of what what the optimal path is well what is that that is walked ran because this cost five and discussed nine so with these weights these random weights or them just come up with I'm going to pick what contra and that is my prediction okay so now what we want to do is you want to update our w's based on the fact that our true label is walk walk walk and our prediction is walking on and the algorithm that kind of does this it's just like the most like silliest thing possible so so what it does is it's going to first look at the truth value of W okay so it's going to look at so so so the weights are starting from so I decided that this guy is three and I decided that this guy's too and I'm gonna update them so I'm gonna look at every action in this path and for every action in this path I'm going to down wait the the weight of that well why am I going to do that because I don't want to penalize that right this is the true thing I want the weight of the true thing to be small so I see walk I'm like okay so I see you walk the weight of that was three I'm going to down rate that by one I'm gonna make that two I see walk again so I'm gonna bring that one I see walk again I'm gonna subtract one again I end up at zero okay now I'm going to go over my prediction and then for every action I see here I'm going to bring it up bring the cost the weight up by one so I see walk again here I'm going to bring it up by one so so these were subtract subtract subtract bring it up by one because it's over my Y Prime and then I see Tran and then because I see tram I'm going to bring this up by one and that ends up in three so my new weights here are going to be three the the weight of walk just became one and then the weight of Tran just became three okay and and now I can kind of repeat doing this and see if that gets me this this optimal solution or not so I'm gonna try running my search algorithm if I run my search algorithm this path and this path cost for this path cost three so I'm actually going to get this path and this path so my new prediction is just going to be walk walk walk there are going to be the same thing my weights are now going to change I'm going to converge so I'm talking about a very simplified version of this but the idea is always one so the very simplified version of this is this version where I'm saying the W is just depend on on actions if you if you make the weights depend on state and actions there is a more generalized form of this this is called the strip and the structure perceptron algorithm you'll talk about who briefly talked about the diversion where there is a state action too but for in this case where it just depends on action you're literally just bring it up by one or play whatever like whatever you're bringing it up here you got to bring it down by the same thing so it's plus and minus a what a raise so what am i doing to minus one so I'll get to that so so when I look at Y here right like this is the thing that I really want it so if I so when I see walk I realize that walking was a good thing so I need to bring down the weight of that but if the weights that I already had like knew that walking is pretty good then like the rates that I already had knew that walking is pretty good I should like cancel that out so that's why we're doing the plus one because like at this stage like I knew walking is pretty good up here like like my prediction also said walk so if I'm subtracting it off should add it to to kind of like get them cancel that but like right here like I didn't know walking is good so I'm going to bring down the weight of that and then bring up the weight of Tran I mistakenly thought ram is the way to go so to avoid that next time around I'm going to make the cost of higher so I don't take that route it anymore there's a questionnaire secured we like the only resource I'd like a sentence my primary another white pine is different from walking yes but then like what if like we have like a long sequence and white time is only differently like one small location and like with that change awaits efficiently yeah so if so you're asking okay so if my yny prom prime are kind of like the same thing walk walk walk or something and at the very end this last one they were going to be different yeah so like we were just then for that last one you're just adding one right so so it does like waited it does actually address that and it just run you can run it until you get this sequences to be exactly the same thing so you don't have any mistakes does it matter for our new class become negative does it matter if our new costs because it depends on what sort of search algorithm you're using at the end of the day it's fine if you're using dynamic programming so I can have like a negative cost here and I'm just calling like dynamic programming at the end of the day with that and that is fine the other is fine if the cost becomes the question is will be God here one and three is this actually right like if you remember like when we define a strand problem we said walking costs one and trial costs 2 but we never got that well the reason we never got that is the solution we are going to get here is just based on our training data so if my training data is just walk walk walk this is like the best thing I can get and I can kind of like converge to the solution where the two end up being equal I don't know how many mistakes on this if I have more like data points that I'm going to do this longer and actually try it out another training data and then I might converge to a different thing so as far as initializing the weights I'm assuming went further away you are from the Asheville truth the bunkers gonna take I please okay so the question is how do we initialize so in the natural algorithm you're just initializing with zero so we're initializing everything by zero it's actually not that bad because you just you just basically have this sequence and in it for the more general case you're computing a feature value that you just compute the full thing and you just do one single so it is not that costly if we have that input and incorporate that into that so you're saying if we have some prior knowledge about the cost can be incorporated yeah that is interesting so in this current format so if you have some prior algorithm maybe you like then that your prediction is going to be better right so if you have some knowledge about it maybe you'll get a better prediction and then based on that you don't update it as much so maybe you can incorporate in the search problem but like in again this is the most like general form of this algorithm the simple kind of with the simplified version of it also like even like for the action so it's not doing anything fancy it's not doing something that hard either overfeeding at all yes it's going to it can told you yeah I'll show some examples on this like we're going to code this up and then we'll see overfitting kind of situations do so so I'll get back to that actually all right all right so all right so let's move on okay so so this is just like the things that are on the slides or what I've already talked about so yeah so here is an example so we start with three for a walk and two for tram and then the idea is like how are we going to change the costs so we get the solution that we were hoping for and and as I was saying well we can assume that the costs only depend on the action so I'm assuming cost of SN a is just W of a in the most general form it can depend on on the state - okay so then if you take any candidate output past then what would be the cost of the path it would just be the sum of these W values over over all the edges so it would just be W of a 1 plus W of a 2 force W of 8 a 3 and as you've seen in this example the cost of a path is just double your work first over your walk for some of your walk or double your walk plus W of so so that's all this slide is saying so that's how we compute the cost all right so so now let's actually look at this algorithm like running in practice okay let me actually go over this video code so so we start analyzing double used to be equal to zero and then after that we're going to iterate for some amount of T and then we have a training set of examples it might not be just one here I just showed this one example like like the only training example I had was was that wok wok wok is a good thing but I can you can imagine having multiple training examples for your search problem and then what you can do is you can compute your prediction so that is y prime given that you have some W and you can start with this W equal to zero and then just compute your prediction Y prime and then basically you can do this plus and minus type of action so for each action that is in your true Y that is in your true label you're going to subtract one so to decrease the cost of true Y and then for each action that is in your prediction you're going to add add one to kind of increase the cost of the predicted Y okay all right so so let's look at implementing this one and let's try to look at some examples here all right so let's go back to the problem so this is again the same trying problem you just want to use the same sort of format I actually went back and wrote up the history here if you remember last time I was saying I'm not returning the history now we have a way of returning history by each one of these algorithms because we were going to call dynamic programming and we need the history all right so let's go back to our transportation problems so we had a cost of 1 and 2 for walking and tram but what we want to do is we want to put parameters there so we want to actually put this weight and we can give that to our transportation problem so in addition to the number of bucks now I'm going to actually give like the weight of different actions okay all right so then walking has a weight and tram has weight so now I have updated my transportation problem to generally take different values so so now we want to be able to generate some some training examples so that's what I want to do I want to generate different types of training examples that we can call so we can get these true labels so let's assume that the true weights for our training example is just one on two so that is what we really want okay and then you're going to just write this prediction function that we can call up later to get different values of Y so the prediction function is going to get the number of blocks so so it's going to get an N the number of blocks here and it is going to act with this path that we want so it's going to act with these Y values this different time okay so all right so the whole point of prediction is is basically like running this f of X function and we can define our transportation problem its width and weight and the way we were going to get this is by calling dynamic programming so someone asked earlier could the cost be negative well yes because now I'm calling dynamic programming and if like it's problem has negative cost that is fine too so and the history is going to get and the action new state and an cost right so but the thing that I actually want to return from my predict function is a sequence of actions so I'll just get the action out of this history that I get from dynamic programming some calling dynamic programming on my problem that is going to return a history or get the sequence of actions from that and that is my predict function and I can just call that later so let's go back to generating examples so so I'm just going to go for try out and end to go from 1 to 10 so one block to 10 bucks and we're calling the predict function on these true weights to get the true y-values so these are my true labels okay and those are my examples so my examples are just calling generate examples here okay so let's just print out our examples see how it looks like we haven't done anything like in terms of like the algorithm or anything we're just like creating these training examples by calling this predict function on the true weights I have a typo here generate examples I need parentheses I'll fix the typo okay so that kind of looks right right so that's my training example one through nine and then what is what is the path that you would want to do if you have these true weights the one and two okay so now I have my examples so I'm ready to write this structured perceptron algorithm it gets my examples it gets the training examples which are these paths and then we're going to iterate for some range and then we can basically go over all the examples that we have in our true true y values and then we can't we can basically go and update our weights based on based on that and based on our predictions so let's initialize the weights to just be zero so that's for walking in tram they're just zero and prediction actions this is when we're calling predict based on the current weights so if my current weights are zero then pred actions is just that y prime so pred actions is y prime true actions is y like the things that we had on the slides if okay and then I want to count the number of mistakes I'm making too so if the two are not equal to each other then I'm going to just keep it counter for number of mistakes if the two become equal then then my number of mistakes is zero I'm going to break it in maybe I'm happy then okay so I make a prediction and then after that I'm going to update the weight values okay so how do I update well basically subtract if you're in true actions which is why the labels that have created from my training examples and then two plus one if you're in prediction actions based on the current weight values and then that's pretty much it like you like that is structured perceptron okay so let's just print things nicely so we can print the iteration and number of mistakes we have and what is actually the weight values that we have and I'm just breaking this whenever I have like no mistakes so if number of mistakes is zero oh I'll just break this okay that sounds good so if number of mistakes is zero then I'll break okay so all good I'm gonna run this it's not gonna do anything because I didn't call it so I'll go back and actually call it I have another typo here if you guys can guess like where does my typos this is gonna give an error well I called you the weights not wait so I'll go fix that okay this is driving okay so and then this is what we get so let's actually look at this so what we got is the first iteration number of mistakes was six and then we ended up actually at the first iteration we ended up converging to one two so then the second iteration the number of mistakes just became zero and then we just got one two which is which is the weights that we were hoping for okay so that kind of looks okay to me that's my training data everything looks fine there's a question actually I like integers yeah so in this case you're sending all the way time give it our update model as well we're assuming that the number of walks of the number of trams are different where trim was in a different location but the number of Lawson appearances correct you would still salute I see what you're asking no you treat it like it should figure figure that out so um you we can go over an example after after the class and I'll show you like how how it actually doesn't all right so okay so let's try 1 & 3 so we'd 1 & 3 takes a little bit longer and but it does recover so 1 & 4 is actually the interesting one because it does recover something it does recover to 8 it doesn't recover 1 & 4 but like given my data actually 2 8 is like like there is no reason for me to get 1 1 & 4 like the ratio of them is the thing that I actually care about so even if I get 2 & 8 like like that is a reasonable set of weights that one could get I'm gonna try a couple more things so let's try one in five so I'm gonna try one in five and this is what I get so I get the way to walk to be minus one an afraid of charm to be one no more mistakes is zero so why is this happening now your training is just all walking so starting to just walk yeah so what's happening here is if you look at my training data up here my training data is just has like walk like all walks it hasn't seen tram ever so it has no idea like what the cost of tram is with respect to cost the walk so it's not gonna learn that so we're gonna fix that like one way to fix that is to go and change the training data and actually like get more data so we can kind of do that so like just one thing to remember is this is just going to fit your training data whatever it is so yeah so when we fix that then walk becomes - and tram becomes nine which is not one and five but it is getting there like it's a better ratio and number of mistakes is still zero so it really depends on what you're looking for like if you're trying to like match your data and know if your number of mistakes is zero and you're happy with this you can just go with this and even though like it happened like actually recovered the exact value the ratios you can that's fine or maybe you're looking for the exact ratios and you should like run it longer more iteration it's a question structured perceptron like suspect - getting stuck in local optima sorry I was looking at that is a good question so actually let me think about that the Eustis and then i'll Pirie do you actually know if this gets into local optima i have experienced it personally I feel like there's there's reasons for it to do this it's doing this kind of me think about this because even in a more general form of it it's commonly used in like like matching like sentence like words and sentences so I haven't experienced it either but I can look into that and back to you house guys are we just being at all of your optimal pass yes yeah yeah if we do figure out all the alcohol paths and technically you should just be complex right because I can just match up and if you're feeding it all the optimal path it should you just matching path you're saying is yeah so in terms of okay so yeah so in terms of like bringing down the number of mistakes then then it should always match it but if you have some true like weights that you're looking for and it's not represented in your data set then it's not necessarily like like learning that so so in those settings they could fall into local optima so kind of like a another version of this is when you're doing like reward learning and then you actually have this true reward you want to find like in those settings you can totally fall into like local optima because you want to find out what the reward function is but you're right like if you're just matching the data so the scaling would be a different problem right so the scaling is kind of yeah so you can you have reward shaping so you can have different versions of the reverse function and if you get any of them that is fine but but you might still I get into local optima that's not explained by reward shaping so okay so that we can talk about these things are fine maybe I should just move on to the next topics cuz we have some more stuff going on okay so I was actually going to skip these slides because we have stuff coming up but this is a more general form of it so remember I was saying this w is a function of a but you could you could have a more general form where your cost function is not just W as a function of a it is actually W times a set of features and and then the cost of a path is W times the features of the path and that's just the sum of features over the edges so so you can have this more general form go over the slides later on maybe because we got to move to the next part but just real quick the update here is this more general form up update which is update your W based on subtracting the features over your true true path plus the features over your predicted path so more general form of this it's called Cohen's algorithm so Mike Collins was working on this and in natural language processing he was actually interested in it in the setting of part the speech target and tagging so so you might have like a sentence and you want to tag each one of the each one of the labels here as a noun or verb or determiner or now and again so so he was thinking he was basically looking at this problem as a search problem and he was using like similar type of algorithms to try to figure out like like match what what the value like match noun or like each one of these parts of speech tags to the sentence so he has some scores and then based on the scores and and his data set he goes like up and down he moves the scores up and down which uses the same idea you can use the same idea again in machine translation so you can have like if you have heard of like beam search you can have multiple types like like a bunch of translations of some phrase and then you can operate and down rate them based on your training data okay all right okay so now let's move to a ice a ice a star not a I star a star search alright so okay so we've talked about this idea of learning costs right so we've talked about search problems in general doing inference and then doing learning on top of them and then now I want to talk a little bit about kind of making things faster using smarter ideas and smarter heuristics there's a question I would say what is the loss function that we are trying to minimize this in this structure so in this is this is a prediction problem right so in that prediction problem we are trying to basically figure out what W WS are as closely as possible as we are matching these w wi prime to Y right so so basically like the way we are solving this is not necessarily as an optimization the way that we have solve other types of learning problems the way we are solving it what is by just like tweaking these weights to try to match my Y as closely as possible to 2y time okay all right okay so let's get taught to talk about a star so I don't have internet so I can't show these but I think the link for these should work when you go to their to the file so the idea is if you go back to uniform cost search like an uniform cost search what we wanted to do was you want to get from a point to some solution but we would uniformly like increase explore the states around us until we get to some final state the idea of a a-star is to basically do a uniform cost search but do it a little bit smarter and move towards the direction of the goal state so if I have a goal state particularly like in that corner maybe I can I can move in that direction smarter right okay so here is like an example of that pictorially so I can start from a start and and if I'm using uniform cost search again I'm uniformly kind of exploring all the states possible until I hit my s end and then I'm happy I'm done I've solved my search problem everything is good but the thing is I've done all these like wasted effort on this side which is just not that great okay so uniform cost search and in that sense has this problem of just exploring a bunch of states for no good reason and what we want to do is we want to take into account that we're just going from a star to s and so we don't really like need to do all of that we can actually just try to get to the get to the end state okay so so going back to maybe I'm gonna go on this side so going back to how these search problems work the idea is to start from a start and then get to some state s and then we have this s end okay and what uniform cost search does is it basically orders the states based on past cost of s and then explores everything around it based on past cost of f s until it reaches s end okay but when you're in state s like there is also this thing called future cost earnest right and ideally when I'm on the state s I don't want to explore other things on this side I actually want to want to move in the direction of kind of reducing my my future cost and getting to my to my end State okay so so the cost of me getting from s start to us and it's really just like past cost of s plus future cost of s and if I knew what future cost of s was I would just move in that direction but if I knew what future cost of S is well the problem was solved right like I had the answers of my search problem like I'm solving a problem so in reality I don't have access to future costs sorry I have no idea what future cost is but I do have access to something like I can't potentially have access to something else and I'm gonna call that H of s and that is an estimate of future costs so I'm going to add a function called H of s and this is called a heuristic and a Serie C could estimate what future cost is and if I have access to this heuristic maybe I can update my cost to be something as what the past cost is in addition to that like I can add this heuristic and that helps me to be a little bit smarter when I'm running my algorithm okay so so the idea is ideally like what I would want to do is I want to explore in the order of past cost plus future cost I don't have future costs if I had future costs I had the answer to my search problem instead what if star does is its it exports in the order of past costs plus some H of s ok so remember uniform cost search it explores just in the order of past costs so in uniform cost search like we don't have that H of s okay and H of s is a heuristic it's an estimate of the future cost all right so what does a star do it actually there's something really simple so so a star basically just does uniform cost search so all it does is uniform cost search with a new cost so before I had this blue cost cost of SN a this was my cost before now I'm going to update my cost to be discussed prime of SN a which is just cost plus the heuristic over the successor of SN a minus the heuristic so so that is the new cost and I can just run uniform cost search on this new cost so so I'm gonna call it cost prime listener well what is that equal to that is equal to cost of SN a which is what we had before when we're doing uniform cost search plus heuristic over successor of SN a - heuristic over s so why do I want this well what this is saying is if I'm at in some state s ok and there is some water state successor of SN a so I can take an action a and end up in successor of SN a and there is some s end here that I'm really trying to get to remember H was my estimate of future cost what this is saying is my estimate of future cost for getting from successor to SN minus my estimate of getting from future cost of s to us and should be the thing I'm adding to my cost function I should penalize that and what this is really enforcing is it basically makes me move in the direction of s end because because if I end up in some other state that is not in the direction of s and then then that thing that I'm adding here is basically going to penalize that right it's going to be saying well it's really bad that you're you're going in that action I'm going to put more costs on that so you never go in that direction you should go in the direction that go it's ghost towards your s end and that all depends on like what your H function is and how good like of an H function you have and how you're designing your your heuristics but that's kind of the idea behind it so here is an example actually so let's say that we have this example where we have ABCD and E and we have cost of one on all of these edges and what we want to do is you want to go from C to e that's our plan okay so if I'm running uniform cost search well what would I do I'm at C I'm going to explore B and D because they have a cost of 1 and then after that I'm going to explore a and E and then finally I get to get to eat but why did I spend all of that time we can get a and B I shouldn't have done that right like a and B are not in the direction of getting to a cent so instead what I can do is if someone comes in tells me oh I have this heuristic function you can evaluate it on your state and this heuristic function is going to give you four three two and one and zero for each one of these states then you can update your cost and maybe you'll have a better way of getting to a cent so this heuristic in this case is actually perfect because it's actually equal to future costs like the point of the heuristic is to get as close as possible to the future cost this is exactly equal to future class so with this heuristic what's going to happen is my new cost is going to change how is it going to change well it's going to become the cost of whatever the cost of the edge was before which was 1 plus h of the case of for example cost of going from C to B if you work at C to B it's the old cost which was 1 plus heuristic at B which is 3 minus heuristic at C which is 2 so that ends up giving me 1 plus 3 minus 2 that is equal to 2 and then similarly you can compute like all these like new cost values the purple values and and that has a cost of 2 for going in this direction and cost of 0 for going towards E and if I just run uniform cost search again here then I can get to like much easier there's a a sterling result in 3d approaches where this opportunities like go back with them better loss star result in my greedy approaches like where user greedy greedy yes yeah so okay so so it's all so so the question is is a star like causing like greedy approaches so no actually we were going to talk about that a little bit a star dependent depends on the heuristic you are choosing so depending on the heuristic you are choosing a star is actually going to be like return the optimal value but yeah it does depend on the heuristic so it actually does the exact same thing as uniform cost search if you choose a good heuristic what why is cost of C C look really bad when it's really not since to become so cost of CB o because oh I see what you're saying that's what we started with so this is like the graph that I started with so I started with the cost like the blue cost being all one but now I'm saying those costs are not good I'm going to update them based on this heuristic so I can get closer to the goal like as fast as possible and state well you returned like the actual cause you wouldn't that's right so so the question is what costs are you going to return at the end and you do want to return the actual cost so you return the actual cost but you can run your algorithm with this heuristic thing added in because that allows you to explore less things and just be more efficient okay oh I got a move on all right so okay so a good question to ask is well what is this heuristic how does this heuristic look like like can any it does any heuristic like work well so turns out that not every heuristic works so here's an example so again the blue things are the costs that are already given these are the things that I already have and I can just run my search algorithm bit the red things are the values of the heuristic someone gave them to me for now in general we would want to design them so someone comes in and gives me this this heuristic values and then what I want to do is I want to compute the new cost values so the question is is this heuristic good so I get my new cost values they look like this like does this work we don't have time so I'm gonna answer that it's not gonna work but the reason this is not gonna work is though we just got a negative edge there right so I'm running uniform cost search at the end of the day like a star is just uniform cost search and I can't have negative edges so I'm not like I'd like that it's just not a good heuristic to have here so so the heuristics need to have specific properties and and we should you should think about what those properties are so one property that you would want to have the heuristics to have is this idea of consistency this is actually like the most important property really so so we talked about heuristics I'm going to talk about properties of them here heuristics H they should be consistent so the consistent heuristic has two conditions the first condition is it's going to satisfy the triangle inequality and and what that means is like the cost that you're the updated cost that you have should be it should be non-negative so so this costs prime of this yesterday this should be positive so that means that the old cost of sna plus h of successor i'm gonna use as prime for that - H of S is greater or equal to zero okay so that is the first condition and then the second condition that we are going to put is that future cost of SN is going to be equal to zero right because the future cost of the end state should be zero so then the heuristic at the end state is also equal to zero so so these are kind of the properties that we would want to have if you want to talk about consistent heuristics okay and they're kind of like natural things that we would want to have right like like the first one is basically saying well the cost you are going to end up at should be should be greater than or equals 0 and you can run uniform cost search on it but it's really like talking about this triangle inequality that we want to have right like we H of s it's kind of an estimate of this future cost so if I'm going to from s take an action with that cost of s and a dad added up H of successor of s SN a should be greater than just H of s the estimate of future cost of s so so that's that's all it is saying and then the last one also makes sense right like I do want my future cost of s and to be 0 right so then the heuristic at s end should also be equal to 0 because again heuristic is just an estimate of the future cost ok all right so so what do I know about a star beyond that so one thing that we know is that if if H is consistent so if I have this consistency property then I know that a star is correct so that there's a theorem that says a star is going to be correct if H is consistent well we can kind of look at that through an example so so let's say that I am at the zero and I take a 1 and I end up at this one and I take a 2 and a minus 3 and 0 its to take a 3 so let's say that I have I have kind of like a path that looks like this okay so then if I'm looking at the cost of each each one of these right I'm looking at cost of cost prime of 0 and a 1 well what is that equal to that's that's my updated cost updated cost is old cost which is cost of a 0 and a plus heuristic value at s1 minus heuristic value is 0 value this one point is heuristic value I this year okay so so that is the cost of going from s 0 and taking a 1 I'm going to just write all the costs for for the rest of this to figure out what's the cost of the path the cost of the path is just the sum of these costs so s 1 a 2 is cost of s 1 a 2 plus heuristic is it as 2 - heuristic it is 1 so that is the new cost of this edge and the new cost of the last edge which is cost prime of s 2 a 3 and that is equal to the old cost stuff s 2 a 3 plus heuristic add s 3 minus heuristic is ok so I just wrote up all these costs if I'm talking about the cost of a path then it's just that these costs added up right so if I add up these costs what happens a bunch of things get cancelled out right this guy gets cancelled up like this guy this guy gets cancelled out right and what I end up with is is some of these new cost is cost Prime's of Si minus 1 AI is just equal to some of my old cost of Si minus 1 AI plus my heuristic at this last state is end State - heuristic at this year okay I'm saying my heuristic is a consistent heuristic so what is a property of a consistent heuristic the heuristic value at s nth should be equal to zero so this guy is also equal to zero so what I end up with is is if I look at a path with the new cost the sum of the new cost is just equal to the sum of the old cost minus sum sum constant and this constant is just a heuristic value at s 0 ok so so why is this important because when we talk about the correctness like remember we just proved at the beginning of this lecture that uniform cost search is correct so the cost that it is returning is optimum that is that if this cost a star is just uniform cost search with a new cost so a star is just running on this new cost but this new cost is the same thing that they have as old cost minus a constant so if I'm optimizing the new cost it's the same thing as optimizing the old cost so it is going to return the optimal solution okay all right so that is basically the same things on the slide like ok basically so that's one property right so so we talked about heuristics being consistent we have now just talked about a star being correct because it's uniform cost search it's it's correct only if the heuristic is consistent right like only if you have that property because because that consistency gets us gets us the fact that this guy is equal to 0 and gets us the fact that these guys are going to be positive and I can run uniform cost search on de um the next property that we have here for for a store is a star is actually more efficient than uniform cost search and we kind of have already seen this right like like the whole point of a star is to not explore everything and explore in a directed manner so if you remember uniform cost search like how does it explore well it explores all the states that have a past cost that are less than the past cost of ascent so again remember the uniform cost search you're exploring with the with the order of path cost of states and then we explore all those states that Haskell's less than the den state a star like the thing that they have stored us is it explores less states so it explores states that have a past cost less than past cost of the end state - the heuristic so so if you kind of look at the right side the right side just became become smaller right like the right side for uniform cost search was just past cost of SN now it is past cost of ascent - the heuristic so it just became smaller and then why did it become smaller because now I'm doing this more directed search I'm not searching everything uniformly around me and and that's the whole point of the heuristic okay and that makes it actually more efficient so and then kind of the interpretation of this is if H is larger than then that's better right like if my heuristic is as large as possible well well that is better because then I am kind of exploring a smaller like area to get to the solution the proof of this is like two lines so I'm gonna escape that so let me actually show how this looks like so if I'm trying to get from a start to s and again if I'm doing uniform cost search I'm uniformly exploring so like all states around me and that is equivalent to assuming that the heuristic is equal to zero like it's basically uniform cost search is a star when the heuristic is equal to zero so what is the point of the heuristic the point of the heuristic is to estimate what the future cost this if I know what the future costs is then then H of s is just equal to future cost and then a and that would be awesome and and I only need to like explore that green kind of space and then the thing I'm exploring is it's just the notes that are under minimum past cough and call cost path and I'm not exploring anything extra right like that's the most like efficient thing one can do in practice like I don't have access to future costs right and in practice if I had access to future costs like the problem was solved I have access to some heuristic that is some estimate of the future cost it's not as bad as uniform cost search it's getting close to future costs like look the value of future cost and you're kind of somewhere in between so it is going to be more efficient than uniform cost search in some sense okay all right so so basically the whole idea of a star is it kind of distorts edge edge cost and favor sees and States so I'm going to add here that a star is efficient so that is the other thing okay all right so so these are all cool properties one more property about here is six and then after that we can talk about your lack stations so so there's also this other property called admissibility which is something that we have kind of been talking about already right like we've been talking about how this heuristic should get close to future cost and should be an estimate of the future cost so an admissible heuristic is a heuristic where H of s is less than or equal to future cost and then the cool thing is if you already have consistency then you have admissibility - so if you already have this property then you have admissibility - so another property is admissible which means H of s is less than or equal to future cost of all right so the proofs of these are again like just one-liners so this one is more than one line but it is actually quite easy it's in the notes so you can use induction here to prove to prove that if you have consistency then you're going to have admissibility - okay so so we've just talked about how a star is the sufficient thing you have talked about how like we can come up with we haven't talked about how you come up with your six but we have talked about consistent heuristics that are going to be useful and they're going to give us admissibility and they're going to give us correctness and how like a star is going to be this very efficient thing but we actually have not talked about how to come up with heuristics so let's spend the next yeah couple of minutes talking about talking about how to come up with heuristics and then the main idea here is just relax the problem just relaxation so so what are so so the way we come up with heuristics is we pick the problem and just make it easier and solve that easier problem so so that is kind of the whole idea of it so remember the HMS is supposed to be close to future cost and some of these problems can be really difficult right so this so if you have a lot of constraints and it becomes harder to solve the problem so if you relax it and we just remove the constraints we are solving a much easier problem and that could be used as a heuristic as a value of heuristic that estimates what the future cost this so so you want to remove constraints and when we remove constraints the cool thing that happens is sometimes we have closed form solutions sometimes you just have easier search problems that we can solve and sometimes you have like independence of problems and we can find a solutions to them and that gives us a good heuristic so so that is my goal right like I would want to find these heuristics so let me just go through a couple of examples for that so so let's say I have a search problem and I want to get the triangle to get to the circle and that is what I want to do and I have all these like walls there and that just seems really difficult so what is a good heuristic here I'm going to just relax the problem I'm gonna remove like all those walls just knock down the walls and have that problem that just seems much easier okay so so well like now I actually have a closed form solution for getting the triangle get to the get to the circle I can just compute them in half in this sense and I can use that as a heuristic again it's not going to be there like actually like what future cost is but it is an approximation for it so so usually you can think of the heuristics as these optimistic views of what the future costs is like like it's an optimistic view of the problem like what if there was like no walls like if there are no walls here then how would I get from location to another location the solution to that is going to give you this future cost Val its estimate of future cost value which is which is H of s okay or the Tran problem let's say we have the Tran problem but we have a more difficult version of it where we have a constraint and this constraint says you can't have more tram actions than walk actions so now this is my search problem I need to solve this this seems kind of difficult like we talked about how to come up with States for it last time and even that seems difficult like I need to have the location I need to have the difference between the walk and tram that seems kind of difficult like like I have an order of N squared states now so instead of doing that well let me just remove the constraint I'm just gonna remove the constraint relax it and then after relaxing it then I have a much easier search problem I need to deal with I only have this location and then I can just go with that location and everything will be great okay all right so so the idea here was like for this middle part is if I if I remove these constraints I'm going to have these easier search problems these relaxations and I can compute the future cost of these relaxations using my favorite techniques like dynamic programming or uniform cost search but but the one thing to notice is I need to compute that for one through n because this heuristic is a function of state right so I actually need to compute future cost for this relaxed problem for all states from 1 through n and that allows me to have like a better estimate of this there are some like engineering things that you might need to do here so so for example you might so here we are looking for future cost so if you plan to use uniform cost search for whatever reason like maybe dynamic programming doesn't work in this setting you need to use uniform cost search you need to make a few engineering things to make it work because if you remember uniform cost search would only work on past cost doesn't work on future costs so you need to like create a reverse problem where you can actually compute future costs so so a few engineering things but beyond that it is basically just running our search algorithms that we know on these relaxed problems and that will give us a heuristic value and we'll put that in our problem and we'll go and solve it ok and another cool thing that heuristics give us is this idea of having independent subproblems so so here's another example I want to solve this this eight puzzle and move blocks here and there and come up with this new configuration that seems hard again a relaxation of that is just assume that the tiles can overlap so the original problem says the tiles cannot overlap I'm just gonna relax it and say oh you can just go wherever and you can overlap ok so that is again much simpler and now I have eight independent problems for getting each one of these points from one location to another location and I have a closed form solution for that because that's again Manhattan distance so that gives me a heuristic that that's an estimate that's not perfect it's an estimate and I can use that estimate in my original search problem to solve the search problem so here it was just some examples of this idea of removing constant removing constraints and coming up a bit better heuristics so like knocking down walls like why can't ramp freely overlapping pieces and he says and that allows you to kind of solve this new problem and then the idea is you're reducing these edge costs from infinity to some finite finite cost okay all right so yeah so I'm gonna wrap up here and I guess we're going to talk about these last few slides next time since you're running wait but I think you guys have got like the main idea so what's our next time