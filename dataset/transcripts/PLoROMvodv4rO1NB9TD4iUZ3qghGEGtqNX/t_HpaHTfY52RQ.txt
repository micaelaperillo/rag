so this lecture is going to be on reinforcement learning I will in interest this time skip the quiz so so the way to think about how reinforcement learning fits into what we've done so far is you remember this class has this picture right so we talked about different models and we talked about different algorithms inference algorithms to be able to predict using these models and answer queries and then we have learning which is how do you actually learn these models right so every type of model we go through we have to kind of check the boxes for each of these pieces so last lecture we talked about Markov decision processes this is a kind of a modeling framework allows you to define models for example for crossing volcanoes or playing dice games or tram taking trams what about inference so what do we have here last time we just had value iteration and which allows you to compute the optimal policy and policy evaluation which allows you to estimate the value of a particular policy so these are algorithms that operate on the MVP right and we looked at these algorithms last time so this lecture is going to be about learning I'll just put RL for now RL is not an algorithm it's a kind of refers to the family of algorithms that fits in this week but that's a way you should think about it RL allows you to either explicitly or implicitly as to my MVP s and then once you have that you can do all these inference algorithms to figure out what the optimal policy is okay so just to review so what is the MVP the clearest way remember to think about it is it's in terms of a graph so you have a set of states so in this dice game we have in and n so we have a set of states from every state you have a set of actions coming out so in this case stay and quit actions take you to chance nodes where the you don't get to control what happens but nature does and there's randomness so out of these chance nodes are transitions each transition takes you into a state it has some probability associated with it so two-thirds in this case it also has some reward associated with it which you pick up along the way so naturally this has to be 1 3 4 and remember the last time this was probability 1/10 okay so and then there is no the discount factor which gamma which is a number between 0 & 1 tells you how much you value the future for default you can think about it as 1 for simplicity okay so this is a Markov decision process and what do you do with one of these things we have a notion of a policy and a policy let's see I'll write it over here so a policy denoted PI let me use screen so a policy pi is a mapping from States to action it tells you a policy when you apply it says when I land here where should I go should I do stay or quit if I land well I mean this is kind of a simple MVP otherwise it usually would be more States and for every state blue circle it will tell you where to go and when you run a policy what happens you get a path which I'm going to call an episode so what do you do you start in state s0 not so that would be in in this particular example you take an action a1 let's say stay you get some reward in this case it would be for you end up in a new state oops s1 and suppose you go back to n and then you take another action maybe it's stay reward is for again and and so on right so this sequence is a path or in RL speak it's an episode let's see so let me let me erase this comment and so this is an episode and until you hit the end state and what happens out of an episode you can look at a utility we're going to denote you which is the discounted some of the rewards along the way right so if you you know stayed three times and then went there you would have utility of four plus four plus four plus four so that would be sixteen okay so the last lecture we didn't really work with the episodes and their utility because we were able to define set of recurrences that computed the expected utility so remember that we want to you know we don't know what's going to happen so there's a distribution and in order to optimize something we have to turn it to a number that's what expectation does so there's two concepts that we had from last time one is the value function of a particular policy so V PI of S is the expected utility if you follow PI from s what does that mean that means if you take a particular s so let's take in and I put you there and you run the policy so stay and you traverse this graph you will have different utilities coming out and the average of those is going to be V PI of s similarly there's a Q value expected utility if you first take an action from state s and then fall pi so what does that mean that means if I put you on one of these read chance nodes and you basically play out the game and average the resulting utilities that you get what number do you get okay and we saw recurrences that related these two so V PI of s is U recurrence the name of game is two kind of Dell to some kind of simple problem so you first look up what you're supposed to do in s that's pious and that takes you in a chance node which is s comma PI R sub s and then you say hey how much utility am I going to get from that node and similarly from the the chance nodes you have to look at all the possible successors the probability of going into that successor of the immediate reward that you get along the edge plus the discounted reward of the kind of the future when you end up in s prime okay so any questions about this this is kind of a review of Markov decision processes from last time okay so now we're able to do something different okay so if you say goodbye to the transition and rewards that's called reinforcement learning so remember Markov decision processes I give you everything here and you just have to find the optimal policy and now I'm gonna make life difficult by not even telling you what rewards and what transitions you have to get okay so just to give a kind of flavor of what that's like let's play a game so I'm gonna need a volunteer I'll give you the game but this volunteer you have to have a lot of grit and persistence because this is not gonna be an easy game he has to be one of those people that even though you're losing a lot you're still gonna not give up okay so here's how the game works so for each round R equals one two three four or five six and so on you're just gonna choose a or b red pill or blue pill I guess and you you move to a new state so the state is here and you get some rewards which I'm gonna show here okay and the state is five comma zero that's an initial state okay so everything clear about the rules of the game that's reinforcement learning all right we don't know anything about how okay so any volunteers how about you in the front okay that's an empty piece so that case I hope I warned you I'm glad this worked because last time it took a lot longer but you know so what did you have to do I mean you don't know what you try so you try a and B and then hopefully you're building MVP and you're your head right yeah right okay just smile and nod and you have to figure out how the game works right so maybe you notice that hey a is no decrementing and B is going up but then there's this other bit that gets flipped so can you figure this out in a process you're also trying to maximize reward which apparently I guess wasn't it doesn't come until the very end because it's a cruel game okay so how do we gonna algorithm to kind of do this and how do we think about cross doing this so just to kind of make the contrast between MVPs and reinforcement learning sharper so Markov decision process is an offline thing right so you already have a mental model of how the work world works that's the MVP that's all the rewards in the transitions and the states and actions and you have to find a policy to collect maximum rewards you have it all in your head so you just kind of think really hard about you know what is the best thing is I know if I do this action and I'll go here and you know look at the probabilities take the max or whatever so reinforcement learning is very different you don't know how the world works so you can't just sit there and think because thinking isn't gonna help you figure out how the world works so you have to just go out and perform actions in the world right and in doing so you hopefully you'll learn something but also your you'll get some rewards okay so since maybe formalize the paradigm of RL so you can think about as an agent that's that's you and do you have the environment which is everything else that's not agent the agent takes actions so that I sense action to the environment an environment just sends you back rewards and a new state and you keep on doing this so what you have to do is figure out first of all how am I going to act if I'm in a particular state s t minus one what action should I choose okay so that's one one question and then you're gonna get this reward and observe a new state how what should I do to update my mental model of the world okay so these are the main two questions I'm gonna talk first about how to update the parameters and then later in the lecture I'm gonna come back to how do you actually go and you know explore okay so I'm not gonna say much here but you know in the context of volcano crossing just to kind of think through things every time you play the game right you're gonna get some utility so you take of so this is the episode over here so ARS you're gonna sometimes you you know fall into a pit sometimes you go to a hut and based on these experiences if I didn't hadn't told you what any of the actions do and what's the probability or anything how would you kind of go about just kind of solving this problem that's that's the question okay so there's a bunch of algorithms I think there's going to be one two three four at least our algorithms that we're gonna talk about with different characteristics but they're all gonna kind of build onto each other in some way so the first class of algorithms Monte Carlo methods right so okay so when you're ever you're doing RL or any sort of learning the first thing you get is you just have data let's let's suppose that you run even a random policies you're just gonna because in the beginning you don't know any better so you're just gonna try random actions and but in the process you're gonna see hey I tried this action and a lot to this reward and and so on so in the concrete example just to make things open more crisp it's going to look something like in and then you take you know you did let's see let me try to color coordinate this so you're in n you do stay and you get a free water four and then you're back in in you do a stay and then you get four and then maybe you're done you're out okay so this is an example episode just to make things concrete so this is s 0 a 1 R 1 s 2 s 1 incrementing too quickly a - R - s - ok ok so what should you do here all right so any ideas model-based Monte Carlo so if you have the MVP you would be done but we don't have them be key we have data so what can we do yeah yeah let's try to build an MDP from that data okay so the idea is estimated MDP so intuitively we just need to figure out what the transitions and rewards are and when we're done right so how do you do the transitions so the transition says if I'm in state s and I take action a what will happen I don't know what will happen but let's see in the data what happened so I can go look at the number of times I went into a particular s Prime and then divided over a number of times I attempted any this action from that state at all and just take the ratio okay and for the rewards this is actually fairly you know easy when I because when I observe a reward from s a and s Prime I just write it down and say that's the reward okay okay so on the concrete example what does this look like so remember now here's mt b graph I don't know what the the transition distribution or the rewards are so let's suppose I get this trajectory what should I do so I get stay stay stay stay and I'm out okay so first I can write down the rewards of for here and then I can estimate the probability of you know transitioning so three out of four times I went back to N one out of four times I went to N so I'm gonna estimate this as 3/4 1/4 okay but then I suppose I get a new data point so I have stay stay and so what do I do I can add to these counts so everything is kind of cumulative so two more times I started one more time I went into N and another time I went to n so this becomes 4 out of 6 2 out of 6 and suppose I see another time when I just go into n so I'm just gonna increment this counter and now it's 3 out of 7 and 4 out of 7 ok so pretty pretty simple okay so for reasons I'm not going to get into this process actually you know converges to the if you do this kind of you know a million times you'll get pretty accurate that question do we know the number of states yes a question is you don't know the rewards or the transitions but yes you do know the set of states and the actions set of states I guess you don't know have to know them all events but you have just observe them as they come the actions you need to know because you your agent and you need to play the game yeah good question okay so yeah so the question is does this work with variable rewards and if the reward is not a function of SAS prime you would just take the average of the rewards that you see yeah okay so so what do you do with this so after you ask them eight the MVP so you know you needed a transitions and rewards then now we have MVP in mind it may not be the exact right MVP because this is estimated from data so it's not gonna match it exactly but nonetheless we already have these tools from last time you can do value iteration to compute the optimal policy on it and then you just you know you're done you run it on in practice you were probably kind of interleave the learning and the optimization but for simplicity we can think about as a two stage where you gather a bunch of data you estimate the MTP and then you are off okay there's one problem here does you wanna know what the problem might be you can actually see it by looking on the slide yeah well with your face policy of always staying on never explore the wait branch of the world yeah yeah you didn't explore this at all so you actually don't know how much reward is here maybe like you know 100 right so so this is this problem this kind of actually a pretty big problem that unless you have a policy that actually goes and covers all the the states you just won't know right and this is kind of natural because there could always be you know a lot of reward hiding under kind of one state but unless you see it do you you don't do just so no okay so this is a kind of key idea key challenge I would say in reinforcement learning is exploration so you need to be able to explore the state space this is different from normal machine learning where data just comes in passively and you learn a nice function and then you're you're done here you actually have to figure out how to get the data and that's that's kind of one of the the key challenges of RL so we're gonna go back to this this problem and I'm not really going to try to solve it now um for now you can just think about PI as a random policy because a random policy you eventually will just you know hit everything for you know finite small state spaces okay so okay so that's the basically end of the first algorithm let me just write this over here so algorithms we have model-based Monte Carlo and the model base is referring to the fact that we're estimating a model that in particular to MVP the Monte Carlo part is just referring to the fact that we're using samples to estimate a model or you're basically applying the policy multiple times and then estimating the model based on averages okay so so now I'm going to present a different algorithm and it's called model free to call oh and you might from the name guess what we might want to do is maybe we don't have to estimate this small okay and Wyatt why is that well what do we do with this model what we did was we you know presumably use value iteration - you know compute the optimal policy and they remember this recurrence for computing Q OTT it's in terms of T and reward but at the end of the day all you need is Q opt if I told you Q oft si which is what is aq opt si it's the the maximum possible utility I could get if I am in chance node si and I followed often policy so clearly if I knew that then I would just produce often policy and our you've done I don't even need to know understand the rewards and transitions okay so with that insight is model free learning which is that we're just going to try to estimate Q opt you know directly sometimes it can be a little bit confusing what is meant by model free so Q opt itself you can think about it as a model but in the context of MDPs and reinforcement learning generally people when they say model free refers to the fact that there's no MDP model not that there's no model in general okay so so we're not going to get to Q opt yet that'll come later in a lecture so let's warm up a little bit so here's our data staring at us remember let's look at a related quantity so Q pi remember what Q pi is Q PI of Si is the expected utility if we start at s and you first take action a and then follow a policy pi right so in in I guess another way to write this is if you are at a particular time step T you can define you t as the the discounted some of the rewards from that point on which is you know the reward immediately that you would get plus a discounted pot and then I'm next time sad plus you know squared discounted and then to time steps in the future and so on and what you can do is you can try to estimate Q PI from this utility right so this is the utility that you get at a particular time step so suppose you do the following so suppose you average the utilities that you get only on the time steps where I was in a particular state s and I took an action a ok so you have it let's suppose you have a bunch of episodes right so here pictorially let's see here's another way to think about it so I get a bunch of episodes I'm gonna do some abstract drawing here so every time you have you know si shows up here maybe it shows up here maybe it shows up here maybe it shows up here you're gonna look at how much reward I get from that point on how much reward I get from here on how much reward do I get from here on and average them right so and there's a kind of a technicality which is that if si appears here and it also appears after it then I'm not going to count that because I'm kind of if I do both I'm kind of double counting in fact it works both ways but just conceptually it's easier to think about just taking the NS a if the same you don't kind of go back to the same you know s position ok so let's do that on a concrete example so Q PI let's just write it Q PI as a is a thing where we're trying to estimate and this is a value associated with every chance node si so in particular I've drawn it here I need a value here and a value here okay so suppose I get some data I say and then I got go to the end so what's my utility here it's not a trick question for yes some of for is for okay so now I can say okay it's for that's my best guess so far I mean I haven't seen anything else maybe it's for so what happens if I play the game again and I get four four so what's the utility here eight so then I update this to the average of four and eight do it again I get sixteen to deny average in the sixteen okay and and again you know I'm using stay so I don't learn anything about this in practice you would actually go explore this and figure out how much utility is saying there so in particular notice I'm not updating the rewards nor the transitions because I'm model free I just care about the Q values that I get which are the values that sit at the no it's not on the edges okay so one caveat is that we are estimating Q PI naught Q opt will revisit this later and another thing to kind of note is the difference between what is called on policy and off policy okay so in reinforcement learning you're always following some policy to get around the world right and that's generally called an exploration policy or the control policy and then there's usually some other thing that you're trying to estimate usually the the value of a particular policy and that policy could be the same or could be different so on policy means that we're estimating the value of the policy that we're following the generate data generate policy our policy means that we're not okay so so in particular is model free Monte Carlo on policy or off policy it's on policy because I'm estimating q pi knock you opt okay that's on policy and off policy what about model-based Montecarlo I mean it's a little bit of a slightly weird question but in model-based monte-carlo we're following some policy maybe even random policy but we're estimating the transitions and rewards and from that we can compute the optimal policy so you can you can think about is a off policy but you know that's maybe not completely standard okay so any questions about what model free Monte Carlo is doing so let me just actually write so what does the model-based Monte College is doing it's trying to estimate the the transition and rewards and model 3 Monte Carlo is trying to estimate the Q pi okay and just as a note I put hats on any letter that is supposed to be a quantity that's estimated from data and that's what you know statisticians do to differentiate them between whenever IQ PI that's the true value of that you know policy which you know I don't have ok any questions about model 3 Monte Carlo both of these algorithms are pretty simple right you just you know you look at the data and you take averages yeah so question is model free making changes to a policy or as a fixed box so so this version I've given you is only for a fixed policy the general idea of model free as we'll see you later you can also optimize a policy okay so so now what we're gonna do is we're gonna do theme and variations on model free Monte Carlo actually where it's gonna be the same algorithm but I just want to interpret it in kind of slightly different ways that will help us generalize it in the future yeah problems where model three dozen employees are there certain problems where model free is better than model-based so this is actually a really interesting question right so you can show that if your model is correct if your model of the world is correct model base is kind of the way to go because it will be more sample efficient meaning that you need fewer data points but it's really hard to get the model correct in the real world so recently especially with neo deep reinforcement learning people have gotten a lot of mileage by just going model free because then jump your head a little bit you can model this as a kind of a deep neural network and that gives you extraordinary flexibility and power without having to solve the hard problem of Noah constructing the MDP okay so so there's kind of three ways you can think about this so the first we already talked about it is you know this average idea so we're just looking at the utilities that you see whenever you counter in SNA and you just average them okay so here is an equivalent formulation and the way it works is that for every si you that you see so every time you see a particular sau sau sau and so on I'm going to perform the following update on so I'm gonna take my existing value and I'm going to do a what is called a convex combination so you know 1 - ADA and a - sum to 1 so it's you know kind of balancing between two things balancing between the old value that I had and the the new utility that I saw ok and the ADA is set to be 1 over a 1 plus the number of updates ok so let me do a concrete example I think it'll make this very clear what's what's going on so suppose my data looks like this so I hacked at 4 and then a 1 and a 1 so these are the utilities right that's that's a you hear I'm ignoring the SNA I'm just gonna assume that there's some something okay so first let's assume that Q pi is zero okay so the first time I do let's see number of updates I haven't done anything so it's 1 1 minus 0 so 0 times 0 plus 1 times 4 which is the first year that comes in okay so this is 4 okay so then that what about the next data point that comes in so I'm gonna take 1/2 now times 4 plus 1/2 times 1 which is a new value that comes in and that is I'm gonna write it as 4 plus 1 over 2 okay so now okay just to keep track of things this results in this this results in this and then now we're running out of space but hopefully I can so now on the third one I do two thirds so I have 4 plus 1 over 2 times 2/3 plus actually I guess I should do 2/3 to be consistent 2/3 times 4 plus 1 over 2 which is a previous value that City and Q PI plus a 1/3 times 1 which is a new value and that gives me 4 plus 1 plus 1 over 3 right so you can see what's going on here is that you know each each time I have this you know sum over all the acuity I've seen over the number of times occurs and this ADA is set so that next time I kind of cancel out the old account and I add the new count to the denominator and it kind of all works out so that and every time step what actually is in Q PI is just the plain average over all the numbers I've seen before okay this is just kind of an algebraic trick to get this original formulation which is the notion of average into this formulation which is a notion of kind of you're trying to take a little bit of the old thing and add a little bit of a new thing okay so I guess I'm gonna call this I guess combination I guess so that's the second interpretation there's a third interpretation here which you can think about is in terms of stochastic gradient descent so this is actually a kind of a simple algebraic manipulation so if you look at this expression what is this so you have 1 times Q pi so I'm going to pull it out and put it down here and then I'm gonna have minus a 2 times Q pi that's this thing and then I also have an ADA a u so I'm gonna put kind of - you here and this is inside this print so if you just you know do the algebra you can see that these two are equivalent so what's the point of this right so where have you kind of seen this before something like maybe not not this exact expression but something like it any ideas yeah when you looked at a saccadic radius and in the context of the square loss for linear regression right so remember we had these updates that all looked like kind of prediction - Targa which was you know the residual and that was used to kind of update so one way to interpret this is this is kind of implicitly trying to do stochastic gradient descent on the objective which is a squared loss on the the q pi value that you you you're trying to set and you which is the new piece of data that you got so think about in regression this is the Y this is y you know the what the output is and you this is the model that's trying to predict it and you want those to be close to each other okay okay so so those are kind of three views on basically this idea of averaging or incremental updates okay so it'll become clear why you know I did this this isn't just to you know have fun okay so now let's see an example of model free Monte Carlo in action on this the volcano game so remember here we have as an example and I'm going to set the number of episodes to let's say a thousand let's see what happens so here okay so what is this kind of a grid like structure degree of triangles denote so this remember is a state this is 2 comma 1 so what i'm doing here is dividing into four pieces which corresponds to the four different action so this triangle is 2 comma 1 north this triangle is 2 comma 1 east and so on okay so and the number here is the q pi value that i'm estimating along the way okay so the policy I'm using is a complete random just move randomly and I've run this a thousand times and we see that the average utility is no minus 18 which is obviously not great okay but this is an estimate of how well the random policy is doing so you know as advertised no random policy you would expect to fall into a volcano quite often okay and you can run this and sometimes you get slightly different results but you know it's pretty much stable around minus 19 - 18 ok any questions about this before we move on to different algorithms okay so model base Monte Carlo were estimating the MVP lot of free Monte Carlo we're just estimating a Q values of a particular policy for now okay so so let's really visit what model free Monty Hall is doing so if you use a policy PI eco stay for the dice game you know you might get a bunch of different trajectories that come out these are possible episodes and each episode you have a utility you know associated with that and what model free Monte Carlo is doing is it's using these utilities to kind of update towards update uq pi right so in particular like for example this you're saying okay I'm in I'm in the in state and I you know take an action stay we know what will happen well in this case I got you know 16 and this case I got twelve and notice that there's you know quite a bit of variance so on average this actually does the right thing right so um just by definition this is our unbiased you know estimate if you do this a million times in average you're just gonna get the right value which is twelve in this case but the variance is huge so if for example if you only do it a few times you're not gonna get twelve you might get something you know cited so how can we kind of counteract all this this variance so the key idea behind what we're gonna call bootstrapping is is that you know we actually have you know some more information here so we have this Q PI that we're estimating along the way right so so this view is saying okay we're trying to estimate Q PI and then we're going to try to basically regress it against you know this data that we're seeing but you know can we actually use q pi itself to help you reduce the variance so so the idea here is I'm going to look at although paces where I you know I started in and I take stay I get a for okay so I'm gonna say I get a for but then after that point I'm actually just going to substitute this 11 in okay this is kind of weird right because normally I would just see okay what would happen but what happens is kind of random on average is gonna be right but you know on any given case I'm a kid like you know 24 or something and the hope here is that by using my current estimate which isn't going to be right because if I were they were right I would be done but hopefully it's kind of somewhat right and that will you know be no better than using the the kind of the raw roll out value yeah question yes the question is would you update the furnace after each episode yes so all these algorithms I haven't been explicit about it is that you see an episode you update after you see it and then you get a new episode and so on yeah sometimes you would even update before you're done with episode okay so let me show this what this algorithm so this is a new algorithm it's called sarsa does anyone know why it's called sarsa oh yeah right so if you look at this it's bill starts up and that's literally the reason why it's called sarsa so what is this algorithm say so you're in a state s you took action a you've got a reward and then you ended up in state s prime and then you took another action a prime so for every kind of quintuple that you see you're gonna perform this update okay so what is this update doing so this is the convex combination remember that we saw from before where you take a part of the old value and then you try to merge them with the new value so what is the new value here this is looking at just the immediate reward not the full utility just a media reward which is this for here and you're adding the discount which is one for now of your estimate and remember what is the estimate trying to do estimate is trying to be the expectation of rewards that you will get in the future so if this were actually coupon occupy hat then this will actually just be strictly no better because that would be just reducing the variance but you know of course this is not exactly right there is bias so it's 11 not 12 but the hope is that you know this is not you know bias bite you know too much okay so these would be the kind of the values that you will be updating rather than these kind of raw values here okay so just to kind of compare them well okay okay any questions about what sarsa is doing before I move on so maybe I'll write something to try to be helpful here so Q PI Marquis Monticello estimates Q PI based on you and sarsa is still Q PI hat but it's based on reward plus essentially Q PI hat I mean this is not like a velvet expression but hopefully it's some symbols that will evoke the right memories okay so let's discuss the difference is so this is this whenever people say kind of bootstrapping in the context of reinforcement learning this is kind of what they mean is that instead of using you as the prediction target you're using R plus Q PI and this is kind of your pulling up yourself from your bootstraps because you're trying to ask me Q PI but you don't know Q pi by using q pi two minutes okay so us based on one path in salsa your based on the estimate which is based on all your previous kind of experiences which means that this is unbiased model free monitor colors bias but sarsa is bias want to have has large variance sarsa has you know smaller variance and one I guess a consequence of the way the algorithm set up is that model fee Monte Carlo you have to kind of roll out the entire game basically play the game or the MVP until you reach the terminal state and then you can now you have your you to update where as sarsa when or any sort of bootstrapping algorithm you can just immediately update because all you need to do is you need to see this like a very local window of SA RSA and then you can just update and that can happen kind of you know anywhere you don't have to wait until the very end to get the value okay so just as a quick sanity check which of the following algorithms allows you to estimate Q opt okay so model-based Monte Carlo model free Monte Carlo or sarsa okay so I'll give you maybe 10 seconds to ponder this okay how many more many more time okay let's get a report I think I didn't reset it from last year so this is includes last year's uh participants so model-based Monte Carlo allows you to get Q opt right because once you have that me P you can get whatever you want you can kick you up motive free Monte Carlo estimates q pi doesn't have to make you opt and sarsa also has some it's q pi but doesn't has to make you opt okay all right so so that's a kind of a problem like I mean these algorithms are fine for estimating the value of a policy but you really want the optimal policy right in fact these can be used to improve the policy as well because you can do something called policy improvement which I didn't talk about once you have the Q values you can define the new policy based on the Q values but there's actually a kind of a more direct way to do this okay so so here's the kind of the way mental framework you should have in your head so there's two values Q PI and Q opt so in MDPs we saw that policy evaluation allows you to get Q PI value iteration get allows you get a Q opt and now we're doing reinforcement learning here we saw a model free Monte Carlo in star sellout you get Q PI and now we need I'm gonna show you a new algorithm called Q learning that allows you to get Q optin so this gives you Q opt and it's based on reward plus Q opt okay so this is going to be very similar to sarsa it's only gonna differ by essentially as you might guess the same difference between policy evaluation and value iteration okay so it's helpful to go back to kind of the MVP recurrences so even though MVP recurrences can only apply when you know the MVP for deriving and reinforcement learning algorithms it's they can kind of give you inspiration for the actual algorithm okay so remember a Q opt what is Q opt to the Q opt is considering all possible successors the probability immediate reward plus future returns okay so the Q learning is this actually really kind of clever idea and it's it could also be called czars stars I guess but maybe you don't want to call it that and what it does is as follows so this has the same form the convex combination of the old value and the the new value right so what is the new value so if you look at Q opt Q opt is looking at different successors reward plus V opt what we're gonna do is well we don't have all we're not going to be able to some of our successors because when our reinforcement learning setting and we only saw one particular successor so let's just use that successor so on that successor we're going to get the reward so R is a stand-in for the actual reward I mean is the stand-in for the reward reward function and then you have gamma times and then V opt I am going to replace it with our estimate of what V opt is and what should it be estimate of V off to be so what relates V up to Q opt yeah yeah exactly so you define V off to be the max over all possible actions of Q opto of s in that particular action then this is V opt right so Q is saying I'm in a chance node how much what is optimal utility I can get provided I took an action clearly the best thing to do if you're at a state is just choose action that gives you the maximum Q value that you get okay so that that's just no Q learning so let's put it side-by-side with sarsa okay so sarsa these two are very similar right so sarsa remember updates against r plus a q pi and now we're updating against our plus this max over Q opt okay and you can see that salsa requires knowing what action I'm gonna take next kind of a one step look ahead a prime and it plugs it into here whereas Q learning it doesn't matter what a you took because I'm good just gonna take the one that maximizes right so you can see why salsa dip is estimating the value of a policy because you know what a prime shows up here is you know a function of a policy and here I'm kind of insulated from that because I'm just taking the maximum overall actions this is the same intuition as for value iteration versus a policy evaluation okay I'll pause here any questions Q learning versus sarsa Q learning on policy or off policy it's off policy because I'm following whatever policy I'm following and I you get to estimate the value of the optimal policy which is probably not the one I'm following how they seem the beginning okay so let's look at the example here so here's sarsa and run for a thousand iterations and like model free Monte Carlo this I'm estimating that average the average utility I'm getting is minus 20 and in particularly the values I'm getting are all very negative because this is Q PI this is a policy I'm following which is the random policy if I replace this with Q what happens so first notice that the average utility is still minus 19 because I actually haven't changed my exploration policy I'm still doing random exploration well yeah I'm still doing random exploration but notice that the value the Q up values are all around you know 20 right and this is because the optimal policy remember is just 2 and this is a slip probability 0 so although policy is just to go down here and get your 20 okay and Q and I guess it's kind of interesting that Q learning I'm just blindly following the policy running you know off off the cliff into the volcano all the time but you know I'm learning something and I'm learning how to behave optimally even though I'm not behaving optimally and that's a kind of a hallmark of off policy learning okay so any questions about these four algorithms so model based Monte Carlo estimate MVP model free Monte Carlo estimate the Q value of this policy based on the actual returns that you get the actual sum of the rewards sarsa is bootstrapping to estimating the same thing but with kind of a one step look ahead and Q learning is like sarsa except for I'm estimating the optimal instead of a fixed policy pi yeah Garza is on policy because I'm estimating Q PI all right okay so now let's talk about covering the unknown so these are the algorithms so at this point if I just hand you some data if I told you here's a fixed policy here's some data you can actually ask me all these quantities but now there's a question of exploration which we saw was really important because if you don't even even see all the states how can you possibly act optimally so so which exploration policy should you use so here are kind of two extremes so the first extreme is let's just set the exploration policy so so imagine we're doing Q learning now so you have this Q opt estimate so it's not their true Q Appa you have an estimate of Q opt the naivety thing to do is just take a use that kuat figure out which action is best and just always do that action okay so what happens when you do this is you don't do very well so why don't you do very well because initially well you explore randomly and soon you you find the two and once you've found that two you say uh well two is better than zero zero zero so I'm just gonna keep on going down to the two which is your all exploitation no exploration right you don't realize you that there's all this other stuff over here so in the other direction we have no exploitation all exploration here you kind of have the opposite set up where I'm running a q-learning right so as we saw before I'm actually able to estimate the the the Q opt values so I learn a lot but the average utility which is the actual utility I'm getting by playing this game is pretty bad in pretty good it's the you to you get from just you know moving randomly so kind of what you really want to do is balance you know exploration exploitation so just kind of a kind of a site or a commentary is that I really feel reinforcement learning kind of captures our life pretty well because in life there's you know you don't know what's going on you want to get rewards you know you want to do well and but at the same time time you have to kind of learn about how the world works so that you can kind of improve your your policy so if you think about going to restaurants or finding the shortest path better way to get to to school or to work or in research even when you're trying to figure out a problem you can work on the thing that you know how to do and will definitely work or you know do you try to do something new in hopes of you learning something but maybe it won't get you as high reward so hopefully reinforcement learning is I know it's kind of a metaphor for life anyways okay so back to concrete stuff so here is one you can balance exploration exploitation right so it's called the epsilon greedy policy and this assumes that you're doing something like q-learning so you have these Q up values and the idea is that you know with probability 1 minus epsilon where epsilon is let's say like point 1 you're just gonna give exploit we're just gonna do give you a give it all you you have and then once in a while you're just gonna do something random okay so this is actually not a bad policy to act in life so once in a while maybe you should just do something random and you kind of see what happens so if you do this what do you get okay so what I've done here is I've set epsilon to be starting with 1 so 1 is all exploration and then I'm going to change the value a third of the way into 0.5 and then I'm gonna 2/3 of way I'm going to change it to 0 okay so if I do this then I actually estimate the values really really well and also I get utility which is you know pretty good you know 32 okay and this is also kind of something that happens as you get older you tend to explore earth less and explore it just happens ok alright so that was exploration so let's put some stuff on the board here do I need this anymore maybe this ok [Applause] okay so covering the unknown so we talked about your exploration your epsilon greedy and there's other ways to do this epsilon greedy is just kind of a simplest thing that actually you know works remarkably you know well even the state of our systems so the other problem now I'm going to talk about is you know generalization so remember when we said exploration well if you don't see a particular state then you don't know what to do with this I mean think about for a moment that's kind of unreasonable because you know in life you're never gonna be an exact same you know situation and yet we are we need to be able to act properly right so general problem is at the state space that you you might deal with in a kind of a real-world situation is enormous and there's no way you're going to go track down every possible state okay so this state space is actually not that enormous but this is the biggest state space I could draw on this on the screen and you can see that this you know the average utility is you're pretty bad here okay so what can we do about this so I guess let's talk about a large state space so this is the problem so now this is where the second the third interpretation of model Fremont color will come in handy so let's take a look at q-learning okay so in the context of SGD it looks like this right so it's a kind of a gradient step where you take the old value and you - ADA and something that kind of looks like it could be a gradient which is the residual here so one thing to note is that under the the kind of formulations of a q-learning that I've talked about so far this is what we call kind of roll earning right which if we were you know two weeks ago we were said this is you know kind of ridiculous because it's not really learning we're generalizing at all right now it's basically for every single state in action I have a value if I have a difference in action completely different value I don't I don't there's no kind of sharing of information and naturally if I do that I can't generalize between states and actions okay so here's the key idea that will allow us to actually overcome this so it's called function approximation in the context of reinforcement learning in normal machine learning it's just called normal machine learning so the way it works is this so we're going to define this Q opt si it's not going to be a lookup table it's going to depend on some parameters here W and I'm going to define this function to be W dot fee si okay so I'm going to define this feature vector very similar to how we did in kind of machine in the machine learning section like except for instead of si we had X and now all the weights are going to be kind of us you know saying okay so what kind of features might you have you might have for example features on you know actions so these are indicator features let's say hey maybe it's better to go east than to go west or maybe it's better to be in the fifth row or it's good to be in a sixth column and you know things like that so you have a smaller set of features and you try to use that to kind of generalize across I know all the different states that you might see so what this looks like is now with the features is actually the same as before except for now we have something that really looks like you know the machine learning of lectures is that you take your weight vector and you do an update of the residual times the feature vector okay so how many of you this looks familiar from linear regression okay all right so so just a contrast so before we were just updating the Q opt values but the residual is exactly the same and there's nothing over here and now what we're doing is we're updating not the Q values we're updating the weights the residual is the same and the thing that connects the the Q values with the the residual width or the weights is the kind of a feature vector okay as a sanity check this has the same dimension this is a vector this is a scalar this is a vector which has the same dimensionality as W okay and if you want to derive this you can actually think about the implied objective function as simply you know linear regression you have a model that's trying to predict a value from an input si so si is like X and Q opt is like kind of Y and then your record this target is like the Y that you're trying to predict and you're just trying to make this prediction close to the target yeah question yeah so a good question so what is this a door now is it the same as before or is it new so when we first started talking about these algorithms right ADA was supposed to be one over the number of updates and so on but once you get into the sgf form like this then now this is just behaves as a step size and you can tune it to your heart's conduct all right so that's all I'll say about these two challenges one is how do you do exploration you can use epsilon greedy which allows you to kind of balance exploration with exploitation oh and then the second thing is that for large state spaces epsilon greedy isn't going to cut it because you you're not going to see all the states even if you try really hard and you need something like function approximation to tell you about new states that you fundamentally haven't seen before okay so summary so far online learning we're in the online setting this is the game of reinforcement learning you have to learn and take actions in real world one of the key challenges is this exploration exploitation trade-off we saw for algorithms there's kind of two key ideas here one is Monte Carlo which is that from data alone you can basically use averages to estimate quantities that you care about for example transitions rewards and Q values and the second key idea is this bootstrapping which shows up in salsa and Q learning which is that you're updating towards a target that depends on your estimate of what you're trying to predict not just the the kind of raw data that you see okay so now I'm gonna maybe step back a little bit and talk about reinforcement learning in the context of some kind of other things so there's kind of two things that happen when we went from binary classification which was you know two weeks ago to reinforcement learning now and it's worth kind of decoupling there's two things one is state and one is feedback so the idea about partial feedback is that you can only learn about actions you take right I mean this is kind of obvious in reinforcement learning if you didn't don't quit in this game you never know how much money you'll yoyo get and the other idea is the notion of state which is that you rewards depend on your previous actions so if you're going through a volcano you have to there's a kind of a different situation depending on where you are in in the map and there's actually kind of so so this is kind of a you can draw two by two grid where you go from supervised learning which is stateless and full feedback right so there's no state every iteration you just get in new example and that doesn't have you know there's no dependency and in terms of prediction on the previous examples and full feedback and because in supervised learning you're told which is the correct label even though there's even if there might be you know a thousand labels for example in image classification you're just told which ones a correct label and now in real for still learning both of those are made harder there's two other interesting points so what is called multi armed bandits is kind of a you can think about as a warm-up to reinforce for learning where there's partial feedback but there's no state which makes it easier and there's also you can get full feedback by their state so in structure prediction for example a machine translation you're told what the translation output should be but clearly do actions depend on previous actions because you know you can't just translate words in isolation essentially okay so one of the things I would just mention very briefly is you know this deeper reinforcement learning has been you know very popular in recent years so reinforce ammonia there was kind of a lot of interest in the the kind of 90s where a lot of the algorithms were kind of in theory work um was kind of developed and then there was a period where kind of not that much not as much happened and since I guess 2013 there's been a revival of reinforcement of research a lot of it's due to I guess at the D mind where they published a paper showing how they can do use robbery and first morning to play Atari so this will be talked about more in a section this Friday but the basic idea of deep reinforcement learning just to kind of demystify things is that you're using a neural network for cue opt essentially that's what it is and there's also a lot of tricks to make this kind of work which are necessary when you're dealing with enormous state spaces so what of the things that's different about deep reinforcement learning is that people are much more ambitious about handling problems where the state spaces are kind of enormous so for this the state is that just the you know the pixels right so there's no huge number of pixels and whereas before people were kind of in what is known as a tabular case which there's a number of states you can kind of enumerate so there's a lot of kind of details here to care right one general comment is that reinforcement learning is tech news it's really hard right because of this statefulness and also the delayed feedback so just when you're maybe thinking about final projects I mean it's a really cool area but you know don't underestimate how much work and compute you need to do some other things I won't have a time to talk about is so far we've talked about methods that try to estimate there the cue function there's also a way to even do without the cue function and just try to estimate the policy directly that's called you know methods like policy gradient there's also methods like actor critic that tried to combine of these value based methods and policy based methods these are used in deep minds you know I'll forego and alpha0 programs for you know crushing humans I go this one actually will be deferred to next week section because this is in the context of games there's a bunch of other applications you know you can fly helicopters play backgammon this is actually one of the early examples TD gamma was one of early examples in there the nineties up on they're kind of one of success stories of using reinforcement learning and particulars you know self play for non games you know reinforcement can be used to do have later scheduling and managing data centers and so on okay so that concludes a section on Markov decision processes which we the ideas that we are playing against nature so nature is kind of random but you know kind of neutral next time we're going to play against an opponent where they're out together so we'll see about that