so the goal of this video is to prove the correctness of Kazar raju's two paast depth first search-based linear time algorithm that computes the strongly kinetic components of a directed graph so I've given you the full specification of the algorithm I've also given you a plausibility argument of why it might work and that at least it does something sensible on an example namely it first does a passive depth for search on the reverse graph it computes this magical ordering and what's so special about this ordering is then when we do a depth first search using this ordering on the forward graph it seems to do exactly what we want every invocation of depth first search to some new node discovers exactly the nodes of a strong component and no extra stuff remember that was our first observation about that was unclear whether depth first search would be useful or not for computing strong components if you call depth first search from just the right place you're going to get exactly the nodes of an SEC and nothing more if you call it from the wrong place you might get all of the nodes of the graph and get no information at all about the structure of the strong components and at least in this example this first pass with the finishing time seems to be accomplishing seems to be leading us to invoking depth for search from exactly the right places so remember how this worked in the example so in the in the top graph I've shown you the graph with the arcs reversed this is where we first invoke DFS uh loop with a loop over the nodes going from the highest node name nine all the way down to the node name one and here we compute finishing times that's the bookkeeping that we do in the first pass so we just keep a running count of how many nodes we've finished processing that is how many we've both explored that node as well as explored all of the outgoing arcs and so that giv us these numbers in red these finishing times between 1 and one and N for the various nodes those became the new node names in the second graph and then we reversed the arcs again to get the original graph back and then we saw that every time we invoked DFS in our second pass we uncovered exactly the nodes of an SEC so when we invoked it from the node 9 we discovered 98 and 7even those all have the leader vertex nine then when we next invoked DFS from six we discovered 65 and one and nothing else and then finally we invoked it from four we discovered two three and four and nothing else and those are exactly the three uh secc's of this graph so let's Now understand why this works in any directed graph not just this in this one example so let's begin with a simple observation about directed graphs which is actually interesting in its own right the claim is that every directed graph has two levels of granularity if you squint if you sort of zoom out then what you see is a directed ayylic graph course comprising its strongly connected components and if you want you can zoom in and focus on the fine grain structure with one SC a little bit more precisely the claim is that the strongly kinetic components of a directed graph induced in a natural way an a cyclic metagraph so what is this metagraph what are the nodes and what are the arcs what are the edges well the metan noodes are just the sec's so we think of every strong ktic component as being a single node in this metag graph so call them say C1 up to CK so what are the arcs in this metag graph well they're basically just the ones corresponding to the arcs between secc's and the original graph that is we include in the metagraph an arc between the strong components C and C hat from C to C hat if and only if there's an arc from a node in C to a node in C hat in the original graph G so for example if this is your C and this other triangle is your C hat and you have one or maybe multiple edges going from C to C hat then in the corresponding metagraph you're just going to have a node for c a node for C hat and a directed Arc from C to C hat so if we go back to some of the directed graphs that we've used as running examples so if we go back to the one that at the beginning of the previous video which looked maybe something like this the corresponding directed ayylic graph has four nodes and four arcs and for the running example we Ed to illustrate Kazar raju's algorithm with the three triangles the corresponding metagraph would just be a path with three nodes so why is this metagraph guaranteed to be a cyclic well remember metan noes correspond to strong components and in a strong component you can get from anywhere to anywhere else so if you had a cycle that involved two different metan noes that is two different strong conected components remember on a directed cycle you can also get from anywhere to anywhere else so if you had two supposedly distinct secc's but you could get from the one to the other and vice versa they would collapse into a single SEC you can get from anywhere to anywhere in one anywhere from anywhere in the other one and you can also go between them at will so you can get from anywhere in one you can get from anywhere in this Union to anywhere in Union so not just in this context of computing strong components but also just more generally this is a useful fact to know about directed graphs on the one hand they can have very complex structure within a strong component you know you have paths going from everywhere everywhere else and it may be sort of complicated looking but at a higher level if you abstract out at the level of secc's you're guaranteed to have this simple dag the simple directed aycc graph structure so to reinforce these Concepts and also segue into thinking about Kazar raji's algorithm in particular let me ask you a question about how reversing arcs affects the strong components of a directed graph so the correct answer to this quiz is the fourth one the strong components are exactly the same as they were before in fact the relation that we described is exactly the same as it was before so therefore the equivalence classes or the strong components is exactly the same so if two nodes were pre are related in the original graph that is there's a path from U to V and a path from V to U that's still true after you reverse all the arcs you just use the reversal of the two paths that you had before similarly if the two nodes weren't related before for example because you could not get from U to V well then after you reverse everything then you can't get from V to you so again you don't have this relation holding so the sec's are exactly the same in the forward or the backward uh graph in particular in kar's algorithm the strong component structure is exactly the same uh in the first passive DFS and in the second passive DFS so now that we understand how every directed graph has a metagraph where the nodes correspond to its trtic components and you have an arc from one to another if there's any Arc from any node in that sec to the other sec in the original graph I'm in a position to State what's the key limma that drives the correctness of Kazar raju's two pass algorithm for computing the strong ktic components of a directed graph so here's the LMA statement it considers two string kinetic components that are adjacent in the sense that there's an arc from one node in one of them to one node in the other one so let's say we have one SEC C1 with a node I and another SEC C2 with a node J and that in G in the graph there's an arc directly from I to J so in this sense we say that these sec's are adjacent with the second one being in some sense after the first one now let's suppose we've already run the first pass of the DFS Loop sub routine and remember that works on the reverse graph so we've invoked it on the reverse graph we computed these finishing times as usual we'll let F denote the finishing times computed in that depth first search sub routine on on the reverse graph the Lemma then asserts the following it says first amongst all the nodes in C1 look at the one with the largest finishing time similarly amongst all nodes in C2 look at the one with the biggest finishing time amongst all of these the claim is the biggest finishing time will be in C2 not in C1 so what I want to do next is I want to assume that this Lemma is true temporarily and I want to explore the consequences of that assumption and in particular what I want to show you is that if this Lemma holds then we can complete the proof of correctness of Kazar raju's two pass SEC computation algorithm okay so if the Lemma is true then after I'll give you the argument about why we're done about why we just peel off the strongly connect components one at a time with the second passive depth first search now of course a proof of the hole in it isn't a proof so at the end of the lecture I'm going to fill in the hole that is I'm going to supply a proof of this key Lemma but for now as a working hypothesis let's assume that it's true let's begin with a Cory that is a statement which follows essentially immediately from the statement of the Lemma so for the Cory let's forget about just trying to find the maximum vert the maximum finishing time in a single SEC let's think about the maximum finishing time in the entire graph now why do we care about the maximum finishing time in the entire graph well notice that's exactly where the second passive depth first search is going to begin right so it processes nodes in order from largest finishing time to smallest finishing time so equivalently let's think about the node at which the second passive depth first search is going to begin IE the node with a maximum finishing time where could it be well the corollary is that it has to be in what I'm going to call a sync strongly connected component that is a strongly connected component without any outgoing arcs so for example let's go back to the uh metagraph of fcc's for the very first directed graph we looked at you recall that in the very first directed graph we looked at in uh when we started talking about this algorithm there were four secc's so there was a C1 a C2 a C3 and a C4 and of course within each of these uh components there could be multiple nodes but they're all strong connected to each other now let's use F1 F2 F3 and F4 to denote the maximum finishing time in each of these scc's so we have fub1 F2 F3 and 4 so now we have four different opportunities to apply this Lemma right there's four different pairs of adjacent sec's and so what do we find we find that well comparing FS1 and FS2 because C2 comes after C1 that is there's an arc from C1 to C2 the max finishing time in C2 has to be bigger than that in C1 that is FS2 is bigger than fub1 for the same reasoning F3 has to be bigger than FS1 symmetrically we can apply the limit to the pairs C2 C4 and C 3 C4 and we get that F4 has to dominate both of them you'll notice we actually have no idea whether F2 or F3 is bigger so that pair we can't resolve but we do know these relationships okay F1 is the smallest and F4 is the smallest and you'll also notice that C4 is a sync SCC in the sense that it has no outgoing arcs and if you think about it that's a totally General consequence of this Lemma so a simple proof by contradiction would go as follows consider the strongly kinetic component with the maximum F value suppose it was not a sync secc then it has an outgoing Arc follow that outgoing Arc to get to some other SCC by the Lemma the SEC you've gotten to has an even bigger maximum finishing time so that contradicts the fact that you started in the SEC with a maximum finishing time okay so just like in this cartoon where the unique sync SEC has to have the largest finishing time that's totally General as another sanity check we might return to the N node graph where we actually ran Kazar raju's algorithm and looking at the forward version of the graph which is the one on the bottom we see that the maximum finishing times in the three sec's are four 6 and n okayy it turns out they're the same as the leader nodes which is not an accident if you think about it for a little while and again you'll observe the maximum finishing time in this graph namely nine is indeed in the leftmost SEC which is the only SEC with no outgoing arcs okay but it's totally General uh basically you can keep following arcs and you keep seeing bigger and bigger finishing times so the biggest one of all it has to be somewhere where you get stuck where you can't go forward where there's no outgoing arcs and that's what I'm calling a sync SEC okay so assuming the L is true we now though this Cory is true now using this coraly let's finish the proof of correctness of Kazar raju's algorithm modular appr proof of the key Lemma so I'm not going to do this super rigorously although everything I say is correct and can be made made rigorous and if you want a more rigorous version I'll post some notes on the course website which you can consult for more details so what the previous coril accomplished it allows us to locate the node with maximum finishing time we can locate it in some somewhere in some sync sec let me remind you about the discussion we had at the very beginning of talking about Computing strong components we're trying to understand whether depth for search would be a useful Workhorse for finding the strong components and the key observation was that it depends where you begin that depth for search so for example in this uh graph with four sccs shown in Blue on the right a really bad place to start a DFS call depth for search would be somewhere in C1 somewhere in this Source SEC so this is a bad DFS why is it bad well remember what depth first search does it finds everything findable from its starting point and from C1 you can get to the entire world you can get to all the nodes in the entire graph so you'll discover everything and this is totally useless because we wanted to discover much more fine grain structure we wanted to discover C1 C2 C3 and C4 individually so that would be a disaster if we invoked bre uh depth for search somewhere from C1 fortunately that's not what's going to happen where we computed this magic Ling in the first pass to ensure that we look at the node with the maximum finishing time first and by the corollary the maximum finishing time is going to be somewhere in C4 that's going to be a good DFS in the sense that when we start exploring from anywhere in C4 there's no outgoing arcs so of course we're going to find everything in C4 everything in C4 is strongly connected to each other but we can't get out we will not have the option of trespassing on other strong components we're not going to find them so we're only going to find C4 nothing more now here's where I'm going to be a little informal although again everything I'm going to say is going to be correct so what happens now once we've discovered everything in C4 well all the nodes in C4 get marked as explored as we're doing depth first search and then they're basically dead to us right the rest of our depth first search Loop will never never explore them again they're already marked as explored if we ever see them we don't even go there so the way to think about that is when we proceed with the rest of our for Loop in DFS loop it's as if we're starting a fresh we're doing depth first search from scratch on a smaller graph on the residual graph the graph G with this newly discovered strong component C star deleted so in this example on the right all of the nodes and C4 are dead to us and it's as if we run DFS a new just on the graph uh containing the strong components C1 C2 and C3 so in particular where is the next indication of depth first search going to come from it's going to come from some sync sec in the residual graph right it's going to start at the node that remains and that has the largest finishing time left so there's some ambiguity in this picture again recall we don't know whether F2 is bigger or F3 is bigger it could be either one so maybe F2 is the largest remaining finishing time in which case the next DFS invocation is going to begin somewhere from C2 again then the only things outgoing from C2 are these already explored nodes they're effectively deleted we're not going to go there again so this is essentially a sync sccc we discover we newly discover the nodes in C2 and nothing else those are now effectively deleted now the next indication of DFS will come from somewhere in F3 somewhere in C3 that's the only remaining sync SCC in the residual graph so the third call of DFS will discover this stuff and now of course we're left only with C1 and so the final indication of DFS will will emerge from and discover the nodes in C1 and in this sense because we've ordered the nodes by finishing times in DFS with reverse graph that ordering has this incredible property that when we process the nodes in the second pass we will just peel off the strongly connected components one at a time if you think about it it's in Reverse topological order with respect to the directed a cyclic graph of the strongly connected components so we've constructed a proof of correctness of Kazar raju's uh algorithm for computing St components but again there's a hole in it so we completed the argument assuming a statement that we haven't proved so let's fill in that last Gap in the proof and we'll be done and so what we need to do is prove the key Lemma let me remind you what it says it says if you have two adjacent secc's C1 and C2 there's an arc from a node in C1 call it I to a node in C2 say J then the max finishing time in C2 is bigger than the max finishing time in C1 where as always these finishing times are computed in that first pass of depth first search Loop in the Reversed graph all right now the finishing times are computed in the Reversed graph so let's actually reverse all the arcs and reason about what's happening there we still have C1 it still contains the node I we still have C2 it still contains the node J but now of course the orientation of the arc has reversed so the arc now points from J to I recall we had a quiz which said asked you to understand the effect of reversing all arcs on the sec's and in particular there is no effect so the sec's in the reverse graph are exactly the same as in the forward graph so now we're going to have two cases in this proof and the cases correspond to where we first encounter a node of C1 Union C2 now remember when we do this DFS Loop this second pass because we have this outer for Loop that iterates over all of the nodes we're guaranteed to explore every single node of the graph at some point so in particular we're going to explore at some point every single node in C1 Union C2 what I want you to do is pause the algorithm when it first for the first time explores some node that's in either C1 or C2 there's going to be two cases of course because that node might be in C1 you might see that first or it might be in C2 you might see something from C2 first so our case one is going to be when the first node that we see from either one happens to lie in C1 and the second case is where the first node V that we see happens to lie in C2 so clearly exactly one of these will occur so let's think about case one when we see a node of C1 before we see any nodes of C2 so in this case where we encounter a node in C1 before we encounter any node in C2 the claim is that we're going to explore everything in C1 before we ever see anything in C2 why is that true the reason is there cannot be a path that starts somewhere in C1 like for example at the vertex V and reaches C2 and this is where we're using the fact that the metagraph on strong conect components is a cyclic right C1 is strong connected C2 is strong connected you can get from C2 to C1 and if you can also get from C1 back to C2 this all collapses into a single strong kinetic component but that would be a contradiction we're assuming C1 and C2 are distinct strong components therefore you can't have paths in both directions we already have a path from right to left via ji so there's no path from left to right that's why if you originate a depth for search from somewhere inside C1 like this vertex V you finish exploring all of C1 before you ever are going to see C2 you're only going to see C2 at some later point in the outer for Loop so what's the consequence that you completely finish with C1 before you ever see C2 well it means every single finishing time in C1 is going to be smaller than every single finishing time in C2 so that's even stronger than what we're claiming we're just claiming that the biggest thing in C2 is bigger than the biggest of C1 but actually finishing times in C2 totally dominate those in C1 because you finish C1 before you ever see C2 so let's now have a look at case one actually in action let's return to the N node graph on which we actually ran kajar raju's algorithm to completion so if we go back to this graph which has the three connected components and remember it's the the bottom version is the forward version the top version is the Reversed version so if you think about the middle SEC is being C1 playing the role of C1 and the leftmost SEC playing the role of C2 then what we have exactly is case one of the key Lemma so which was the first of these six vertices visited during the DFS Loop in the Reversed graph well that would just be the node with the highest name so the node nine so this was the first of these six vertices that depth first search ever looked at and the first pass that lies in what we're calling C1 and indeed everything in C1 was discovered in that past before anything in C2 and that's why all of the finishing times in C2 the 7 8 and N are bigger than all of the finishing times in C1 the one five and six so we're good to go in case two we've proven uh sorry in case one we've proven the Lemma when it's the case that amongst the vertices in C1 Union C2 depth first search in the first pass seees something from C1 first first so now let's look at this other case this gray case which could also happen totally possible where the first thing we see when depth first searching in the first pass is something from C2 and here now is where we truly use the fact that we're using depth first search rather than some other Graph Search algorithm like breath first search there's a lot of places in this algorithm you could swap in breath first search but in this case too you'll see why it's important we're using depth first search to compute the finishing times and what's the key point the key point is that when we invoke depth first search beginning from this node V which is now assuming the line C2 remember depth for search will not complete we won't be done with v until we found everything there is to find from it right so we recursively explore all of the outgoing arcs they recurly explore all of their outgoing arcs and so on it's only when all paths going out of V have been totally explored and exhausted that we finally backtrack all the way to V and we consider ourselves done with it that is is depth first search in the reverse graph initiated at V won't finish until everything findable has been completely explored because there's an arc from C2 to C1 obviously everything in C2 is findable from V that's strong connected we can get from C2 to C1 just using this Arc from J to I C1 being strongly connected we can then find all of that maybe we can find other strong strongly connected components as well but for sure death first search starting from V we'll find everything in C1 Union C2 maybe some other things and we won't finish with v until we finish with everything else that's the de first search property for that reason the finishing time of this vertex V will be the largest of anything reachable from it so in particular it'll be larger than everything in C2 but more to the point it'll be larger than everything in C1 which is what we were trying to prove again let's just see this quickly in action in the N9 node Network on which we traced through Kazar raju's algorithm so to show the role that case 2 is playing in this concrete example let's think of the rightmost strong strongly connectic component as being C1 and let's think of the middle strongly connectic component as being C2 remember last time uh we called the middle one C1 and the leftmost one C2 now we're calling the rightmost one C1 and the middle one C2 so again we have to ask the question you know of the six nodes in C1 Union C2 what is the first one encountered in the depth first search that we do in the first pass and that again is the node nine okay the the node which is originally labeled nine so that's the same node that was relevant in the previous case but now with this relabeling of the components nine appears in the strong contic component C2 not in the one labeled C1 so that's the reason now we're in case 2 not in case one and what you'll see is what is the finishing time that this originally labeled nine node gets it gets the finishing time six and you'll notice six is bigger than any of the other finishing times of any of the other nodes in C1 or C2 all the other five nodes had the finishing times 1 through 5 and that's exactly because when we ran depth for search in the first pass and we started it at the node originally labeled nine it discovered these other five nodes and finished exploring them first before finally backtracking all the way back to Nine and deeming Nine fully explored it was only at that point that nine got its finishing time after everything reachable from it had already gotten their lower finishing times so that wraps it up we had two cases depending on whether in these two adjacent secc's the first vertex encountered was in the C1 or in C 2 either way it doesn't matter the largest finishing time has to be in C2 sometimes it's bigger than everything sometimes it's just bigger than the biggest in C1 but it's all the same to us and to recap how the rest of the proof goes we had a Cory based on this Lemma which says maximum finishing times have to lie in sync strong connected components and that's exactly where we want our depth first search to initiate if you're initiated in a strong component with no outgoing arcs you do DFS the stuff you find is just the stuff in that strong connected component you do not have any Avenues by which to trespass on other strong components so you find exactly one sec in effect you can peel that off and recurse on the rest of the graph and our slick way of implementing this recursion is to just do the single second uh DFS pass where you just treat the nodes in decreasing order of finishing times that in effect uh unve unveils all of the secc's in Reverse topological ordering so that's it karaji algorithm and the complete proof of correctness a blazingly fast graph primitive that in any directec the graph will tell you its strong components