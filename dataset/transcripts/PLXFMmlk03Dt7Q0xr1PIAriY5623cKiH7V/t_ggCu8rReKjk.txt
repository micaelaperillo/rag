So now that we understand why we can't have a single hash function which always does well on every single data set that is every hash function is subject to pathological data sets. We'll discuss the randomized solution of how you can have a family of hash functions and if you make a real-time decision about which hash function to use, you're guaranteed to do well on average no matter what the data is. So let me remind you of the three-prong plan that I have for this part of the material. So in this video we'll be covering the first two. So part one which we'll accomplish on the next slide will be to propose a mathematical definition of a good random hash function. So formally we're going to define a universal family of hash functions. Now what makes this definition useful? Well two things. And so part two will show that there are examples of simple and easy to compute hash functions that meet this definition that are universal in the sense described on the next slide. So that's important. And then the third part which we'll do in the next video will be the mathematical analysis of the performance of hashing specifically with chaining when you use universal hashing. And we'll show that if you pick a random function from a universal family then the expected performance of all of the operations are constant assuming of course that the number of buckets is comparable uh to the number of objects in the hash table which we saw earlier is a necessary condition for good performance. So let's go ahead and get started. Let's say what we mean by a good random hash function. So for this definition, we'll assume that the universe is fixed. So maybe it's IP addresses, maybe it's our friends names, maybe it's configurations of a chessboard, whatever, but there's some fixed universe U. And we'll also assume we've decided on the number of buckets N. Then we call the set H universal if and only if it meets the following condition. In English, the condition says that for each pair of distinct elements, the probability that they collide should be no larger than with the gold standard of perfectly uniform random hashing. So for all distinct keys from the universe, call them X and Y, what we want is that the probability if we choose a random hash function little h from the set script H, the probability that X and Y collide, and again just to be clear, what that means is that X and Y hash to exactly the same bucket under this hash function little h. This should be no more than 1 over n. And don't forget n is the number of buckets. And again to interpret this, you know, 1 / n, where does this come from? So we said earlier that an impractical but in some sense gold standard hash function would be to just independently for each key assign it a bucket uniformly at random with different keys being assigned independently. Remember the reason this is not a practical hash function is because you'd have to remember where everybody went and then that would basically require maintaining a list which would devolve to the list solution. So you don't want that. You want hash functions where you have to store almost nothing and where you can evaluate them in constant time. But if we throw out those requirements of small space and small time then random functions should spread stuff out pretty evenly, right? I mean that's what they're doing. You're throwing darts completely at random at these end buckets. So what would be the collision probability of two given keys? Say of Alice and of Bob. If you're doing everything independently and uniformly at random, well, you know, first Alice shows up and it goes to some totally random bucket, say bucket number 17. Now Bob shows up. So what's the probability that it collides with Alice? Well, there are these n buckets that Bob could go to. Each is equally likely. And there's a collision between Alice and Bob if and only if Bob goes to bucket 17. Since each bucket's equally likely, that's only a one and n probability. So really what this condition is saying is that for each pair of elements the collision probability should be as small as good as with the holy grail of perfectly random hashing. So this is a pretty subtle definition perhaps the most subtle one that we've seen this entire course. So, to help you just get some facility with this and to force you to think about it a little bit deeply, the next quiz, which is probably harder than a typical in-class quiz, asks you to compare this definition of universal hashing with another definition and ask you to figure out to what extent they're the same definition. So, the correct answer to this quiz question is the third one that there are hash function families h that satisfy the condition on this slide that are not universal. On the other hand, there are hash function families capital H which satisfy this property and are universal. So I'm going to give you an example of each. Uh I'd encourage you to think carefully about why this is an example and a non-example uh offline. So an easy example to show that sometimes the answer is yes, you have universal hash function families H which also satisfy this property on the slide would be to just take capital H to be the set of all functions from mapping the universe to the number of buckets. So that's an awful lot of functions that's a huge set but it's a set nonetheless. And by symmetry of having all of the functions it both satisfies the property on this slide. It is indeed true that exactly a one overn fraction of all functions uh map arbitrary key K to an arbitrary bucket I. And by the same reasoning, by the same symmetry properties, this is universal. So really, if you think about it, choosing a function at random from capital H is now just choosing a completely random function. So that's exactly what we've been calling perfect random hashing. And as we discussed in the last slide, that would indeed have a collision probability of exactly 1 / n uh for each pair of distinct keys. So this shows sometimes you can have both this property and the be universal. An example where you have the property in this slide, but you're not universal would be to take h to be a quite small family. A family of exactly n functions each of which is a constant function. So there's going to be one function which always maps everything to bucket zero. That's a totally stupid hash function. There's going to be another hash function which always maps everything to bucket number one. That's a different but also totally stupid hash function. And so on. And then the nth function will be the constant function that always maps everything to bucket n minus one. And if you think about it, this very silly set h does indeed satisfy this very reasonable looking property on this slide. Fix any key, fix any bucket. You know, say bucket number 31. What's the probability that you pick a hash function that maps this key to bucket number 31? Well, independent of what the key is, it's going to be the probability that you pick the constant hash function whose output is always 31. Since there's n different constant functions, there's a one and n probability. So, that's an example showing that in some sense this is not as useful a property as the property of universal hashing. So, this is really not what you want or this is not strong enough. Universal hashing, that's what you want for strong guarantees. So now that we've spent some time trying to assimilate probably the subtlest definition we've seen so far in this class, let me let you in on a little secret about the role of definitions in mathematics. So on the one hand, I think mathematical definitions often get short shrift, especially in, you know, the popular discussion of mathematical research. That said, you know, it's easy to come up with one reason why that's true, which is that any schmo can come up and write down a mathematical definition. Nobody's stopping you. So what you really need to do is you need to prove that a mathematical definition is useful. So how do you indicate usefulness of a definition? Well, you got to do two things. First of all, you have to show that the definition is satisfied by objects of interest. For us right now, objects of interest are hash functions we might imagine implementing. So they should be easy to store, easy to evaluate. So they better be such hash functions meeting that complicated universal hash function definition. The second thing is is something good better happen if you meet the definition. And in the context of hashing, what what good thing do we want to have happen? We want to have good performance. So those are the two things that I owe you in these lectures. First of all, a construction of practical hash functions that meet that definition. That's what we'll start on right now. Second of all, why meeting that definition is a sufficient condition for good hasht performance. That'll be the next video. So in this example, I'm going to focus on IP addresses. Although the hash function construction is general as I hope will be reasonably clear. And as many of you know an IP address is a 32-bit integer consisting of four different 8- bit parts. So let's just go ahead and think of an IP address as a four tuple the way you often see it. And since each of the four parts is eight bits, it's going to be a number between 0 and 255. And the hash function we're going to construct, it's really not going to be so different than the quick and dirty hash functions we talked about last video. Although in this case, we'll be able to prove that the hash function family is in fact universal. And we're again going to use the same compression function. We're going to take the modulus with respect to a prime number of buckets. The only difference is we're going to multiply these x i's by a random set of coefficients. We're going to take a random linear combination of x1, x2, x3, and x4. So let be a little more precise. So we're going to choose a number of buckets n. And as we've said over and over, the number of buckets should be chosen so that it's in the same ballpark of the number of objects you're storing. So you know, let's say that n should be roughly double the number of objects that you're storing as a initial rule of thumb. So for example, maybe we only want to maintain something in the ballpark of 500 IP addresses and we could choose n to be a prime like 997. So here's the construction. Remember, we want to produce not just one hash function, but the definition is about a universal family of hash functions. So, we need a whole set of hash functions that we're ultimately going to choose one member from at random. So, how do we construct a whole bunch of hash functions in a simple way? Here's how we do it. So, we define one hash function which I'm going to denote by H sub A. A here is a four tupil. the components of which I'm going to call a1, a2, a3, and a4. And all of the components of a are integers between zero and n minus one. So they're exactly in correspondence with the indices of the buckets. So if we have 997 buckets, then each of these ais is an integer between zero and 996. So it's clear that this defines, you know, a whole bunch of functions. So in fact for each of the uh four coefficients so that's four independent choices you have n options okay so uh each of the integers between 0 and n minus one for each of the four coefficients so that's fine I've given a name to n to the fourth different functions but what is any given function how do you actually evaluate one of these functions okay so remember what a hash function is supposed to do remember you know how a type checks it takes as input something from the universe in this case an IP address and outputs a bucket number and The way we evaluate the hash function h sub a and again remember a here is a four tupil and remember an IP address is also a four tupil. Okay. So each component of the IP address is between 0 and 255. Each component of a is between 0 and n minus one. So for example between 0 and 996. What we do is we just take the dotproduct or the inner product of the vector a and the vector x and then we take the modulus with respect to the number of buckets. So that is we take a1 * x1 + a2 * x2 plus a3 * x3 plus a4 * x4. Now of course remember the x's lie between 0 and 255. The ais lie between 0 and n minus one. So say 0 and 996 you know so you do these four multiplications and add them up might get a pretty big number. You might well overshoot the number of buckets n. So to get back in the range of the what the buckets are actually indexed at the end we take the modu modulus uh the number of buckets. So in the end we do output a number between 0 and n minus one as desired. So that's a set of a whole bunch of hash functions n to the fourth hash functions and each one meets the criteria of being a good hash function from an implementation perspective. Right? So remember we don't want to have to store much to evaluate a function. And for a given hash function in this family, all we got to remember are the coefficients A1, A2, A3, and A4. We just got to remember these four numbers. And then to evaluate a hash function on an IP address, we clearly do a constant amount of work. We just do these four multiplications, the three additions, and then taking the modulus by the number of buckets in. So it's constant time to evaluate, constant space to store. And what's cool is using just these very simple hash functions, which are constant time to evaluate and constant space to store. This is already enough to meet the definition of a universal family of hash functions. So this fulfills the first promise that I owed you after subjecting you to that definition of universal hashing. Remember the first promise was that there are simple or there are useful examples that meet the definition and then of course I still owe you why meaning this definition is useful. Why does it lead to good performance? But I want to conclude this video by actually proving this theorem to you arguing that this is in fact a universal family of hash functions. All right. So, this will be a mostly complete proof and certainly we'll have all of the conceptual ingredients of why the proof works. There'll be one spot where I'm a little handwavy because we need a little number theory and I don't want to have a big detour into number theory. And uh if you think about it, you shouldn't be surprised that basic number theory plays at least some role, right? Because I said we should choose the number of buckets to be prime. So that means at some point in the proof, you should expect us to use the assumption that n is prime. And pretty much always you're going to use that assumption to involve at least elementary number theory. Okay, but I'll be clear about where I'm being handwavy. So what do we have to prove? Let's just quickly review the definition of a universal hash function. So we have our set H that we that we know exactly what it is. What does it mean that it's universal? It means for each pair of distinct keys. So in our context, it's for each pair of IP addresses. The probability that a random hash function from our family script H causes a collision maps these two IP addresses to the same bucket should be no worse than with perfectly random hashing. So no worse than 1 / n where n is the number of buckets say like 997. So the definition we need to meet is a condition for every pair of distinct keys. So let's just start by fixing two distinct keys. So, I'm going to assume for this proof that these two IP addresses differ in their fourth component. That is, I'm going to assume that X4 is different than Y4. So, I hope it's intuitively clear that, you know, it shouldn't matter, you know, which which set of eight bits I'm looking at. So, they're different IP addresses. They differ somewhere. If I really wanted, I could have four cases that were totally identical depending on whether they differ in the first eight bits, the next eight bits, the next eight bits, or the last eight bits. I'm going to show you one case because the other three are the same. So let's just think of the last eight bits as being different. And now remember what the definition asked us to prove. It asks us to prove that the probability that these two IP addresses are going to collide is at most 1 overn. So we need an upper bound on the collision probability with respect to a random hash function from our set of nth hash functions. So I want to be clear on the quantifiers. We're thinking about two fixed IP addresses. So for example, the IP address for the New York Times website and the IP address for the CNN website. We're asking for these two fixed IP addresses, what fraction of our hash functions cause them to collide, right? We'll have some hash functions which map the New York Times and CNN IP addresses to the same bucket and we'll have other hash functions which do not map those two IP addresses to the same bucket. And we're trying to say that the overwhelming majority sends them to different buckets. Only a one overn fraction at most sends them to the same bucket. So we're asking about the probability for the choice of a random hash function from our set H that the function maps the two IP addresses to the same place. So the next step is just algebra. I'm just going to take this equation which indicates when the two IP addresses collide under a hash function. I'm going to expand the definition of our hash function. Remember it's just this inner product modulo the number of buckets n. And I'm going to rewrite this condition in a more convenient way. All right. So after the algebra and the dust has settled, we're left with this equation being equivalent to the two IP addresses colliding. So again, we're interested in the fraction of choices of A1, A2, A3, and A4 such that this condition holds. Right? Sometimes it'll hold for some choices of the AIS. Sometimes it won't hold for other choices. And we're wanting to show that it almost never holds. Okay? So it fails for all but a one overn fraction of the choices of the AIS. So next we're going to do something a little sneaky. This trick is sometimes called the principle of deferred decisions. And the idea is when you have a bunch of random coin flips, it's sometimes convenient to flip some but not all of them. So sometimes fixing parts of the randomness clarifies the role that the remaining randomness is going to play. That's what's going to happen here. So let's go ahead and flip the coins which tell us the random choice of A1, A2, and A3. So again, remember in the definition of a universal hash function, you analyze collision probability under a random choice of a hash function. What does it mean to choose a random hash function for us? It means a random choice of A1 and A2 and A3 and A4. So we're making four random choices. And what I'm saying is let's condition on the outcomes of the first three. Suppose we knew that A1 turns up 173, A2 shows up 122, and A3 shows up 723. But we don't know what A4 is. A4 is still equally likely to be any of 012 all the way up to n minus one. So remember that what we want to prove is that at most a 1 overn fraction of the choices of a1, a2, a3, and a4 cause this underlined equation to be true, cause a collision. So what we're going to show is that for each fixed choice of A1, A2, and A3, at most a 1 overn fraction of the choices of A4 cause this equation to hold. And if we can show that for every single choice of A1, A2, and A3, no matter how those random coin flips come out, at most a 1 overn fraction of the remaining outcomes satisfy the equation, then we're done. It means at most a 1 over n fraction of the overall outcomes can cause the equation to be true. So if you haven't seen the principle of deferred decisions before, you might want to think about this a little bit offline, but it's easily justified by just say two lines of algebra. Okay, so we're done with this setup and we're ready for the meat of the argument. So what we've done is we've identified an equation which is now in green which occurs if and only if we have a collision between the two IP addresses. And the question we need to ask is for our fixed choices of A1, A2, and A3, how frequently will the choice of A4 cause this equation to be satisfied? Cause a collision. Now, here's why we did this trick of the principle of deferred decisions by fixing A1, A2, and A3, the right hand side of this equation is now just some fixed number between 0 and n minus one. So maybe this is 773, right? The x i's were fixed up front. The y eyes were fixed up front. We fixed a1, a2, and a3 at the beginning at the end of the last slide. And those are the only ones involved on the right hand side. So this is 773. And over on the left hand side, x4 is fixed, y4 is fixed, but a4 is still random. This is an integer equally likely to be any value between zero and n minus one. Now, here's the key claim, which is that the left hand side of this green equation is equally likely to be any number between 0 and n minus one. And I'll tell you the reasons why this key claim is true, although this is the point where we need a little bit of number theory, so I'll be kind of hand wavy about it. So, there's three things we have going for us. The first is that X4 and Y4 are different. Remember, our assumption at the beginning of the proof was that, you know, the IP addresses differ somewhere. So why not just assume that they differ in the last eight bits of the proof. Again, this is not important. If you really wanted to be pedantic, you could have three other cases depending on the other possible bits in which the IP addresses might differ. But anyway, so because X4 and Y4 are different, what that means is that X4 minus Y4 is not zero. And in fact, now that I write this, it's jogging my memory of something that I should have told you earlier and forgot, which is that the number of buckets n should be at least as large as the maximum coefficient value. So for example, we definitely want the number of buckets n in this equation to be bigger than x4 and bigger than y4. And the reason is otherwise you could have x4 and y4 being different from each other, but they the difference still winds up being 0 mod n. Right? So for example, suppose n was 4 and x4 was 6 and y4 was 10. Then x4 - 10 would be minus4 and that's actually zero modulo 4. So that's definitely not what you want. You want to make sure that if x4 and y4 are different then their difference is non zero modulo n. And the way you ensure that is you just make sure n is bigger than each. So you should choose the number of buckets bigger than the maximum value of a coefficient. So in our IP address example, remember the coefficients don't get bigger than 255. And I was suggesting a number of buckets equal to say 997. Now in general this is is never a big deal in practice. If you only wanted to use say a 100 buckets, you didn't want to use a thousand, you wanted 100, well then you could just use smaller coefficients, right? you could just break up the IP address. Instead of into eight bit chunks, you could break it into six bit chunks or four bit chunks and that would keep the coefficient size smaller than the number of buckets. Okay, so really you choose the buckets first and then you choose how many bits to chunk your data into and that's how you make sure this is satisfied. So back to the three things we have going for us and trying to prove this key claim. So x4 and y4 are different. So their difference is non zero modulo n. So second of all, n is prime. That was part of the definition, part of the construction. And then third, a4, this final coefficient is equally likely to take on any value between 0 and n minus one. So just as a plausibility argument, let me give you a proof by example. Again, I don't want to detour into elementary number theory, although it's beautiful stuff. So, you know, I encourage those of you who are interested to go learn some uh and figure out exactly how you prove it. You really only need the barest elementary number theory uh to give a formal proof of this. But just to show you that this is true in some simple examples. So let's think about a very small prime. Let's just say there's seven buckets and uh let's suppose that the difference between x4 and y4 is two. Okay, so having chosen the parameters, I've set n equal 7. I've set the difference equal to two. What I want to do is I want to step through the seven possible choices of a4 and look at what we get in this blue circled quantity in the left hand side of the green equation. So we want to say the left hand side's equally likely to be any of the seven numbers between 0 and six. So that means that as we try our seven different choices for A4, we better get the seven different possible numbers as output. So for example, if we set A4 equal to zero, then the blue circle quantity is certainly itself zero. If we set it equal to 1, then it's 1 * 2, so we get two. For two, two, we get 2 * 2, which is four. For three, we get 3 * 2, which is 6. Now, when we set a4 equal to 4, we get 4 * 2, which is 8. modulo 7 is 1. 5 * 2 mod 7 is 3. 6 * 2 mod 7 is 5. So as we cycle through a4 0 through 6, we get the values 02 46135. So indeed we cycle through the seven possible outcomes one by one. So if a4 is chosen uniformly at random, then indeed this blue circled quantity will also be uniform at random. So just to give another quick example, we could also keep n= 7 and think about the difference of x4 and y4. Again, we have no idea what it is other than that it's non zero. So you know maybe instead of three, maybe maybe instead of two, it's three. So now again, let's step through the seven choices of A4 and see what we get. So now we're going to get zero, then three, then six, then two, then five, and then one, and then four. So again stepping through the seven choices of A4 we get all of the seven different possibilities of this left hand side and it's not an accident of these choices of parameters. As long as n is prime x4 and y4 are different and y4 ranges over all possibilities so will the value on the left hand side. So by choosing a4 uniformly at random indeed the left hand side is equally likely to be any of its possible values 012 up to n minus one. And so what does that mean? Well, basically that means we're done with our proof because remember the right hand side that's circled in pink is fixed. We fixed A1, A2, and A3. The X's and Y's have been fixed all along. So, this is just some number like 773. And so, we know that there's exactly one choice of A4 that will cause the left hand side to also be equal to 773. Now, A4 has n different possible values and it's equally likely to take one on. So there's only a one in chance that we're going to get the unlucky choice of A4 that causes the left hand side to be equal to 773. And of course there's nothing special about 773. Doesn't matter how the right hand side comes out. We have an only one in chance of being unlucky and having a collision. And that is exactly the condition we are trying to prove. And that establishes the universality of this function h of N to the 4th. Very simple, very easy to evaluate hash functions.