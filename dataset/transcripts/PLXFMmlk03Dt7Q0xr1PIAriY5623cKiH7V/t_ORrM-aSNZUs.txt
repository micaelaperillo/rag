in this video we'll apply the divide and conquer algorithm design Paradigm to the problem of multiplying matrices this will culminate in the study of stren's matrix multiplication algorithm and this is a super cool algorithm for two reasons first of all stren's algorithm is completely non-trivial it's totally non- obvious very clever not at all clear how stren ever came up with it the second cool feature is it's for such a fundamental problem so computers as long as they've been in use from the time they're invented up till today a lot of their Cycles is spent multiplying matrices just comes up all the time in important applications so let me first just uh make sure we're all clear on what the what the problem is of multiplying two matrices so we're going to be interested in three matrices X Y and Z they're all going to I'm going to assume they all have the same dimensions uh n byn the ideas we'll talk about are also relevant for multiplying non-square matrices but we're not going to discuss it in this video the entries in these matrices you know you could think of it as whatever you want maybe they're integers maybe they're rationals maybe they're from some finite field it depends on the application but the point is they're just entries that we can add and multiply so how is it that you take two n byn matrices X and Y and multiply them producing a new n byn Matrix Z well recall that the IJ entry of Z that means the entry in the I row and the jth column is simply the dotproduct of the I row of X with the jth column of Y so if IJ was this red square this red entry over in the Z Matrix that would be derived from the corresponding row of the X Matrix and the corresponding column of the Y Matrix and recall what I mean by dotproduct that just means you take the products of the individual components and then add up the results so ultimately the Z entry boils down to a sum Over N Things where each of the constituent products is just the x i entry uh the ik entry of the Matrix x with the KJ entry uh of the Matrix y or your K is ranging from 1 to n so that's how Z J is defined for a given uh pair of indices I and J one thing to note is that where we often use n to denote the input size here we're using n to denote the dimension of each of these matrices the input size is not n the input size is quite a bit bigger than n uh specifically each of these are n byn matrices it contains n s and R so since presumably we have to read the input which has size n squ and we have to produce the output which also has size n squ the best we could really hope for for a matrix multiplication algorithm would be a running time of n^2 so the question is how close can we get to it before we talk about algorithms for matrix multiplication let me just make sure we're all crystal clear on exactly what the problem is so let's just actually spell out what would be the result of multiplying two different 2x two matrices so we we can parameterize two generic 2x two matrices by just giving the first one entries a b c and d where these four entries could all be anything and then we're multiplying by a second 2x two Matrix let's call this entries EF G and H now what's the result of multiplying these where again it's going to be a 2X two Matrix where each entry is just the corresponding dotproduct of the relevant row of the first Matrix and column of the second Matrix so to get the upper left entry we take the dot product of the upper row of the first Matrix and the First Column of uh the left column of the second Matrix so that results in ae+ BG to get the upper right entry we take the dotproduct of the top row of the left Matrix with the right column of the second Matrix so that gives us AF f+ BH and then filling in the other entries in the same way we get C plus plus DG and cf+ DH okay so that's the multiplying two matrices and we've already discussed the definition in general now suppose you had to write a program to actually compute the result of multiplying two n byn matrices one natural way to do that would just be to return to the definition and which defines each of the N squ entries in the Z Matrix as a suitable sum of products of entries entries of the X and Y matrices so this the next Quiz I'd like you to uh figure out exactly what would be the running time of that algorithm as a function of the Matrix Dimension n where as usual we count the addition or multiplication of two individual entries as a constant time operation so the correct response to this quiz is the third answer that the running time of the straightforward inter of algorithm runs in cubic time relative to The Matrix Dimension n to see this just recall with the definition of the matrix multiplication was the definition tells us that each entry Z J of the output Matrix Z is defined as the sum from K = 1 to n of x i k * y KJ that is the dotproduct of the I row of the X Matrix and the jth column of the Y Matrix I'm certainly assuming that we have the matrices represented in a way that we can access a given entry in constant time and under that assumption remember each of these each of these products only takes constant time and so then to compute Z J we just have to add up these n products so that's going to be uh Theta of n time to compute a given Z J and then there's an N squared entries that we have to compute there's n choices for i n choices for J so that gives us uh uh n^2 * n or cubic running time overall for the natural algorithm which is really just a triple for Loop uh which computes each entry of the output array separately using the dot product so the question question as always for the Keen algorithm designer is can we do better can we beat uh nqb time from multiplying two matrices and we might be especially emboldened with the progress that we've already seen in terms of multiplying two integers we apply the divide and conquer algorithm uh to the problem of multiplying two ndit integers and we had both a naive recursive algorithm and a seemingly better algorithm due to gaus which made only three recursive calls now we haven't yet analyze the running time of that algorithm but as we'll see later that does indeed uh beat the quadratic running time of the grade school algorithm so it's very natural to ask can we do exactly the same thing here there's the obvious n cubed algorithm which follows straight from the definition perhaps analogous to gaus we can have some clever dividing conquer method which beats cubic time so that's what we're going to explore next let's recall the divide and conquer Paradigm what does it mean to use it well we first have to identify smaller sub problems so if we want to multiply two n byn matrices we have have to identify multiplications of smaller matrices that we can solve recursively once we figured out how we want to divide the given problem into smaller ones then in the conquer step we simply invoke our own algorithm recursively that's going to uh recursively multiply the smaller matrices together and then you know in general we'll have to combine the results of the recursive calls to get the solution for the original problem in our case to get the product of the original two matrices from the product of whatever submatrices we identify so how would we apply the divide and conquered Paradigm to matrices so we're given two n byn matrices and we have to somehow identify smaller pairs of square matrices that we can multiply recursively so the idea I think is fairly natural so we start with a big and by n Matrix X right so there's n rows and N columns we have to somehow divide it into smaller pieces now the first thing you might think about is you put it into its left half and its right half analogous to what we've been doing with arrays but then we're going to break X into two matrices which are no longer Square which are n /2 in one dimension and have length n in the other dimension and we want to recursively call a sub routine that multiplies Square matrices so what seems like the clear thing to do is to divide X into quadrants okay so we have four pieces of X each is going to be n /2 by n /2 corresponding to the different quarters of this Matrix so let's call these different quadrants or blocks in Matrix terminology a b c and d all of these are n /2 by n /2 matrices as usual for Simplicity I'm assuming that n is even and as usual it doesn't really matter and we can do the same trick with Y so we'll divide Y into quadrants n over 2 by n/ two matrices which we'll call E F G and H so one thing that's cool about matrices is when you split them into blocks and you multiply them the blocks just behave as if they were Atomic elements so what I mean by that is that the product of X and Y can be expressed in terms of its quadrants and each of its four quadrants each of its Four Corners uh can be written as a suitable arithmetic expression of the quadrants of X and Y so here's exactly what those formulas are they're exactly analogous to when we just multiplied a pair of 2x2 matrices so I'm not going to formally prove this fact I'm sure many of you uh have I've seen it before familiar with it and if you haven't it's actually quite easy to prove it's not obvious you can't see it off the top of your head necessarily but if you go back to the definition it's quite easy to verify that indeed when you multiply X and Y you can express its quadrants its blocks in terms of the blocks of X and Y so what we just did is completely analogous to when talking about integer multiplication and we wanted to multiply two integers Little X and little Y and we broke them into pairs of n/2 digits and then we just took the expansion and we observed how that expansion could be written in terms of uh products of n over two-digit numbers same thing going on here except with matrices so now we're in business as far as a recursive approach we want to multiply X and Y they're n byn matrices we recognize we can express that product X X Y in terms of the products of n /2 by n /2 matrices things were're able to multiply recursively plus additions and additions is clearly easy to multiply uh two different matrices with say n 2qu entries each is going to be linear in the number of entries so it's going to be uh n squ time to add two matrices that are n byn so this immediately leads us to our first recursive algorithm to describe it let me quickly rewrite that expression we just saw on the previous slide and now our first recursive algorithm is simply to evaluate all of these expressions in the obvious way so specifically in step one we recursively compute all of the necessary products and observe that there are eight products that we have to compute eight products of n /2 by n /2 matrices there are four entries in this expansion of X X Y each of the each of the blocks is the sum of two products and none of the products reoccur they're all distinct so naively if you want to evaluate this we have to do eight different products of n /2 by n/2 matrices once those recursive calls complete then all we do is do the uh necessary four additions as we discussed that takes time proportional to the number of entries in The Matrix so this is going to take quadratic time overall quadratic and n linear in the number of entries now the question you should be asking is is this a good algorithm was this good for anything this recursive approach splitting X and Y into these blocks expanding the product in terms of these blocks then recursively Computing each of the blocks and I want to say it's totally not obvious it is not clear what the running time of this recursive algorithm is uh I'm going to go ahead and uh give you a spoiler which is going to follow from the master method that we'll talk about in the next lecture but it turns out with this kind of recur recursive algorithm where you do eight recursive calls each on a problem with Dimension half as much as what you started with and then do quadratic time outside the running time is going to be cubic so exactly the same as with the straightforward iterative algorithm that follows from the definition that was cubic it turns out and that was clearly cubic this one although it's not obvious is cubic as well so no better no worse than the straightforward iterative algorithm so in case you're feeling disappointed that we went through all this work and this sort of seemingly clever divide and conquer approach for matrix multiplication and and came out at the end no better than the interative algorithm well there's really no reason to despair because remember back in integer multiplication we had a straightforward recursive algorithm where we had to do four recursive calls products of n two-digit numbers but then we had gauss's trick which said oh if we only did more clever products and more clever additions and subtractions then we can get away with only three recursive calls and we'll see later that that is indeed a significant saving in the time needed for energer multiplication and we've done nothing analogously clever to gauss's trick for this matrix multiplication problem all we did is the naive expansion in terms of submatrices and the naive evaluation of the resulting Expressions so The $64,000 Question is then can we do something clever to reduce the number of recursive calls from eight down to something lower and that is where stren's algorithm comes in so at a high level St algorithm has two steps just like the first recursive algorithm that we discussed it recursively computes some products of smaller matrices n /2 roughly n/2 by n/2 matrices but there's only going to be seven of them they will be much less straightforward they will be much more cleverly chosen than in the first recursive algorithm and step two then is to actually produce the product of X and Y produce each of those four blocks that we saw with suitable uh additions and subtractions of these seven products and again these are much less straight forward than in the first recursive algorithm and so while the additions and subtractions involved will be a little bit more numerous uh than they were in the naive recursive algorithm it's only going to change the work in that part of the algorithm by a constant Factor so we'll still spend only Theta of n^2 work adding and subtracting things and we get a huge win in decreasing the number of recursive calls from 8 to 7: now just in case you have the intuition that shaving off one of eight recursive calls should only decrease the running time of an algorithm by 1/8 by 12.5% in fact it has a tremendously more Amplified effect because we do one less recursive call over and over and over again as we keep recursing in the algorithm so it makes a fundamental difference in the eventual running time of the algorithm as we'll explore in detail in the next set of lectures when we discuss the master method so again a bit of a spoiler alert what you're going to see in the next set of lectures that indeed stren's algorithm does beat cubic time it's better than n Cub time you'll have to watch the next set of lectures if you want to know just what the running time is but I'm going to tell you now that Savings of the eth recursive call is what changes the algorithm from cubic to subcubic now 1969 was obviously quite a bit before my time but by all accounts from people I've talked to who are around then and from you know what the books say sasson's algorithm totally blew people's minds at the time everybody was assuming that there's no way you could do better than the iterative algorithm the cubic algorithm it just seemed that matrix multiplication intuitively fundamentally required all of the calculations that are spelled out in the definition so stren's algorithm is an early glimpse of the magic and of the power of clever algorithm design that if you really have a serious Ingenuity even for super fundamental problems you can come up with fundamental savings over the more straightforward solution so those are the main points I wanted to talk about sten's algorithm how you can beat cubic Time by saving a recursive call with suitably chosen clever products and clever additions attractions maybe a few of you are wondering you know what are these cleverly chosen products can you really do this and I don't blame you there's no reason to believe me just because I sort of spelled out this high level idea it's not obvious this should work you might want to actually see the products so for those of you like that this last slide is for you so here is stren's algorithm in its somewhat gory detail so let me tell you what the seven products are that we're going to form I'm going to label them P1 through p7 and they're all going to be defined in terms of the blocks of the input matricies X and Y so let me just remind you that we think of X in terms of its blocks A B C D and we think of Y in terms of its blocks e fgh and remember a through H are all n /2 by n /2 submatrices so here are the seven products P1 through p7 P1 is a * quantity Fus H P2 is quantity A + B * H P3 is C + D * e P4 is D * G minus E P5 is quantity a + d e * quantity e + H P6 is quantity B minus D * quantity G + H and finally p7 is quantity a minus c e+ f so I hope you'll agree that these are indeed only seven products and we could compute these with seven recursive calls so we pre-process with a little bit of additions and subtractions we have to compute f H A plus b c plus d and so on WE compute all of these new metrices from the blocks and we can then recursively with seven recursive calls do these seven products that operate on N /2 by n /2 matrices now the question is why is this useful why on Earth would we want to know the values of these seven products so the amazing other part of the algorithm is that from just these seven products we can using only addition and subtraction recover all four of the blocks of a x x y so x * y you'll recall we expanded in terms of the blocks so we previously computed this to be AE plus BG in the upper left corner and then similarly expressions for the upper right lower left and lower right blocks so this we already know so the content of the claim is that these four blocks also arise from the seven products in the following way so the claim here is that these two different expressions for x * y are exactly the same and they're the same block by block so in other words what the claim is that this crazy expression P5 + P4 minus P2 plus P6 where those are four of the products that we've listed above that is precisely ae+ BG similarly we're we're claiming that P1 plus P2 is exactly AF Plus BH that's actually easy to see P3 + P4 is C plus DG that's also easy to see and and then the other complicated one is that P1 + P5 - P3 - p7 is exactly the same as CF + DH so all four of those hold so let me just so you believe me because I don't know why you would believe me unless I actually showed you some of this derivation let's just look at the proof of one of the cases of the upper left corner so that is let's just expand out this crazy expression P5 + P4 - P2 + P6 what do we get well from P5 we get AE plus ah plus d plus DH and we add P4 so that's going to give us a plus DG minus D then we subtract P2 so that gives us a minus ah minus BH and then we add in P6 so that gives us a BG plus BH minus DG minus DH okay so what happens next well now we look for cancellations so we cancel the ah's we cancel the de we cancel the dh's we cancel the dgs we cancel the bh's and holy cow what do we get we get a e plus BG that is we get exactly what we were supposed to in the upper left block of x * y so we just actually verify that this equation holds for the upper left Block it's quite easy to see that it holds for the upper right and lower left blocks and comparable calculation verifies it for the lower right uh blocks of the two so summarizing because this claim holds because actually we can recover the four blocks of x * y from these seven products sten's algorithm works in the following way you compute the seven products P1 through p7 using seven recursive calls then you just compute the four blocks using some extra additions and subtractions as shown in the claim so seven recursive calls on N by n /2 by n/2 matrices plus N squared work to do the necessary additions and as you'll see in the master method lecture that is actually sufficient for subcubic ton now I sympathize with you if you have the following question which is how on Earth did Sten come up with this and indeed this sort of illustrates uh the difference between checking somebody's proof and coming up with a proof given that I told you the magical seven products and how you from them can recover the four desired blocks of x * y it's really just mechanical to see that it works it's a totally different story of how do you come up with P1 through p7 in the first place so how did stren come up with them honestly your guess is as good as mine