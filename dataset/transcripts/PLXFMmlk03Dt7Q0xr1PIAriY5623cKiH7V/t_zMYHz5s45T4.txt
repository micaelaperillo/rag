this video is the second of three that describes the proof of the master method in the first of these three videos we mimicked the analysis of merge sort we used a recursion tree approach which gave us an upper bound on the running time of an algorithm which is governed by a recurrence of the specified form unfortunately that video left us with a bit of an alphabet soup this complicated expression and so in the second video we're not going to do any computations we're just going to look at that expression attach some semantics to it and look at how that interpretation naturally leads to three cases and also give it intuition for some of the running times that we see in the master method so recall from the previous video that the way we've bounded the work done by the algorithm is resumed in our particular level J of the recursion tree the computation which was the number of subproblems at that level a to the J times the work done per sub-problem that was the constant C times quantity and over B to the J raised to the D and that gave us this expression C n to the D times the ratio of a over B to the D raised to the J at a given level J the expression star that we concluded the previous video with was just some of these expressions over all of the logarithmic number of levels yet now as messy as this expression might seem perhaps are on the right track in the following sense the master method has three different cases and which case you're in is governed by how a compares to B to the D and here in this expression we are seeing precisely that ratio a divided by B to the D so let's drill down and understand why this ratio is fundamental to the performance of a divide and conquer recursive algorithm so really what's going on in the master method is a tug-of-war between two opposing forces one which is forces of good and one which is forces of evil and those correspond to the quantities B to the D and a respectively so let me be more precise let's start with the parameter a so a you'll recall is defined as the number of recursive calls made by the algorithm so it's the number of children that a node in the recursion tree has so fundamentally what a is it's the rate at which subproblems proliferate as you pass deeper in the recursion tree it's the factor by which there are more subproblems at the next level than the previous one so let's think of a in this way as the rate of sub-problem proliferation or RSP and when I say rates I mean as a function of the recursion the level J so these are the forces of evil this is why our algorithm might run slowly is because as we go down the tree there are more and more subproblems and that's a little scary the forces of good what we have going for us is that when each recursion level J we do less work for sub problem and then you extent to which we do less work is precisely B to the D so I'll abbreviate this rate of work shrinkage or this quantity B to the D by RWS now perhaps you're wondering why is it B the d why is it not be so remember what B denotes that's the factor by which the input size shrinks with the recursion level J so for example if B equals 2 then each sub-problem at the next level is only half as big as that at the previous level but we don't really care about the input size of a sub-problem except in as much as it determines the amount of work that we do solving that sub-problem so that's where this parameter D comes into play think for example about the cases where you have a linear amount of work outside the recursive calls versus a quadratic amount of work that is consider the cases where D equals 1 or 2 if B equals 2 and D equals 1 that is if you recurse on half the input and do linear work then not only is the input size dropping by a factor 2 but so is the amount of work that you do per sub-problem and that's exactly the situation we had in merge sort where we had linear work outside the recursive calls but think about D equals 2 suppose you did quadratic work per sub-problem as a function of the input size then again if B equals 2 if you cut the input in half the recursive calls only going to do 25% as much work as what you did at the current level the input size goes down by a factor 2 and that gets squared because you do quadratic work as a function of the input size so that would be B to the D 2 raised to the 2 or 4 so in general the input size goes down by a factor B but what we really care about how much less work we do percept goes down by B to the D that's why B to the D is the fundamental quantity the Qantas governs the forces of good the extent to which we work less hard with each recursion level J so the question then is just what happens in this tug-of-war between these two opposing forces so fundamentally what the three cases of the master method correspond to is the three possible outcomes in this tug-of-war between the forces of good mainly the rate of work shrinkage and the forces of evil namely the rate of sub-problem proliferation there are three cases one for the case of a tie one for the case in which the forces of evil win that is in which a is bigger than B to the D and a case in which good wins that is B to the D is bigger than a to understand this a little bit better what I want you to think about is the following think about the recursion tree that we drew in the previous slide and as a function of a versus B to the D think about the amount of work done per level when is that going up per level when is it going down per level and when is it exactly the same in each level so the answer is all of these statements are true except for the third one so let's take them one at a time so first of all let's consider the first one suppose that the rate of sub-problem proliferation a is strictly less than the rate of work shrinkage B to the D this is where the forces of good the rate at which we're doing less work per sub problem is out is outpacing the rate at which start problems are proliferating and so the number of subproblems goes up but the savings per sub-problem goes up by even more so in this case it means we're gonna be doing less work with each recursion tree level the forces of good outweigh the forces of evil the second one is true for exactly the same reason if subproblems are proliferating so rapidly that it outpaces the savings that we get per sub-problem then we're going to see an increasing amount of work as we go down the recursion tree it will increase with the level J given that these two are true the third one is false we can draw conclusions depending on whether the rate of sub-problem proliferation is strictly bigger strictly less than the rate of work shrinkage and finally the fourth statement is also true this is the perfect equilibrium between the forces good in the forces of evil sub problems are proliferating but our savings per sub problem is increasing at exactly the same rate the two forces will then cancel out and we'll get exactly the same amount of work done at each level of the recursion tree this is precisely what happened when they analyzed the merge sort algorithm so let's summarize and conclude with the interpretation and even understand how this interpretation lends us to forecast some of the running time bounds that we see in the master method were summarizing the three cases of the master method correspond to the three possible outcomes in the battle between subproblems proliferating and the work per sub problem of shrinking one for a tie one for one sub problem is filtrating faster and look for when the work shrinkage is happening faster in the case where the rates are exactly the same and they cancel out then the amount of work should be the same at every level of the recursion tree and in this case we can easily predict what the running time should work out to be and particularly we know there's a logarithmic number of levels the amount of work is the same at every level and we certainly know how much work is getting done at the root right because that's just the original recurrence which tells us that there's asymptotically n to the D work done at the root so with anybody work for each of the log levels we expect a running time of n to the D times log n as we just discussed when the rate of work done per sub-problem is shrinking even faster than subproblems proliferate then we do less and less work with each level of the recursion tree so in particular the biggest amount of work the worst level is at the root level now the simplest possible thing that might be true would be that actually the root level just dominates the overall running time of the algorithm and the other levels really don't matter up to a constant factor so it's not obvious that's true but if we keep our fingers crossed and hope for the simplest possible outcome with the root has the most work we might expect a running time that's just proportional to the world work running time at the root as we just discussed we already know that that's into the B because that's just the outermost call to the algorithm by the same reasoning when this inequality is flipped in subproblems proliferate so rapidly that it's outpacing the savings we get per subproblem the amount of work is increasing the recursion level and here the worst case is going to be at the leaves that's where the that level is going to have the most compared to any other level and again if you keep your fingers crossed and hope that the simplest possible outcome is actually true perhaps the leaves just dominate and up to a constant factor they govern the running time of the algorithm in this third case given that we do a constant amount of work for each of the leaves since those correspond to base cases here we'd expect a running time in the simplest scenario proportional to the number of leaves in the recursion tree so let's summarize what we've learned in this video we now understand that fundamentally there are three different types of recursion trees those in which the work done per level is the same at every level those in which the work is decreasing with the level in which case the root is the worst level and those in which the amount of work is increasing in the level where the leaves of the worst level furthermore it's exactly the ratio between a the rate of sub-problem proliferation and B to the D the rate of work shrinkage per sub-problem the governs which of the through these three recursion trees were dealing with furthermore intuitively we've now have predictions about what kind of running time we expect to see in each of the three cases there ended the deal again that were pretty confident about there's a hope that in the second case where the root is the worst level that may be the running time is n to the D and there's a hope in the third case where the leaves are the worst level and we do constant time per leaf per base case that it's going to be proportional to the number of leaves let's now sanity check this intuition against the formal statement of the master method which will prove more formally in the next video so in the three cases we see they match up at least to at a three with exactly what our intuition lies so in the first case we see the expected end of the D times login and the second case where the root is the worst level indeed the simplest possible outcome of Big O of n to the D is the assertion now the third case that remains a mystery to be explained our intuition said this should hopefully be proportional to the number of leaves and instead we've got this funny formula Big O of end of the log base B of A so in the next video we'll demystify that connection as well as supply a formal proof for these assertions