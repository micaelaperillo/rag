okay so let's move on and actually discuss the pseudo code for the merge sword algorithm first let me just tell you the pseudo code leaving aside exactly how the merging sub routine is implemented and this high level should be very simple and clear at this point so there's going to be two recursive calls and then there's going to be a merging step now I owe you a few comments because I'm being a little sloppy again as I promised this isn't something you would directly translate into code although it's pretty close but so what are a couple of the ways that I'm being sloppy well first of all there's you know in any recursive them you got to have some base cases you got to have this idea that when the input's sufficiently small you don't do any recursion you just return some trivial answer so in the Sorting problem the base case would be if you're handed an array that has either zero one elements well it's already sorted there's nothing to do so you just return it without any recursion okay so to be clear I haven't written down the base cases although of course you would if you were actually implementing merge sort so let me make a note of that couple other things I'm ignoring I'm ignoring what what uh what to do if the array has odd length so if it has say nine elements obviously you have to somehow break that into five and four or four and five so you would do that just in either way and that would be fine uh and then secondly I'm ignoring the details of what it really means to sort of recursively sort so for example I'm not discussing exactly how you would pass these subarz onto the recursive calls that's something that really would depend somewhat on the programming language so that's exactly what I want to avoid I really want to talk about the concepts which transcend any particular programming language uh implementation so that's why I'm going to describe uh algorithms at at this level okay all right so the hard part relatively speaking then is how do you implement the merge step the recursive calls have done their work we have these two sorted subarrays of uh half the numbers the left half and the right half how do we combine them into one and in English I already told you on the last Slide the idea is you just populate the output array in sorted order by traversing pointers or just traversing through the two uh sorted subarrays in parallel so let's look at that in some more detail okay so here is the pseudo code for the merge step so let me Begin by introducing some names for the characters in what we're about to discuss uh so let's use C to denote the output array so this is what we're supposed to spit out with the numbers in sorted order and then I'm going to use a and b to denote the results of the two recursive calls okay so the first recursive call has given us array a which contains the left half of the input array in sorted order similarly B contains the right half of the input array again in sorted order so as I said we're going to need to Traverse the two uh sorted subarrays and and B in parallel so I'm going to introduce a counter I to Traverse through a j to Traverse through b i and J will both be initialize to one to be at the beginning of their respective arrays and now we're going to do we're going to do a single pass through the output array Poppy and get in increasing order always taking the smallest from the union of the two sorted subarrays and if you if there's one idea in this merge step it's just the realization that the minimum element that you haven't yet looked at in A and B has to be at the front of one of the two lists right so for example at the very beginning of the algorithm where is the minimum element overall well whichever of the two arrays it lands in A or B it has to be the smallest one there okay so the smallest element overall is either the smallest element in a or it's the smallest element in B so you just check both places the smaller one's the smallest you copy it over and you repeat and that's it so the purpose of K is just to Traverse the output array from left to right that's the order we're going to populate it currently looking at position I in the first array and position J in the second array so that's how far we've gotten How Deeply we've probed into both of those two arrays we look at which one has the current smallest and we copy the smallest one over okay so if the if uh the entry in the I position of a is smaller we copy that one over of course we have to increment I we probe one deeper into the list a and symmetrically for the case where the current position in B has the smaller element now again I'm being a little bit sloppy so that we can focus on the forest and not sort of uh and not get bogged down with the trees I'm ignoring some end cases so if if you really wanted to implement this you'd have to add a little bit uh to keep track of when you fall off uh e either A or B okay so you'd have additional checks for when I or J reaches the end of the array at which point You' copy over all the remaining elements into C all right so I'm going to give you a cleaned up version of uh that pseudo code so that you don't have to tolerate my questionable handwriting any longer than it's absolutely necessary this again is just the same thing that we wrote on the last slide okay the pseudo code for the merge step now so that's the merge sord algorithm now let's get to the part of this lecture which is okay so merge T produces a uh sorted array what makes it if anything better than much simpler non- divide and conquer algorithms like say insertion short in other words what is the running time of the merge sort algorithm now I'm not going to give you a completely precise def definition of what I mean by running time and there's good reason for that as we'll discuss shortly but intuitively you should think of the running time of an algorithm you should imagine that you're just running the algorithm in a debugger and and every time you press enter you advance one line in the program through the debugger and then basically the running time is just the number of operations executed the number of lines of codes executed so the question is how many times do you have to hit enter on your debugger before the uh program finally terminates so we're interested in how many such uh lines of lines of code get executed for merge sort when the input array has end numbers okay so that's a fairly complicated question so let's start with a more modest schal rather than thinking about the number of operations executed by merge sort which is this crazy recursive algorithm which is calling itself over and over and over again let's just think about how many operations are going to get executed when we do a single merge of two sorted subarrays that seems like it should be an easier place to start so let me remind you the pseudo code of the merge sub routine here it is so let's just go and count up how many operations they're going are going to get used so there's the initialization step uh so let's say that I'm going to charge us um one operation for each of these two initializations so let's call this two operations to set IAL 1 and jal1 then we have this for Loop so clearly the for Loop executes a total number of n times so in each of these n iterations of this for Loop how many instructions get executed well we have one here we have a comparison so we compare AI to BJ and either way the comparison comes up we then do two more operations we do an assignment here or here and then we do an increment of the relevant variable either here or here so that's going to be three operations per iteration and then maybe I'll also say that uh in order to increment K uh we're going to call it a fourth iteration okay so for each of these n iterations of the for loop we're going to do four operations all right so putting it all together what do we have is the running time for merge so let's say the upshot so the upshot is that the running time of the merge sub routine given an array of M numbers is at most 4 m + 2 so a couple of comments first of all I've changed a letter on you so don't get confused in the previous slide uh we were thinking about an input size of uh n here I've just made it I've changed the name of the variable to M that's going to be convenient once we think about merge sort which is recursing on smaller sub problems but it's exactly the same thing MN whatever so an array of M entries that does at most 4 M plus to lines of code the second thing is there's some ambiguity in exactly how we counted lines of code on the previous slot so maybe you might argue that um you know really each Loop iteration should count as two operations not just one because you don't just have to increment K but you also have to compare it to the uh upper bound of n yeah maybe would have been 5 n plus 2 instead of 4 n plus 2 so it turns out these small differences in how you count up uh the number of lines of code executed are not going to matter and we'll see why shortly so amongst friends let's just agree let's call it 4 M plus2 operations for merge uh to execute an array of exactly M entries so let me abuse our friendship now a little bit further with an inequality which is true but extremely sloppy but I promise it'll make our lives just easier in some future calculations so rather than 4M plus 2 this two is sort of getting on my nerves let's just call this at most 6m because m is at least one okay you have to admit it's true 6m always is at least 4M plus 2 it's very sloppy these numbers are not anything Clos to each other for M large but let's just go ahead and be sloppy in the interests of future Simplicity okay now I don't expect anyone to be impressed with this rather crude upper bound and the number of lines of code that the merge sub routine needs to finish to execute the key question you recall was how many lines of code does merge sort require to correctly sort the input array not just this sub routine and in fact analyzing merge sort seems a lot more intimidating because it keeps spawning off these recursive versions of itself so the number of recursive calls the number of things we have to analyze is blowing up exponentially as we think about various levels of the recursion now if there's one thing that we have going for us it's that every time we make a recursive call it's on a quite a bit smaller input than what we started with it's on array of only half the size of the input array so there's some kind of tension between on the one hand explosion of sub problems a proliferation of sub problems and the fact that successive sub problems only have to solve smaller and smaller sub problems and res resolving these two forces is what's going to drive our analysis of merge sword so the good news is is I'll be able to show you a complete analysis of exactly how many lines of code merge s takes and I'll be able to give you in fact a very precise upper bound okay so here's going to be the claim that we're going to prove in the remainder of this lecture so the claim is that merge sort never needs more than 6 * n * the logarithm of n log base 2 if you're keeping track plus an extra 6n operations to correctly sort an input array of end numbers okay so let's discuss for a second is this good is this a win knowing that uh this is an upper bound on the number of lines of code that merge sort takes well yes it is and it shows the benefits of the divide and conquer Paradigm recall uh in the simpler sorting methods that we briefly discussed like insertion sort selection sort and bubble sort I claimed that their performance was governed by a quadratic function of the input size that is they need a constant times N squared number of operations to sort an input array of length n merge short by contrast needs at most a constant times n * log n not n squar but n * log in lines of code to correctly sort and input array so to get a feel for what kind of win this is let me just remind you for those of you that are Rusty or for whatever reason have lived in fear of the logarithm just exactly what the logarithm is okay so the way to think about the logarithm is as follows so you have the x- Axis or you have n which is going from one up to infinity and uh for comparison let's think about just the identity function okay so the function which is just f ofns n okay and let's contrast this with the logarithm so what is the logarithm well for our purposes we can just think of the logarithm as follows okay so the log of n log base 2 of n is you type the number n into your calculator okay then you hit divide by two and then you keep repeating dividing by two and you count how many times you divide by two until you get a number that drops below one okay so if you plug in 32 you got to divide five times by two to get down to one log base 2 of 32 is five if you put in 1,24 you have to divide by two 10 times till you get down to one so log base 2 10,24 is 10 and so on okay so the point is you can already see this if log of a th000 roughly is something like 10 then the logarithm is much much smaller than the input okay so graphically uh what the logarithm is going to look like is it's going to look like a curve which becomes very flat very quickly as n grows large okay so F of n being log base 2 of n and I encourage you to do this perhaps a little more precisely on a computer or graphing calculator uh at home but log is wearing much much much slower than the identity function and as a result uh sorting algorithm which runs in time proportional to n * log n is much much faster especially as n grows large than a sorting algorithm with a running time that's a constant times n s