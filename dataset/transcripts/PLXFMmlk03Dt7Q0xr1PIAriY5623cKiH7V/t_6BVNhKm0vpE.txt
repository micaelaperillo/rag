in this video we'll begin the proof of the master method the master method you'll recall is a generic solution to recurrences of a given form recurrences in which there's a recursive calls each on a sub problem of the same size size n over B assuming that the original problem had size n and plus there is Big O of n the D work done by the algorithm outside of these a recursive calls the solution that the master method provides has three cases depending on how a compares to B to the D now this proof will be the longest one that we've seen so far by a significant margin uh it'll span this video as well as the next two so let me say a few words upfront about what you might want to focus on overall I think the proof is quite conceptual there's a couple of spots where we're going to have to do some computations and the computations I think are worth seeing once in your life I don't know that they're worth really committing to long-term memory what I do think is worth remembering in the long term however is the conceptual meaning of the three cases of the master method in particular the proof will follow a recursion approach just like we used in the running time analysis of the merge short algorithm and it's worth remembering what three different types of recursion trees the three cases of the master method corresponds to if you can remember that there will be absolutely no need to memorize any of these three running times including the third rather exotic looking one rather you'll be able to reverse engineer those running times just from your conceptual understanding of what the three cases mean and how they correspond to recursion trees of different type so one final comment before we embark on the proof so as usual I'm uninterested in formality in its own sake the reason we use mathematical analysis in this course is because it provides an explanation of fundamentally why things are the way they are for example why the master method has three cases and what those three cases mean so I'll be giving you an essentially complete proof of the master method in the sense that it has all of the key ingredients I will cut corners on occasion where I don't think it hinders understanding and where it's easy to fill in the details so it won't be 100% rigorous I won't Dot i and cross every T but it will be a complete proof on the conceptual level that being said let me begin with a couple of minor assumptions I'm going to make to make our lives a little easier so first we're going to assume that the recurrence has the following form so here essentially all I've done is I've taken our previous assumption about the format of a recurrence and I've written out all of the constants so I'm assuming that the base case kicks in when the input size is one and I'm assuming that the number of operations in the base cases a most C and that that concept C is the same one that was hidden in the Big O notation of the general case of the recurrence the constant C here isn't going to matter in the analysis it's just all going to be a wash but to make keep everything clear I'm going to write out all of the constants that were previously hidden in the Big O notation another assumption I'm going to make analogous to our merge sort analysis is that N is a power of B the general case would be basically the same just a little more tedious at the highest level the proof of the master method should strike you as very natural really all we're we going to do is revisit the way that we analyze merge short recall our recursion tree method worked great and gave us this n log nbound on the running time of merge s so we're just going to mimic that recursion tree and see how far we get so let me remind you what a recursion tree is at the root at level zero we have the outermost the initial invocation of the recursive algorithm at level one we have the first batch of recursive calls at level two we have the recursive calls made by that first batch of recursive calls and so on all the way down to the Le Le of the tree which correspond to the base cases where there's no further recursion now you might recall from the merge short analysis that we identified a pattern that was crucial in analyzing the running time and that pattern that we had to understand was at a given depth J at a given level J of this recursion tree first of all how many distinct sub problems are there at level J how many different level J recursive calls are there and secondly what is the input size that each of those level J sub problems has to operate on so think about that a little bit and give your answer in the following quiz so the correct answer is the second one at level J of this recursion tree there are Ada to the J sub problems and each has an input of size n over V to the J so first of all why are there a to the J sub problems well when J equals zero at the root there's just the one problem the original indication uh of the recursive algorithm and then each call to the algorithm makes a further calls for that reason the number of sub problems goes up by a factor of a with each level leading to a to the J sub problems at level J similarly B is exactly the factor by which the input size shrinks once you make a recursive call so J levels into the recursion the input size has been shrunk J Times by a factor of B each time so the input size at level J is n over B to the J that's also the reason why if you look at the question statement we've identified the number of levels as being log base B of n back in merge short B was two we recursed on half the array so the leaves all resided at level log base 2 of n in general if we're dividing by a factor B each time then it takes log base B of n times before we get down to the base cases of size one so the number of levels overall zero through log Bas B of n for a total of log base B of n + one levels here then is what the recursion tree looks like at level zero we have the roots corresponding to the outermost call and the input size here is n the original problem children of a node correspond to the recursive calls because there are a recursive calls by assumption there are a children or a branches level one is the first batch of recursive calls Each of which operates on an input of size n / B at level log base B of n we've cut the input size by a factor B this many times so we're down to one so that triggers the base case so now the plan is to Simply mimic our previous analysis of merge sort so let's recall how that worked what we did is we zoomed in in a given level and for a given level J we counted the total amount of work that was done at level J sub problems not counting work that was going to be done later by recursive calls then given a bound on the amount of work at a given level J we just summed up over all the levels to capture all of the work done by all of the uh recursive indications of the algorithm so inspired by our previous success let's zoom in on a given level J and see how much work gets done with level J sub problems we're going to compute this in exactly the way we did in merge sort namely we're just going to look at the number of problems that are at level J and we're going to multiply that by a bound on the work done per sub problem we just identified the number of level J sub problems as a to the J to understand the amount of work done for each level J sub problem let's do it in two parts so first first of all let's focus on the size of the input for each level J sub problem that's what we just identified in the previous quiz question since the input size is being decreased by a factor B each time the size of each level J sub problem is n / B to the J now we only care about the size of a level J sub problem in as much as it determines the amount of work the number of operations that we perform per level J sub problem and to understand the relationship between those two Quant quantities we just return to the recurrence the recurrence says how much work gets done in a given sub problem well there's a bunch of work done by recursive calls the a recursive calls and we're not counting that we're just counting up the work done here at level J and the recurrence also tells us how much work is done outside of the recursive calls namely it's no more than the constant c times the input size raised to the D power so here the input size is n/ B to the J so that gets multiplied by the constant C and it gets raised to the D power okay so C * quantity n / B the J that's the input size raised to the D power next I want to simplify this expression a little bit and I want to separate out the terms which depend on the level number J and the terms which are independent of the level number J so if you look at it A and B are both functions of J where the C and N to the D terms are independent of J so let's just separate those out and you will notice that we have now our grand entrance of the ratio between A and B to the D and foreshadowing a little recall that the three cases of the master method are governed by the relationship between A and B to the D and this is the first time of the analysis where we get a clue that the relative magnitude of those two quantities might be important so now that we've zoomed in on a particular level J and done the necessary computation to figure out how much work is done just of that that level let's sum over all of the levels so that we capture all of the work done by the algorithm so this is just going to be the sum of the expression we saw on the previous slide now since C and the D doesn't depend on J I can yank that out in front of the sum and I'll sum the expression over all J that results in the following so believe it or not we've now reached an important milestone in the proof of the master method specifically this somewhat messy looking formula here which I'll put a green box around is going to be crucial and the rest of the proof will be devoted to interpreting and understanding this expression and understanding how it leads to the three different running time bounds in the three different cases now I realize that at the moment this expression star probably just looks like alphabet soup probably just looks like a bunch of mathematical gibberish but actually interpreted correctly this has a very natural interpretation so we'll discuss that in the next video