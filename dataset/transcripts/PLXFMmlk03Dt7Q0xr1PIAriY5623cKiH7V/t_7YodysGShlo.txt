so we've now put in a lot of work designing and analyzing super fast algorithms for reasoning about graphs so in this optional video what I want to do is show you why you might want such a primitive especially for computation on extremely large graphs specifically we're going to look at the results of a famous study that computes the strongly connected components of the web graph so what is the web graph well it's the graph in which the vertices correspond to web pages so for example I have my own web page where I you know list my research papers and also links to courses such as this one and the edges are going to be directed and they correspond precisely to hyperlinks so the links that bring you from one web page to another note of course these are directed edges where the tail is the page that contains the hyperlink and the head is the P page that you go to if you click the hyperlink and so this is a directed graph so from my homepage you can get to my papers you can get to my courses sometimes I have random links up to things I like like saying my favorite record store and of course for many of these web pages there are additional links going out or going in so for example from my papers I might link to some of my co-authors some of my co-authors might be linking from their homepages to me or of course there's web pages out there which list the currently available free online courses and so on so obviously this is just part of a massive web graph just a tiny tiny piece of it so the origins of the web were probably around 1990 or so uh but it started to really explode in the mid '90s and by the year 2000 it was sort of already Beyond Comprehension even though uh in Internet years the year 2000 is sort of the Stone Age relative to right now relative to 2012 but still even by 2000 people were so overwhelmed with the massive scale of the web graph they wanted to understand anything about it even the most basic things now of course one issue with understanding what the graph looks like is you don't even have it locally right it's distributed over all these different servers over the entire world so the first thing people really focused on when they wanted to answer this question was on techniques for crawling so having uh software which just follows lots of hyperlinks reports back to the home base from which you can assemble uh at least some kind of sketch of what this graph actually is so but then the question is even once you have this crawled information even if once you've accessed a good chunk of the nodes and the edges of this of this network what does it look like so what makes this a difficult question more difficult than say for any other directed graph you might encounter well it's simply the massive scale of the web graph it's just so big so for the graph used in the particular study I'm going to discuss you know like we said it was in the year 2000 which was sort of the Stone Age compared to 2012 so the graph was small relatively but still the graph was really really big so it was something like 200 million nodes and 1 billion edges really 1 and a half billion edges so the reference for the work on I'm going to discuss is a paper by a number of authors the first author is Andre broer and then he has many co-authors and this was a paper that appeared in the dubdub duub conference of the year 2000 that's the worldwide web conference and I encourage you those of you who are interested to go track down the paper online and read the original Source uh so Andre broer the lead author at this time he was at a company that was called altav Vista so how many of you remember a company called altav Vista well some of you especially you know the youngest ones among you maybe have never heard of altav Vista and the youngest ones among you maybe can't even conceive of a world in which we didn't have Google uh but in fact there was a time when we had web search uh that but Google did not yet exist uh that was sort of in the you know maybe '97 or so and um so this was in the very embryonic years of Google and this this data set actually came out of out of altav Vista instead so brother all wanted to give some answers to this question what does the web graph look like and they approached it in a few ways but the one I'm going to focus on here is they asked well you know what's the most detailed structure we can get about this web graph uh without doing an infeasible amount of computation really just sticking to linear time algorithms uh at at the worst and what have we seen we've seen that in a director graph you can get full connectivity information just really using depth first search you can compute strongly connectic components in linear time with small constants and that's one of the major things that they did in this study now if you wanted to do the same computation today you'd have one thing going against you and one thing going for you the obvious thing that you'd have going against you is that the web is still very much bigger uh than it was here certainly by an order of magnitude the thing that you'd have going for you is now there's specialized systems which are meant to operate on massive data sets and in particular they can do things like compute connectivity information on graph data so what you have to remember for those of you who are aware of these terms in 2000 there was no map reduce there was no Hadoop uh there were no tools for automated processing of large data sets these guys really had to do it from scratch so let me tell you about what broer at all found when they did strong connectivity computations on the web graph they explain their results in what they called The Bow high picture of the web so let's begin with the center or the knot of the Bow high so in the middle we have what we're going to call a giant strongly connected component with the interpretation being this is the core of the web in some sense all right so all of you know what an SEC is at this point a as strong cantic component is the region from which you can get from any point to any other point along a directed path so in the context of the web graph with this giant secc what this means is that from any web page inside this blob you can get to any other web page inside this blob just by traversing a sequence of hyperlinks and hopefully it doesn't strike you as too surprising that a big chunk of the web is strongly connected is well connected in this sense right so if you think about all the different universities in the world you know probably all of the web pages corresponding to all of the different universities uh you can get from any one place to any other place for example from the homepage on which I put my papers I often include links to my co-authors which uh very commonly are at other universities so that already provides a web link from some Stanford page to some page at say Berkeley or Cornell or whatever and of course I'm just one person I'm just one of many faculty members at Stanford so you put all of these together you would expect all of the different uh sec's corresponding to different universities to collapse into a single one and so on for other uh sectors as well and then of course if you knew that a huge chunk of the web was in the same strongly connected component so let's say 10% of the web which would be tens of millions of web pages uh you wouldn't expect there to be a second one right it would be super weird if there were two different blobs 10 million web pages each that somehow were not mutually reachable from each other that would just all all it takes to collapse two sec's into one is a loan Arc going from one to the other and then a loan Arc in the reverse Direction and then those two sec's collapse into one so we do expect a giant secc just sort of thinking anecdotally about what the web looks like and then once we realize there's one giant secc we don't expect there to be more than one all right so is that the whole story is the web graph just one big SEC well one of the perhaps interesting findings of this rotor all paper is that you know there is a giant SEC but it doesn't actually take up the whole web or anything really that close to the entire web so what else would there be in such a picture well there's the other two ends of the bow tie which are called the in and the out regions in the out regions you have a bunch of strong ktic components not giant sec's we've established there shouldn't be other any other giant sec's but small sec's which you can reach from the Giant strong kinetic component but from which you cannot go back to the giant strong kinetic component I encourage you to think about what types of websites you would expect to see uh in this out part of the bow tie I'll give you one example very often if you look at a corporate site including those of well-known corporations which you would definitely expect to be reachable from the giant SEC there it's actually a corporate policy that no hyperlinks can go from something in the corporate site to something outside the corporate site so that means the corporate site is going to be a collection of web pages which is certainly strongly connected because it's a major corporation you can certainly get there from the giant SEC but because of this corporate policy you can't get back out symmetrically in the in part of the bow TI you have strong T components generally small ones from which you can reach the giant SEC but you cannot get to them from the giant SEC again I encourage you to think about all the different types of web pag Pages you might expect to see in this in part of the bow tie uh certainly think one really obvious example would be new web pages so if you just create something and then you know if I just created a web page and pointed it to Stanford University that would immediately be in this in component or this in collection of components now if you think about it this does not exhaust all of the possibilities of where nodes can LIE there's a few other cases uh the frankly are pretty weird but they're there you can have passive hyperlinks which bypass the Jun SCC and go straight from the in part of the bow tie to the out part so broer all suggested calling these tubes and then there's also kind of very curious outgrowths going out of the in component but which don't make it all the way to the giant SEC and similarly there's stuff which goes into the out component and broer all recommended calling these strange creatur tendrils and then in fact you can also just have some weird isolated islands of sec's that are not connected uh even weakly to the giant SEC so this is the picture that emerged from Brer at all's strong component computation on the web graph and here's qualitatively some of the main findings that they came up with so first of all that picture on the previous slide I drew Drew roughly to scale in the sense that all four parts so the giant SEC the in part the out part and then the residual stuff the tubes and tendrils have roughly the same size you know more or less 25% of the nodes uh in the graph I think this surprised some people I think some people might have thought that the core that the giant SEC might have been a little bit bigger than just 25 or 28% but it turns out there's a lot of other stuff outside of this strongly connected core you might wonder if this is just an artifact of the this data set being from the Stone Age being from 2000 or so but uh people have rerun this experiment on uh the on the web graph again in later years and of course the numbers are changing because the graph is growing rapidly but these qualitatively findings qualitative findings have seemed pretty stable uh throughout subsequent uh re-evaluations of the structure of the web on the other hand while the core of the web is not as big as you might have expected it's extremely well connected perhaps better connected than you might have expected now you'd be right to ask the question you know what could I mean by unusually well connected we've already established that this uh giant core of the web is strongly connected you can get from any one place to any other place via a sequence of hyperlinks what else could you want well in fact it has a very richer notion of connectivity called the small world property so let me tell you about the small world property or the phenomenon colloquially known as Six Degrees of Separation so this is an idea that had been in the air at least since the early 20th century uh but it really kind of was studied uh in a major way and popularized by Stanley mgram who's a social scientist uh back in 1967 so mgrm was interested in in understanding you know are people at Great distance in fact connected by Short Change of intermediaries so the way he evaluated this uh he ran the following experiment he gave uh he identified a friend in Boston Massachusetts a doctor I believe and uh so this was going to be the Target and then he identified a bunch of people uh who were thought to be far away both culturally uh and geographically specifically Omaha so for those of you who don't live in the US just take it on faith that many people in the US would regard Boston and and Omaha as being fairly far apart geographically and otherwise and uh what he did is he wrote each of these uh residents of Omaha the following letters who said look here's the name and address of this doctor who lives in Boston okay your job is to get this letter to this doctor in Boston now you're not allowed to mail the letter directly to the doctor instead you need to mail it to an intermediary someone who you know on a first name basis so of course if you knew the doctor on a first name basis you could mail it straight to them but that was very unlikely so you know what people would do in Omaha they'd say well you know I don't know any doctors or I don't know anyone in Boston but at least I know somebody in Pittsburgh and at least that's closer to Boston than Omaha that's further Eastward or maybe someone would say well I don't really know anyone on the East Coast but at least I do know some doctors here in Omaha and so they' give the letter to somebody that they knew on a first name basis in Omaha and then the situation would repeat whoever got the letter again they'd be given the same instructions if you know this doctor in Boston on a first day mesis send them the letter otherwise pass the letter on to somebody who seems more likely closer to them uh than you are now of course many of these letters never reached their destination but shocking at least to me is that a lot of them did so something like uh 25% at least of the letters that they started with made it all the way to Boston which I think says something about people in the late 60s just having more free time on their hands uh than they do in the early 21st century I I find this hard to imagine but it's a fact so you had dozens and dozens of letters uh reaching this doctor in Boston and they were able to trace exactly which path of individual ual the letter went along before it eventually reached this doctor in Boston and so then what they did is they looked at the distribution of chain lengths so how many intermediaries were required to get from some random person in Omaha to this doctor in Boston some were as few as two some were as big as nine but the average number of hops the average number of intermediaries was in the range of 5 and a half or six and so this is what has given rise to the colloquialism uh even the name of a popular play the Six Degrees of Separation so that's the origin myth that's where this phrase comes from uh these sort of experiments with physical letters but now in network science the small world property is meant to be a network which on the one hand is richly connected but also in some sense there are enough cues about which links are likely to get closer to some Target so that if you need to Route information from point A to point B not only is there a short path but if you in some sense follow your nose then you'll actually exhibit a short path so in some sense routing information is easy in small world networks and this is exactly the the property that bro all identified within this giant SEC very rich with short paths and if you want to get from point A to point B just follow your nose and you'll do great you don't need a very sophisticated shortest path algorithm to find a short path some of you may have heard of sley mgram not for this small world experiment but for another famous or maybe Infamous experiment he did earlier in the' 60s uh which consisted into tricking volunteers into thinking they were subjecting other human beings to massive doses of electric shocks so that wound up you know causing a rewrite to certain standards of Ethics uh in experimental psychology you don't hear about that so much when people are talking about networks but that's another reason uh why mgm's work is welln and just as a point a contrast outside of this giant strongly ktic component which has this Rich small world structure uh very poor connectivity in the other parts of the web graph so there's lots of cool research going on these days about the study of information networks like like the web graph so I don't want you to get the Imp impression that the entire interaction between algorithms and thinking about information networks has just been this one strong ktic component computation in 2000 of course there's all kinds of interactions I just singled one out that was easy to explain and also highly influential and interesting back in the day but you know these days lots of stuff's going on people are thinking about uh information networks in all kinds of different ways and of course algorithms like in almost everything is playing a very fundamental role so let me just dash off sort of a few examples maybe to W your appetite maybe you want to go explore uh this topic in Greater depth uh outside of this course so one super interesting question is rather than looking at a static snapshot of the web like we were doing so far in this video right the web's changing all the time new pages are getting created new links are getting created and destroyed and so on and how does this Evolution proceed can we have an mathematical model which Faithfully reproduces uh the most important first order properties of this evolutionary process so a second issue is to think not just about the Dynamics of the graph itself but the Dynamics of information that gets carried by the graph and you could ask this both about the web graph and about other social networks like say Facebook or Twitter another really important topic which there's been a lot of work on but we still don't fully understand by any means is getting at the finer grain structure in networks including the web graph in particular what we really like to do is have foolproof methods for identifying communities so groups of nodes these could either be web pages in the web graph or individuals in a social network which we should think of as grouped together we discussed this a little bit when we talked about applications of cuts one motivation for cuts is to identify communities if you think of communities as being relatively densely connected inside and sparsely connected outside and that's just a but that's just a baby step really we need much better techniques for both defining and Computing communities in these kinds of networks so I think these questions are super interesting both from a mathematical technical level but also they're very timely answering them really helps us understand our world better uh unfortunately these are going to be well outside the course of just the bread and butter design analysis of algorithms which is what uh I'm tasked with covering here but I will leave you with a reference a book that I recommend if you want to read more about these topics namely the quite recent book by David Easley and John kleinberg called networks crowds and markets