I've said pretty much everything I want to say about sorting at this point but I do want to cover one more related topic namely the selection problem this is the problem of computing order statistics of an array with Computing the median of array being a special case analogous to our coverage of quick sort the goal is going to be the design and Analysis of a super practical randomized algorithm that solves the problem and uh this time we'll even achieve an expected running time that is linear in the length of the input array that is Big O of n for input arrays of length n as opposed to the O of n log time that we had for the expected running time of quicksort like quicksort the mathematical analysis is also going to be quite elegant so in addition these two required videos on this very practical algorithm will motivate two optional videos that are on very cool topics but of a somewhat more theoretical nature uh the first optional video is going to be on how you solve the selection problem in deterministic linear time that is without using randomization and the second optional video uh will be a sorting lower bound that is why no comparison-based sort can be better than merge can have better running time uh than Big O of and login so a few words about what you should have fresh in your mind before you watch this video uh I'm definitely assuming that you've watched the quicksort videos and not just watch them but that you have that material pretty fresh in your mind so in particular the video quick sort about the partition sub routine so this is where you take an input array you choose a pivot and you do by repeated swaps you rearrange the array so that everything less than the pivot is to the left of it everything bigger than the pivot is to the right of it you should remember that sub routine you should also remember the previous discussion about pivot choices the idea that the quality of a pivot depends on how balanced to split into two different sub problems it gives you those are both going to be important for the analysis of this randomized linear time selection algorithm I need you to be remember the concepts from probability review part one in particular random variables their expectation and linearity of expectation that said let's move on and formally Define what the selection problem is the input is the same as for the Sorting problem just you're given an array of n distinct entries but in addition you're told what order statistic you're looking for so that's going to be a number I which is an integer between 1 and n and the goal is to Output just a single number namely the E order statistic that is the I smallest entry in this input array so just to be clear if you had an array entry of let's just say four elements containing the numbers 10 8 2 and four and you were looking for say the third order statistic that would be this eight the first order statistic is just the minimum ele element of the array that's easy to find with a linear scan the nth order statistic is just the maximum again easier easy to find with a linear scan uh the middle element is the median and you should think of that as the canonical version of the selection problem now when n is odd it's obvious what the median is that's just the middle element so the n+ one over tooth order statistic if the array has even length there's two possible medians so let's just take the smaller of them that's the in tooth or statistic you might wonder why you'd ever want to compute the median of an array rather than the mean that is the average it's easier to see that you can compute the average just with a simple linear scan and uh the median you can one motivation is it's a more robust version of the mean so if you just have a data entry problem and it corrupts one element of an input array it can totally screw up the average value of the array but it has has generally very little impact on the median final comment about the problem is I am going to assume that the array entries are distinct that is there's no repeated elements but just like in our discussions of sorting this is not a big assumption uh I can encourage you to think about how to adapt these algorithms to work even if the arrays do have duplicates you can indeed uh still get the same very practical very fast algorithms with duplicate elements now if you think about it we already have a pretty darn good algorithm that solves the selection problem here's the algorithm it's two simple steps and it runs in O of n log n time step one sort the input array we have various subroutines to do that let's say we pick merge sort now what is it we're trying to do we're trying to find the I smallest element of the input array well once we've sorted it we certainly know where the I smallest element is it's in the I position of the sorted array so that's pretty cool we've just done what a computer scientist would call a reduction and that's a super useful and super fundamental concept it's when you realize that you can solve one problem by reducing it to another problem that you already know how to solve so what we just showed is that the selection problem reduces easily to the Sorting problem we already know how to solve the Sorting problem in nlog end time so that gives us an nlog end time solution to the selection problem but again remember the Mantra of any algorithm designer worth their salt is can we do better we should avoid content just cuz we got n log n doesn't mean we should stop there maybe we can be even faster now certainly we're going to have to look at all of the elements in the input array in the worst case we shouldn't expect to do better than linear but hey why not linear time actually if you think about it we probably should asked that question back when we were studying the Sorting problem why were we so content with the nlog end time Bound for merge sort and the O of n log end time on average Bound for quick sort well it turns out we have a really good reason to be happy with our n log in Upper bounds for the Sorting problem it turns out and this is not obvious and would be the subject of a optional video you actually can't sort an input array of length n better than n log n time either in the worst case or on average so in other words if we insist on solving the selection problem via a reduction to the Sorting problem then we're stuck with this nlog end time Mound okay strictly speaking that's for something called comparison sorts see the video for more details but the upshot is if we want a general purpose algorithm and we want to do better than n logn for selection we have to do it using Ingenuity Beyond this reduction we have to prove that selection is a strictly easier problem than sorting that's the only way we're going to have an algorithm that beats n log in it's the only way we can conceivably get a linear time algorithm and that is exactly what is up next on our plate we're going to show selection is indeed fundamentally easier than sorting we can have a linear time algorithm for it even though we can't get a linear time algorithm for sorting you can think of the algorithm we're going to discuss is a modification of quick sort and In The Same Spirit of quick sort it will be a randomized algorithm and the running time will be an expected running time that will hold for any input array now for the Sorting problem we know that quick sort that's un log end time on average where the average is over the coin flips done by the code but we also know that if we wanted to we could get a sorting algorithm in N log and time that doesn't use randomization the merge sort algorithm is one such solution so here we're giving a linear time solution for selection for finding orders to statistics that uses randomization and it would be natural to wonder is there an analog to merge sort is there an algorithm which does not use randomization and gets this exact same linear time Bound in fact there is the algorithms a little more complicated and therefore not quite as practical as as randomized algorithm but it's still very cool it's a really fun algorithm to learn and teach so I will have an optional video about linear time selection without randomization so for those of you who aren't going to watch that video or want to know what's the key idea uh the idea is to choose the pivot deterministically in a very careful way using a trick called the median of medians that's all I'm going to say about it now you should watch the optional video if you want more details I do feel compelled to warn you that if you're going to actually Implement a selection algorithm you should do the one that we discuss in this video not the linear time one because the one we'll discuss in this video has both smaller constants and works in place so what I want to do next is develop the idea that we can modify the quick sort Paradigm in order to directly solve the selection problem so to get an idea of how that works let me review the partition sub routine like in quick sort this sub routine will be our Workhorse for the selection algorithm so what the partition subroutine does it takes us input some jumbled up array and it's going to solve a problem which is much more modest than sorting so in partitioning it's going to first choose a pivot element somehow we'll have to discuss what's a good strategy for choosing a pivot element but suppose you know in this particular input array it chooses the first element this three as the pivot element the responsibility of the partition sub routine then is to rearrange the elements in this array so that the following properties are satisfied anything less than the pivot is to the left of it it can be in jumbled order but if you're less than the pivot you better be to the left like this two and one is less than the three if you're bigger than the pivot then again you can be in jumbled order amongst those elements but all of them have to be to the right of the pivot and that's true for the numbers four through eight they all are to the right of the pivot three in a jumbled order so this in particular puts the pivot in its rightful position where it would belong in the final sorted array and at least for quick sort it enabled us to recursively sort to smaller sub problems so this is where I want you to think a little bit about how we should adapt this Paradigm so suppose I told you the first step of our selection algorithm is going to be to choose a pivot and partition the array now the question is how are we going to recurse we need to understand how to find the E order statistic of the original input array it suffices to recurse on just one sub problem of smaller size and find a suitable order statistic in it so how should we do that let me ask you that in with some very concrete examples about what pivot we choose and what order statistic we're looking for and see what you think so the correct answer to this quiz is the second answer so we can get away with recursing just once and in this particular example we're going to recurse on the right side of the array and instead of looking for the fifth order statistic like we were originally we're going to recursively search for the second order statistic so why is that well first why do we recurse on the right side of the array so by assumption we have this array with 10 elements we choose the pivot we do partitioning remember the pivot winds up in its rightful position that's what partitioning does so in if the bivet winds up in the third position we know it's the third smallest element in the array now that's not what we were looking for we were looking for the fifth smallest element in the array that of course is bigger than the third smallest element of the array so by partitioning where is The Fifth Element going to be it's got to be to the right of this third smallest element to the right of the pivot so we know for sure that the fifth order statistic of the original array lies to the right of the pivot that is guaranteed so we know where to recurse on the right hand side now what are we looking for we are no longer looking for the fifth order statistic the fifth smallest element why well we've thrown out both the pivot and everything smaller than it remember we're only recursing on the right hand side so we've thrown out the pivot the third element and everything less than it the minimum and the second minimum having deleted the three smallest elements and originally looking for the fifth smallest of what remains of what we're recursing on we're looking for the second smallest element so the selection algorithm in general is just the generalization of this idea to arbitrary arrays and arbitrary situations of whether the pivot comes back equal to less than or bigger than the element you're looking for so let me be more precise I'm going to call this algorithm R select for randomized selection and according to the problem definition it takes as input as usual an array a of some length n but then also the order statistic that we're looking for so we're going to call that I and of course we assume that I is some integer between 1 and N inclusive so for the base case case that's going to be if the array has size one then the only element we could be looking for is the oneth order statistic and we just return the sole element of the array now we have to partition the array around the pivot element and just like in quick sort we're not we're going to be very lazy about choosing the pivot we're going to choose it uniformly at random from the end possibilities and hope things work out and that'll be the Crux of the analysis proving that random pivots are good enough sufficiently often having chosen the pivot we now just invoke the standard partitioning sub rou as usual that's going to give us the partitioned array you'll have the pivot element you'll have everything less than the pivot to the left everything bigger than the pivot to the right as usual I'll call everything to the left the first part of the partitioned array and everything bigger the second part now we have a couple of cases depending on whether the pivot is bigger or less than the element we're looking for so I need a little notation to to talk about that so let's let J be uh the order statistic that P is so if T winds up being the third smallest element like in the quiz then J is going to be equal to three equivalently we can think of J is defined as the position of the pivot in the partitioned version of the array now there's one case which is very unlikely to occur but we should include it just for completeness if we're really lucky then in fact our random pivot just happens to be the order statistic we were looking for that's when I equals J we're looking for the ice smallest element if by dumb luck the pivot winds up being the ice smallest element we're done we can just return it we don't have to recurse now in general of course we don't randomly choose the element we're looking for we choose something that well it could be bigger or it could be smaller than it in the quiz we chose a pivot that was smaller than what we're looking for actually that's the harder case so let's first start with a case where the pivot winds up being bigger than the element we're looking for so that means the J is bigger than I we're looking for the I smallest we randomly chose the J's smallest for J bigger than I so this is the opposite case of the quiz this is where we know what we're looking for has to be to the left of the pivot the pivot is the J smallest everything less than it's to the left we're looking for the I smallest I is less than J so that's got to be on the left that's where we recurse moreover it's clear we're looking for exactly the same order statistic if we're looking for the third smallest element we're only throwing out stuff which is bigger than something even bigger than the third smallest element so we're still looking for the third smallest of what remains and naturally the new array size is J minus one because that's what is to the left of the pivot and then finally the final case is when the random element that we choose is less than what we're looking for and then we're just like the quiz so namely what we're looking for is bigger than the pivot it's got to be on the right hand side we know we got a recurse on the right hand side we know the right hand side has n minus J elements we threw out everything up to the pivot so we threw out J things there's n minus J left and all of those J things we threw out are less than what we're looking for so if we used to be looking for the ice smallest element now we're looking for the IUS J smallest element so that is the whole algorithm that is how we adopt the approach we took to the Sorting problem and quick sort and adapt it to the problem of selection so is this algorithm any good let's start studying its properties and understand how well it works so let's begin with correctness so the claim is that no matter how the algorithm's coin flips come up no matter what random pivots we choose the algorithm is correct in the sense that it's guaranteed to Output the I order statistic the proof is by induction it proceeds very similarly to quick sort so I'm not going to give it here if you're curious about how these proofs go there's an optional video about the correctness of quick sort if you watch that and understand it it should be clear how to adapt that inductive argument uh to apply to this selection algorithm as well so as usual for divide and conquer algorithms the interesting part is not so much knowing understanding why the algorithm works but rather understanding how fast it runs so the big question is what is the running time of this selection algorithm now to understand this we have to understand the ramifications of pivot choices on the running time so you've seen the quick sort videos they're fresh in your mind so what should be clear is that just like in quicksort how fast this algorithm runs is going to depend on how good the pivots are and what good pivots means is pivots the guarantee balance to split so the next Quiz will make sure that you understand this point and asks you to think about just how bad the running time of The Selection algorithm could be if you get extremely unlucky in your pivot choices so the correct answer to this question is exactly the same as the answer for quick sort uh the worst case running time if the pivots are chosen just in a really unlucky way is actually quadratic in the array length remember we're shooting for linear time so this quadratic is a total disaster so how could this happen well suppose you're looking for the median and suppose you choose the minimum element as the pivot every single time so if this is what happens if every time you choose the pivot to be the minimum just like in quick sort this means every time you recur all you succeed in doing is peeling off a single element from the input array now you're not going to find the median element until you've done roughly n over two recursive calls each on an array that has size at least a constant fraction of the original one so that's a linear number of recursive calls each on an array of size at least some constant times n so that gives you a total running time of quadratic overall of course this is an absurdly unlikely event frankly your computer is more likely to be struck by a meteor than it is for the pivot to be chosen as the minimum m in every recursive call but if you really had an absolutely worst case choice of pivots it would give this quadratic runtime bound so the upshot then is that the running time of this randomized selection algorithm depends on how good our pivots are and for a worst case choice of pivots the running time can be as large as n squ now hopefully most of the time we're going to have much better pivots and so the analysis proceeds by making that idea precise so the key to a fast running time is going to be the the usual property that we want to see in divide conquer algorithms namely every time recurse the every time we recurse the problem size better not just be smaller but it better be smaller by a significant factor how would that happen in this selection approach based on the partition sub routine well if both of the sub problems are not too big then we're guaranteed that when we recurse we make a lot of progress so let's think about what the best possible pivot would be in the sense of giving a balanc split right so of course in some sense the best pivot is you just choose the order statistic you're looking for uh but and that then you're done in constant time but that's extremely unlikely and it's not worth worrying about so ignore the fact that we might guess the pivot what's the best pivot if we want to guarantee an aggressive decrease in the input size before the next iteration well the best pivot is the one that gives us as most balanced split as possible so what's the pivot that gives us the most balanced split a 50/50 split well if you think about it it's exactly the median of course this is not super helpful because the median might well be what we're looking for in the first place so this is sort of a circular idea but for intuition it's still worth exploring what kind of running time we would get in the best case right if we're not going to get linear time even in this magical best case we certainly wouldn't expect to get it on average over random choices of the pivots so what would happen if we actually did luckily choose the median as the pivot every single time well we get the recurrence that the uh running time that the algorithm requires on array of length n well there's only going to be one recursive call so this is the big difference from quick sort where we had to recurse on both sides we had two recurs recursive calls so here we're going to have only one recursive call in the magical case where our pivots are always equal to the median both sub problem sizes contain are only half as large as the original one so when we recurse it's on a problem size guaranteed to be at most n over2 and then outside of the recursive call pretty much all we do is a partitioning invocation and we know that that's linear time so the recurrence we get is T of n is the most T of n / 2 plus Big O of n this is totally ready to get plugged into the master method it winds up being case two of the master method and indeed we get exactly what we wanted linear time to reiterate this is not interesting in its own right this is just for intuition this was a sanity check that at least for a best case choice of pivots we get what we want a linear time algorithm and we do now the question is how well do we do with random pivots now the intuition the hope is exactly as it was for quick swort which is the random pivots are a perfectly good surrogate for the median for the perfect pivot so having the analysis of quick sort under our belt where indeed random pivots do approximate very closely the performance You' get with best case pivots maybe now we have reason to believe that this is hopefully true that said as a mathematical statement this is totally not obvious and it's going to take a proof that's the subject for the next video but let me just be clear exactly what we're claiming here is the running time guarantee that randomized selection provides for an arbitrary input array of length n the average running time of this randomized selection algorithm is linear Big O of n let me reiterate a couple points I made uh for the analogous guarantee for the quick sort algorithm the first is that we're making no assumptions about the data whatsoever in particular we're not assuming that the data is random this guarantee holds no matter what input array you feed into this randomized algorithm in that sense this is a totally general purpose sub routine so where then does this averaging come from where does the expectation come from the randomness is not in the data rather the randomness is in the code and we put it there ourselves now let's proceed to the analysis