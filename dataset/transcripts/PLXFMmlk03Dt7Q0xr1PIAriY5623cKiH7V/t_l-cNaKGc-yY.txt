In this sequence of lectures, we're going to learn asmtoic analysis. This is the language by which every serious computer programmer and computer scientist discusses the highle performance of computer algorithms. As such, it's a totally crucial topic. In this video, the plan is to segue between the highle discussion you've already seen in the course introduction and the mathematical formalism which we're going to start developing in the next video. Before getting into that mathematical formalism, however, I want to make sure that the topic is well motivated, that you have solid intuition for what it's trying to accomplish, and also that you've seen a couple simple intuitive examples. Let's get started. Asmintoic analysis provides basic vocabulary for discussing the design and analysis of algorithms. And while it is a mathematical concept, it is by no means math for math's sake. You will very frequently hear serious programmers saying that such and such code runs in ON time whereas such and such other code runs in O of N squared time. It's important you know what programmers mean when they make statements like that. The reason this vocabulary is so ubiquitous is that it identifies a sweet spot for discussing the highle performance of algorithms. What I mean by that is it is on the one hand coarse enough to suppress all of the details that you want to ignore. Details that depend on the choice of architecture, the choice of programming language, the choice of compiler and so on. On the other hand, it's sharp enough to be useful in particular to make predictive comparisons between different high-level algorithmic approaches to solving a common problem. This is going to be especially true for large inputs. And remember, as we discussed, in some sense, large inputs are the interesting ones. Those are the ones for which we need algorithmic ingenuity. For example, asmtoic analysis will allow us to differentiate between better and worse approaches to sorting, better and worse approaches to multiplying two integers and so on. Now, most serious programmers if you ask them what's the deal with asmtoic analysis anyways, they'll tell you reasonably that the main point is to suppress both leading constant factors and lower order terms. Now, as we'll see, there's more to asmtoic analysis than just these seven words here. But long-term, 10 years from now, if you only remember seven words about asmtoic analysis, I'll be reasonably happy if these are the seven words that you remember. So, how do we justify adopting a formalism which essentially by definition suppresses constant factors and lower order terms? where lower order terms basically by definition become increasingly irrelevant as you focus on large inputs which as we've argued are the interesting inputs the ones where algorithmic ingenuity is important as far as constant factors these are going to be highly dependent on the details of the environment the compiler the language and so on so if we want to ignore those details it makes sense to have a formalism which doesn't focus unduly on leading constant factors here's an example Remember when we analyzed the merge sort algorithm, we gave an upper bound on its running time that was 6 * n log n + 6n where n was the input length, the number of numbers in the input array. So the lower order term here is the 6n that's growing more slowly than n login. So we just drop that. And then the leading constant factor is the six. So we suppress that as well. After those two suppressions, we're left with a much simpler expression n login. The terminology would then be to say that the running time of merge sort is big O of N log N. So in other words, when you say that an algorithm's running time is bigo of some function, what you mean is that after you've dropped the lower order terms and suppressed the leing leading constant factor, you're left with the function f of n. Intuitively, that is what bigo notation means. So to be clear, I am certainly not asserting that constant factors never matter when you're designing an al analyzing algorithms. Rather, I'm just saying that when you think about high-level algorithmic approaches, when you might want to make a comparison between fundamentally different ways of solving a problem, asintoic analysis is often the right tool for giving you guidance about which one is going to perform better, especially on reasonably large inputs. Now, once you've committed to a particular algorithmic solution to a problem, of course, you might want to then work harder to improve the leading constant factor, perhaps even to improve the lower order terms. By all means, if the future of your startup depends on how efficiently you implement some particular set of lines of code, have at it. Make it as fast as you can. In the rest of this video, I want to go through four very simple examples. In fact, these examples are so simple, if you have any experience with big O notation, you're probably better off just skipping the rest of this video and moving on to the mathematical formalism that we begin in the next video. But if you've never seen it before, I hope these simple examples will get you oriented. So, let's begin with a very basic problem. Searching an array for a given integer. Let's analyze the straightforward algorithm for this problem where we just do a linear scan through the through the array checking each entry to see if it is the desired integer t. That is the code just checks each array entry in turn. If it ever finds the integer t, it returns true. If it falls off the end of the array without finding it, it returns false. So what do you think? We haven't formally defined bigo notation, but I've given you an intuitive description. What would you say is the running time of this algorithm as a function of the length of the array capital A? So the answer I'm looking for is C big O of N or equivalently we would say that the running time of this algorithm is linear in the input length N. Why is that true? Well, let's think about how many operations this piece of code is going to execute. Actually, the lines of code executed is going to depend on the input. It depends on whether or not the target T is contained in the array A and if so, where in the array A it lies. But in the worst case, this code will do an unsuccessful search. T will not be in the array and the code will scan through the entire array A and return false. The number of operations then is a constant. there's some initial setup perhaps and maybe it's an operation to return this final boolean value but outside of that constant which will get suppressed in the big O notation it does a constant number of operations per entry in the array and you could argue about what the constant is if it's 2 three four operations per entry in the array but the point is whatever that constant is 2 three or four it gets uh conveniently suppressed by the bigo notation so as a result the total number of operations will be linear and N and so the big O notation will just be O of N. So that was the first example. In the last three examples, I want to look at different ways that we could have two loops. And in this example, I want to think about one loop followed by another. So two loops in sequence. I want to study almost the same problem as the previous one where now we're just given two arrays, capital A and capital B. Let's say both of the same length n. And we want to know whether the target T is in either one of them. Again, we'll look at the straightforward algorithm where we just search through A. And if we fail to find T and A, we search through B. If we don't find T and B either, then we have to return false. So the question then is exactly the same as last time given this new longer piece of code. What in bigo notation is its running time? Well, the question was the same and in this case the answer was the same. So this algorithm just like the last one has running time bigo of N. If we actually compound count the number of operations of course it won't be exactly the same as last time. It'll be roughly twice as many operations as the previous piece of code. That's because we have to search two different arrays each of length n. So whatever work we did before we now do it twice as many times. Of course that two being a constant independent of the input length n is going to get suppressed once we pass to bigo notation. So this like the previous algorithm is a linear time algorithm it has running time bigo of n. Let's look at a more interesting example of two loops where rather than processing each loop in sequence they're going to be nested. In particular, let's look at the problem of searching whether two given input arrays, each of length n, contain a common number. The code that we're going to look at for solving this problem is the most straightforward one you can you can imagine where we just compare all possibilities. So for each index i into the array a, each index j into the array b, we just see if a i is the same number as bj. If it is, we return true. If we exhaust all of the possibilities without ever finding equal elements, then we're safe in returning false. The question of course is in terms of bigo notation asmtoic analysis as a function of the array length n, what is the running time of this piece of code? So this time the answer has changed for this piece of code. The running time is not big O of N, but it is bigo O of N squared. So we might also call this a quadratic time algorithm because the running time is quadratic in the input length N. So this is one of those kinds of algorithms where if you double the input length, then the running time of the algorithm will go up by a factor of four rather than by a factor of two like in the previous two pieces of code. So why is this? Why does it have quadratic running time bigo of N squared? Well, again, there's some constant setup cost which gets suppressed in the bigo notation. Again, for each fixed choice of an entry I into array A and an index J for array B, for each fixed choice of I and J, we only do a constant number of operations. The particular constant's irrelevant because it gets suppressed in the big O notation. What's different is that there's a total of n squared iterations of this double for loop. In the first example, we only had n iterations of a single for loop. In our second example, because one for loop completed before the second one began, we had only two n iterations overall. Here, for each of the n iterations of the outer for loop, we do n iterations of the inner for loop. So that gives us the n * n, i.e. n squared total iterations. So that's going to be the running time of this piece of code. Let's wrap up with one final example. It will again be nested for loops, but this time we're going to be looking for duplicates in a single array A rather than needing to compare two distinct arrays A and B. So here's the piece of code we're going to analyze for solving this problem for detecting whether or not the input array A has duplicate entries. There's only two small differences relative to the code that we went through on the previous slide when we had two different arrays. Uh the first surprise the first change won't surprise you at all which is instead of referencing the array B I change that B to an A. Right? So I'm just going to compare the E entry of A with the J entry of A. The second change is a little bit more subtle which is I changed the inner for loop so that the index J begins at I + 1 where I is the current value of the outer for loops index rather than starting at the index one. I could have had it start at the index one. That would still be correct, but it would be wasteful. And you should think about why. If we started the inner for loops index at one, then this code would actually compare each distinct pair of elements of A to each other twice, which of course is silly. You only need to compare two different elements of A to each other once to know whether they're equal or not. So this is the piece of code. The question is the same as it always is. What in terms of bigo notation and the input length N is the running time of this piece of code? So the answer to this question same as the last one big O of N squared that is this piece of code is also a has quadratic running time. So what I hope was clear was that you know whatever the running time of this piece of code is it's proportional to the number of iterations of this double for loop. Like in all the examples we do constant work per iteration. We don't care about the constant. It gets suppressed by the big O notation. So all we got to do is figure out how many iterations there are of this double for loop. My claim is that there's roughly n^2 over two iterations of this double for loop. There's a couple ways to see that. Informally, we discussed how the difference between this code and the previous one is that instead of counting something twice, we're counting it once. So that saves us a factor of two in the number of iterations. Of course, this 1/ half factor gets suppressed by the big O notation anyways. So the big O running time doesn't change. A different argument would just say you know how many there's one iteration for every distinct choice of i and j of indices between one and n. And a simple counting argument says that there's n choose two such choices of distinct i and j where n choose 2 is the number n * n minus one over two. And again suppressing lower order terms in the constant factor we still get a quadratic dependence on the length of the input array a. So that wraps up some of the sort of just simple basic examples. I hope this gets you oriented. You have a strong intuitive sense for what bigo O notation is trying to accomplish and how it's defined mathematically. Let's now move on to both uh the mathematical development and some more interesting algorithms.