so in this video we're going to start reasoning about the performance of hash tables and in particular we'll make precise this idea that properly implemented they guarantee constant time lookup so let me just briefly remind you of the road map that we're in the middle of so we observe that every fixed hash function is subject to a pathological data set and so exploring the solution of making a real-time decision of what hash function to use and we've already gone over this really quite interesting definition of universal hash functions and that's the proposed definition of a good random hash function moreover in the previous video I showed you that there are simple such families of hash functions they don't take much space to store they don't take much time to evaluate and the plan for this video is to prove formally that if you draw a hash function uniformly at random from a universal family of hash functions like the one we saw in the previous video then you're guaranteed expected constant time for all of the supported operations so here's the definition once more of a universal family of hash functions we'll be using this definition of course when we prove that these hash functions give good performance so remember we're talking now about a set of hash functions these are all of the conceivable real-time decisions you might make about which hash function to use so the universe is fixed this is something like IP addresses the number of buckets is fixed you know that's going to be something like 10,000 say and what it means for a family to be Universal is that the probability that any given pair of items collides is as good as small as with the gold standard of completely perfect un uniform random hashing that is for each pair XY of distinct elements of the universe so for example for each distinct pair of IP addresses the probability over the choice of the random hash function little H from the family script H is no more than 1 over n where n is the number of buckets so if you have 10,000 buckets the probability that any given pair of IP addresses winds up getting mapped to the same bucket is at most 1 in 10,000 Let Me Now spell out the precise guarantee we're going to prove if you use a randomly chosen hash function from a universal family so for this video we're going to only talk about hash tables implemented with chaining with one link list per bucket we'll be able to give a completely precise mathematical analysis with this Collision resolution scheme we're going to analyze the performance of this hash table in expectation over the choice of a hash function little H drawn uniformly from a universal family script H so so for example for the family that we constructed in the previous video this just amounts to choosing each of the four coefficients uniformly at random that's how you select a universal that's how you select a hash function uniformly at random so this theorem and also the definition of universal hash functions dates back to a 1979 research paper by Carter and wigman uh the idea of hashing dates back quite a bit before that certainly to the 50s so this just kind of shows how sometimes ideas have to be percolating for a while uh before you find the right way to explain what's going on so Carter and w provided this very clean and modular way of thinking about performance of hashing through this Universal hashing definition and the guarantee is exactly the one that I promised way back when we talked about what operations are supported by hash tables and what kind of performance should you expect you should expect constant Time Performance as always with hashing there is some fine print so let me just be precise about what the caveats of this guarantee are so first of all necessarily this guarantee is in expect ation so it's on average over the choice of the hash function little H but I want to reiterate that this guarantee does hold for an arbitrary data set so this guarantee is quite reminiscent of the one we had for the randomized quicks sort algorithm in quicksort we made no assumptions about the data it was a completely arbitrary input array and the guarantee said on average over the real-time randomized decisions of the algorithm no matter what the input is the expected running time was n log n here we're saying again no assumptions about the data it doesn't matter what you're storing in the hash table in expectation over the real time random decision of what hash function you use you should expect constant Time Performance no matter what that data set is so the second caveat is something we've talked about before remember the key to having good hash table performance not only do you need a good hash function which is what this universality guarantee is providing us but you also need to control the load of the hash table so of course to get Constantin performance as we've discussed a necessary condition is that you have enough buckets to hold more or less the stuff that you're storing that is the load Alpha the number of objects in the table divided by the number of buckets should be constant for this theorem to hold and finally whenever you do a hash table operation you have to in particular invoke the hash function on whatever key you're given so in order to have constant Time Performance it better be the case that it only takes constant time to evaluate your hash function and that's of course something we also discussed in the previous video when we emphasized the importance of having simple Universal hash functions like those random linear combinations we discussed in the previous video in general the mathematical analysis of hashtable performance is a quite Deep Field and there are some uh quite mathematically interesting results that are well outside the scope of this course but what's cool is this theorem I will be able to provide you a full and rigorous proof so for hash tables with chaining and using randomly chosen Universal hash functions I'm going to now prove that you do get constant Time Performance all right so hash tables uh support various operations insert delete and lookup but really if we can just bound the running time of an unsuccessful lookup that's going to be enough to bound the running time of all of these operations so remember in a hash table with chaining you first hash to the appropriate bucket and then you do the appropriate insert delete or lookup in the link list in that bucket so the worst case as far as traversing through this link list is you're looking for something that's not there cuz then you got to look at every single element in the list and fall off the end of the list before you can conclude that they look up has failed of course insertion as we discussed is always constant time deletion and successful searches well you might get lucky and stop early before you hit the end of the list so all we're going to do is bother to analyze unsuccessful lookups that will carry over to all of the other operations so a little more precisely Let's uh let s be the data set this is the objects that we're storing in our hash table and remember that to get constant time look up it certainly needs to be the case that the load is constant so we're assuming that the size of s is Big O of the number of buckets n and let's suppose we're searching for some other object which is not an S call it X and again I want to emphasize we're making no assumptions about what this data set s is other than that the size is comparable to the number of buckets so conceptually doing a lookup in a hash table with chaining is a very simple thing you just hash to the appropriate bucket and then you scan through the link list in that bucket so conceptually it's very easy to write down what the running time of this unsuccessful lookup is it's got two parts so the first thing you do is you evaluate the hash function to figure out the right bucket and again remember we're assuming that we have a simple enough hash function that this takes constant time now of course the magic of hash functions is that given this hash value we can zip right to where the linked list is to search for X using random access into our array of buckets so we go straight to the appropriate place in our array of buckets and we just search through the list ultimately failing to find what we're looking for X traversing a linked list as you all know takes time proportional to the length of the list so we find something we discussed informally in the past which is that the running time of hashtable operations implemented with chaining is governed by the list lengths so that's really the key quantity we have to understand so let's call this let's give this a name capital L it's important enough to give a name so what I want you to appreciate at this point is that this key quantity capital l the list of the length in X's bucket is a random variable it is a function of which hash function little H we wind up selecting in real time so for some choices of our hash function little H this list length might be as small as zero but for other choices of this hash function H this L list length could be bigger so this is exactly analogous to in quick sort where depending on the realtime decision of which random pivot element you use you're going to get a different number of comparisons a different running time so the list length and hence the time for the unsuccessful search depends on the hash function little H which we're choosing at random so let's recall what it is we're trying to prove we're trying to prove an upper bound on the running time of an unsuccessful lookup on average where the on average is over the choice of the hash function little H we've expressed this lookup time in terms of the average list length in X's bucket where again the average is over the random choice of H summarizing we've reduced what we care about expected time for lookup to understanding the expected value of this random variable capital L the average list length in X's bucket so that's what we got to do we got to compute the expected value of this random variable capital L now to do that I want to jog your memory of a general technique for analyzing expectations which you haven't seen in a while the last time we saw it I believe was when we were doing the analysis of randomized quick sort and Counting its comparisons so here's a general decomposition principle which we're now going to use in exactly the same way as we did in quicksort here to analyze the performance of hashing with chaining so this is where you want to understand the EXP expectation of a random variable uh which is complicated but which you can express as the sum of much simpler random variables ideally Z1 or indicator random variables so the first step is to figure out a random variable capital Y is what I'm calling it here that you really care about now we finished the last slide completing step one what we really care about is capital L the list length uh in X's bucket so that governs the running time of an unsuccessful lookup clearly that's what we really care about step two of the decomposition principle is well you know the random variable you care about is complicated hard to analyze directly but decompose it as a sum of 01 indicator random variables so that's what we're going to do at the beginning of the next slide why is it useful to decompose a complicated random variable into the sum of 01 random variables well then you're in the Wheelhouse of linearity of expectation you get that the expected value of the random variable that you care about is just the sum of the expected values of the different indicator random variables and those in expectations are generally much easier to understand and that will again be the case here in this uh theorem about the performance of hash tables with chaining so let's apply this three-step decomposition principle to complete the proof of the Carter Wegman theorem so for the record let me just remind you about the random variable that we actually really care about that's capital L the reason that's a random variable is it because it depends on the choice of the hash function little H this number could vary between zero and something much much bigger than zero depending on which H you choose So This is complicated hard to analyze directly so let's try to express it as the sum of 01 random variables so here the 01 random variables that are going to be the constituents of capital L we're going to have one such variable for each object y in the data set now remember this is an unsuccessful search X is not in the data set capital S so if Y is in the data set X and Y are necessarily different and we will Define each random variable Z suby as follows we'll Define it as one if y collides with X under h and zero otherwise so for a given zy we have fixed objects X and Y so X is some IP address say Y is some distinct IP address X is not in our hash table Y is in our hash table and now depending on which hash function we wind up using these two distinct IP addresses may or may not get mapped to the same bucket of our hash table so this indicator random variables just indicating whether or not they Collide whether or not we unluckily choose a hash function little AG that sends these distinct IP addresses X and Y to exactly the same bucket okay so that's zy clearly by definition it's a Z1 random variable now here's what's cool about these random variables is that capital L the list length that we care about decomposes precisely into the sum of the Z Y that is we can write capital L as being equal to the sum over the objects in the hash table of zy so if you think about it this equation is always true no matter what the hash function H is right so if you choose some hash function that Maps this IP address X to say bucket number 17 capital L is just counting how many other objects in the hash table wound up getting mapped to bucket number 17 so maybe 10 different objects got mapped to bucket number 17 those are exactly the 10 different values of Y that will have their zy equal to one right so so L is just counting the number of objects in the data set s that collide with X a given zy is just counting whether or not a given object y in the hash table is colliding with X so summing over all the possible things that could collide with X summing over the zy we of course get the total number of things that collide with X which is exactly equal to the number the population of x's bucket in the hash table all right so we've got all of our ducks lined up in a row now if we just remember all of the things we have going for us we can just follow our nose and nail the proof of this theorem so what is it that we have going for us well in addition to this decomposition of the list length into indicator random variables we've got linearity of expectation going for us we've got the fact that our hash function is drawn from a universal family going for us and we've got the fact that we've chosen the number of buckets n to be comparable to the data set size so we want to use all of those assumptions and finish the proof that the expected performance is constant so we're going to have a few inequalities and we're going to begin with the thing that we really care about we care about the average list length in X's bucket remember we saw on the previous slide this is what governs the expected performance of the lookup if we can prove that the expected value of capital L is constant we're done we finished the theorem so the whole point of this decomposition principle is to apply linearity of expectation which says that the expected value of a sum of random variables equals the sum of the expected values so because L can be expressed as the sum of these zys we can reverse the summation in the expectation and we can first sum over the Y's over the objects in the hash table uh and then take the expected value of zy now something which came up in our quick sort analysis but what you might have forgotten is that Z1 random variables have particularly simple expectations so the next Quiz is just going to jog your memory about why 01 random variables are so nice in this context okay the answer to this quiz is the third one the expected value of zy is simply the probability that X and Y Collide that just follows from the definition of the random variable zy and the definition of expectation namely recall how do we Define zy this just is one if this object y in the hash table happens to collide with the object X that we're looking for under the hash function X and zero otherwise again this will be this will be one for some hash functions and zero for other hash functions and then we just have to compute expectations so one way to compute the expected value of a 01 random variable is you just say well you know there are the cases where the random variable evaluates to zero and then there's the cases where the random variable evaluates to one and of course we can cancel the zero so this just equals the probability that zy equals 1 and since zy being 1 is exactly the same thing as X and Y by colliding that's what gives us the answer okay so just the probability that X collides with Y so plugging that into our derivation now we again have the sum over all the objects Y in our hash table and instead of the expected value of zy let's write that in the more interpretable form the probability that this particular object in the hash table y collides with the thing we're looking for X now we know something pretty cool about the probability that a given of distinct elements like X and Y collide with each other what is it that we know okay so I hope you answered the second response to this quiz uh this is really in some sense the key point of the analysis this is the role that being a universal family of hash functions plays in this performance guarantee what does it mean to be Universal it means for any pair of objects distinct like X and Y in that proof If you make a random choice of a hash function the probability of collision is as good as with perfectly random hash hashing namely at most one over n where n is the number of buckets so now I can return to the derivation what that quiz reminds you is that the definition of a universal family of hash functions guarantees that this probability for each y is a most one over n where n is the number of buckets in the hash table so let's just rewrite that so we've upper bounded the expected list length by a sum over the objects in the data set of 1/ N and this of course is just equal to the number of objects in the data set the cardinality of s divided by n and what is this this is simply the load this is the definition of the load Alpha which we're assuming is constant remember that was the third caveat in the theorem so that's why as long as you have a hash function which you can compute quickly in constant time and as long as you keep the load control so the number of buckets is commensurate with the size of the data set that you're storing that's why Universal hash functions in a hash table with chaining guarantee expected constant Time Performance