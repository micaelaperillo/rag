so now we come to one of my favorite sequence of lectures where we get to discuss the famous quicksort algorithm if you ask professional computer scientists and professional programmers to drop a list of their top five top 10 favorite algorithms I'll bet you'd see quick sort on many of those uh those people's lists so why is that after all we've already discussed sorting we already have a quite good and practical uh sorting algorithm namely the merge sort algorithm well quick sort in addition to being very practical it's competitive with and often Superior to merge sort uh so in addition to being very practical and used all the time in the real world and in programming libraries uh it's just a extremely elegant algorithm when you see the code it's just so succinct uh it's so elegant you just sort of wish you had come up with it yourself moreover the mathematical analysis which explains why quick sort runs so fast and that mathematical analysis will cover in detail uh it's very slick so it's something I can cover in just about half an hour or so so more precisely what we'll prove about the quicks swort algorithm is that a suitable randomized implementation runs in time n log n on average and I'll tell you exactly what I mean by on average later on in the sequence of lectures and moreover the constants hidden in the Big O notation are extremely small and that'll be evident from the analysis that we do finally and this is one thing that differentiates quick sort from the merge sort algorithm is it operates in place that is it needs very little additional storage Beyond what's given in the input array in order to accomplish the goal of sorting essentially what quick swort does is just repeated swaps within the space of the input array until it finally concludes with a sorted version of the given array the final thing I want to mention on this first slide is that unlike most of the videos this set of videos will actually have an accompanying set of lecture notes which I've posted on in PDF uh from the course website those are largely uh redundant they're optional but if you want another treatment of what I'm going to discuss a written treatment I encourage you to look at the lecture notes uh on the course website so for the rest of this video I'm going to give you an overview of the ingredients of quick sort and what we have to discuss in detail and the rest of the lectures will give details of the implementation as well as the mathematical analysis so let's begin by recalling the Sorting problem this is exactly the same problem we discussed back when we covered merge sort so we're given as input an array of n numbers in arbitrary order so for example perhaps the input looks like this array here and then what do we got to do we just got to Output a version of these same numbers but in increasing order like when we discuss merge sort I'm going to make a simplifying assumption just to keep the lectures as simple as possible namely I'm going to assume the input array has no duplicates that is all of the entries are distinct and like with merge short I encourage you to think about how you would uh alter the implementation of quick sort so that it deals correctly with ti with duplicate entries to discuss how quick sort works at a high level I need to introduce you to the key sub routine and this is really the key great idea in quick sort which is to use a sub routine which partitions an array around a pivot element so what does this mean well the first thing you got to do is you got to pick one element in your array to act as a pivot element now eventually we'll worry quite a bit about exactly how we choose this magical pivot element but for now you can just think of it that we pluck out the very first element in the array to act as the pivot so for example in the input array that I mentioned uh on the previous slide we could just use three as the pivot element after you've chosen a pivot element you then rearrange the array and you rearrange it so that every all the elements which come to the left of the pivot element are less than the pivot and all the elements which come after the pivot element are greater than the pivot so for example given this input array one legitimate way to rearrange it so that this holds is the following perhaps in the first two uh entries we have the two and the one then comes the pivot element and then comes the elements 4 through 8 in some perhaps jumbled order so notice that the elements to the left of the pivot the two and the one are indeed less than the pivot which is three and the five elements to the right of the pivot to the right of the three are indeed all greater than three notice in the partitioning subroutine we do not insist that we get the relative order correct amongst those elements less than the pivot or amongst those elements bigger than the pivot so in some sense we're doing some kind of partial sorting we're just bucketing the elements of the array into one bucket those less than the pivot and then a second bucket those bigger than the pivot and we don't care about uh getting right the order amongst each within each of those two buckets so partitioning is certainly a more modest goal than sorting but it does make progress towards sorting in particular the pivot element itself winds up in its rightful position that is the pivot element winds up where it should be in the final sorted version of the array you'll notice in the example we chose as the pivot the third largest element and it does indeed wind up in the third position of the array so more generally where should the pivot be in the final sorted version well it should be to the right of everything less than it it should be to the left of everything bigger than it and that's exactly what partitioning does by definition so why is it such a good idea to have a partitioning sub routine after all we don't really care about partitioning what we want to do is sort well the point is that partitioning can be done quickly it can be done in linear time and it's a way of making progress toward having a sorted version of an array and it's going to enable a divide and conquer approach toward sorting the input array so in a little bit more detail let me tell you about two cool facts about a partition sub routine I'm not going to give you the code for partitioning here I'm going to give it to you on the next video but here are the two Salient properties of the partition Ser routine discussed in detail in the next video so the first cool fact is that it can be implemented in linear that is Big O of n time where n is the size of the input array and moreover not just linear time but linear time with essentially no extra overhead so we're going to give a linear time implementation where all you do is repeated swaps you not allocate any additional memory and that's key to the uh practical performance of the quick sword algorithm secondly it cuts down the problem size so it enables a divide and conquer approach namely after we've partitioned an array around some pivot elements all we have to do is recursively sort the elements that lie on the left of the pivot and recursively sort the elements that lie on the right of the pivot and then we'll be done so so that leads us to the highle description of the quicksort algorithm before I give the highle description I should mention that this uh algorithm was discovered by uh Tony roughly 1961 or so this was at the very beginning of wh's career he was just about 26 27 years old uh he went on to do a lot of other contributions and eventually wind up winning the highest honor in computer science the ACM Turing award uh in 1980 and when you see this code uh I'll bet you feel like you wish you would come up with this yourself it's hard not to be envious of the inventor of this very elegant quicksort algorithm so just like in merge short this is going to be a divide and conquer algorithm so it takes an array of some length n and uh if it's an array of length n it's already sorted and that's the base case and we can return otherwise we're going to have two recursive calls the big difference from mer short is that whereas in merge short we first split the array into two pieces recurse and then combine the results here the recursive calls come last so the first thing we're going to do is choose a pivot element then partition the array around that pivot element and then do two recursive calls and then we'll be done there will be no combined step no merge step so in the general case the first thing you do is choose a pivot element for the moment I'm going to lose leave the choose pivot sub routine unimplemented there's going to be an interesting discussion about exactly how you should do this for now you just do it in some way that for somehow you come up with one pivot element for example a naive way would be to just choose the first element then you invoke the partition sub routine that we discussed in the last couple slides so weall that results in a version of the array in which the pivot element p is in its rightful position everything to the left of p is less than P everything to the right of the pivot is bigger than the pivot and now all I have to do to finish up is recurse on both sides so let's call the elements less than P the first part of the partitioned array and the elements greater than P the second part of the recursive array and now we just call quick sort again to recursively sort the first part and then to recursively sort the second part and that is it that is the entire quick sort algorithm at a high level this is one of the relatively rare recursive or divide and conquer algorithms that you're going to see where you literally do no work after solving the sub problems there is no combined step no merg step once you've partitioned you just sort the two sides and you're done so that's the high level description of the quick swort algorithm let me give you a quick tour of what the rest of the videos are going to be about uh so first of all I owe you details on this partition sub routine I promised you it could be implemented in linear time with no additional memory so I'll show you an implmentation of that on the next video uh we'll have a short video that formally proves correctness of the quick swort algorithm uh I think most of you will kind of see intuitively why it's correct so that's a video you can skip if You' want but if you do want to see what a formal proof of correctness for a divide and conquer algorithm looks like you might want to check out that video then we'll be discussing exactly how the pivot is chosen it turns out the running time of quicksort Depends depends on what pivot you choose so we're going to have to think carefully about that then we'll introduce randomized quick sort which is where you choose a pivot element uniformally at random from the given array hoping that a random pivot is going to be pretty good sufficiently often uh and then we'll give the mathematical analysis in three parts we'll prove that the quick swort algorithm runs an N log n time with small constants on average for a randomly chosen pivot uh in the first analysis video I'll introduce a general decomposition principle of how you take a complicated random variable break it into indicate a random variables and use linearity of expectation to get a relatively simple analysis that's something we'll use a couple more times in the course for example when we study hashing uh then we'll discuss sort of the key Insight behind the quick sort analysis which is about understanding the probability that a given pair of elements gets compared at some point in the algorithm that'll be the second part then there's going to be some mathematical computations just to sort of tie everything together and that'll give us the bound on the quick sore running time another video that's available is a review of some basic probabil ility concepts for those of you that are Rusty uh that we'll be using in the analysis of quick sort okay so that's it for the overview let's move on to the details