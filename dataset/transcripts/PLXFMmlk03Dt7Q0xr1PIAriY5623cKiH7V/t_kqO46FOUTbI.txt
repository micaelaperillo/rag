so let's review the stories so far we've been discussing the quicksort algorithm here again is its high-level description so quicksort you call two subroutines first and then you make two recursive calls so the first subroutine choose pivot we haven't discussed yet at all that'll be one of the main topics of this video but the job of the choose trivet subroutine is to somehow select one of the n elements in the input array to act as a pivot element now what does it mean to be a pivot well that comes into play in the second subroutine the partition subroutine which we did discuss quite a bit in a previous video so a partition does is it rearranges the elements in the input array so that it has the following property so that the pivot P winds up in its rightful position that is its to the right of all of the elements less than it and it's to the left of all of the elements bigger than it the stuff less than its to the left in some jumbled order the stuff bigger than its to the right in some jumbled order that's what's listed here as the first part in the second part of the partitioned array now once you've done this partitioning you're good to go you just recursively solve you recursively sort the first part I think exam in the right order you call quicksort again to recursively sort the right part and bingo the entire array is sorted you don't need a combined step you don't need a root step moreover recall in a previous video we saw that the practitioner rate can be implemented in linear time and moreover it works in place with essentially no additional storage we also in an optional video formally prove the correctness of quicksort and remember quicksort is independent of how you implement the choose pivot subroutine so what we're going to do now is discuss the running time of the quicksort algorithm and this is where the choice of the pivot is very important so what everybody should be wondering about at this point is is quicksort of good algorithm does it run fast the bar is pretty high we already have merge sort which is a very excellent practical n log n algorithm the key point to realize that this juncture is that we are not currently in a position to discuss the running time of the quicksort algorithm the reason is we do not have enough information the running time of quicksort depends crucially on how you choose the pivot it depends crucially on the quality of the pivot chosen you'd be right to wonder what I mean by a pivots quality and basically what I mean is a pivot is good if it splits the British and array into roughly two equal sized sub problems and it's bad it's of low quality if we get very unbalanced of problems so to understand both what I mean in the ramifications of having good quality and bad quality pivots let's walk through a couple of quiz questions this first quick quiz question is meant to explore a sort of worst case execution of the quicksort algorithm what happens when you choose pivots that are very poorly suited for the particular input array may be more specific suppose we use the most naive choose pivot implementation like we were discussing in the partition video so remember here we just pluck out the first element of the array and we use that as the pivot so suppose that's how we implement the choose Tibbett subroutine and moreover suppose that the input array to quicksort is an array that's already in sorted order so for example if they just have the numbers 1 through 8 it would be 1 2 3 4 5 6 7 8 in order my question for you is what is the running time of this recursive quicksort algorithm on an already sorted array if we always use the first element of a sub array as the pivot okay so this is a slightly tricky but actually a very important question so the answer is the fourth one so it turns out the quicksort if you pass it an already sorted array and you're using the first element as pivot elements it runs in quadratic time and remember for a sorting algorithm quadratic is bad it's bad in the sense that we can do better merge sort runs in time n log n which is much better than N squared and if we are happy with an N squared running time we wouldn't have to resort to these sort of relatively exotic sorting algorithms we could just use insertion sort and we'd be fine we'd get that same quadratic running time okay so now I owe you an explanation why is it that quicksort can actually run in quadratic time in this unlucky case of being passed as already sorted input array well to understand let's think about what pivot gets chosen and what are the ramifications of that pivot choice for how the array gets partitioned and then what the recursion looks like so let's just think of the array as being the numbers 1 through n in sorted order what is going to be our pivot well by definition we're choosing the first element of the pivot so the pivots is just going to be 1 now we're going to invoke the partition subroutine and if you go back to the pseudocode of the partition subroutine you'll notice that if we pass it an already sorted array it's going to do essentially nothing because it's just going to advance the index J until it falls off the end of the array it's just going to return back to us the same array that it was passed as input so partition subroutine if given an already sorted away returns and already sorted array and so we have just the pivot one in the first position and then the number is 2 through n in order and the remainder of the positions so if we draw our usual picture of what a partitioned array looks like with everything less than the pivot to the left everything bigger than the pivot to the right well since nothing is less than the pivot this stuff is going to be empty this will not exist and to the right of the pivot this will have length n minus 1 and moreover it will still be sorted so once partition completes we go back to the outer call of quicksort which then calls itself recursively twice now in this case one of the recursive calls is just vacuous there's just an empty array there's nothing to do so really there's only one recursive call and that happens on a problem of size only one less so this is about the most unbalanced split we could possibly see right where one side has zero elements one size n minus 1 so let's don't really get any worse than that and this is going to keep happening over and over and over again we're going to recurse on the numbers 2 through n we're going to choose the first element the two as the pivot again we'll feed it to partition we'll get back the exact same sub-array that we handed it in we get through the numbers 2 through n in sorted order we exclude the pivot - we recurse on the numbers 3 through n a sub array of length n minus 2 the next recursion level we recurse on an array of size of length n minus 3 that n minus 4 that n minus 5 and so on until finally after added recursion depth of n roughly we got down to just the last element in the base case kicks in and we return that in quicksort completes so that's how quicksort is going to execute on this particular input with these particular pivot choices so what running time does that give to us well the first observation is that you know in each recursive call we do have to invoke the partition subroutine and the prediction subroutine does look at every element in the array it is passed as input so if we pass partition an array of length K it's going to do at least K operations because it looks at each element at least once so the run time is going to be bounded below by the work we do in the outermost call which is on an array of length n plus the amount we do in the second level of recursion which is on a sub array of length n minus 1 plus n minus 2 plus blah blah blah blah blah all the way down to plus 1 for the very last level of recursion so this is a lower bound on our running time and this is already theta of N squared so one easy way to see why this some n plus n minus 1 plus etc etc leads to a bound of N squared is to just focus on the first half of the terms so the first n over two terms in the sum are all of magnitude at least n over 2 so the sum is at least n squared over four it's also evident that this sum is at most n squared so overall the running time of quicksort on this bad input is going to be quadratic now having understood what the worst-case performance the quicksort algorithm is let's move on to discuss its best-case running time now we don't generally care about the best-case performance of algorithms for its own sake the reason that we want to think about quicksort in the best case first of all it will give us better intuition for how the algorithm works second of all it will draw a line in the sand its average case running time certainly can't be better than the best case so this will give us a our good for what we're shooting for in our subsequent mathematical analysis so what was the best case what is the highest quality pivot we could hope for well again we think of the quality of a pivot as the amount of balance that it provides between the two sub problems so ideally we choose a pivot which gave us two sub problems both of sus and over two or less and there's a name for the element that would give us that perfectly balanced split it's the median element of the array can the element where exactly half of the elements are less than it and half the elements are bigger than that would give us a essentially perfect 50/50 split of the input array so here's the question suppose we had some input and rear and put sword and everything just worked out in our favor in the magically in the best possible way that is in every single recursive invocation of quicksort on any sub array of the original input array suppose we happen to get as our pivots the median element that is suppose in every single recursive call we wind up getting a perfect 50/50 split of the input array before we recurse this question asks you to analyze the running time of this algorithm in this magical best-case scenario so the answer to this third to this question is the third option the answer is it runs in n log n time why is that well the reason is that then the recurrence which governs the running time of quicksort is exactly matches the recurrence that governs the merge short running time which we already know is n log n that is the running time quicksort requires in this magical special case on an array of length n well as usual you have a recurrence in two parts there's the work that gets done by the recursive calls and there's the work that gets done now now by assumption we wind up picking the median as the pivot so there's going to be two recursive calls each of which will be on an input of size at most n over two and we can write this this is because the pivot equals the median so this is not true for quick sort of general it's only true in this magical case where the pivot is the median so that's what gets done by the two recursive calls and then how much work do we do outside of the recursive calls well we have to do the truth pivot subroutine and I guess strictly speaking I haven't said how that was implemented but let's assume they choose pivot does only a linear amount of work and then as we've seen the partition subroutine only does a linear amount of work as well so let's say oh then for work outside of the recursive calls and what do we know we know this implies say by using the master method or just by using the exact same argument as for merge sort this gives us a running time bound of n log N and then again something I hadn't written emphasizing that which is true is that actually we can write theta of n log N and that's because in the recurrence in fact we know that the work done outside of the recursive calls is exactly theta of n ok partition needs really linear time not just Big O of n time in fact the work done outside of the recursive calls is theta of n that's because the partition subroutine does indeed look at every entry in the array that it passed and as a result we didn't really discuss this so much in the master method but as I mentioned in passing if you have recurrences which are tight in this sense and then the results of the master method can also be strengthened to be theta instead of just big out but those are just some extra details the upshot of this quiz is that even in the best case even if we magically get perfect pivots throughout the entire trajectory of quicksort the best we can hope for is an N log n upper bound it's not going to get any better than that so the question is how do we have a principled way of choosing pivots so that we get this best case or something like it this best case n log n running time so that's what the problem that we have to solve next so the last couple quizzes have identified a super important question as far as implementation of quicksort which is how are we going to choose these pivots right we now know that they have a big influence on the running time of our algorithm it could be as bad as n squared or as good as n log N and we really want to be on the n log inside so the key question how to choose pivots and quicksort is the first killer application we're going to see of the idea of randomized algorithms that is allowing your algorithms to flip coins in the code so that you get some kind of good performance on average so the big idea is random pivots by which I mean for every time we recursively call quicksort and we are passed some sub array of length K among the K candidate pivot elements in the sub array we're going to choose each one with equally likely with probability of 1 over K and we're going to make a new random choice every time we have a recursive call and then we're just going to see how the algorithm does so this is our first example of a randomized algorithm this is an algorithm where if you feed it exactly the same input it will actually run differently on different executions and that's because there's randomness internal to the code of the algorithm now it's not necessarily intuitive that randomization should have any purpose in computation and software design and algorithm design but in fact this has been sort of one of the real breakthroughs in algorithm design mostly in the 70s realizing how important this is that the use of randomization can make algorithms more elegant simpler easier to code more faster or just simply you can solve problems that you could not solve or at least not solve as easily without the use of randomization so it's really one thing that should be in your toolbox as an algorithm designer randomization puts sort of be the first counter killer application but we'll see a couple more later in the course now by the end of the sequence of videos all i've given you a complete rigorous argument about why this works why with random pivots quicksort always runs very quickly on average but you know before moving in descent anything too formal let's develop a little bit of intuition or at least kind of a daydream about why on earth could this possibly work wine earth could this possibly be a good idea to have randomness internal to our quicksort implementation well so first just you know very high-level what would be sort of the hope or the dream the hope would be you know random pivots are not going to be perfect I mean you're not going to just sort of guess the median or you have only have one an in chance of figuring out which one the median is but the hope is that most choices of a pivot will be good enough so that's pretty fuzzy let's drill down a little bit and develop this intuition further let me describe it in two steps the first claim is that you know in our last quiz we said suppose we get lucky and we always pick the median in every single recursive call and we observed we do great we could n log n running time so now let's observe that actually to get the n log n running time it's not important that we magically get the median every single recursive call if we get any kind of reasonable pivot by which a pivot that gives us some kind of approximately balanced split of the problems again we're going to be good okay so the last quiz really wasn't particularly getting the exact median near medians are also fine to be concrete suppose we always pick a pivot which guarantees us a split of 2575 or better that is both recursive calls should be called on a raise of size and most 75% of the one that we started with so precisely if we always get a 25 75 split or better in every recursive call I claim that the running time of quicksort in that event will be Big O of n log n just like it was in the last quiz where we're actually assuming something much stronger that we're getting the median now this is not so obvious the fact that 2575 splits in guarantee and log n running time for those of you that are feeling keen you might want to try to prove this you can prove this using a recursion tree argument but because you don't have balanced sub problems you have to work a little bit harder than you do in the cases covered by the master method so that's the first part of the intuition and this is what we mean by a pitted being good enough if we get a 25 75 split or better we're good to go we get our desired at our target in log n running time so the second part of the intuition is to realize that actually we don't have to get all that lucky to just be getting a 25/75 split that's actually a pretty modest goal and even this modest goal is enough to get the N log on running time right so suppose our array contains the numbers the integers between 1 and so it's an array of length 100 think for a second which of those elements is going to give us a split that's 25 75 or better so if we pick any element between 26 and 75 inclusive will be totally good right if we pick something that's at least 26 that means the left sub-problem is going to have at least the elements 1 through 25 that'll give them have at least 25% the elements if we pick something less than 75 then the right subproblem will have all of the elements 76 through 100 after we partition so that'll also have at least 25% of the elements so anything between 26 and 75 gives us a 75 25 split or better but that's a full half of the elements so it's as good as just flipping what's fair coin and hoping we get heads so with 50 percent probability we get to split good enough to get the good enough to get this n log n bound and so again the high level hope is that often enough you know half of the time we get these you know good enough splits 25 selling high splits are better so that would seem to suggest an n log n running time on average is a legitimate hope so that's the high-level intuition but if I were you I would certainly not be content with this somewhat hand wavy explanation that I've given you so far when I've told you sort of the hope the dream why there is at least a chance this might work but but the question remains and I would encourage such skepticism which is does this really work and to answer that we're going to have to do some actual mathematical analysis and that's we're going to show you I'm going to show you a complete rigorous analysis of the quicksort algorithm with random pivots and what so that will show that yes in fact it does really work and this highlights what's going to be a recurring theme in this course and a recurring theme just in the study and understanding of algorithms which is it's quite often there's some fundamental problem when you're trying to code up a solution and you come up with a novel idea it might be brilliant and it might suck and you have no idea now obviously you can code up the idea run it on some concrete instances and get a feel for you know whether it seems like a good idea or not but if you really want to know fundamentally what makes the idea good or what makes good idea bad really you need to turn to math to give you a complete explanation and that's exactly what we're going to do with quicksort and it will explain a very deep way why it works so well specifically in the next sequence of three videos I'm going to show you an analysis a proof of the following theorem about quicksort so under no assumptions about the data that is for every input array of a given length say n the average running time of quicksort implemented with random pivots is Big O of n log n and again in fact it's theta of n log n but we'll just focus on the Big O of n log n part so this is a very very cool theorem about this randomized quicksort algorithm one thing I want to be clear so that you don't undersell this guarantee in your own mind this is a worst-case guarantee with respect to the input okay so notice at the beginning of this theorem what do we say for every input array of length n all right so we have absolutely no assumptions about the data this is a totally general purpose sorting subroutine which you can use whenever you want even if you have no idea where the data is coming from and these guarantees are still going to be true this of course is something I held forth about at some length back in our guiding principles video when I argued that if you can get away with it what you really want is general purpose algorithms which make no data assumption so they can be used over and over again in all kinds of different contexts and that still have great guarantees and quicksort is one of those so basically if you have a data set and it fits in the main memory of your machine again sorting is a forfree subroutine in particular quicksort the quicksort implementation is for free so this just runs so blazingly fast doesn't matter what the array is maybe you don't even know why you want to sort it but go ahead why not maybe it'll make your life easier like it did for example in the closest pair algorithm for those of you that watch those two optional videos now the word average does appear in this theorem and you know as I've been harping on this average is not over any assumptions on the data we're certainly not assuming that the input array is random in any sense the input array can be anything so where is the averaging coming from the averaging is coming only from randomness which is internal to our algorithm randomness that we put in the code ourselves that we are responsible for so remember randomized algorithms have the interesting property that even if you run it on the same input over and over again you're going to get different executions so the running time of a randomized algorithm depend you know can vary as you run it on the same input over and over again the quizzes have taught us that the running time of quicksort on the given input fluctuates from anywhere between the best case of n log n to the worst case of N squared so what this theorem is telling us is that for every possible input array while the running time does indeed fluctuate between an upper bound of n squared and the lower bound of n log n the best case is dominating on average it's n log n on average it's almost as good as the best case that's what's so amazing about quicksort that was n a square that can pop up once in a while has doesn't matter you're never going to see it you're always going to see this n log n like behavior and randomized quicksort so for some of you I'll see you next in a video on probability review that's optional for the rest of you I'll see you in the analysis of this theorem