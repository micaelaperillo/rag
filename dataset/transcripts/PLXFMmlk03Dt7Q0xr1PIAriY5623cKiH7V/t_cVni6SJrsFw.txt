let's complete the proof of the master method let me remind you about the story so far the first thing we did is we analyzed the work done by a recursive algorithm using a recursion tree so we zoomed in in a give a level J we identified the total amount of work done at level J and then we summed up over all of the levels resulting in this rather intimidating expression star CN the D times a sum over the levels J from0 to log Bas B of n of quantity A over B the D raised to the J having derived this expression start we then spent some time interpreting itach attaching to it some semantics and we realize that the role of this ratio A over B to the D is to distinguish between three fundamentally different types of recursion trees those in which a equals B to the D and therefore the amount of work is the same at every level those in which a is less than b to the D and therefore the amount of work is going down with the level and those in which a is bigger than b to the D in which case the amount of work is growing with the level this gave us intuition about the three cases of the master method method and even even gave us predictions for the kind of running times that we might see so what remains to do is really turn this hopeful intuition into a rigorous proof so we need to verify that in fact the simplest possible scenarios outlined in the previous video actually occur in addition we need to demystify the third case and understand what the expression has to do with the number of leaves of the recursion tree let's begin with the simplest case which is case one recall in case one we're assuming that a equals B the D this is the case where we have a perfect equilibrium between the forces of Good and Evil where the rate of sub problem proliferation exactly cancels out with the rate at which we do less work per sub problem and now examining the expression star we can see how easy Our Lives get when a equals B to the D in that case this ratio is equal to one so naturally this ratio raised to the J is also equal to one for all J and then of course this sum evaluates to something very simple namely one summed with itself log base B of n + 1 * so the sum simply equals log base B of n + one and that's going to get multiplied by the C end of the D term which is independent of the sum so summarizing when a equals b d we find that star equals CN the D time log base B of n + 1 writing this in Big O notation we would write Big O of n the D log n and again I'm going to suppress the base of the logarithm since all logarithms differ only by a constant Factor uh we don't have to specify the base that's just suppressed by the constant hidden in the bigo notation so that's it for case one like I said this is the easy case so what do we do when a is not equal to B to the D and remember a can be either less than or bigger than b to the D to answer that question let's take a short detour into geometric series for this single slide detour we're going to think about a single constant number R now what you want to think about is R representing that ratio A over B the D from the previous slide but for this slide only let's just call it R this is a constant it's bigger than zero and it's not equal to one now suppose we sum up powers of R stopping let's say at the K power of R I claim that this sum has a nice closed form formula specifically Al it is exactly r k +1 -1 / R -1 now whenever you see a general formula like this it's useful to keep in mind a couple of canonical values of the parameters that you can plug in to develop intuition and for this expression you might want to think canonically about the cases r equal 2 and Ral 1/2 so when r equal 2 we're summing up powers of 2 1 + 2 + 4 + 8 + 16 and so on when R is a half we're summing up 1 plus a half plus a/4 plus an e and so on now I'm not going to prove this for you I'd like you to prove this yourself if you don't already know this fact so the way to prove this is simply by induction and I will leave this as an exercise what I want to focus on instead is what this fact can do for us the way we'll use this fact is to formalize the idea that in recursion trees where the amount of work is increasing in the levels the leaves dominate the overall running time and where in recursion trees where the amount of work is decreasing in the level the root dominates the running time in the sense that we can ignore all of the other levels of the recursion tree so in the notation in this slide we have two upshots first of all for the case when R is less than one and in this case this expression on the right hand side r k + 1 - 1/ Rus one can be upper bounded by 1 over 1 - R so again remember you might want to have a canonical value of R in mind here namely 1/2 so what we're claiming here is that the right hand side is no more than two for the the case of r equal 1/2 and that's easy to see if you think about 1 + 1/2 plus a/4 plus 1/8 and so on that sum is converging to two as K grows large so in general for R less than one a constant the sum is bounded by 1 - 1 / R now we're not actually going to care about this Formula 1 - 1 / R the point for us is just that this is a constant and by constant I mean independent of K independent of how many terms we sum up obviously it depends on R the ratio but it does not depend on how many things we sum up on K so the way to think about this is when we sum up a bunch of terms where R is less than one then the very first term dominates the first term is equal to one and no matter how many terms we sum up we never get grow bigger than some constant a similar situation holds for the case where R is a constant bigger than one when R is bigger than one a tiny bit of algebra shows that we can upper bound the right hand side by R the K times something which is constant independent of K so again let's interpret the second upshot in terms of a canonical value of R namely Ral 2 then our sum is 1 + 2 + 4 + 8 + 16 and so on and what this is saying is that no matter how many terms we sum up the overall sum is never going to be more than twice the largest and final term so if we sum up to say 128 the sum you'll notice will be 255 which is at most twice that largest term 128 and that same is true for any K the entire sum is no more than twice that of the largest term in this sense the largest term of the series dominates the whole thing so to summarize this slide in just one sentence when we sum up powers of a constant R when R is bigger than one the largest power of that constant dominates the sum when R is smaller than one then the sum is just a constant let's now apply this to prove case two of the master method in case two of the master method we assume that a is less than b to the D then that is the rate at which sub problems are proliferating is drowned out by the rate at which we do less work per sub problem so this is the case where the amount of work is decreasing with each level of the recursion tree and our intuition said that well in the simplest possible scenario we might hope that all of the work up to a constant factor is being done at the root so let's make that intuition precise by using the basic sums fact on the previous slide so since a is less than b to the D this ratio is less than one so let's call this ratio equal to R so R you'll notice does depend on the three parameters a b and d but R is a constant it does not depend on N so what is this sum the sum is just we're just summing up powers of this constant R where R is less than one what did we just learn we just learned that any such sum is bounded above by a constant independent of the number of terms that you sum up so therefore what do this expression star evaluates to it EV valuates to C which is a constant time n the D time another constant so suppressing the product of these two constants in Big O notation we can say that the expression star is upper bounded by Big O of n the D and this makes precise our intuition that indeed the overall running time of the algorithm in this type of recursion tree with decreasing work per level is dominated by the root the overall amount of work is only a constant Factor larger than the work done merely at level Z of the tree let's move on to the final and most challenging part of the proof the final case in case three we assume that a is bigger than b to the D so in conceptual terms we're assuming the rate at which sub problems proliferate is exceeding the rate at which we do less work per sub problem so these are recursion trees where the amount of work is increasing with each level with the most work being done at the leaves and once again using the basic sums fact we can make precise the hope that that in fact we only have to worry about the leaves we can throw away the rest of the work losing only a constant Factor so to see that we will again denote this ratio between A and B to the D as R and in this case r is bigger than one so this sum is a sum of a bunch of powers of R where R is bigger than one what did we just learn about that two slides ago in the basic sums fact we learned that such sums are dominated by the largest and last term of the sum okay so they're bounded by a constant Factor times the largest ter term therefore we can we can simplify the expression star to the following I'm going to write it in terms of Big O notation and like on the last slide I'll use it to suppress two different constants on the one hand I'm going to be suppressing the constant C which we inherited way back when from the original recurrence and on the other hand I'm going to use it to also suppress this constant that comes from the basic sums fact so ignoring those two constants what do we have left we have n the D times the largest term of the sum so what is the largest term of the sum where it's the last one so we plug in the biggest value of J that we're ever going to see so what's the biggest value of J we're ever going to see well it's just this log base B of n so we get the ratio A over B to the D raised to the log base B of n power now don't despair how messy this looks we can do some remarkable simplifications so what I want do next is I want to focus just on this 1 over B to the D raised to the log base B of n term so that's going to be I can write that as B the minus D log base B of n which if I factor this exponent into two successive Parts I can write this as B raised to the log base B of n power and only then raised to the minus D and now of course what happens is that taking the logarithm of NBAs B followed by taking raising it to the B power those are inverse operations that cancel so that leaves us just with the N so this results in a n the minus D and now remarkably this n the minus D is just going to cancel out with this n the D leaving us with merely a raised the log base B of N and thus out of this crazy C of letters Rises a formula we can actually understand so a to the log base B of n if we step back and think for a minute is actually a supernatural quantity it describes something about the recursion trees that we already knew was supposed to pop up in the analysis I'll let I'll let you think through exactly what that is in the following quiz so the correct answer to this quiz is the fourth response a raised to the log Bas B of n is precisely the number of leaves of the recursion tree and remember in our intuition for case three recursion trees where the amount of work is increasing per level we thought that perhaps the work would be dominated by the work done at the leaves which is just proportional to the number of leaves so why is this the answer we'll just remember what recursion trees look like at level zero we have a single node and then with each level we have eight times as many nodes as before that is with each level of the recursion tree the number of nodes goes up by a factor of a how far does this how long does this process go on well it goes on till we reach down into the leaves recall that the input size starts at n up at the root it gets divided by a factor B each time and it terminates once we get down to one so the leaves reside at precisely level log base B of n so therefore the number of leaves is just the branching factor which is a raised to the number of times that we actually multiply by a which is just the number of levels which is log base B of n so each time we go down a level we increase the number of nodes by a factor of a and we go down a level log Bas B of n * leaving us with a number of leaves equal to a raised to the log Bas B of n so what we've done is we've mathematically confirmed in a very cool way our intuition about what case three should look like in the master method we've proven that in case three when a is bigger than b to the D the running time is O of the number of leaves in the recursion tree just as the intuition predicted but this leaves us with one final mystery if you go back to the statement of the master method we didn't say a to the log B B of n in case 3 it says the running time is n to the log base B of A and not only that we've used this case 3 formula over and over again to evaluate gauss's recursive algorithm for integer multiplication to evaluate st's matrix multiplication algorithm and so on so what's the story how come we're not getting the same thing as in the statement of the master method well there's a very simple explanation which is simply that believe it or not a log base B of n and n the log base B of A are exactly the same thing this looks like the kind of mistake you'd make in freshman algebra but actually if you think about it these are simply the same quantity if you don't believe me just take the logarithm base B of both sides and you'll get the same thing on both sides now you might well be wondering why I didn't just state in the master method that the running time of case three is this very sensible and meaningful expression a raised to the log base B of n I.E the number of leaves in the recursion tree well it turns out that while this expression on the left hand side is the more meaningful conceptually the right hand side end the log Bas B OFA is the easiest one to apply so recall when we worked through a bunch of examples of the master method this right hand side was super convenient when we evaluated the running times of algorithms when we plugged in the numbers of A and B in any case whether or not you want to think about the running time in case three as proportional to the number of leaves in the tree or as proportional to end of the log base B OFA we're done we've proved it that's case three that was the last one so we're done with the master method QED so that was a lot of hard work proving the master method method and I would never expect someone to be able to regurgitate all of the details uh of this proof you know it's something like a cocktail party well maybe except at the nerdiest of all cocktail parties but I do think there's a few highle conceptual points of this proof that are worth remembering in the long term so we started by just writing down a recursion tree for the recursive algorithm and in a generic way and going level by level we counted up the work done by the algorithm and this part of the proof had nothing to do with how A and B the D related to each other then we recognized that there are three fun Fally different types of recursion trees those with the same amount of work per level those where it increases with a level and those where it decreases with a level if you can remember that you can even remember what the running times of the three cases should be in the case where you do the same amount at every work at each level we know there's a logarithmic number of levels we know we do n the D work at the root so that gives us the running time in case one of n the D log n when the amount of work is decreasing with the levels we now know that the root dominates up to a constant Factor we can throw out the rest of the levels and we know end of the D work gets done at the root so that's the overall running time and in the third case where it's increasing in the levels the leaves dominate the number of leaves is a raised to the log base B of N and that's the same as n to the log base B of A and that's the proportional the running time in case three of the master method