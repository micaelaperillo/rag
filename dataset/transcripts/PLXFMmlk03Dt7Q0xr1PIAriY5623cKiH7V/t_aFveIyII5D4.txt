this optional video will be more or less the last word that we have on sorting for the purposes of this course and it'll answer the question can we do better remember that's the mantra of any good algorithm designer I've shown you in log n algorithms for sorting merge sort in the worst case quicksort on average can we do better than n log n indeed for the selection problem we saw we could do better than n log n we could do linear time maybe we can do linear time for sorting as well the purpose of this video is to explain to you why we cannot do sorting in linear time so this is a rare problem where we understand quite precisely how well it can be solved at least for a particular class of algorithms called comparison based sorts which I'll explain in a moment so here's the form of theorem I want to give you the gist of in this video so in addition to restricting the comparison based sorts which is necessary as we'll see in a second I'm going to make a second assumption which is not necessary but it's convenient for the lecture which is that I'm going to think only about deterministic algorithms for the moment I encourage you to think about why the same style of argument gives an N log n lower bound on the expected running time of any randomized algorithm maybe I'll put that on the coarse side as an optional theory problem so in particular quicksort is optimal in the randomized sense it has averaged n log n time and again the claim is that no comparison based sort can be better than that even on average so I need to tell you what I mean by a comparison based sorting algorithm what it means it's a sorting algorithm that accesses the elements of the input array only via comparisons this does not do any kind of direct manipulation on a single array element all it does is it picks pairs of elements and ask the question is the left one bigger or is the right one bigger I like to think of comparison based sorts as a general-purpose sorting routines they make no assumptions about what the data is other than that is from some totally ordered set I like to think of it really as a function that takes as an argument a function pointer that allows it to do comparisons between abstract data types there's no way to access the guts of the elements all you can do is go through this API which allows you to make comparisons and indeed if you look at the sorting routine and say the UNIX operating system that's exactly how its set up you just pass it a function pointer to a comparison operator I know this sounds super abstract so I think it becomes clear once we talk about some examples there's famous examples of comparison based sorts including everything we've discussed in the class so far there's also famous examples of non comparison based sorts which we're not going to cover but perhaps some of you have heard of or at the very least they're easy to look up on Wikipedia or wherever so examples include the - showing algorithms we use discussed so far mergesort the only way the merge sort interacts with the elements in the input array is by comparing them and by copying them similarly the only thing quicksort does with the input array elements is compare them and swap them in place for those of you that know about the heap data structure which we'll be reviewing later in the class heapsort where you just keep the file bunch of elements and then extract the minimum and times that also uses only comparisons so what are some famous non-examples I think this will make it even more clear or what we're talking about so bucket sort is one very useful one so bucket sort is used most frequently when you have some kind of distributional assumption on the data that you're sorting and remember that's exactly what I'm focusing on avoiding in this class I'm focusing on general purpose subroutines where you don't know anything about the data if you do know stuff about the data bucket sort can sometimes be a really useful method for example suppose you can model your data as iid samples from the uniform distribution on 0 1 so they're all rational numbers bigger than 0 less than 1 and you expect them to be evenly spread through that interval then what you can do in bucket sort is you can just pre-allocate in buckets where you going to collect these elements each one is going to have the same width with 1 over n the first bucket you just do linear pass through the input array everything that's between 0 & 1 over N you stick in the first bucket everything between 1 over N and two over N you stick in the second bucket 2 over N and 3 over N you stick in the third bucket and so on so with a single pass you've classified the input elements according to which bucket they belong in now because the data is assumed to be uniform at random that means you expect each of the buckets to have a very small population just a few elements in it so remember if elements are drawn uniform from the interval 0 1 then it's equally likely to be in each the N available buckets and since there's n elements that means you only expect one element per bucket so each one is gonna have a very small population having bucketed the data you can now just you say insertion sort on each bucket independently you're going to be doing insertion sort on a tiny number of elements so that'll run in constant time and then there's gonna be linear there are but gets with linear time overall so the upshot is if you're willing to make really strong assumptions about your data like it's drawn uniformly at random from the interval 0 1 then there is not an N log n lower bound in fact you can elude the lower bound and sort in linear time so just to be clear in what sense is bucket sort not comparison based and what sense does a look at the guts of its elements and do something other than access them by pairs of comparisons well it actually looks at an element in the input array and it says what is its value and the checks of its value is point 1 7 versus point through 7 versus 0.77 and according to what value it sees inside this element that makes the decision of which bucket to allocate it to so it actually stairs of the guts of an element to decide to have what to do next another non example which can be quite useful is counting sort so this sorting algorithm is good when your data again going to make an assumption on the data when they're integers and they're small integers so say they're between 0 and K where K is say ideally at most linear and n so then what you do is you do a single pass through the input array again you just bucket the elements according to what their value is it's somewhere between 0 and K and it's an integer by assumption so you need K buckets and then you do a pass and you should depopulate the buckets and copy them into an output array and that gives you a sorting algorithm which runs in time o of n plus K or K is the size of the biggest integer so the upshot with counting sort is that if you're willing to assume the datas are integers bounded above by some factor linear and n proportional to n then you can sort them in linear time again counting sort does not access the array elements merely through comparisons it actually stares at an element figures out what its value is and uses that value to determine what bucket to put the element in so in that sense it's not a comparison case sort and it can under various assumptions meet the N log N lower bound so a final example is the well known algorithm called radix sort I think if this is sort of an extension of counting sort although you don't have to use counting sort of the inner loop you can use other so-called stable sorts as well and there's a stuff you can read about in many programming books or or on the web and the upshot a great extort is you've prot you you again you assume that the data are integers you think of them in digit representations a binary representation and now you just sort one bit at a time starting from the least significant bits and going all the way out to the most significant bits and so the upshot of writing sort its extension of counting sort in the sense that if your data is integers that are not too big polynomial e bounded and n then it lets you sort in linear time so summarizing a comparison based sorting only access the input array through this API that lets you do comparisons between two elements you cannot access the value of an element so in particular you cannot do any kind of bucketing technique buckets or counting sort and radix sort all fundamentally are doing some kind of bucketing and that's why when you're willing to make assumptions about what the data is and how you are permitted to access that data that's what you can bypass in all of those cases this n log n lower bound but if you're stuck with a comparison based sorting wanna have something general-purpose you're going to be doing and log n comparisons in the worst case let's see why so we have to prove a lower bound for every single comparison based sorting method so a fixed one and let's focus on a particular input length call it in okay so now let's simplify our lives now that we're focused on a comparison based sorting method one that doesn't look at the values of the array elements just in the relative order we may as well think of the array is just containing the elements 1 2 3 all the way up to N in some jumbled order now some other algorithm could make use of the fact that everything is small integers for the comparison based sorting method cannot so there's no loss I'm just thinking about an unsorted array containing the integers one at one day and inclusive now despite seemingly restricting the space of inputs that we're thinking about even here there's kind of a lot of different inputs we got to worry about right so n elements can I can show up and in fact oriole different orderings are those n choices for who the first element is than n minus 1 choices for the second element and minus 2 choices for the third element and so on so there's n factorial so is for how these elements are in our arrange in the input array so I don't wanna prove this super formally but I want to give you the gist I think the good intuition now we're just in lower bounding the number of comparisons that this method makes in the worst case so let's introduce a parameter K which is its worst case number of comparisons that is for every input each of these in vectorial inputs by assumption this method makes no more than K comparisons the idea behind the proof is that because we have n factorial fundamentally different inputs the sorting method has to execute in a fundamentally different way on each of those inputs but since the only thing that causes a branch in the execution of the sorting method is the resolution of a comparison and we have only taken para sins it can only have 2 to the K different execution paths so that forces 2 to the K to be at least n factorial and a calculation then shows that that forces K to be at least Omega n log n so let me just quickly fill in the details so across all n factorial possible inputs just as a thought experiment we can imagine running this method n factorial times and just looking at the pattern of how did comparisons resolved right for each of these n factorial inputs we run it through the sorting method it makes comparison number 1 the comparison number 2 the comparison number 3 and a comparison number for the person number 5 and you know it cuts back to 0 then a 1 and a 1 then is 0 they've been some other input it gets back a 1 than a 1 then is 0 then a 0 and so on the point is for each of these n factorial inputs it makes up most K comparisons we can associate that with a K bit string and because there's there's only K bits we're only going to see two of the K difference K bits trims to the K different ways that a sequence of comparisons resolves now to finish the proof we are going to apply something which I don't get to use as much as I'd like an algorithms class but it's always fun when it comes up which is the pigeonhole principle the pigeonhole principle you'll recall is the essentially obvious fact that if you try to stuff k plus 1 pigeons and to just K cubbyholes one of those cubbyholes has got to get two of the pigeons ok at least one of the cubbyholes gets at least two pigeons so for us what are the pigeons and what are the holes so our pigeons are these n factorial different inputs the different ways you can scramble the integers one through and what are our holes those are the two the K different executions of the story method can possibly take on now if the number of comparisons K used is so small that 2 to the K the number of distinct executions number of distinct ways comparisons can resolve themselves is less then the number of different inputs that have to get correctly sorted then by the pigeonhole principle one cubby gets two holes that is two different inputs get treated in exactly the same way by the sorting method they are asked exactly the same K comparisons and the comparisons resolve identically okay so it's one jumbling of 1 through n and you get a 0 1 1 0 1 and it's a totally different jumbling of n and again you get a 0 1 1 0 1 and if this happens the algorithm is toast in the sense that is definitely not correct right because we've fed it to different inputs and it is unable to resolve which of the two it is right so it may be one permutation of one surrender with this totally different permutation of 1 through n the algorithm has tried to learn about what the input is through these K comparisons but it has exactly the same data about the input into the two cases so if it outputs the correct sorted version in one case it's going to get the other one wrong so you can't have a common execution of a sorting algorithm unscramble totally different permutations it cannot be done so what have we learned we've learned that by correctness 2 to the K is in fact at least in the factorial so how does that help us well on a lower bound cake here's the number of comparisons this arbitrary storing method is using we want to show that's at least n log n so we two lower bound K we better lower bound n factorial so you know you could use Stirling's approximation or something fancy but we don't need anything fancy here we'll just do something super crude will just say well look this is the product of N Things right n times n minus 1 times n minus 2 bla bla bla bla and the largest of those is the end of our two largest of those and terms are all at least n over 2 the rest we'll just ignore pretty sloppy but it gives us a lower bound of n divided by 2 raised to the N - now we'll just take log base 2 of both sides and we get the K is at least n over 2 log base 2 of n over 2 also known as Omega of n log N and that my friends is why he corrects deterministic sorting algorithm that's comparison based has got to use n log n comparisons in the worst case