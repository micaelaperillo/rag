in this lecture we'll continue our formal treatment of ASM totic notation we've already discussed bigo notation which is by far the most important in ubiquitous concept that's part of ASM totic notation but for completeness I do want to tell you about a couple of close relatives of bigo namely Omega and Theta if Big O is analogous to less than or equal to then Omega and Theta are analogous to greater than or equal to and equal to respectively but let's treat them a little more precisely the formal definition of Omega notation closely mirrors that of Big O notation we say that one function T of n is Big Omega of another function f of n If eventually that is for sufficiently large n it's lower bounded by a constant multiple of F ofn and we quantify the ideas of a constant multiple and eventually in exactly the same way as before namely via explicitly giving two constants C and N such that t of n is bounded below by C * F of n for all sufficiently large n that is for all n at least n not there's a picture just like there was for Big O notation perhaps we have a function T of n which looks something like this green curve and then we have another function f of n which is above T of n but then when we multiply F of n by 1 12 we get something that eventually is always below T of n so in this picture this is an example where T of n is indeed big Omega of f ofn as far as what the constants are well the multiple that we use C is obviously just 1/2 that's what we're multiplying F of n by and as before n KN is the crossing point between the two functions so n KN is the point after which C * F ofn always lies below T of n forever more so that's big Omega Theta notation is the equivalent of equals and so it just means that the function is both Big O of F ofn and Omega of f ofn an equivalent way to think about this is that eventually T of n is sandwiched between two different constant multiples of f ofn all write that down and I'll leave it to you to verify that the two Notions are equivalent that is one implies the other and vice versa so what do I mean by T ofn is eventually sandwiched between two multiples of f ofn well just mean we choose two constants a small one C1 and a big constant C2 and for all n at least n not t of n lies between those two constant multiples one way that algorithm designers can be quite sloppy is by using O notation instead of theta notation so that's a common convention and I will follow that convention often in this class let me give you an example suppose we have a sub rutine which does a linear scan through an array of length n it looks at each entry in the array and does a constant amount of work with each entry so the merge subroutine would be more or less an example of a subroutine of that type so even though the running time of such an algorithm a sub routine is patently Theta of n it does constant work for each of n entries so it's exactly Theta of n we'll often just say that it has running time o of n we won't bother to make the strong stronger statement that it's Theta of n the reason we do that is because you know as algorithm designers what we really care about is upper bounds we want guarantees on how long our algorithms are going to run so naturally we focus on the upper bounds and not so much on the lower bound side so don't get confused once in a while there'll be a quantity which is obviously Theta of F ofn and I'll just make the weaker statement that it's O of f ofn the next Quiz is meant to check your understanding of these three concepts big O big Omega and big Theta notation so the final three responses are all correct and I hope the high level intuition for y is fairly clear T of n is definitely a quadratic function we know that the linear term doesn't matter much uh as it grows as n grows large so since it has quadratic growth then the third response should be correct it's Theta of N squared and it is Omega of n so Omega of n is not a very good lower bound on the ASM totic rate of growth of T of n but it is legitimate indeed as a quadratic growing function it grows at least as fast as a linear function so it's Omega of n for the the same reason Big O of n cubed it's not a very good upper bound but it is a legitimate one it is correct the rate of growth of T of n is at most cubic in fact it's at most quadratic but it is indeed at most cubic now if you wanted to prove these three statements formally you would just exhibit the appropriate constants so for proving that it's big Omega of n you could take n equal to 1 and c equal to2 for the final statement again you can take n equal to 1 and c equal to say four and prove that it's Theta of n^2 you can do something similar just using the two constants combined so n would be one you can take C1 to be2 and C2 to B4 and I'll leave it to you to verify that the formal definitions of big Omega big Theta and Big O would be satisfied with these choices of constants one final piece of ASM totic notation we're not really going to use this much but you do see it from time to time so I want to di mention it briefly this is called little O notation in contrast to Big O notation so while Big O notation inform Al is a less than or equal to type relation little O is a strictly less than relation so intuitively it means that one function is growing strictly less quickly than another so formally we say that our function T of n is little o of f of n if it only if for all constants C there is a constant n not Beyond which T of n is upper bounded by this constant multiple C * F ofn so the difference between this definition and that of biger notation is that to prove that one function is bigger of another we only have to exhibit one measly constant C such that c * F of f f ofn is a upper bound eventually for T of n by contrast to prove that something is little low of another function we have to prove something quite a bit stronger we have to prove that for every single constant C no matter how small for every C there exists some large enough and not Beyond which T of n is bounded above by C * F of in so for those of you looking for a little more facility with little L notation I'll leave it as an exercise to prove that as you'd expect for all polinomial powers K in fact n the Kus one is little o of n the K there is an analogous notion of little Omega notation expressing that one function grows strictly quicker than another but that one you don't see very often and I'm not going to say anything more about it so let me conclude this video with a quote from an article back from 1976 by my colleague Don can widely regarded as the grandfather of the formal analysis of algorithms and uh it's rare that you can pinpoint why and where some kind of notation became universally adopted in the field but in the case of asmic notation indeed it's very clear where it came from the notation was not invented by algorithm designers or computer scientists it's been in use in number Theory since the 19th century but it was Don canth in 76 that proposed that this become the standard language for discussing rate of growth and in particular for the running time of algorithms so in particular he says in this article on the basis of the issues discussed here I propose that members of sigact this is the special interest group of the ACM which is concerned with uh theoretical computer science in particular the analysis of algorithms so I proposed that the members of siga and editors of Computer Science and Mathematics journals adopt the O Omega and Theta notations as defined above unless a better alternative can be found reasonably soon so clearly a better alternative was not found and ever since that time this has been the standard way of discussing the rate of growth of running times of algorithms and that's what we be used here