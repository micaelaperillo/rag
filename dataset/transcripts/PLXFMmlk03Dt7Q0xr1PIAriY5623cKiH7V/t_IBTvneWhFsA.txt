so we're almost at the finish line of our analysis of quick sort let me remind you what we're proving we're proving that for the randomized implementation of quick sort where we always choose a pivot element to partition around uniformly at random we're showing that for every array every input of length n the average running time of quick sort over the random choices of pivots is Big O of n log n so we've done a lot of work in the last couple videos let me just remind you about the story so far in the first video what we did is we identified the relevant random variable that we cared about Capital C the number of comparisons that quick sort makes among pairs of elements in the input array then we applied a decomposition approach we expressed Capital C the overall number of comparisons as a sum of indicator or 01 random variables where each of those variables x i j just counted the number of comparisons involving the I smallest and J's smallest entries in the array and that's going to be either zero or one then we applied linearity of expectation to realize all we really needed to understand was the comparison probabilities uh for different pairs of elements in the second video we nailed what that comparison probability is specifically for the I smallest and the J smallest elements in the array the probability that quick sort Compares them when you always make random pivot choices is exactly 2 / the quantity Jus I + 1 so putting that all together yields the following expression governing the average number of comparisons made by quicksort and one thing I want you to appreciate is is in the last couple videos we've been sort of amazingly exact as algorithmic analysis goes specifically we've done nothing sloppy whatsoever we've done no estimates the number of comparisons the quick sort makes on average is exactly this double s now shortly we'll do some inequalities to make our lives a little bit easier but up to this point everything has been completely exact and this lets you see why there's small constants also in in uh in quick sort it's basically going to be this Factor two now the next question to ask is what are we shooting for remember the theorem we want to prove is that the expected number of comparisons really expected runtime is O of n log n so are we already done well not quite we're going to have to be a little bit clever so if we look at this double sum and we ask how big are the summands and how many terms are there well the biggest summs we're ever going to see are when I and J are right next to each other where J is one bigger than I in that case this fraction is going to be 1/2 so the terms can be as big is 1/2 how many terms are there well there's a quadratic number of terms so it' be very easy to derive an upper bound that's quadratic in N but that's not what we want we want one that's n log n so to derive that we're going to have to be a little bit more clever about how we evaluate this sum so the idea is what we're going to do is we're going to think about a fixed value of I in this outermost sum and then we're going to ask how big could the inner sum B so let's fix some value of I the the value of the index in the outer sum and then let's look at the inner sum where J ranges from I + one up to n and the value of the sum end is 1 over the quantity J - I + 1 so how big can this be well let's first understand what the terms actually are so J starts at I + 1 and then it ascends to n and as J gets bigger the denominator gets bigger so the summands gets smaller so the biggest sum is going to be the very first one when J is as small as possible namely I + 1 When J is I + 1 the sum and is 1/2 then J gets incremented in the sum and so that's we're going to pick up a 1/3 term followed by 1/4th term and so on so it's going to be for every inter sum is going to have a of this form 1/2 plus 1/3 plus 1/4 and then it's going to sort of run out at some point when J equals n and the biggest term we're ever going to see uh is going to be a 1/ n in the case where I equals 1 so let's make our lives Easier by taking this expression we started with star and instead of having a double sum let's just upper bound this with a single sum so what are the ingredients of the single sum well there's this two can't forget the two then there's nend choices for I actually there's n minus one choices for I but let's just be sloppy and say n choices so that gives us a factor n and then how big can an inter suum be well an inter suum is just a bunch of these terms 1 12 plus 1/3 and so on the biggest of those inner sums is the one occurring when I equals 1 at which point the last term is 1/ n so we're going to just do a change of variable and express the inm an upper bound on each inter sum as the sum from k equal 2 to n of 1 over K so that's looking more manageable just having this single sum uh involving this uh index K andless going to get really good when we prove the next claim which is that this sum cannot be very big it's only logarithmic in N even though there's a linear number of su ANS the the overall value of the sum is only logarithmic that of course is going to complete the proof because that'll give us an overall bound of 2 * n * the natural log of n so it's an N logn bound with really quite reasonable constants so why is this true why is this some only logari ially large well let's do a proof by a picture I'm going to write this sum in a geometric fashion so on the x-axis let me Mark off points corresponding to the positive integers and on the y- axis let me Mark off points corresponding to fractions of the form 1 over K and what I'm going to do is I'm going to draw a bunch of rectangle Ang Les of decreasing area specifically they all have width one and the heights are going to be like 1 over K so the area of this guy is one the area of this guy is 1/2 the area of this guy is 1/3 and so on and now I'm going to overlay on this picture the graph of the function The Continuous function f ofx = 1 /x so notice that is going to go through these three points it's going to kiss all of these rectangles on their upper right Corners now what is it we're trying to prove the claim we're trying to prove is that this sum of 1/2 plus 1/3 and so on is upper bounded by something so this sum can be just thought of as the areas in these rectangles the 1/2 the 1/3 and so on and we're going to Upper bound it by the area under the blue curve so you notice the area under the blue curve is at least as big as the some of there is the rectangles because the curve hits each of these rectangles in its northeast corner so putting that into mathematics the sum from K = 2 to n of 1 K is mounted above by the integral and we'll start the area under the curve at one and then we need it to go all the way up to n of the function 1 /x DX so that's the area under the curve and if you remember a little bit of calculus the integral of 1 /x is the natural log of x so this equals the natural log of x evaluated uh N1 also known as log nus log 1 and of course log 1 would be zero so that gives us our log n so that completes the proof of the claim that indeed the sum of these 1 over KS is Bed above by the natural log of N and that in fact completes the proof of the theorem we've got that the expected number of comparison is a most 2 N times this sum which is a most log n putting it all together we find that the expected number of comparisons that quick sort makes on an arbitrary input of length n is 2 * n * the natural log of n so that would be Big O of n log n with quite reasonable constants uh now this is just the number of comparisons but as we observed earlier the running time of quicksort on average is not much more than that the running time is dominated by the number of comparisons that it makes moreover as we discussed when we were talking about the details of the implementation it works in place essentially no extra storage is necessary so that is a complete and mathematically rigorous explanation of just why quick sort is so quick