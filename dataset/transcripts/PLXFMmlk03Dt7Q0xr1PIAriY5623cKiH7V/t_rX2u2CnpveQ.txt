in this video I'll explain the mathematical analysis of the randomized linear time selection algorithm that we studied in the previous video specifically I'm going to prove to you the following guarantee for that algorithm for every single input array of length n the running time of this randomized selection algorithm on average will be linear pretty amazing if you think about it because that's barely more than the time it takes just to read the input and in particular this linear time algorithm is even faster than sorted so this shows that selection is a fundamentally easier problem than sorting you don't need to reduce the shorten you can solve it directly in Big O of n time I want to reiterate the same points I made about quicksort the guarantee is the same it is a general purpose subroutine we make no assumptions about data this theorem holds no matter what the input array is the expectation the average that's in the theorem statement is only over the coin flips made by the algorithm made inside it's code of our own devising before we plunge into the analysis let me just make sure you remember what the algorithm is so it's like quicksort we partition around the pivot except we only recurse once not twice so we're given an array with some length n we're looking for the I or statistic the I smallest element the base case is obvious if you're not in the base case you choose a pivot P uniformly at random from the input array just like we did in quicksort we partition around the pivot just like we didn't pick in quicksort that splits the array into a first part those elements less than the pivot and the second part those elements which are bigger than the pivot now we have a couple cases the case which is very unlikely so we don't really worry about it is if we're lucky enough to guess the pivot as the 8th order statistic what we're looking for that's when the new position J of the pivot element happens to equal I what we're looking for then of course we just return it that was exactly what we wanted in the general case the pivot is going to be in the position J which is either bigger than what we're looking for I that's when the pivot is too big or J its position will be at less than the order statistic I that we're looking for that's when the pivot is too small so if the pivot is too big if J is bigger than I then we know that what we're looking for is on the left hand side amongst the elements less than the pivot so that's where we recurse we've thrown out both the pivot and everything to the right of it that leaves us with a array of J minus 1 L and we're still looking for the ight smallest among these J minus one smallest elements and in the final case this is what we went through in the quiz in the last video is if we choose a pivot whose smaller than what we're looking for that's when J is less than I then it means we're safe to throw out the pivot and everything less than it we're safe recursing on the second part those elements bigger than the pivot having thrown out the j smallest elements were rehearsing on an element of length n minus J and we're looking for the I minus J smallest element in those that remain having already thrown out the J smallest from the input array so that's randomized selection let's discuss why it's linear time on average the first thought that you might have and this would be a good thought would be that we should proceed exactly the same way that we did in quicksort you'll recall that when we analyzed quicksort we set up these indicator random variables X IJ determining whether or not a given pair of elements got compared at any point in the algorithm and then we just realize the sum of the comparison is just the sum over all of these X on J's we fled linearity of expectation and it boiled down to just figuring out the probability that a given pair of elements gets compared you can analyze this randomized selection algorithm in exactly the same way and it does give you a linear time bound on average but it's a little messy why is it being not quite as clean as in the quicksort analysis moreover because of the special structure of the selection problem we can proceed in an even more slick way here then the way we did with quicksort so again we'll have some Constituent random variables will again apply linearity of expectation but the definition of those random variables is going to be a little bit different than it was in quicksort so first a preliminary observation which is that the workhorse for this randomized selection procedure is exactly the same as it was in quicksort namely it's the partition subroutine essentially all of the work that gets done outside of the recursive calls just partitions the input array around some pivot element as we discussed in detail in a separate video that takes linear time so usually when we say something's linear time we just use Big O notation I'm going to go ahead and explicitly use a constant C here for the operations outside the recursive call that will make it clear that I'm not hiding anything up by sleeves when we do the rest of the analysis now what I want to do on this slide is introduce some vocabulary some notation which will allow us to cleanly track the progress of this recursive selection algorithm and by progress I mean the length of the array on which is currently operating remember we're hoping for a big win over quicksort because here we do only one recursive call not - we don't have to recurse on both sides of the pivot just on one of them so it stands to reason that we can think about the algorithm as making more and more progress as a single recursive call is operating on a raise of smaller and smaller lengths so the notion that will be important for this proof is that of a phase this quantifies how much progress we've made so far with higher numbered phases corresponding to more and more progress well say that the our select algorithm at some midpoint of its execution is in the middle of phase J if the array size that the current recursive call is working on has length between 3/4 raised to the J times N and the smaller number of 3/4 raised to the J plus 1 times n for example think about the case where J equals 0 that says phase 0 recursive calls operate on a raise with size between n that's the length of the original input array and 75% of n so certainly the outermost recursive call is going to be in phase zero because the input array has size N and then depending on the choice of the pivot you may or may not get out of phase zero in the next recursive call if you choose a good pivot and you end up recursing on something that has at most 75% of the original elements you will no longer be in phase zero if you recurse on something that has more than 75% of what you started with of the input array then you're still going to be in phase zero even in the second recursive call so overall the phase number J quantifies the number of times we've made 75 percent progress relative to the original input array and the other piece of notation that's going to be important is going I'm going to call XJ so for a phase J XJ simply counts the number of recursive calls in which a randomized selection algorithm is in phase J so this is going to be some integer it could be as small as 0 if you think about it for some of the phases or it could be larger so why am i doing this why am I making these definitions of phases and of these X J's what's the point well again remember the point was we want to be able to cleanly talk about the progress that the randomized selection algorithm makes through its recursion and what I want to now show you is that in terms of these X J's counting the number of iterations in each phase we can derive a relatively simple upper bound on the number of operations that our algorithm requires specifically the running time of our algorithm can be bounded above by the running time in a given phase and then summing those quantities over all of the possible phases so we're going to start with a big sum over all the phases J we're going to look at the number of recursive calls that we have to endure during phase J so that's X J by definition and then we're to look at the work that we do outside of the recursive calls in each recursive call during phase J now in a given a recursive call outside of its recursive call we do C times M operations where m is the length of the input array and during phase J we have an upper bound on the length of the input array by definition it's at most 3/4 raised to the J times n so that is we multiply the running time times this constant C this we inherit from the partition subroutine and then we can for the input length we can put an upper bound of 3/4 raised to the J times n so just to review where all of these terms come from this 3/4 J times n is an upper bound on the array size during phase J this is by the definition of the phase then if we multiply that times C that's the amount of work that we do on each phase J sub-problem how much work do we do in phase J overall well we just take the work per sub-problem that's what's circled in yellow and we multiply it times the number of such subproblems we have and of course we don't want to forget any of our sub-problem so we just make sure we sum over all of the phases J to ensure that at every point we count the work done in each sub-problems okay so that's the upshot of this slide we can upper bound the running time of our randomized algorithm very simply in terms of phases and the X J's the number of subproblems that we have to endure during phase J so this upper bound on our running time is important of to give some notation we'll call this star this will be the starting point of our final derivation when we complete the proof of this theorem now don't forget we're analyzing a randomized algorithm so therefore the left hand side of this inequality the running time of our select that's a random variable so that's a different number depending on the outcome of the random coin flips of the algorithm depending on the random pivots chosen you'll get different running times similarly the right hand side of this inequality is also a random variable that's because the X J's are random variables the number of subproblems in phase J depends on which pivots get chosen so to analyze what we care about is the expectations of these quantities their average values so we're going to start modestly and as usual this will extend our modest accomplishments to much more impressive ones using linearity of expectation but our first modest goal is just to understand the average value of an XJ the expected value of X chip we're going to do that in two steps on the next slide I'm going to argue that to analyze the expectation of XJ it's sufficient to understand the expectation a very simple coin flipping experiment then we'll analyze that coin flipping experiment then we'll have the dominoes all set up in a row and the final slide will knock them down and finish the proof so let's try to understand the average number of recursive calls we expect to see in a given phase so again just so you don't forget XJ is defined as the number of recursive calls during phase J where a recursive call is in phase J if and only if the current sub array length lies between three quarters raised to the J plus 1 times N and then the larger number of three quarters raised to the J times n so again for example phase zero is just the recursive calls in which the array length is between 75% of the original 100% of the original elements so what I want to do next is point out that a very simple sufficient condition guarantees that will proceed from a given phase on to the next phase so its condition guaranteeing termination of the current phase and it's an event that we've discussed in previous videos namely that the pivot that we choose gives a reasonably balanced blit 25 75 or better so recall how positioning works we choose a pivot P it one-zip rubber ones up and the stuff to the left of it's less than P the stuffs to the right of it's bigger than P so 25/75 split are better what I mean is that each of these each the first part in the second part has of most 75% of the elements in the input array both have to have at least 25% and at most 75% and the key point is that if we want end up choosing a pivot that gives us a split that's at least this good the current phase must end why must the current phase end well together 25/75 split it better then no matter which case we wind up in in the algorithm we're guaranteed to recurse on a subproblem has the most 75% of what we started with that guarantees that whatever phase were in now we're going to be in an even bigger phase when we were curse now I want you to remember something that we talked about before which is that you've got a decent chance when you pick a random pivot of getting something that gives you a 25/75 split or better in fact the probability is 50% right if you have an array that has the integers from 1 to 100 inclusive anything from 76262 75 will do the trick that will ensure that at least the first 25 elements are excluded from the rightmost call and at least the rightmost 25 elements are excluded from the left recursive call so this is why we can reduce our analysis of the number of recursive calls during a given phase to a simple experiment involving flipping coins specifically the expected number of recursive calls that we are going to see in a given phase J is no more than the expected number of coin flips and the thawing experiment okay so you have a fair coin 50% heads 50 % tails you it's - flipping it until you see the head and the question is how many coin flips does it take up to and including the first head that you see okay so at minimum that's going to be one coin flip if you get a head the first time it's one if you get a tails then a head then it's two if it's tails tails heads it's three and so on and you always stop when you get that first hit so what's the correspondence well think of heads as being you're in phase J and if you get a good pivot that gives you a 25/75 split call that heads and then guarantees that you exit this phase J just like it guarantees that you get determinate the coin flipping experience experiment now if you get a pivot which doesn't give you a 25/75 split you may or may not pass to a higher phase J but in the worst case you don't you stick in phase J if you get a bad split and that's like getting a tails in the coin flipping experiment and you have to try again this correspondence gives us a very elementary way to think about the progress that a randomized selection algorithm is making so there's one recursive call in every at every step in our algorithm and each time we either choose a good pivot or a bad divot both can happen 50/50 probability good pivot means we get a 75 25 split or better bad pivot means by definition we get a split worse than 25 75 so what have we accomplished we've reduced the task of upper bounding the expected number of recursive calls in a phase J to understanding the expected number of times you have to flip a fair coin before you get one heads so on the next slide we'll give you the classical and precise answer to these coin flipping experiment so let me use capital n to denote the random variable which we were just talking about the number of coin flips you need to do before you see the first heads and it's not very important but you should know that these random variables have their own name this would be a geometric random variable with parameter 1/2 so you can use a few different methods to compute the expected value of a geometric random variable such as this and brute force using the definition of expectation works fine as long as you know how to manipulate infinite sums but for the sake of variety let me give you a very sneaky proof of what its expectation is so the sneaky approach is to write to the expected value of this random variable in terms of itself and then solve for the unknown solve for the expectation so let's think about it so how many coin flips do you need well for sure you're going to need one that's the best-case scenario and now two things could happen either you get heads and that happens with thickness and probability you stop or you get tails that happens with 50% probability and now you start all over again again you just flip coins until you get the first heads on average how many times does that take well by the definition of capital n you expect the expectation of n coin flips in the case where you get tails and you have to start all over so this one represents the first coin flip the 1/2 is the probability that you can't stop that you have to start all over probability of tails and then because it's a memoryless process because when you start a new on the second coin flip haven't gotten the tails it's as if you're back at time 1 all over again so now we have a trivial equation in terms of the unknown expected value of n and the unique solution the unique value that the expected value of capital n could have in light of this equation is 2 so on average if you flip a fair coin and stop and you get heads you're going to see two coin flips on average to make sure you haven't sort of lost the forest for the trees let me remind you why we were talking about this coin flipping analysis in the first place so recall on the previous slide we showed that XJ and remember XJ is the number of recursive calls you expect to see in a given phase J and we argued that the number of recursive calls you're going to see is bounded above by the expected number of coin flips until the heads so this exact calculation of 2 for the coin flips gives us an upper bound of 2 for the number of recursive calls on average in any given phase J so now we got all our ducks lined up in a row let's wrap up the proof on this final slide so inherited from Part one of the proof we have an upper bound on the expected running time of the our select algorithm this is what we were calling star on the first proof slide in star that looked a little messy but we had the sum over the phases J but we had two things that were independent of J the constant C and the original input length n so let me just yank this C in the n out front and then we have this residual sum over the phases J of 3/4 raised to the J remember that comes from our upper bound on the sub-problem size during phase J and then of course we have to keep track of how many phase J subproblems we got to solve that by definition is X J now star was written as a rant inequality in terms of the random variables now we're going to go ahead and take the expectations and again I've said this over and over but don't forget where is the expectation come from this is over the random pivot choices that our code makes so the expensive running time of the algorithm is mostly expectation of this starred quantity so like I said earlier pretty much every time we're going to do any analysis of a randomize process we're going to wind up using linearity of expectation at some point here is where we do it many of expectation says the expectation of a sum is just the sum of the expectations so we yank the C and the end outside of the expectation we yank this sum over phases outside of the expectation we yank this 3/4 raised to the J outside of the expectation and then we just have the expected value of XJ the average number of recursive calls we expect to see in phase J now in the previous two slides we figured out an upper bound on how many recursive calls we expect to see in each phase so first by the coin flip analysis by the reduction of the coin flip analysis this is the most expected number of coin flips n which on the previous slide we argued was exactly two so bringing that two out in front of the sum that no longer depends on J so we get at most to see n times the sum over phases J of 3/4 raised to the J now this kind of sum we have seen previously in the course it came up when we are analyzing the master method and we summed up our running time upper bounds over the levels of our recursion tree and if we're not in case one if you were in case two or three we had geometric sums that were non-trivial that required a certain formula to calculate so let me remind you of that formula here when 3/4 is the number being powered up to the J so this has value at most 1 over 1 minus the number that's getting it powered so in this case it's 3/4 so 1 minus 3/4 is 1/4 you take the reciprocal you get 4 and the upshot is that the expected number of operations that's this randomized selection algorithm uses to find the I ordered statistic in a given input array is 8 times C times n where C is the hidden cost in the linear running time of partition and so that completes the proof the input array was arbitrary we showed the expected running time over the random choices of the algorithm is linear in n that is only a constant factor larger than what is required to read the input pretty amazing