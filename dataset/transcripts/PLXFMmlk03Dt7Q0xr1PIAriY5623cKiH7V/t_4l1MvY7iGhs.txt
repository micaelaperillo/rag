in this video we'll put the master method to use by instantiating it for six different examples but first let's recall what the master method says so the master method takes as input recurrences of a particular format in particular recurrences that are parameterized by three different constants a b and d a refers to the number of recursive calls or the number of sub problems that get solved B is the factor by which the sub problem size is smaller than the original problem size and D is the exponent in the running time of the work done outside of the recursive calls so the recurrence has the form T of n the running time on input of size n is no more than a the number of sub problems times the time required to solve each sub problem which is T of n / B because the input size of a sub problem is n/ B plus o of n the D the work outside of the recursive calls there's also a base case which I haven't written down so once the problem size drops below a particular constant then there should be no more recursion and you can just solve the problem immediately that is in constant time now given a recurrence in this permitted format the running time is given by one of three formulas depending on the relationship between a the number of recursive calls and B raised to the D power case one of the master method is when these two quantities are the same AAL B to the D then the running time is n the D log n no more than that in case two the number of recursive calls a is strictly smaller than b to the D then we get a better running time upper bound of O of n the D and when a is bigger than b to the D we get this somewhat funky looking running time of O of n raised to the log base B of A Power we'll understand what that where that formula comes from a little later so that's the master method it's a little hard to interpret the first time you see it so let's look at some concrete examples let's begin with an algorithm that we already know the answer to we already know the running time namely let's look at merge short so again what's so great about the master method is all we have to do is identify the values of the three relevant parameters a b and d and we're done we just plug them in and we get the answer so a remember is the number of recursive calls so in merge short recall we get two recursive calls B is the factor by which the sub problem size is smaller than that in the original well we recurse on half the array so the sub problem so the sub problem size is half that of the original so B is equal to two and recall that outside of the recursive calls all merge short does is merge and that's a linear time sub routine so the exponent D is one in reflection of the fact that it's linear time so remember the key trigger which determines which of the three cases is the relationship between A and B to the D so a obviously is two and B to the D is also equal to two so this puts us in case one and remember in case one we have that the running time is bounded above by o of n the D log n in our case D equal one so this is just oen login which of course we already knew okay but at least this is a sanity check the master method is at least reconfirming facts which we've already proven by direct means so let's look at a second example the second example is going to be for the binary search algorithm in assorted array now we haven't talked explicitly about binary search and I'm not planning to so if you don't know what binary search is please read about it in a textbook or just look it up on the web and it'll be easy to find descriptions but the upshot is this is basically how you look up a phone number in a phone book now I realize probably the youngest viewers of this video haven't actually had the experience of using a physical telephone book but for the rest of you as you know you don't actually start with the A's and then go to the B's and then go to the C's if you're looking for a given name you more sensibly split the telephone book roughly in the middle and then depending on what if you're looking for is earlier or later in the alphabet you effectively recurse on the relevant half of the telephone book so binary search is just exactly the same algorithm when you're looking for a given element in a particular sort of array you start in the middle of the array and then you recurse on the left or the right half as appropriate depending on if the element you're looking for is bigger or less than the middle element now the master method applies equally well to binary search and it tells us what its running time is so in the next Quiz you'll go through that exercise so the correct answer is the first one to see why let's recall what a b and d mean a is the number of recursive calls now in binary search you only make one recursive call this is unlike merge sword remember you you just compare the element you're looking for to the middle element if it's less than the middle element you recurse on the left half if it's bigger than the middle element you recurse on the right half so any case there's only one recursive call so a is merely one in binary search now in any case you recurse on half the array so like and merge short the value of b equals 2 you recurse on a problem of half the size and outside of the recursive call the only thing you do is one comparison you just determine whether the element you're looking for is bigger than or less than the middle element of the array that you recursed on so that's constant time outside of the recursive call giving us a value for D of zero just like merge sort this is again case one of the master method because we have a equal B to the D both in this case are equal to one so this gives us a recurrence a solution to our recurrence of Big O of n the D log n since D equals z this is simply log in and again many of you probably already know that the running time of binary search is log in or you can figure that out easily again this is just using the master method as a sanity check to reconfirm that it's giving us the answers that we expect let's now move on to some harder examples beginning with the first recursive algorithm for integer multiplication remember this is where we recurse on four different products of n/2 digit numbers and then recombine them in the obvious way using padding by zero and some linear time additions so in in the first integer multiplication algorithm which does not make use of gauss's trick where we do the four different recursive calls in the naive way we have a the number of recursive calls is equal to four now in each case whenever we take a product of two smaller numbers the numbers have n/ two digits so that's half as many digits as we started with so just like in the previous two examples B is going to be equal to two the input size drops by a by a factor 2 when we recurse now how much work do we do outside the recursive cuse well again all it is doing is additions padding by zeros and that can be done in linear time linear time corresponds to a parameter value of D equal to 1 so next we determine which case of the master method we're in a = 4 B the D = 2 which in this case is less than a so this corresponds to case three of the master method and this is where we get the somewhat strange formula for the running time of the recurrence T of n is Big O of n the log base B of A which with our parameter values is n the log base 2 of 4 also known as o of n^2 so let's compare this to the simple algorithm that we all learned back in grade school recall that the iterative algorithm for multiplying two integers also takes an N squ number of operations so this was a clever idea to attack the problem recursively but at least in the absence of gauss's trick where you just naively compute each of the the four uh necessary uh products separately you do not get any improvement over the iterative algorithm that you learned in grade school either way it's an N squared number of operations but what if we do make use of gauss's trick where we do only three recursive calls instead of four surely the running time won't be any worse than N squared and hopefully it's going to be better so I'll let you work out the details on this next Quiz so the correct answer to this quiz is the fourth option it's not hard to see what the relevant values of a b and d R remember the whole point of ga's trick is to reduce the number of recursive calls from four down to three so the value of a is going to be three as usual we're recursing on a problem size which is half of that of the original in this case n over two-digit numbers so B Remains Two and just like in the more naive recursive algorithm we only do linear work outside of the recursive calls all that's needed to do some additions and paddings by zero so that puts us parameter values a b and d then we have to figure out which case of the master method that is so we have AAL 3 B raised to the D equal to 2 so a has dropped by one relative to the more naive algorithm but we're still in case three of the master method a is still bigger than b to the D so the running time is still governed by that rather exotic looking formula namely T of n is Big O of n to the log base B which in our case is two of a which is now three instead of four okay so the master method just tells us the solution to this recurrence the running time of this algorithm is Big O of n the log base 2 of 3 so what is log of the uh Lo what is log base 2 of three well plug it in your computer or your calculator and you'll find that it's roughly 1.59 so we get a running time of n^ the 1.59 which is certainly better than N squared it's not as fast as n logn not as fast as the merge short recurrence which makes only two recursive calls but it's quite a bit better than quadratic so summarizing you did in fact learn a suboptimal algorithm for integer multiplication way back in grade school you can beat the iterative algorithm using a combination of recursion plus gauss's trick to save on the number of recursive calls let's quickly move on to our final two examples example number five is for those of you that watch the video on sten's matrix multiplication algorithm so recall the salian properties of sten's algorithm the key idea is similar to Eng gusa strict for integer multiplication first you set up the problem recursively one observes that the naive way to solve the problem recursively would lead to eight sub problems but if you're clever about saving some computations you can get it down to just seven recursive calls seven sub problems so a in ston algorithm is equal to seven as usual each sub size is half that of the original one so B is going to be equal to two and the amount of work done outside of the recursive cause is linear in The Matrix size so quadratic in N quadratic in the dimension because there's quadratic number of entries in terms of the dimension so n^2 work outside the recursive cause leading to a value of D equal to 2 so as far as which case of the master method we're in well it's the same as in the last couple examples a = 7 u d = 4 Which is less than a so once again we're in case three and now the running time of staton's algorithm T of n is Big O of n to the log base 2 of 7 which is more or less n the 2.81 and again this is a win once we use uh the the savings to get down to just seven recursive calls this beats the naive Ed of algorithm which recall would require cubic time so that's another win for a clever divide and conquer uh in matrix multiplication via stren's algorithm and once again the Master's method just by plugging in parameter tells us exactly what the right answer to this recurrence is so for the final example I feel a little guilty because I've shown you five examples and none of them have triggered case two we've had two in case one of the master method and three now in case three so this will be sort of a fictitious recurrence just to illustrate case two but you know there are examples of of recurrences that come up uh where case 2 is the relevant one so let's just look at uh at the following recurrence so this recurrence is just like merge short we recurse twice there's two recursive calls each on half the problem sign size the only difference is in this recurrence we're working a little bit harder in a combined step instead of linear time outside of the recursive cuse we're doing a quadratic amount of work okay so a = 2 b = 2 and DS 2 so B to the D was equal to four strictly bigger than a and that's exactly the trigger for case two now recall what the running time is in case two it's simply end the D where D is the exponent in the combined step in our case D is 2 so we get a running time of n^2 and you might find this a little counterintuitive right given the merge short all we do with merge short is change the combined step from linear to quadratic and merge short has a running time of n log n you might have expected the running time here to be n^2 log n but that would be an overestimate so the master method gives us a tighter upper bound shows that it's only quadratic work so put differently the running time of the entire algorithm is governed by the work outside of the recursive calls just in the outermost call to the algorithm just at the root of the recursion tree