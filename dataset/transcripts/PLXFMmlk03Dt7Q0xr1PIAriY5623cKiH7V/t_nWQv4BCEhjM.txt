in the last video we discussed the performance of hash tables that are implemented using chaining using one linked list per bucket in fact we proved mathematically that if you use a hash function chosen uniformly at random from a universal family and if you keep the buckets number of buckets comparable to the size of your data set then in fact you're guaranteed constant on expected performance but recall that chaining is not the only implementation of hash tables there's a second paradigm which is also very important called open addressing this is where you're only allowed to store one object in each slot and you keep searching for an empty slot when you need to insert a new object into your hash table now it turns out it's much harder to mathematically analyze hash tables implemented using open addressing but I do want to say a few words about it to give you the gist of what kind of performance you might hope to expect from those sorts of hash tables so recall how open addressing works were only permitted to store one object in each slot so this is unlike the case with chaining where we can have an arbitrarily long list in a given bucket of the hash table with the most one object per slot obviously open addressing only makes sense when the load factor alpha is less than 1 when the number of objects you're storing in your table is less than the number of slots available because of this requirements that we have at most one object per slot we need to demand more of our hash function so our hash function might ask us to put a given object say with some IP address into say bucket number seventeen but bucket number seventeen might already be full might already be populated so in that case we go back to our hash function and ask it where to put the low where to look for a Nutty slot next so maybe it tells us to next looking bucket forty-one if 41 is full of tells us to look in bucket number seven and so on two specific strategies for producing a probe sequence that we mentioned earlier were double hashing and linear probing double hashing is where you use two different hash functions h1 and h2 h1 that tells you which slot in which to search first and then every time you find a full slot you add an increment which is specified by the second hash function H - linear probing is even simpler you just have one hash function that tells you where to look first and then you just add one to the slot until you find an empty slot as I mentioned at the beginning it is quite non-trivial to mathematically analyze the performance of hash tables using these various open addressing strategies it's not impossible there is some quite beautiful and quite informative theoretical work that does tell us how hash tables perform but that's well outside the scope of this course so instead what I want to do is I want to give you a quick and dirty calculation that suggests at least in an idealized world what kind of performance we should expect from a hash table with open addressing if it's well implemented as a function of the load factor alpha precisely I'm going to introduce a heuristic assumption it's certainly not true but we'll do it just for a quick and dirty calculation that we're using a hash function in which each of the N factorial possible probe sequences is equally likely now no hash function you're ever going to use is actually going to satisfy this assumption and if you think about it for a little bit you'll realize that if you use double hashing or linear probing there's certainly not going to be satisfying that assumption so this will still give us a kind of best-case scenario against which you can you can compare the performance of your own hash table implementations so if you have them in a hash table and you're seeing performance as good as what's suggested by this idealized heuristic analysis then you're home free you know your hash table is performing great so what is the line in the sand that gets drawn under this heuristic assumption what is this I'll the idealized hash function performance as a function of the load alpha well here it is I'm gonna argue next is that under this heuristic assumption the expected amount of time to insert a new object into the hash table is going to be essentially 1 over 1 minus alpha where alpha is the load remember the load is the number of objects in the hash table divided by the number of available slots so if the hash table is half-full then alpha is going to be point 5 if it's 75% full then alpha is going to be 3/4 so what this means is that in this idealized scenario if you keep the load pretty under control so say that the load is 50% then the insertion time is going to be great right if alpha is 0.5 then 1 over 1 minus alpha is equal to 2 so you expect just two probes before to successfully insert a new object and of course if you're thinking about lookup that's going to be at least as good as insert so if you're lucky you look up my terminate early if you find what you're looking for in the worst case you go all the way until an empty slot and an unsuccessful search and that's going to be the same as insertion so if alpha is small bounded away from one you're getting constant time performance on the other hand as the hash table gets full as alpha gets close to one this operation time is blowing up it's actually going to infinity as alpha gets close to one so even if you have a 90% full hash table with open addressing you're going to start seeing ten probes so you really want to keep hash tables with open addressing you want to give the load under control certainly no more than probably 0.7 maybe even less than that to refresh your memory with chaining hash tables are perfectly well defined even with load factors bigger than one what we derived is that under Universal hashing under a weaker assumption we had an operation time of 1 plus alpha for a load of alpha so with chaining you just got to keep alpha to be you know in most some reasonably small constant with the open address and you really got to keep it well bounded away below 1 so next let's understand why this observation is true why under the assumption that every probe sequence is equally likely do we expect a 1 over 1 minus alpha running time for hash tables with open addressing so the reason is pretty simple and we can derive it by analogy with a simple coin flipping experiment so to motivate the experiment think just about the very first probe that we do okay so we get some new objects some new IP address we want to insert into our hash table let's say our hash table is currently 70% full say there's 100 slots 70 are already taken by objects well when we look at this first probe by assumption it's equally likely to be any of the 100 slots 70 of which are full 30 of which are empty so with probability 1 minus alpha or in this case 30% our first probe will luckily find an empty slot and we'll be done we'll just concert' the new object into that slot if we get unlucky with probability 70 percent we find a slot that's already occupied and then we have to try again so we try a new slot drawn a random and we again check is it full or is it not full and again with 30% probability essentially it's going to be empty and we can stop and if it's already full then we try yet again so doing random probes looking for an empty slot is tantamount to flipping a coin with probability of heads 1 minus alpha or in this example 30% and the number of probes you need until you successfully insert is just the number of times you've flipped this biased coin until you see a heads in fact this biased coin flipping experiment slightly overestimates the expected time for insertion on the heuristic assumption and that's because in the insertion time we're never going to try us the same slot twice we're going to try all n buckets in some order with each of the N factorial orderings equally likely so back to our example where we have a hash table with 100 slots 70 of which are full the first probit indeed we have a 30 and 100 chance of getting an empty slot if that one fails then we're not going to try the same slot again so there's only 99 residual possibilities again 30 of which are empty the one we checked last time was full so actually have a 30 over 99% chance of getting a empty slot in the second try 30 over 98 on the third try if the second one fails and so on but a valid upper bound is just to assume a 30% success probability with every single probe and that's precisely what this coin flipping experiment will get us so the next quiz will ask you to actually compute the expected value of capital n the number of coin flips needed to get heads when you have a probability of heads of 1 minus alpha as a hint we actually analyze this exact same coin flipping experiment when alpha equals 1/2 back when we discussed the expected running time of randomized linear time selection all right so the correct answer is the first one 1 over 1 minus alpha so to see why let's return to our derivation they were reduced analyzing the expected insertion time to this random variable of the expected number of coin flips until we see a heads so I'm going to solve this exactly the same way that we did it back when we analyzed the randomized selection algorithm and it's quite a sneaky way but very effective what we're going to do is we're going to express the expected value of capital n in terms of itself and then solve so how do we do that well on the left hand side let's write the expected number of coin flips the expected value of capital n and then let's just notice that there's two different cases either the first coin flip as a heads or it's not so in any case you're certainly going to have one coin flip so let's separate that out and count it separately with probability alpha the first coin flip is going to be tails and then then you start all over again and because it's a memoryless process the expected number of further coin flips one requires given that the first coin flip was tails is just the same as the expected number of coin flips in the first place so now it's a simple matter to solve this one linear equation for the expected value of n and we find that it is indeed 1 over 1 minus alpha as claimed summarising under our idealized heuristic assumption that every single probe sequence is equally likely the expected insertion time is upper bounded by the expected number of coin flips which by this argument is the most 1 over 1 minus alpha so as long as your load alpha is well bounded below 1 you're good at least in this idealized analysis your hash table will work extremely quickly now I hope you regarding this idealized an analysis with a bit of skepticism right from a false hypothesis you can literally derive anything you want and we started with this assumption which is not satisfied by hash functions you're actually going to use in practice this heuristic assumption that all probe sequences are equally likely so should you expect this 1 over 1 minus Alpha bound to hold in practice or not well that depends to some extent depends on what open addressing strategy you're using it depends on how good a hash function you're using it depends on whether the data is pathological or not so just to give course rules of thumb if you're using double hashing and you have non pathological data I would go ahead and look for this 1 over 1 minus alpha bound in practice so implement your hash table check its performance as a function of the load vector alpha and shoot for the 1 over 1 minus alpha curve that's really what you'd like to see with linear probing on the other hand you should not expect to see this performance guarantee of 1 over 1 minus alpha even in a totally idealized scenario remember linear probing is the strategy where your initial probe the hash function tells you where to look first and then you just scan linearly through the hash table until you find what you're looking for an empty slot the object you're looking up or whatever so with linear probing even in a best-case scenario it's going to be subject to clumping you to have contiguous groups of slots which are all full and that's because of the linear probing strategy I encourage you to do some experiments with implementations to see this for yourself so because of clumping with linear probing even in the idealized scenario you're not going to see one over one minus alpha however you're going to see something worse but still in idealized situations quite reasonable so it's the last thing I want to tell you about in this video now needless to say with Lena linear probing the heuristic assumption is badly false the heuristic assumption is pretty much always false no matter what hashing strategy you're using but with linear programming it's quote unquote really false so to see that the heuristic assumption said that all n factorial probe sequences are equally likely so your next probe is going to be a uniform at random amongst everything you haven't probed so far but with linear probing it's totally the opposite right once you know the first slot that you're looking into say bucket 17 slot 17 is going to be the first slot you know the rest of the sequence because it's just a linear scan through the table so it's kind of diametrically opposite from each successive probe being independent from the previous ones except for not exploring things twice so to state a conjectured or idealized performance guaranteed for hash tables with linear probing we're going to place replace the blatantly false heuristic assumption by a still false but more reasonable heuristic assumption so what do we know we know that the initial probe with linear probing determines the rest of the sequence so let's at least assume that these initial probes are uniform at random and independent for different keys of course once you know the initial probe you know everything else but let's assume independence and uniformity amongst the initial probes now this is a strong assumption this is way stronger than assuming you have a universal family of hash functions this assumption is not satisfied in practice but the performance guarantees we can derive under this assumption are typically satisfied in practice by well implemented hash tables that use linear probing so the assumption is still useful for deriving the correct idealized performance of this type of hash table so what is that performance well this is a utterly classic result from exactly 50 years ago from 1962 and this is a results by my colleague the living legend Don Knuth author of art of computer programming and what Knuth proved was that under this weaker heuristic assumption suitable for linear probing the expected time to insert an object into a hash table with load factor alpha when you're using linear probing is worse than 1 over 1 minus alpha but it is still a function of the load alpha only and not a function of the number of objects in the hash table so that is with linear probing you will not get as good a performance guarantee but it is still the case that if you keep the load factor bounded away from 1 if you make sure the hash table doesn't get to full you will enjoy constant time operations on average so for example if with linear probing your hash table is 50% full then you're going to get an expected insertion time of roughly 4 probes known however this quantity does approach does blow up pretty rapidly as the hash table grows full if it's 90% full this is already going to be something like a hundred probes on average so you really don't want to let hash tables get too full when you're using linear probing you might well wonder if it's ever worth implementing linear probing given that it has the worst performance curve 1 over 1 minus alpha squared then the performance curve you'd hope from something like double hashing 1 over 1 minus alpha and it's a tricky cost benefit analysis between linear probing and more complicated with better performing strategies it really depends on the application there are reasons that you do want to use linear probing sometimes it is actually quite common in practice for example it often interacts very well with memory hierarchies so again as with all of this hashing discussion you know the costs and benefits are very subtle trade-offs between the different approaches if you have mission-critical code that's using a hash table and you want to really optimize it try a bunch of prototypes and just test figure out which one is the best for your particular application so let me conclude the video with a quote from Knuth himself where talks about the rapture of proving this 1 over 1 minus alpha squared theorem and how it was life-changing he says I first formulated the following derivation meaning the proof of that last theorem in 1962 ever since that day the analysis at algorithms has in fact been one of the major themes in my life