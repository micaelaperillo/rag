so this is the first video of three in which will mathematically analyze the running time of the randomized implementation of quicksort so in particular we're going to prove that the average running time of quicksort is Big O of n log n now this is the first randomized algorithm that we've seen in the course and therefore and its analysis will be the first time that we're going to need any kind of probability theory so let me just explain upfront what I might expect you to know in the following analysis basically I need you to know the first few ingredients of discrete probability theory so I need you to know about sample spaces that is how to model all of the different things that could happen all of the ways that random choices could resolve themselves I need you to know about random variables functions on sample spaces which take on real values I need you to know about expectations that is average values of random variables and very simple with very key property we're going to need in the analysis of quicksort is linearity of expectation so if you haven't seen this before or if you're too rusty definitely you should review this stuff before you watch this video some places you can go to get that necessary review you can look at the probability review part one video that's up on the courses website if you prefer to read something like I said at the beginning of the course I recommend the free online lecture notes by Eric Lehman and Tom Layton mathematics for computer science that covers everything we'll need to know plus much much more there's also a wiki book on discrete probability which is a perfectly fine obviously free source in which you can learn the necessary material ok so after you've got that sort of fresh in your mind then you're ready to watch the rest of this video and in particular we're ready to prove the following theorem stated in the previous video so the quicksort algorithm with a randomized implementation that is where in every single recursive sub call you pick a pivot uniformly at random we stated the following assertion that for every single input so for a worst-case input array of length n the average running time of quicksort with random pivots is o of n log n and again to be clear where the randomness is the randomness is not in the data we make no assumptions about the data as per our guiding principles no matter what the input array is averaging only over the randomness in our own code the randomness internal to the algorithm we get a running time of n log we saw in the past that the best case behavior of quicksort is n log n it's the worst case behavior as M Squared so the snare missus is asserting that no matter what the input array is the typical behavior of quicksort is far closer to the best case behavior than it is to the worst case behavior so that's what we're going to prove in the next few videos so let's go ahead and get started so first I'm going to set up the necessary notation and be clear about exactly what is the sample space what is the random variable that we care about and so on so we're going to fix an arbitrary array of length n that's going to be the input to the quicksort algorithm and we'll be working with this fixed but arbitrary input array for the remainder of the analysis okay so just fix a single input in your mind now what's the relevant sample space well recall what a sample space is it's just all the possible outcomes of the randomness in the world so is all the distinct things that could happen now here the randomness is of our own devising it's just the random pivot sequences the random pivots chosen by quicksort so Omega is just the set of all possible random pivots the quicksort could choose now the whole point of this theorem proving that the average will average running time of quicksort is small boils down to computing the expectation of a single random variable so here's the random variable we're going to care about for a given pivot sequence remember that random variables are real valued functions define on the sample space so for a given point the sample space or pivot sequence Sigma we're going to define capital C of Sigma as the number of comparisons that quicksort makes where by comparison I don't mean something like with an array index in a for loop that's not what I mean by comparison I mean a comparison between two different entries of the input array like comparing the third entry in the array against the seventh entry in the array to see whether the third entry or the seventh entry is smaller notice that this is indeed a random variable that is given knowledge of the pivot sequence Sigma the choices of all pivots you can think of quicksort at that point is just a deterministic algorithm with all of the pivot choices predetermined and so a deterministic version of quicksort makes some deterministic number of comparisons so forgiven pivot sequence Sigma we're just calling C of Sigma to be whatever however many comparisons it makes given those choices of pivots now the theorem I stated is not about the number of comparisons of quicksort but rather about the running time of quicksort but really if you think about it kind of the only real work that the quicksort algorithm does is make comparisons between two between pairs of elements in the input array yes there's a little bit of other bookkeeping but that's all noise that's all second order stuff all real quick so really does is comparisons between the pairs of elements in the input array if you want to know what I mean by that a little more formally dominated by comparisons I mean that there exists a constant C so that the total number of operations of any type that quicksort executes is at most a constant factor larger than the number of comparisons so let's say that by RT I mean the number of primitive operations of any form the quicksort uses and for every pivot sequence Sigma the total number of operations is no more than a constant times the total number of comparisons and if you want to proof of this it's not that interesting so I'm not going to talk about it here but in the notes posted on the website there is a sketch of why this is true how you can formally argue that there isn't much work beyond just the comparisons but I hope most of you find that to be pretty intuitive so given this given that the running time of quicksort boils down just to the number of comparisons we want to prove the average running time is n log n all we got to do quote unquote all we have to do is prove that the average number of comparisons the quicksort makes is over n log N and that's we're going to do so that's what the the rest of these lectures are about so that's what we gotta prove we gotta prove that the expectation of this random variable C which counts up the number of comparisons quicksort makes is for this arbitrary input array a of length n bound is my Big O of n log n so the higher or bit of this lecture is a decomposition principle we've identified this random variable C the number of comparisons and it's exactly what we care about it governs the average running time of quicksort the problem is it's quite complicated it's very hard to understand what this capital C is it's fluctuating between n log N and N squared and it's hard to know how to get a handle on it so how are going to go about proving this assertion that the expected number of comparisons the quicksort makes is on average just over n log N at this point we've actually have a fair amount of experience with divide and conquer algorithms we've seen a number of examples and whenever we had to do a running time analysis of such an algorithm we run out our recurrence we applied the master method or in the worst case we wrote out a recursion tree to figure out the solution to that recurrence so you'd be very right to expect something similar to happen here but as we probe deeper and we think about quicksort we quickly realized that the master method just doesn't apply or at least not in the form that we're used to the problem is twofold so first of all the size of the two subproblems is random right as we discussed in the last video the quality of the pivot is what determines how balanced a split we get into the two subproblems it could be as bad as a subproblem of size 0 and 1 of size n minus 1 or it could be as good as a perfectly balanced split into two subproblems of equal sizes but we don't know it's going to depend on our random choice of the pivot moreover the master method at least as we discussed it required solve subproblems to have the same size and unless you're extremely lucky that's not going to happen in the quicksort algorithm it is possible to develop a theory of recurrence relations for randomized algorithms and to apply that to quicksort in particular but I'm not going to go that route for two reasons the first one is this really quite messy it gets pretty tentacle to talk about solutions to recurrences for randomized algorithms or to think about random recursion trees both of those get get pretty complicated the second reason is I really want to introduce you to what I call a decomposition principle by which you take a random variable that's complicated but that you care about a lot you decompose it into simple random variables which you don't really care about in their own right but which are easy to analyze and then you stitch those two things together using linearity of expectation so that's going to be the workhorse for our analysis of the quicksort of the quicksort algorithm and it's going to come up again a couple times in the rest of the course for example when we study hashing so to explain how this decomposition principle applies to quicksort in particular I'm going to need to introduce you to the building blocks simple random variables which will make up the complicated random variable that we care about the number of comparisons so here's some notation recall that we fixed in the background an arbitrary array of length N and that's denoted by capital A and some notation which is simple but also quite important by Z sub I what I mean is the eighth smallest element in the input array capital A also known as the ice order statistic so let me tell you what Zi is not okay what Zi is not in general is the element in the eighth position of the inputs unsorted array what Zi is is it's the element we're just going to wind up in the eighth element of the array once we sort it okay so if you fast forward to the end of a sorting algorithm in position I you're going to find the Zi so let me give you an example so suppose we had just a simple array here unsorted with the number six eight ten and two then z1 well that's the first smallest the one smallest or just the minimum so z1 would be the two Z 2 would be the six Z three would eight + Z four would be the ten for this particular input already okay so Zi is just the ice smallest number wherever it may lie in the original unsorted array that's what Zi refers to so we already defined the sample space that's just all possible choices of pivots the quicksort might make I already described one random variable the number of comparisons that quicksort makes on a particular choice of pivots now I'm gonna introduce a family of much simpler random variables which count merely the comparisons involving a given pair of elements in the input array not all elements just a given pair so for a given choice of pivots are given a sigma and for a given choices of I and J both of which serve between 1 and N and so we only count things once I'm going to insist that I is less than J always and now here's a definition by X IJ and this is a random variable so it's a function of the pivots chosen this is going to be the number of times that Zi and ZJ are compared in the execution of quicksort okay so this is going to be an important definition in our analysis it's important to understand it so for something like if the third smallest element and the seventh smallest element X IJ is asking that's when I equals 3 and J equals 7 X 3 7 is asking how many times those two elements get compared as quick sort of precedes and this is a random variable in the sense that if the pivot choices are all predetermined if we think of those being chosen in advance then there's just some fixed deterministic number of times that Zi and ZJ get compared so it's important you understand these random variables X IJ so the next quiz is going to ask a basic question about the range of values that a given X IJ can take on so for this quiz we're considering as usual some fixed input array and now furthermore fixed to specific elements of the input array for example the third smallest element whatever wherever it may lie and the seventh smallest element wherever it may lie think about just these pair of two elements what is the range of values that the corresponding random variable X IJ can take on that is what are the different numbers of times given pair elements might conceivably get compared in the execution of the quicksort algorithm all right so the correct answer to this quiz is the second option this is not a trivial quiz this is a little tricky to see so the assertion is that a given pair of elements they might not be compared at all they might be compared once and they're not going to get get paired up more than once okay so here what I'm going to discuss is why it's not possible for two given pair of elements to be compared twice during the execution of quicksort it'll be clear later on if it's not already clear now that both 0 & 1 or legitimate possibilities a pair of elements might never get compared and they might get compared once again we'll go into more detail on that in the next video so but why is it impossible to be compared twice well think about two elements say the third element and the seventh element and let's recall how the partition subroutine works observe that in quicksort the only place in the code were comparisons between pairs of the input array elements happens it only happens in the partition subroutine so that's where we have to drill down so what are the comparisons that get made in the partition subroutine well go back and look at that code the pivot elements is compared to each other element in the input array exactly once ok so the pivot just hangs out in the first entry of the array we have this for loop this index J which marches over the rest of the array and for each value of J the jate element of the input array gets compared to the pivot okay so summarizing in an invocation of partition every single comparison involves the pivot element okay so two elements get compared if and only if one is the pivot all right so let's go back to the question why can a given pair of elements in the input array get compared two or more times well think about the first time they ever get compared in quicksort it must be the case that at that moment were in a recursive call where either one of those two is the pivot element so it's the third smallest element or the seventh smallest element the first time those two elements had compared to each other either the third smallest or the seventh smallest is currently the pivot because all comparisons involve the pivot elements therefore what's going to happen in the recursion while the pivot is excluded from both recursive calls so for example if the seventh smallest element is currently the pivot that's not going to be passed on the recursive call which contains the third smallest element therefore if you're compared once one of the elements is the pivot and they'll never be compared again because the pivot will not even show up in any future recursive calls so let me just remind you of some terminology so a random variable which can only take on the value 0 or 1 is often called an indicator random variable because it's just indicating whether or not a certain thing that happens so in that terminology each X IJ is indicating whether or not the I smallest element in the array in the chase smallest element in the array ever get compared it can't happen more than once it may or may not happen and X IJ is 1 precisely when it happens so that's the event that it's indicating having defined the building blocks I need these indicator random variables these X I J's now I can introduce you to the decomposition principle as applied to quicksort so there's a random variable that we really care about which is denoted capital C the number of comparisons the quicksort makes that's really hard to get a handle on in and of itself but we can express C as a sum of indicator random variables of these X I J's and those we don't care about in their own right but they're going to be much easier to understand so let me just rewrite the definitions of C and the X IJ so we're all clear on them so C recall counts all of the comparisons between pairs of input elements that quick so whatever makes whereas an X IJ only counts the number and it's going to be 0 or 1 comparisons that involve the ice smallest and the J's smallest elements in particular now since every comparison involves precisely one pair of elements some I and some J with I less than J we can write C as the sum of the X I J's so don't get intimidated by this fancy double sum all this is doing is it's iterating over all of the ordered pairs so all of the pairs IJ where I and J are both between 1 and N and where I is strictly less than n this double sum is just a convenient way to do that iteration and of course no matter what the pivots chosen are we have this equality okay the comparisons us are somehow split up amongst the various pairs of elements the various isin js why is it useful to express a complicated random variable as a sum of simple random variables well because an equation like this is now right in the wheelhouse of linearity of expectation so let's just go ahead and apply that remember and this is super super important linearity of expectation says that the expectation of a sum equals the sum of the expectations and moreover this is true whether or not the random variables are independent and I'm not going to prove it here but you might want to think about the fact that the X I J's are not in fact independent so we're using the fact that linear expectation works even for non independent random variables and why is this interesting well the left-hand side this is complicated right this is the this is some crazy number of comparisons by some algorithm on some arbitrarily long array and it fluctuates between two pretty far apart numbers n log N and N squared on the other hand this does not seem as intimidating a given X IJ it's just 0 or 1 whether or not these two guys get compared or not so that is the power of this decomposition approach ok so it reduces understanding a complicated random variable to understanding simple random variables in fact because these are indicator random variables we can even clean up this expression some more so for any given X IJ being a zero one random variable if we expand the definition of expectation just as an average over the various values its what is it it's just well there's some probability it takes on the value 0 that's possible and there's some possibility it takes on the value 1 and of course this 0 part we can vary satisfyingly delete cancel and so the expected value of a given X IJ is just the probability that X IJ equals 1 and remember it's an indicator random variable it's one precisely when the ice smallest and the Jade smallest elements get compared so putting it all together you find that what we care about the average value of the number of comparisons made by quicksort on this input array is this double sum which iterates over all ordered pairs where each sum end is the probability that the corresponding X IJ equals 1 that is the probability that Zi and ZJ get compared and this is essentially the stopping point for this video for the first part of the analysis so let's call this star and put a nice circle around it so what's going to happen next is that in the second video for the analysis we're going to drill down on this probability probability that a given pair of elements gets compared then we're going to nail it we're going to get an exact expression as a function of I and J for exactly what this probability is then in the third video we're going to take that exact expression plug it into the sum and then evaluate this sum and it turns out the sum will evaluate to Big O of n log n so that's the plan that's how you apply decomposition in terms of zero one or indicator random variables by linearity of expectation in the next video we'll understand these simple random variables and then we'll wrap up in the third video before we move on to the next part of the analysis I do just want to emphasize that this decomposition principle is relevant not only for quicksort but it's relevant for the analysis of lots of randomized algorithms and we will see more applications at least one more application later in the course so just to kind of really hammer the point home let me spell out the key steps for the general decomposition principle so first you need to figure out what is it you care about so in quicksort we cared about the number of comparisons we have this lemma that said the running time is dominated by comparisons so we understood what we wanted to know the average value for the number of comparisons the second step is to express this random variable Y and as a sum of simpler random variables ideally indicator or zero one random variables now you're in the wheelhouse of linearity of expectation you just apply it and you find that what it is you care about the average value of the random value of the random variable Y is just the sum of the probabilities of various events that given XL random variable is equal to one and so the upshot is to understand the seemingly very complicated left-hand side all you have to do is understand something which in many cases is much simpler which is understand the probability of these various events and the next video I'll show you exactly how that's done in the case of quicksort worthies where we care about the X IJ is the probability that two elements gets compared so let's move on and get an exact expression for that probability