so before we embark on the analysis what are we hoping to understand well what seems intuitively clear is that there's going to be some trade-off between the two resources of a bloom filter one resource is space consumption the other resource is essentially correctness so the more space we use the larger number of bits we'd hope that we'd make fewer and fewer errors and then as we compress the table more and more reuse bits more and more for different objects then presumably the error rate is going to increase so the goal of the analysis that we're about to do is to understand this trade-off precisely at a quantitative level once we understand the trade-off curve between these two resources then we can ask is there a sweet spot which gives us a useful data structure quite small space and quite manageable error probability so the way we're going to proceed with the analysis will be familiar to those of you that watch the open addressing video about hash tables so to make the mathematical analysis tractable I'm going to make a heuristic assumption it's a strong assumption which is not satisfied by hash functions you would use in practice we're going to use that assumption to derive a performance guarantee for Bloom filters but as always in any implementation you should check that your implementation actually is getting performance comparable to what the idealized analysis should suggest that said if you use a good hash function and if you have non-pathological data the hopes and this is borne out in many empirical studies is that you will see performance comparable to what this heuristic analysis will suggest so what is the heuristic assumption well it's going to be again familiar from our hashing discussion we're just going to assume that all the hashing is totally random so for each choice of a hash function hi and for each possible object ax the slot the position of the array which the hash function gives for that object is uniform at random first of all and it's independent from all other outputs of all hash functions on all objects so the setup then is we have nbits we have a data set capital S which we've inserted into our Bloom filter now our eventual goal is to understand the error rate or the false positive probability that is the chance that an object which we haven't inserted into the bloom filter looks as if it has been inserted into the bloom filter but as a preliminary step I want to ask about the population of ones after we've inserted this data set capital S into the bloom filter so specifically let's focus on a particular position of the array and by symmetry it doesn't matter which one and let's ask what is the probability that a given bit a given position in this array has been set to one after we've inserted this entire data set capital S all right so this is uh this is a somewhat difficult quiz question actually the uh correct answer is the second answer it's 1 minus quantity 1 - 1 n raised to the number of hash functions K times the number of objects cardinality of s that's the probability that say the first bit of a bloom filter has been set to one after the data set capital S has been inserted so the maybe easiest way to see this is to First focus on the first answer so the first answer is going to be the probability I claim that the first bit is zero after the entire data set has been inserted then of course the probability that's one is just one minus this quantity which is equal to the second answer so we just need to understand why the first choice is the probability that the first bit equals zero well it's initially zero and remember stuff is only set from 0 to one so we need to analyze the probability that this first bit survives all of these darts that are getting thrown to the bloom filter over the course of this entire data set being inserted so there are these cardinality of s objects each get inserted on an insertion K darts uniformly at random and independent from each other are effectively thrown at the array at the bloom filter any position that a dart hits gets set to one maybe it was one already but if it was Zero it gets set to one if it's one it stays one so how is this first bit going to stay zero well it has to be missed by all of the darts a given Dart a given bit flip is uniformly likely to be any of the the n Bits so the probability that winds up being this bit is only 1/ n the probability that it's fortunately somebody else well that's 1 minus 1/ n so you have a chance of surviving a single Dart with probity 1 minus one/ n there's the number of hash functions K times the number of objects cardinality of as darts being thrown right K per object that gets inserted so the overall probability of eluding all of the darts is 1 - 1 n raised to the number of hash functions K times the number of insertions C value of s and again the probability that it's one is just one minus that Quant quty which is the second option in the quiz so let's go ahead and resume our analysis using the answer to that quiz so what do we discover we discover the probability that a given bit is one is 1 minus quantity 1 - 1 / n where n is the number of position raised to the number of hash functions K times the number of insertions cality of s so that's a kind of messy quantity so let's um recall a simple estimation fact that we used once earlier uh you saw this when we analyzed cargo's contraction algorithm and the benefit of multiple repetitions of cargo's contraction algorithm and the trick here is to estimate a quantity that's of the form 1 plus X or 1 - x by either of the x or either the minus X as the case may be so if you take the function 1 + x which goes through the points -1 0 and 01 and of course is a straight line and then you also look at the function e to the x X well those two functions are going to kiss at the point 0 comma 1 and everywhere else e to the x is going to be above 1+ X so for any real value of x we can always upper bound the quantity 1 plus X by eus X so let's apply this fact to this quantity here 1 - 1 n raed to the K * cality of s we're going to take X to be minus 1/ n so that gives us an upper bound on this a probability of 1 - e to Theus K * the number of insertions Over N okay so that's taking X to be - 1 n let's simplify our lives a little bit further by introducing some notation so I'm going to let B denote the number of bits that we're using per object so we this is the quantity I was telling you to think about as eight previously this is the ratio n the total number of bits divided by the cardinality of s so this green expression becomes 1 - e to the - K over B where B is the number of bits per object and now we're already seeing the type of trade-off that we were expecting remember we were expecting that as we use more and more space then the error rate we think should go down so if you compress the table a lot so you reuse bits for lots of different objects that's when you start going to see a lot of false positives so in this light blue expression if you take the number of bits per object to the number of space the amount of space little B if you take that growing very large tending to Infinity this exponent tends to zero so e to the minus 0 is one so overall this probability of a given bit being one is tending to zero so that is the more bits you have the bigger space you have the smaller the fraction of ones the bigger the fraction of zeros that should translate to a smaller false positive probability and that's what we'll make precise on the next and final slide so let's let's rewrite the upshot from the last slide the probability that a given bit is equal to one is Bed above by 1 - eus K over B where K is the number of hash functions and B is the number of bits we're using per objects now this is not the quantity we care about the quantity we care about is a false positive probability where something looks like it's in the bloom filter even though it's never been inserted so let's focus on some object like some IP address which has never ever been inserted into this Bloom filter so for a given object X which is not in the data set that is has not been inserted Ed into the bloom filter what has to happen for us to have a successful lookup a false positive for this object well each one of its K bits has to be set to one so we already computed the probability that a given bit is set to one so what has to happen for all K of the bits that indicate X's membership in the bloom filter all K of them have to be set to one so we just take the quantity we computed on the previous slide and we rais that to the K power indicating that it has to happen K different times so believe it or not we now have exactly what we wanted what we set out to do which is derive a quantitative understanding of the intuitive tradeoff between on the one hand the space used and on the other hand on the error probability the false positive probability so we're going to call this green circled quantity a name we'll call it Epsilon for the error rate and again all errors are false positives and again as B goes to Infinity as we use more and more space this exponent goes to zero so 1 minus E to that quantity is going going to zero as well and of course once we power it to the Ki power it gets even closer to zero so the bigger b gets the smaller this error rate Epsilon gets so now let's get to the punch line so remember the question is is this data structure actually useful can we actually set all of the parameters in a way that we get both really usefully small space but a tolerable error Epsilon and of course we wouldn't be giving this video if the answer wasn't yes now one thing I've been alluding all along is how do we set K how do we choose the number of hash functions I told you at the very beginning to think of K as a small constant like 2 3 4 5 and now that we have this really nice quantitative version of how the error rate and the space trade off with each other we can answer how to set K namely set K optimally so what do I mean well fix the number of bits that you're using per object 8 16 24 whatever for fixed B you can just choose the K that minimizes this green quantity that minimizes the error rate Epsilon so how do you minimize this quantity well you do it just like you learn in calculus and I'll leave this as an exercise for you to do in the privacy of your own home but for fixed B the way to get this green quantity Epsilon as small as possible is to set the number of hash functions K to be roughly the natural log of two that's a number less than one notice that's like 693 times B so in other words the number of hash functions for the optimal implementation of a bloom filter is scaling linearly with the number of bits that you're using per object it's about 693 times the bits per object of course this is generally not going to be an integer so you just pick K to be either uh this number rounded up or this number rounded Down But continuing theistic analysis now that we know how to set K optimally to minimize the error for a given amount of space we can plug that value of K back in and see well how does the space and the error rate trade off against each other and we get a very nice answer specifically we get that the error rate Epsilon this is under an optimal choice of the number of hash functions K decreases exponentially in the number of bits that you use per object so it's roughly 12 raised to the natural log of 2 or 693 roughly times the number of bits per object B but again the key qualitative Point here is to notice that Epsilon is going down really quickly as you scale B if you dble the number of bits that you're allocating per object you're squaring the error rate and for small error rates squaring it makes it much much much smaller and of course this is just one equation in two variables if you prefer you can solve this equation to express B the space requirement as a function of an error requirement so if you know that the tolerance for false positives in your application is 1% you can just solve this for B and figure out how many bits per object you need to allocate and so rewriting what you get is that the number of bits per object that you need is roughly 1.44 * the log base 2 of 1/ Epsilon so as expected as Epsilon gets smaller and smaller you want fewer and fewer errors the space requirements will increase so the final question is is it a useful data structure can you set all the parameters so that you get uh you know a really interesting space error tradeoff and the answer is totally so let me give you an example let's go back to having 8 Bits of storage per object so that's corresponds to Bal 8 then what this pink formula indicates is we should use five or six hash functions and already you have an error probability of something like 2% which for a lot of the motivating applications we talked about is already good enough and again if you double the number of bits to say 16 per object then this error probability would be really small it would be pushing you know one in 5,000 or something like that so to conclude at least in this idealized analysis which again you should check against in any real world implementation although empirically it is definitely achievable with well implemented Bloom filters and non-pathological data to get this kind of performance even with really a ridiculously minuscule amount of space per object much less generally than storing the object itself you can get fast inserts fast lookups you do have to have false positives but with a very controllable amount of error rate and that's what makes Bloom filters a win in a number of applications