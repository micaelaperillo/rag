So having motivated and hyped up the generality of the master method and its use for analyzing recursive algorithms, let's move on to its precise mathematical statement. Now the master method is in some sense exactly what you want. It's what I'm going to call a black box for solving recurrences. Basically, it takes as input a recurrence in a particular format and it spits out as output a solution to that recurrence, an upper bound on the running time of your recursive algorithm. That is you just plug in a few parameters of your recursive algorithm and boom out pops its running time. Now the master method does require a few assumptions and let me be explicit about one of them right now. Namely, the master method at least the one I'm going to give you is only going to be relevant for problems in which all of the subpros have exactly the same size. So for example in merge sort there are two recursive calls and each is on exactly one half of the array. So merge sort satisfies this assumption. Both sub problems have equal size. Similarly, in all both of our integer multiplication algorithms, all sub problems were on integers with n over2 digits with half as many digits. So those will all also obey this assumption. If for some reason you had a recursive algorithm that recursed on a third of the array and then on the other two/irds of the array, the master method that I'm going to give you will not apply to it. There are generalizations of the master method that I'm going to show you which can accommodate unbalanced subpros sizes, but those are outside the scope of this course. This will be sufficient for almost all of the examples we're going to see. One notable exception for those of you that watch the optional video on a deterministic algorithm for linear time selection. That will be one algorithm which has two recursive calls on different subpros sizes. So to analyze that recurrence, we'll have to use a different method, not the master method. Next, I'm going to describe the format of the recurrences to which the master method applies. As I said, there are more general versions of the master method which apply to even more recurrences, but the one I'm going to give you is going to be reasonably simple and it will cover pretty much all the cases you're likely to ever encounter. So, recurrences have two ingredients. There's the relatively unimportant but uh still necessary base case step and we're going to make the obvious assumption which is just satisfied by every example we're ever going to see in this course which is that at some point in which the input size drops to a sufficiently small amount then the then the recursion stops and the problem is solve the sub problem is solved in constant time since this assumption is pretty much always satisfied in every problem we're going to see I'm not going to discuss it much further let's move on to the general case where there are recursive calls so we assume the recurren is given in the following format. The running time on an input of length n is bounded above by some number of recursive calls. Let's call it a different recursive calls. And then each of these subpros has exactly the same size. And it's one over b fraction of the original input size. So there's a recursive calls each on an input of size n overb. Now, as usual, there's the case where n over b is a fraction and not an integer. And as usual, I'm going to be sloppy and ignore it. And as usual, that sloppiness has no implications for the final conclusion. Everything that we're going to discuss is true for the same reasons in the general case where n overb is not an integer. Now, outside the recursive calls, we do some extra work. And let's say that it's O of N to the D for some parameter D. So, in addition to the input size N, there are three letters here which we need to be very clear on what their meaning is. So first of all there's a which is the number of subpros the number of recursive calls. So a could be as small as one or it might be some larger integer. Then there's b. B is the factor by which the input size shrinks before a recursive call is applied. B is some constant strictly greater than one. So for example, if you recurse on half of the original problem, then B would be equal to two. It better be strictly bigger than one so that eventually you stop recursion. so that eventually that you terminate. Finally, there's D, which is simply the exponent in the running time of the quote unquote combined step. That is the amount of work which is done outside of the recursive calls. And D could be as small as zero, which would indicate constant amount of work outside of the recursive calls. One point to emphasize is that A, B, and D are all constants. That's all. They're all numbers that are independent of n. So a, b, and d are going to be numbers like 1, 2, 3, or four. they do not depend on the input size n. And in fact, let me just redraw the d so that you don't confuse it with the a. So again, a is the number of recursive calls and d is the exponent in the running time governing the work done outside of the recursive calls. Now one comment about that final term that bigo of n to the d. Uh on the one hand, I'm being sort of sloppy. I'm not keeping track of the constant that's hidden inside the bigo notation. uh I'll be explicit with that constant when we actually prove the master method, but it's really not going to matter. It's just going to carry through the analysis uh without affecting anything. So you can go ahead and ignore that constant inside the big O. Obviously the constant and the exponent namely D is very important, right? So depending on what D is depends on whether that amount of time is constant, linear, quadratic or so on. So certainly we care about the constant D. So that's the input to the master method. It is a recurrence of this form. So you can think of it as a recursive algorithm which makes a recursive calls each on sub problems of equal size each of size n over b plus it does n to the d work outside of the recursive calls. So having set up the notation I can now precisely state the master method for you. So given such a recurrence we're going to get an upper bound on the running time. So the running time on inputs of size n is going to be upperbounded by one of three things. So somewhat famously the master method has three cases. So let me tell you about each of them. The trigger which determines which case you're in is a comparison between two numbers. First of all a recall A is the number of recursive calls made and B raised to the D power. Recall B is the factor by which the input size shrinks before you recurse. D is the exponent in the amount of work done outside the recursive call. So we're going to have one case for when they're equal. We're going to have one case for when A is strictly smaller than B to the D and the third case is when A is strictly bigger than B to the D. And in the first case, we get a running time of big O of N to the D * log N and again this is D the same D that was in the final term of the recurrence. Okay, the work done outside of the recursive calls. So the first case the running time is the is the same as the running time in the recurrence outside the recursive calls but we pick up an extra login factor. In the second case where A is smaller than B to the D the running time is merely big O of N to the D. And this case might be somewhat stunning that this could ever occur because of course in the recurrence what do you do? You do some recursion plus you do N to the D work outside of the recursion. So in the second case, it actually says the work is dominated by just what's done outside the recursion in the outermost call. The third case will initially seem the most mysterious. When A is strictly bigger than B to the D, we're going to get a running time of big O of N to the log base B of A where again recall A is the number of recursive calls and B is the factor by which the input size shrinks before you recurse. So that's the master method with its three cases. Let me give this to you in a cleaner slide to make sure there's no ambiguity in my handwriting. So here's the exact same statement, the master method once again with its three cases depending on how a compares to B to the D. So one thing you'll notice about the this version of the master method is that it only gives upper bound. So we only say that the solution to the recurrence is bigo of some function. And that's because if you go back to our recurrence, we used bigo O rather than theta in the recurrence. And this is in the spirit of the course where as algorithm designers, our natural focus is on upper bounds on guarantees for the worst case running time of an algorithm. And we're not going to focus too much most of the time on proving stronger uh bounds in terms of theta notation. Now a good exercise for you to check if you really understand the proof of the master method after we go through it would be to show that if you strengthen the hypothesis and you assume that recurrence has the form t of n equals a * t of n / b plus theta of n the d then in fact all three of these big o's in the statement of the master method become thetas and the solution becomes asmtoically exact. So, one final comment. You'll notice that I'm being asymmetrically sloppy with the two logarithms that appear in these formulas. So, let me just explain why. In particular, you'll notice that in case one with the logarithm, I'm not specifying the base. Why is that true? Well, it's because the logarithm with respect to any two different bases differs by a constant factor. So the logarithm base e that is the natural logarithm and the logarithm base 2 for example differ by only a constant factor independent of the argument n. So you can switch this logarithm to whatever constant base you like. It only changes the leading constant factor which of course is being suppressed in the big notation. Anyways on the other hand in case three where we have a logarithm in the exponent once it's in the exponent we definitely care about that constant. Constance is the difference between say linear time and quadratic time. So we need to keep careful track of the ba the logarithm base in the exponent in case three and that lo and that base is precisely B the factor by which the input shrinks with each recursive with each recursive call. So that's the precise statement of the master method. In the rest of this lecture we'll work toward understanding the master method. So first in the next video we'll look at a number of examples including resolving the running time of Gaus's recursive algorithm for integer multiplication. Following those several examples we'll prove the master method. And I know now these three cases probably look super mysterious, but if I do my job, by the end of the analysis, these three cases will seem like the