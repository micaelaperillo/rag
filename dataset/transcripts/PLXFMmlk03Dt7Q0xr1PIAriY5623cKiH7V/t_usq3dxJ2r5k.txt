So what I want to do next is test your understanding of both the search and insertion procedures by asking you about their running time. So of the following four parameters of a search tree that contains n different keys, which one governs the worstase time of a search or of an insertion? So the correct answer is the third one. So the height of a search tree governs the worstase time of a search or of an insertion. Notice that means merely knowing the number of keys n is not enough to deduce what the worst case search time is. You also have to know something about the structure of the tree. So to see that just let's think about the two examples that we've been running so far. One of which is nice and balanced and the other of which which contains exactly the same five keys is super unbalanced. It's this crazy linked list in effect. So in any search tree, the worst case time to do a search or an insertion is proportional to the largest number of pointers, left or right child pointers that you might have to follow to get from the root all the way to a null pointer. Of course, in a successful search, you're going to terminate before you encounter a null pointer, but in the worst case or an insertion, you go all the way to a null pointer. Now, in the tree on the left, you're going to follow at most three such pointers. So, for example, if you're searching for 2.5, you're going to follow a left pointer followed by a right pointer followed by another right pointer, and then that one's going to be null. So, you're only going to follow three pointers. On the other hand, in the right tree, you might follow as many as five pointers before that fifth pointer is null. For example, if you search for the key zero, you're going to traverse five left pointers in a row, and then you're finally going to encounter the null at the end. So, it is not constant time. Certainly, you have to get to the bottom of the tree. It's going to be proportional to logarithm the logarithm in the number of keys if you have a nicely balanced binary search tree like this one on the left. It's going to be proportional to the number of keys n as in the fourth answer if you have a really lousy search tree like this one on the right. And in general the search time or the insertion time is going to be proportional to the height the largest number of hops you need to take to get from the root to a leaf of the tree. Let's move on to some more operations that search trees support but that for example the dynamic data structures of heaps and hash tables do not. So let's start with the minimum and the maximum. So by contrast in a heap remember you can choose one of the or the two. You can either find the minimum easily or find the maximum easily but not both. In a search tree it's really easy to find either the min or the max. So let's start with the minimum. One way to think of it is that you do a a search for negative infinity in the search tree. So you start at the root and you just keep following left child pointers until you run out until you hit a null. And whatever the last key that you visit has to be the smallest key of the tree, right? Because think about it. Suppose you start at the root, right? Suppose that the root is not the minimum. Then the where's the minimum got to be? It's got to be in the left subree. So you follow the left child pointer and then you just repeat the argument. If you haven't already found the minimum, where's it got to be with respect to the current place? It's got to be in the left subree. And you just iterate until you can't go to the left any further. So for example, in our running search tree, you'll notice that if we just keep following left-handed pointers, we'll start at the three, we'll go to the one, we'll try to go left from the one, we'll hit a null pointer, and we return one. And one is indeed the minimum key in this tree. Now given that we've gone over how to compute the minimum, no prizes to guess how we compute the maximum. Of course, if we want to compute the maximum, instead of following left child pointers, we follow right child pointers by symmetric reasoning. That's guaranteed to find the largest key in the tree. It's like searching for the key plus infinity. All right. So what about computing the predecessor? So remember this means you're given a key in the tree, an element of the tree, and you want to find the next smallest element. So for example, the predecessor of the three is two. The predecessor of the two in this tree is the one. The predecessor of the five is the four. The predecessor of the four is the three. So here I'll be a little handwavy just in the interests of getting through all of the operations in a reasonable amount of time. But let me just point out that there's one really easy case and then there's one slightly trickier case. So the easy case is when the node with the key K has a non-mpy left subree. If that's the case, then what you want is simply the biggest element in this node's left subree. So I'll leave it for you to prove formally that this is indeed the correct way to compute predecessors for keys that do have a non-mpy left subree. Let's just verify it in our example by going through the trees that have a left subree and checking that this is in fact what we want. Now, if you look at it, there's actually only two nodes that have a non-mpy left subree. The three has a non-MPT left subree. And indeed, the largest key in the left subree of three is the two. And that is the predecessor of the three. So, that worked out fine. And then the other node with a non-MP left subree is the five and its left subree is simply the element four. Of course, the maximum in that tree is also four. And you'll notice that is indeed the predecessor of five in this entire search tree. So the trickier case is what happens if you have a key with no left subree at all. Okay. So what are you going to do if you're not in the easy case? Well, given at this node with key K, you only have three pointers. And by assumption, the left one is null. So that's not going to get you anywhere. Now, the right child pointer, if you think about it, is totally pointless for computing the predecessor. Remember the predecessor is going to be a key less than the given key K. The right subree by definition of a search tree only has keys that are bigger than K. So it stands to reason to find the predecessor, we got to follow the parent pointer. Maybe in fact more than one parent pointer. So to motivate exactly how we're going to follow parent pointers, let's look at a couple of examples in our favorite search tree here on the right. So let's start with the node two. So we know we got to follow a parent pointer. When we follow two's parent pointer, we get to one. And boom, one in fact is two's predecessor in this tree. So that was really easy to compute two's predecessor. It seems that all we have to do is follow the parent pointer. So for another example though, let's think about the node four. Now four, when we follow its parent pointer, we get to five. And five is not four's predecessor. It's four successor. Right? We want a key that is less than where we started. We followed the parent pointer and it was bigger. But if we follow one more parent pointer, then we get to the three. So from the two, we needed to follow one parent pointer. From the four, we needed to follow two parent pointers. But the point is, you just need to follow parent pointers until you get to a node with key smaller than your own. And at that point, you can stop and that's guaranteed to be the predecessor. So hopefully you found this intuitive. I should say I have definitely not formally proved that this works. And that is a good exercise for those of you that want to have a deeper understanding of search trees and this magical search tree property and all of the structure that it grants you. The other thing I should mention is another way to interpret the terminating criterion. So what I've said is you stop your search of parent pointers as soon as you get to a key smaller than yours. If you think about a little bit, you'll realize you'll get to a key smaller than yours the very first time you take a left turn. So the very first time that you go from a right child to its parent. Look at the example. When we started from two, we took a left turn, right? We went up a link going leftward. Two is a right child of one. And that's when we got to the predecessor in just one step. By contrast, when we started from the four, our first step was to the right. So, we got to a node that was bigger than where we started. Four is five's left child, so it's got to be smaller than five. But the first time we took a left turn, on the next step, we got to a node that was not only smaller than five, but actually smaller from four, smaller from our starting point. So in fact, you're going to see a key smaller than your starting point the very first time you take a left turn. The very first time you go from a node to a parent and in fact that node is that parent's right child. So this is another statement which I think is intuitive but which formally is not totally obvious and again I encourage you to think carefully about why these two descriptions of the terminating criterion are exactly the same. So it doesn't matter if you stop when you first find a key smaller than your starting point. It doesn't matter if you first stop when you follow a parent pointer that goes from a node that's the right child of a node. Either way, you're going to stop at exactly the same time. So, I encourage you to think about why those are the exact same stopping condition. A couple of other details. If you start from the unique node that has no predecessor at all, you're never going to trigger this terminating condition. So, for example, if you start from the node one in the search tree, not only is the left subree empty, which says you're supposed to start traversing parent pointers, but then when you traverse a parent pointer, you only go to the right. You never turn left. And that's because there's no predecessor. So, that's how you detect that you're at the minimum of a search tree. And then, of course, if you wanted to compute the successor of a key instead of the predecessor, obviously, you just flip left and right throughout this entire description. So that's the highle explanation of all of these different ordering operations. Minimum, maximum, predecessor, and successor work in a search tree. Let me ask you the same question I asked you when we talked about search and insertion. How long do these operations take in the worst case? Well, the answer is the same as it was before. It's proportional to the height of the tree and the explanation is exactly the same as it was before. So to understand the dependence on the height, let's just focus on the maximum operation as is stated in the question. The other three operations, the running time is proportional to the height in the worst case for exactly the same reasons. So what does the max operation do? Well, you start at the root and you just follow right child pointers until you run out of them, until you hit null. So you know that the running time is going to be no worse than the longest such path. It's a particular path from the root to essentially a leaf. So it's never going to have running time more than the height of the tree. On the other hand, for all you know, the path from the root to the maximum key might well be the longest one in the tree. It might be the path that actually determines the height of the search tree. So, for example, in our running unbalanced example, that would be a bad tree for the minimum operation. If you look for the minimum in this tree, then you'd have to traverse every single pointer from five all the way down to one. Of course, there's an analogous bad search tree for the maximum operation where the one is the root and the five is all the way down as a leaf. Another thing you can do with search trees, which mimics what you can do with sorted arrays, is you can print out all of the keys in sorted order in linear time with constant time per element. Obviously, in a sorted array, this is trivial. You just use a for loop starting at the beginning, going to the end of the array, printing out the keys one at a time. And there's a very elegant recursive implementation for doing the exact same thing in a search tree. And this is known as an inorder traversal of a binary search tree. So as always, you begin at the beginning, namely at the root of the search tree. And a little bit of notation, let's call all of the search tree that starts at R's left child T subL and the search tree rooted at R's right child T subR. In our running example, of course, the root is three. T subl would correspond to the search tree comprising only the elements one and two. T subR would correspond to the subree comprising only the elements five and four. Now remember, we want to print out the keys in increasing order. So in particular, the first key we want to print out is the smallest of them all. So something we definitely don't want to do is we don't want to first print out the key at the root. For example, in our search tree example, the roots key is three. We don't want to print that out first. We want to print out the one first. So where's the minimum lie? Well, by the search tree property, it's got to lie in the left subree T subl. So we're just going to recurse on T subl. So by the magic of recursion, or if you prefer induction, what recursing on T subl is going to accomplish is we're going to print out all of the keys in T subl in increasing order from smallest to largest. Now that's pretty cool because T subl contains exactly the keys that are smaller than the key at the root. Remember that's the search tree property. Everything bigger than the roots key has to be in the left subree. Everything bigger than the roots key have to be in its right subree. So in our concrete example, this first recursive call is just going to print out the keys one and then two. And now if you think about it, it's the perfect time to print out the key at the root, right? We want to print out all the keys in increasing order. We've already done everything less than the roots key. We're you know recursing on the right hand side will take care of everything bigger than it. So in between the two recursive calls and this is why it's called an in order traversal. That's when we want to print out R is key. And clearly this works in our concrete example. The reverse recursive call prints out one and two. It's the perfect time to print out three. And then the recursive call will print out four and five. And more generally the recursive call on the right sub tree will print out all of the keys bigger than the roots key in increasing order. again by the magic of recursion or induction. So the fact that this pseudo code is correct, the fact that this so-called in order traversal indeed prints out the keys in increasing order is a fairly straightforward proof by induction. It's very much in the spirit of the proofs by induction of correctness of divide and conquer algorithms that we discussed earlier in the course. So what about the running time of an inorb traversal? The claim is that the running time of this procedure is linear. It's O of N where N is the number of keys in the search tree. And the reason is there's exactly one recursive call for each node of the tree and constant work is done in each of those recursive calls. And a little more detail. So what does the in order traversal do where it prints out the keys in increasing order? In particular, it prints out each key exactly once. Each recursive call prints out exactly one keys value. So there's exactly n recursive calls. And all a recursive call does is print one thing. So n recursive calls constant time for each that gives us a running time of o of n overall. In most data structures deletion is the most difficult operation and in search trees there are no exception. So let's get into it and talk about how deletion works. There are three different cases. So the first order of business is to locate the node that has the key k to locate the node that we want to get rid of. All right. So for starters, maybe we're trying to delete the key two from our running example search tree. So the first thing we need to do is figure out where it is. So there are three possibilities for the number of children that a node in a search tree might have. It might have zero children. It might have one child. It might have two children. Corresponding to those three cases, the deletion pseudo code will also have three cases. So let's start with the happy case where there's only zero children. Like in this case where we're deleting the key two from the search tree. Then of course we can without any reservations just delete the node directly from the search tree. Nothing can go wrong. There's no children depending on that node. Then there's the medium difficult case. This is where the node containing K has one child. An example here would be if we wanted to delete five from the search tree. So the medium case is also not too bad. All you got to do is splice out the node that you want to delete. that creates a hole in the tree, but then the that no deleted node's unique child assumes the previous position of the deleted node. I could make a nerdy joke about Shakespeare right here, but I'll refrain. For example, in our five node search tree, if we wanted to, let's say we haven't actually deleted two out of this one. If we wanted to delete the five, the five, we take it out of the tree. that would leave a hole. But then we just replace the position previously held by five by its unique child four. And if you think about it, that works just fine in the sense that that preserves the search tree property. Remember the search tree property says that everything in say a right subree has to be bigger than everything in the node's key. So we've made four the new right child of three. But four and any children that it might have were always part of three's right subree. So all that stuff has got to be bigger than three. So there's no problem putting four and possibly all of its descendants uh as the right child of three. The search tree property is in fact retained. So the final difficult case then is when the node being deleted has both of its children has two children. So in our running example with five nodes, this would only transpire if we wanted to delete the root. If we wanted to delete the key three from the tree. The problem of course is that you know you can try ripping out this node from the tree but then there's this hole and it's not clear that it's going to work to promote either child into that spot. You might stare at our example search tree and try to understand what would happen if you tried to bring one up to be the root or if you tried to bring five up to be the root. Problems would happen. That's what would happen. This is an interesting contrast to when we faced the same issue with heaps because the heap properties in some senses perhaps less stringent. There we didn't have an issue when we wanted to delete something with two children. We just promoted the smaller of the two children assuming we wanted to export an uh extract min operation. Here we're going to have to work a little harder. In fact, this is going to be a really neat trick. We're going to do something that reduces the case of two children to the previously solved cases of zero or one children. So here's the very sneaky way we identify a node to which we can apply either the case zero or the case one operation. What we're going to do is we're going to start from K and we're going to compute K's predecessor. Remember this is the next smallest key in the tree. So for example, the predecessor of the key three is two. That's the next smallest key in the tree. In general, let's call K's predecessor L. Now this might seem complicated, right? We're trying to implement one tree operation, namely deletion, and all of a sudden we're invoking a different tree operation predecessor, which we covered a couple slides ago. And to some extent, you're right. you know, delete. This is a non-trivial operation, but it's not quite as bad as you think for the following reason. When we compute this predecessor, we're actually in the easy case of the predecessor operation conceptually. Remember, how do you comput a predecessor? Well, it depends. What does it depend on? It depends on whether you've got a non-MPT left subree or not. If you don't have a non-mpt left subree, that's when you got to do this thing where you follow parent pointers upward until you find a key which is smaller than where you started. But if you've got a left subree, then it's easy. You just find the maximum of the left subree, and that's got to be the predecessor. And remember, finding maximum are easy. All you do is follow right child pointers until you can't anymore. Now, what's cool is because we only bother with this predecessor computation in the case where K's node has both children. We only have to do it in the case where it has a non-mpy left subree. So really when we say compute K's predecessor L all you got to do is follow K's left child that's not null because it has both children and then follow right child pointers until you can't anymore and that's the predecessor. Now here's the fairly brilliant part of the way you do implement deletion in a search tree which is you swap these two keys K and L. So, for example, in our running search tree, instead of this three at the root, we would put a two there. And instead of this two at the leaf, we would put a three there. Now, the first time you see this, it should strike you as a little crazy, maybe even cheating, like we're just completely disregarding the rules of rules of search trees. And actually, it is like check out what happened to our example search tree. We swapped the three and the two, and this is not a search tree anymore, right? So, we have this three which is in two's left subree and the three is bigger than the two and that is not allowed. That is a violation of the search tree property. Oops. So, how can we get away with this? We can get away with this because we're going to delete three anyways. So, we're going to wind up with a search tree at the end of the day. So, we may have messed up the search tree property a little bit, but we've swapped K into a position where it's really easy to get rid of. Well, how did we compute K's predecessor L? Ultimately, that was the result of a maximum computation which involves following right child pointers until you get stuck. And L was the place we got stuck. What does it mean to get stuck? It means L's right child pointer is null. It does not have two children. In particular, it does not have a right child. Once we swap K's into L's old position, K now does not have a right child. It may or may not have a left child. In the example on the right, it does not have a left child either in its new position. But in general, it might have a left child, but it definitely doesn't have a right child because that was a position at which a maximum computation got stuck. And if we want to delete a node that has only zero or one child, well, that we know how to do that we covered on the last slide. Either you just delete it, that's what we do in the running example here, or in the case where K's new node does have a left child, you would do the splice out operation. So you would rip out the node that contains K and the unique child of that node would assume uh the previous position of that node. Now an exercise which I'm not going to do here but I strongly encourage you to think through in the privacy of your own home is that in fact this deletion operation retains the search tree property. So roughly speaking when you do this swap you can violate the search tree property as we see in this example but all of the violations involve the node you're about to delete. So once you delete that node, there's no other violations of the search tree property. So bingo, you're left with a search tree. The running time this time, no get no prizes for guessing what it is. Uh because it's basically just one of these predecessor computations plus some pointer rewiring. Uh just like predecessor and search. It's going to be governed by the height of the tree. So let me just say a little bit about the final two operations mentioned earlier, select and rank. So remember select is just the selection problem. I give you an order statistic like 17 and I want you to return the 17th smallest key in the tree. Rank is I give you a key value and I want to know how many keys in the tree are less than or equal to that value. So to implement these operations efficiently, we actually need one small new idea which is to augment binary search trees with additional information at each node. So now a search tree will contain not just a key but also information about the tree itself. So this idea is often called augmenting your data structure. And perhaps the most canonical augmentation of a search tree like these is to keep track at each node not just of a key value but also of the population of tree nodes in the subree that's rooted there. So let's call this size of x which is the number of tree nodes in the subree rooted at x. So to make sure you know what I mean, let me just tell you what the size field should be for each of the five nodes in our running switch tree example. So again, remember we're thinking about how many nodes are in the subree rooted a given node or equivalently following child pointers from that node. How many different tree nodes can you reach? So from the root, of course, you can reach everybody. Everybody's in the tree rooted at the root. So the size there is five. By contrast, if you start at the node one, well, you can get to the one or you can follow the right child pointer to get to the two. So, at the one, the size would be two. At the node with the key value five, for the same reason, the size would be two. At the two leaves, the sub tree root of a leaf is just the leaf itself. So there, the size would be one. There's an easy way to compute the size of a given node once you know the size of its two sub trees. So if a given node in a search tree has children Y and Z, then how many nodes are there in the sub tree rooted at X? Well, there's those that are rooted at Y. There are those in the left subree. There are those that are reachable from Z. That is there are the children that are also uh children of Z. And then there's X itself. Now in general whenever you augment a data structure and this is something we'll talk about again when we discuss red black trees you got to pay the piper. So the extra data that you maintain it might be useful for speeding up certain operations but whenever you have operations that modify the tree specifically insertion and deletion you have to take care to keep that extra data valid to keep it maintained. Now in the case of these subree sizes, they're quite straightforward to maintain under insertion and deletion without affecting the running time of insertion and deletion very much. But that's something you should really think about offline. For example, when you perform an insertion, remember how that works? You do a essentially a search. You follow left and right child pointers down to the bottom of the tree until you hit a null pointer. Then that's where you stick the new node. Now what you have to do is you have to trace back up that path all of the ancestors of the new node you just inserted and increment their subree sizes by one. So let's wrap up this video by showing you how to implement the selection procedure given an order statistic in a search tree that's been augmented so that at every node you know the size of the subree rooted at that node. Well, of course, as always, you start at the beginning, which in a search tree is the root. And let's say the root has subchildren Y and Z. Y or Z could be null. That's no problem. We just think of the size of a null node as being zero. Now, what's the search tree property? It says every the keys that are less than the key stored at X are precisely the ones that are in the left subree at X. The keys in the tree that are bigger than the key at X are precisely the ones you're going to find in X's right subree. So suppose we're asked to find the 17th order statistic in a search tree. The 17th smallest key that's stored in the tree. Where is it going to be? Which sub where should we look? Well, it's going to depend on the structure of the tree. And in fact, it's going to depend on the subree sizes. This is exactly why we're keeping track of them so we can quickly make decisions about how to navigate through the tree. So for a simple example, suppose that X's left subree contains say 25 keys. So remember Y knows locally exactly what the population of its subree is. So in constant time from X, we can figure out how many keys are in Y subree. Let's say it's 25. Now by the defining property of search trees, these are the 25 smallest keys anywhere in the tree, right? X is bigger than all of them. Everything in X's right subree is bigger than all of them. So the 25 smallest order statistics are all in the subree rooted at Y. Clearly that's where we should recurse. Clearly that's where the answer lies. So we can recurse on the subree rooted at Y. And then we are again looking for the 17th order statistic in this new smaller search tree. On the other hand, suppose when we start at X and we look we ask Y how many nodes are there in your subree. It may be Y locally has stored the number 12. So there's only 12 things in X's left subree. Well, okay, X itself is bigger than all of them. So that's going to X is going to be the 13th biggest order statistic. It's going to be the 13th biggest element in the tree. Everything else is in the right subree. So in particular, the 17th order statistic is going to be in the right subree. So we're going to recurse in the right subree. Now what are we looking for? We're not looking for the 17th order statistic anymore. The 12 smallest things are all in X's subree. X itself is the 13th smallest. So we're looking for the fourth smallest of what remains. So the recursion is very much along the lines of what we did in the divide and conquer selection algorithms earlier in the course. So to fill in some more details, let's let A denote the subtree size at Y. And if it happens that X has no left child, we'll we'll define A to be zero. So the super lucky case is when there's exactly I minus one nodes in the left subree. That means the root here X is itself the E order statistic. Remember it's bigger than everything in its left subree. It's smaller than everything in its right subree. But in the general case we're going to be recursing either on the left subree or on the right subree. We recurse on the left subree when its population is large enough that we're guaranteed it encompasses the e order statistic. And that happens exactly when its size is at least I. That's because the left subree has the smallest keys that are anywhere in the search tree. And in the final case, when the left subree is so small that not only does it not contain the e order statistic, but also x is too small to be e order statistic, then we recurse in the right subree knowing that we've thrown away a + one, the a+ one smallest key values anywhere in the original tree. So correctness of this procedure is pretty much exactly the same as the inductive correctness for the selection algorithms we discussed earlier. In effect, the root of the search tree is acting as a pivot element with everything in the left subree being less than the root, everything in the right subree being greater than the element in the root. So that's why the recursion is correct. As far as the running time, I hope it's evident from the pseudo code that we do constant time each time we recurse. How many times can we recurse? while we keep moving down the tree. The maximum number of times we can move down the tree is proportional to the height of the tree. So this again is proportional to the height. So that's the select operation. There's an analogous way to write the rank operation. Remember this is where you're given a key value and you want to count up the number of stored keys that are less than or equal to that target value. Again you use these augmented search trees and again you can get running time proportional to the height. And I encourage you to think through the details of how you'd implement rank offline.