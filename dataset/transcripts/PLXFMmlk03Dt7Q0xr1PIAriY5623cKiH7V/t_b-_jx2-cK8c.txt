so in the last video I left you with a cliffhanger I introduced you to the minimum cut problem I introduced you to a very simple and elegant randomize the algorithm in the form of the contraction algorithm we observed that sometimes it finds them in cut and sometimes it doesn't and so the $64,000 question is just how frequently does it succeed and just how frequently does it fail so now that hopefully you've brushed up on your conditional probability and independence we're going to give a very precise answer that question in this lecture so recall on this problem we're given as input in an undirected graph possibly with parallel edges and the goal is to compute among the exponential number of possible different cuts that's with the fewest number of crossing edges so for example in this graph here which you've seen in a previous video the goal is to compute the cuts a B here because there are only two crossing edges and that's as small as it gets that's the minimum cut problem and Karger proposed the following random contraction algorithm based on a random sampling so we have n minus 2 iterations and the number of vertices gets decremented by one in each iteration so we start with n vertices we get down to two and how do we decrease the number of vertices we do it by contracting or fusing two vertices together how do we pick which pair of edges which pair of vertices defuse well we pick one of the remaining edges uniformly at random so this however many edges there are remaining we pick each one equally likely what if the endpoints of that edge are U and V then we collapse you and me together into a single super node so as what I mean by contracting two nodes into a single vertex and then if that causes any self loops and as we saw the examples we will in general have self loops then we delete them before proceeding after the N minus two iterations only two vertices remain you'll recall that those two vertices naturally corresponds to a cut the first group of the cut a corresponds to the vertices that were fused into one of the super vertices remaining at the end the other super vertex corresponds to the set B the other vertices of the graph so the goal of this length of this video is to give an answer to the following question what is the probability of success whereas by success we mean outputs particular min-cut a comma B so let's set up the basic notation we're going to fix a any one input graph undirected graph and as usual we use n to denote the number of vertices and M to denote the number of edges but also going through fix a minimum cuts a comma B if the graph has only one minimum cut then it's clear what I'm talking about here if a graph has multiple minimum cuts I'm actually selecting just one of them is I'm going to focus on a distinguished minimum cut a comma B and we're only going to define the algorithm as successful if it outputs this particular minimum cut a B fit up with some other minimum cut we don't count it we don't count of this successful case we really want this distinguished minimum cut a comma B in addition to N and M we're going to have a parameter K which is the size of the minimum cut that is it's the number of crossing edges of a minimum cut for example that cross a comma B the K edges that cross the minimum cut a B we're going to call capital F so the picture you want to have in mind is there is out there in the world this minimum cut a B there's lots of edges with both endpoints and a lots of edges possibly with both endpoints and B but there's not a whole lot of edges with one endpoint in a and one endpoint in B so the edges F would be precisely these three crossing edges here so our next step is to get a very clear understanding of exactly when the execution the contraction algorithm can go wrong and exactly when it's going to work exactly when we're going to succeed so let me redraw the same picture from the previous slide so given that we're hoping that the contraction algorithm outputs this cut a B at the end of the day what could possibly go wrong well to see what could go wrong suppose at some iteration one of the edges in capital F remember f are the edges crossing the min cut a B so it's these three magenta edges in the picture suppose at some iteration one of the edges of F gets chosen for contraction well because this edge of F has one endpoint in a and one endpoint in B when it gets chosen for contraction it causes this node from a and this node from B to be fused together what does that mean that means in the cut that the contraction algorithm finally outputs this node from a and this node from B will be in the same side of the output cut thanks for the cut output by the contraction algorithm will have on one side both the node from a and a node from B therefore the output of the contraction algorithm if this happens will be a different cut than a B ok will not output a B if some edge of F is contracted and if you think about it the converse is also true so let's assume now that in each of the N minus 2 iterations the contraction algorithm never contracts an edge from capital F remember capital F for exactly the edges with one endpoint in a in one important B so if it never contracts any edge of F then it only contracts edges where both endpoints line capital A or both endpoints line capital B well if this is the case then vertices from a always stick together in fused nodes and vertices from B always stick together in the fused nodes there is never a iteration where a node from a and a node from B are fused together what does that mean that means that when the algorithm outputs a cuts all of the nodes in a have been grouped together all of the nodes in B have been grouped together in each of the two super nodes which means that the output of the algorithm is indeed the desired min cut a comma B summarizing the contraction algorithm will do what we want it will succeed and outputs the cut a comma B if and only if it never chooses an edge from capital F for contraction therefore the probability of success that is the probability that the output is the distinguishment cut a comma B is exactly the probability that it never contracts an edge of capital F so this is what we're going to be interested in here this will be the object of our mathematical analysis the probability that in all of the N minus 2 iterations we never contract an edge of capital F so to think about that let's think about each iteration in isolation and actually define some events describing that so for an iteration I let si denote the events that we screw up in iteration on with this notation we can succinctly say what our goal is so to compute the probability of success what we want to do is we want to compute the probability that none of the events s 1 s 2 up to n minus s n minus 2 ever occur so I'm going to use this not symbol to say that s 1 does not happen so we don't screw up in iteration 1 we don't screw up in iteration 2 we don't screw up in iteration 3 and so on all the way up to we don't screw up we don't contract anything from capital F in the final iteration either so summarizing analyzing the success probability of the contraction algorithm boils down to analyzing the probability of this event the intersection of all of the knot s eyes with I ranging from iteration 1 the iteration and minus 2 so we're going to take this in baby steps and the next quiz will lead you through the first one which is let's have a more modest goal let's just think about it eration number one let's try to understand what's the chance we screw up what's the chance we don't screw up just in the first iteration so the answer to this quiz is the second option the probability is K over m where K is the number of edges crossing the cut a comma B and M is the total number of edges and that's just because the probability of s 1 probably that we screw up is just the number of crossing edges that's the number of outcomes which are bad which cause which trigger s 1 divided by the number of edges that's the total number of things that and since all edges are equally likely it just boils down to this and by the definition of our notation this is exactly K over m so this gives us an exact calculation of the failure probability in the first iteration as a function of the number of crossing edges and the number of overall edges now it turns out it's going to be more useful or for us to have a bound not quite as exact an inequality that's in terms of the number of vertices n rather than the number of edges M the reason for that is it's a little hard to understand how the number of edges is changing in each iteration it's certainly going down by one each iteration because we contract in that jij iteration but it might go down by more than one when we delete selfloops by contrast the number of vertices is this very steady obvious process one less vertex with each successive iteration so let's rewrite this bound in terms of the number of vertices and to do that in a useful way we make the following key observation I claim that in the original graph G that we were given as inputs every vertex has at least K edges incident on it that is in graph theoretic terminology every Edge has degree at least K where recall K is the number of edges crossing our favorite min cut a comma B so why is that true why must every vertex have a decent number of neighbors a decent number of edges incident to it well it's because if you think about it each vertex defines a cut by itself remember a cut is just any grouping and two of the vertices into two groups that are non-empty that don't overlap so one cut is to take a single vertex and make that the first group a and take the other n minus 1 vertices and make that the second group capital B so how many edges cross this cut well it's exactly the edges that are incident on the first node on the node on the left side so if every single cut if all exponentially many cuts have at least K crossing edges then certainly the end cuts defined by single vertices have at least K crossing edges so therefore the degree of every vertex is at least K so our assumption that every single cut in the graph has at least K crossing it just gives us a lower bound on the number of edges incident on each possible vertex so why is that useful well let's recall the following general facts about any graph which is that if you sum up over the degrees of the nodes so if you go node by node look at how many edges are incident on that node that's the degree of V and then sum them up over all and vertices what do you get you get exactly twice the number of edges okay so this is true for any undirected graph that the sum of the degrees of the vertices is exactly double the number of edges to see this you might think about taking a graph starting with the empty set of edges and then adding the edges of the graph one at a time each time you add a new edge to a graph obviously the number of edges goes up by one and the degree of each of the endpoints of that edge also go up by one and there are of course two endpoints so every time you got an edge the number of edges goes up by one the sum of the degrees goes up by two therefore when you've added all the edges that sum of the degrees is double the number of edges that you can add it that's why this is true now in this graph that we have a hand here every degree is at least K and there's n nodes so this left hand side of course is at least KN for us so therefore if we just divide through by two and flip the inequality around we have the number of edges has to be at least the size of the crossing cut or the degree of every vertex times the number of vertices divided by two so this is just the previous inequality rearranging putting this together with your answer on the quiz since the probability of s 1 is exactly K over m and M is at least KN over 2 if we substitute we get that the probability of s 1 is at worst 2 over n 2 over the number of vertices and the K cancels out so that's sort of our first milestone when you've figured out the chance that we screw up in the first iteration that we pick some edge from the the crosses to cut a B and things look good this is uh this is a small number right so in general the number of vertices might be quite big and this says that the probability which group is only 2 over the number of vertices so so far so good of course this was only the first iteration who knows what happens later so now that we understand the chances of screwing up in the first iteration let's take our next baby step and understand the probability that we don't screw up in either of the first two iterations that is we're going to be interested in the following probability the probability that we don't screw up in the first iteration nor in the second iteration now you should go back to the definition of a conditional probability to realize that we can rewrite an intersection like this in terms of conditional probabilities namely as the probability that we don't screw up in the second iteration given that we didn't do it already times the probability that we didn't screw up in the first iteration ok so the probability that we miss all of these K vulnerable edges in the second iteration given that we didn't contract any of them in the first iteration now notice this we already have a good understanding of on the previous slide we gave a nice lower bound of this we said there's a good chance that we don't screw up probability at least 1 minus 2 to the M and in some sense we also have a very good understanding of this probability we know this is 1 minus the chance that we do screw up and what's the chance that we do screw up well these K edges are still hanging out in the graph remember we didn't contract any in the first iteration that's what's given so there are K ways to screw up and we choose an edge to contract uniformly at random so the total number of choices is the number of remaining edges now the problem is what's nice is we have an exact understanding of this probability this is an equality the problem is we don't have a good understanding of this denominator how many remaining edges are there we have an upper bound on this we know this is at most M minus 1 we certainly got rid of one edge in the previous iteration but actually what think about it well we need on this quantity is a lower bound and that's a little unclear because in addition to contracting the one edge and getting that out of the graph we might have created a bunch of self loops and deleted all of them so it's hard to understand exactly what this quantity is so instead we're gonna rewrite this bound that in terms of the number of remaining vertices and of course we know there's exactly n minus 1 vertices remaining we took 2 in the last iteration we contracted it down to 1 so how did it relate the number of edges to the number of vertices well we do it just in exactly the same way as in the first iteration we make a sort of more general observation for the first iteration we observe that every node in the original graph induces a cut okay with that note on one side the other n minus 1 edges on the other side but in fact that's a more general statement even after we've done a bunch of contractions any single node in the contracted graph even if it represents a union of a bunch of nodes in the original graph we can still think of that as a cut in the original graph right so if there's some super node in the contracted graph which is the result of diffusing 12 different things together that corresponds to a cut where those 12 nodes in the original graph are the one side a and the other n minus 12 vertices are the other side of the cut be thanked so even after contractions as long as we have at least two nodes left and our contracted graph you can take any node and think of it as half of a cut one side of a cut in the original graph now remember K is the number of edges crossing our minimum cut a comma B so any cuts in the original graph G has to have at least K crossing edges so since every note of the contracted graph naturally Maps over to a cuts in the original graph with at least K edges crossing it that means then the contracted graph all of the degrees have to be at least K if you ever had a node in the contracted graph that had only say K minus 1 incident edges well then you'd have a cut in the original graph with only came it is a contradiction so just like in the first iteration now that we have a lower bound on the degree of every single vertex we can derive a lower bound on the number of edges that remain in the graph the number of remaining edges is at least one half that's because when you sum over the degrees of the vertices you double count the edges times the degree of each vertex and we just argued that that's at least K in this contracted graph times the number of vertices and we know there's exactly n minus vertices left in the graph at this point so now what we do is we plug this inequality we plug this lower bound on the number of remaining edges on a zit we substitute that for this denominator so in lower bounding the denominator we upper bound this fraction which gives us a lower bound on one minus that fraction and that's what we want so we find is that the probability that we don't screw up and the second iteration given that we didn't screw up in the first iteration we're again by screwing up means picking one of these K edges crossing a B and to contract is at least 1 minus 2 over n minus 1 so that's pretty cool we took the first iteration we analyzed it we show the probability that we screw up is pretty low we succeed with probably at least 1 minus 2 over N and the second iteration our success probability has dropped a little bit but it's still looking pretty good for reasonable values of n1 minus 2 over n minus 1 now as I hope you've picked up we can generalize this pattern to any number of iterations so the degree of every noted the contracted graph remains at least K the only thing which is changing is the number of vertices is dropping by 1 so extending this pattern to its logical conclusion we get the following lower bound on the probability that the contraction algorithm succeeds so the probability that the contraction algorithm outputs the cut a B your recall we argued is exactly the same thing as the probability that it doesn't contract anything any of the K crossing edges any of the set F in the first iteration nor in the second iteration no on third iteration and so on all the way up to the final and - tooth iteration using the definition of conditional probabilities this is just the probability that we don't screw up in the first iteration times the probability that we don't screw up in the second iteration given that we didn't screw up in the first iteration and so on in the previous two slides we showed that we don't screw up in the first iteration probably at least 1 minus 2 over N in the second iteration was probably at least 1 minus 2 over n minus 1 and of course you can guess what that pattern looks like and that results in the following product now because we stop when we get down to two notes remaining the last iteration in which we actually make a contraction there are three nodes and in the second-to-last iteration in which we make the contraction there are four nodes so that's where these last two terms come from rewriting this is just n minus 2 over n times n minus 3 over n minus 1 and so on and now something very cool happens which is massive cancellation and to this day this is always just incredibly satisfying to be able to cross out so many terms so you get n minus 2 crossing out here it's gonna be a pair of n minus 3 is to get crossed out and minus 4 is and so on on the other side that's going to be a pair of fours that get crossed out and a pair of threes that get crossed out will be left with only the two largest terms on the denominator and the two smallest terms in the numerator which is exactly 2 over n times n minus 1 and to keep things simple among friends let's just be sloppy and lower around this by 1 over N squared so that's it that's our analysis of the success probability of cargas contraction algorithm pretty cool pretty slick huh ok I'll concede probably you're thinking hey wait a minute we're analyzing the probability that the algorithm succeeds and we're thinking of the number of vertices n is being big so what we see here is a success probability of one over in the squared and that kind of sucks so that's a good point let me address that problem this is a low success probability so that's disappointing so why are we talking about this algorithm or this analysis well here's something I want to point out maybe this is not so good one an N squared you're going to succeed but this is still actually shockingly high for an oblivious algorithm which honestly seems to be doing almost nothing this is a non-trivial lower bound a non-trivial success probability because don't forget there's an exponential number of cuts in the graph so if you try to just pick a random cut ie you put every vertex 5050 left or right you'd be doing way worse than this you'd have a success probability of like 1 over 2 to the N so this is way way better than that and the fact that it's an inverse polynomial means that using repeated trials we can turn a success probability that's incredibly small into a failure probability that's incredibly small so let me show you how to do that next so we're gonna boost the success probability of the contraction algorithm in if you think about it a totally obvious way we're gonna run it a bunch of times each one independently using a fresh batch of random coins and we're just going to remember the smallest cuts that we ever see and that's what we're going to return the best of a bunch of repeated trials now the question is how many trials are we going to need before we're pretty confident that we actually find them in cut we're looking for to answer this question rigorously what's to find some suitable events so by T I I mean the event that the ice trial succeeds that is the ice time that we run the contraction algorithm it does output the desired min cut a comma B for those of you that watch the part two of the probability review I said a rule of thumb for dealing with independence is that you should maybe as a working hypothesis assume brand variables are dependent unless they're exquisitely constructed to be independent so here's a case we're just going to define the random variables to be independent we're just going to say that we run carp the contraction algorithm over and over again with fresh randomness so they're going to be independent trials now we know that the probability that a single trial fails could be pretty big could be as big as 1 minus 1 over N squared but here now with repeated trials we're only in trouble if every single trial fails if even one succeeds then we find them in cut so a different way of saying that is we're interested in the intersection of t1 and t2 and so on that's the event that every single trial fails and now we use the fact that the trials are independent so the probability that all of these things happen is just the product of the relevant probabilities so the product from I equal 1 to capital n of the probability of not T I recall that we argued that the success probability of a single trial was bounded below by 1 over N squared so the failure probability is down at above by 1 minus 1 over N squared so since that's true for each of the capital n terms we get an overall failure probability for all capital n trials of 1 minus 1 over little n squared raised to the capital n all right so that's a little calculation don't lose sight of why we're doing the calculation we want to answer this question how many trials do we need how big does capital n need to be before we have our confident that we get the answer that we want well the answer that question I need a quick calculus fact which is both very simple and very useful so for all real numbers X we have the following inequality 1 plus X is bounded above by e to the X so I'll just give you a quick proof by a picture so first think about the line 1 plus X what does that cross through well that crosses through the points when X is minus 1 Y is 0 and when X is 0 Y is 1 and it's a line so this looks like this blue line here what does e to the X look like well if you substitute x equals 0 it's going to be 1 so in fact the two curves kiss each other at x equals 0 but Exponential's grow really quickly so as you jack up x2 higher positive numbers it becomes very very steep and 4x negative numbers it stays non-negative the whole way so this sort of flattens out for the negative numbers so pictorially and I encourage you to type this into your own favorite graphing program you see the e to the X bounds above every where the line the 1 plus X for those of you that want something more rigorous there's a bunch of ways to do it for example you even look at the Taylor expansion of e to the X at the point 0 what's the point the point is this allows us to do some very simple calculations on our previous upper bound on the failure probability by working with Exponential's instead of working with these ugly 1 minus whatever is raised to the whatever term so let's combine our upper bound for the previous slide with the upper bound provided by the calculus fact and to be concrete let's substitute some particular number of capital n so let's use little N squared trials or a little n is the number of vertices of the graph in which case the probability that every single trial fails to recover the cut a comma B is bounded above by e to the minus 1 over N squared that's using the calculus fact applied with X equal to minus 1 over N squared and then we inherit the capital n in the exponent which we just instantiate it to little N squared so of course the N squares are going to cancel there's going to give us e to the minus 1 also known as 1 over E so if you're willing to do little N squared trials then our problem our failure probability has gone from something very close to 1 to something which is more like say 30 some-odd percent now once you get to a constant success probability it's very easy to boost it further by just doing a few more trials so if we just add a natural log factor so instead of little n square trials we do little n squared times the natural log of little m now the probability that everything fails is bounded above by the 1 over e that we had last time but still with a residual natural log of n up top and this is now merely 1 over so I hope it's clear what happened we took a very simple very elegant algorithm that almost always didn't do what we want it almost always failed to output the cut a B it did it with only probability 1 over little N squared but 1 over little N squared is still big enough that we can boost it so that it almost always succeeds just by doing repeated trials and the number of repeated trials that we need is the reciprocal of its original success probability boosted by a further logarithmic factor so that transformed this almost always failing algorithm into an almost always succeeding algorithm and that's a more general lesson more general algorithmic technique which is certainly worth remembering let me conclude with a couple comments about the running time this is probably the first algorithm of a court of the course where we haven't obsessed over just what the running time is and I said it's simple enough it's not hard to figure out what it is but it's actually not that impressive and that's why I haven't been obsessing over it this is not almost linear this is not a for free primitive as I've described it here so it's certainly a polynomial time algorithm it's running time is bounded above by some polynomial and N and M so that's way better than the exponential time you get from brute-force search through all through the impossible cuts but it is certainly the way I've described it you've got to do N squared trials plus a log factor which I'm not even going to bother writing down and also each trial well at the very least you look at all the edges so that's going to be another factor of M so this is a bigger polynomial than in any almost any of the algorithms that we're going to see now I don't want to undersell this application of random sampling to computing cuts because I've just shown you the simplest most elegant most basic but therefore also the slowest implementation of using contractions to compute cuts there has been follow-up work with a lot of extra optimizations in particular doing stuff much more clever than just repeated trials so basically using work that you did in previous trials to inform how you look for cuts and subsequent trials and you can shave large factors off of the running time so there are much better implementations of this randomized contraction algorithm than the one I'm showing you here those are however outside the course scope with this course