in this video we'll be giving a running time analysis of the merge sord algorithm in particular we'll be substantiating the claim that the recursive divide and conquer merge sort algorithm is better has better performance than simpler sorting algorithms that you might know like insertion sort selection sort uh and bubble sort so in particular the goal of this lecture will be to mathematically argue the following claim from an earlier video that in order to sort an array of n numbers the merge sort algorithm needs no more than a constant time n logn operations that's the maximum number of lines of EX of code it will ever execute specifically 6 * n log n plus 6n operations so how are we going to prove this claim we're going to use what is called a recursion tree method the idea of the recursion tree method is to write out all of the work done by the recursive merge sort algorithm in a tree structure with the children of a given node corresponding to the recursive calls made by that node the point of this tree structure is it will facilitate uh interesting way to count up the overall work done by the algorithm and will greatly facilitate uh the analysis so specifically what is this tree so at level zero we have a root and this corresponds to the outer call of merge sword okay so I'm going to call this level zero now this tree is going to be binary in recognition of the fact that each invocation of merge short makes two recursive calls so the two children will correspond to the two recursive calls of merge merge short so at the root we operate on the entire input array so let me draw a big array indicating that and at level one we have one sub problem for the left half and another sub problem for the right half of the input array and I'll call these first two recursive calls level one now of course each of these two level one recursive calls will themselves make two recursive calls each operating on then a quarter of the original input array so those are the level two recursive calls of which there are four and this process will continue until eventually the recursion bottoms out in base cases when there's only an array of size Z or one so now I have a question for you which I'll I'll give you in the form of a quiz which is at the bottom of this recursion tree corresponding to the base cases what is the level number at the bottom so what at what level do the leaves in this tree reside okay so hopefully you guess correctly guess that the uh answer is the second one so namely that uh the number of levels of the recursion tree is essentially logarithmic in the size of the input array the reason is basically that the input size is being decreased by a factor two with each level of the recursion if you have an input size of n at the outer level then each of the first set of recursive calls operates on an array of size n over two at level two each array has size n over4 and so on where does the recursion bottom out well down at the base cases where there's no more recursion which is where the input array has size one or less so in other words the number of levels of recursion is exactly the number number of times you need to divide n by two until you get down to a number that's most one and recall that's exactly the definition of the log logorithm base 2 of n so since the first level is level zero and the last level is level log base 2 of n the total number of levels uh is actually log base 2 of n + 1 and when I write down this expression I'm here assuming that N is a is a power of two which is not a big deal I mean the analysis is easily extended to the case where n is not a power of two but this way we don't have to think about fractions log 2 of n then is an integer okay so let's return to the recursion tree let me just redraw it really quick so again down here at the bottom of the tree we have the leaves I.E the base cases where there's no more recursion which when N is a power of two correspond exactly uh to single element arrays so that's the recursion tree corresponding to an indication of merge sort and the motivation for writing down uh for organizing the work performed by merg sort in this way is it allows us to count up the work level by level and we'll see that that's a particularly convenient way to account for all of the different lines of code that get executed now to see that in more detail I need to ask you to identify a particular pattern so first of all the first question is at a given level J of this recursion exactly how many distinct sub problems are there as a function of the level J that's the first question the second question is for each of those distinct sub problems at level J what is the input size so what is the size of the of the array which is passed to a sub problem residing at level J of this recursion tree so the correct answer is the third one so first of all at a given level J there's precisely two of the J distinct sub problems there's one outermost sub problem at level zero it has two recursive calls those are the two uh sub problems at level one and so on in general since merge short calls itself twice the number of sub problems is d ding each level so that gives us the expression 2 to the J for the number of sub problems at level J on the other hand by a similar argument the input size is having each time with each recursive call you pass it half of the input that you were given so at each level of the recursion tree uh we're seeing half of the input size of the previous level so after J levels since we started with an input size of n after J levels each sub problem will be operating on an array of length n/ 2 to the J okay so now let's put this pattern to use and actually count up all of the lines of code that merge short executes and as I said before the key the key idea is to count up the work level by level now to be clear when I talk about the amount of work done at level J uh what I'm talking about is the work done by those two to the J invocations of merge sort not counting their respective recursive calls not counting work which is going to get done in the recursion lower in the tree now recall merge short is a very simple algorithm it just has three lines of code first there's a recursive call so we're not counting that second there's another recursive call again we're not counting that at level J and then third we just invoke the merge sub routine so really outside the recursive calls all that merge sword does is a single indication of merge further recall we already have a good understanding of the number of lines of code that merge needs on an input of size M it's going to use at most 6 M lines of code that's an analysis that we did in the previous video so let's fix a level J we know how many sub problems there are 2 the J we know the size of each sub problem n / 2 the J and we know how much work merge needs on such an input we just multiply by six and then we just multiply it out and we get the amount of work done at a level J okay at all of the level J sub problems so here it is in more detail all right so we start with just the number of different sub problems at level J and we just noticed that that was at most two to the J we also observed that each level J sub problem is passed an an array as input which which has length n / 2 the J and we know that the merge sub routine when given an input uh given an array of size n/ 2 to the J Will execute at most six times that many number of lines of code so to compute the total amount of work done at level J we just multiply the number of problems times the work done sub problem per sub problem and then something sort of remarkable happens we get this cancellation of the two two to the JS and we get an upper bound 6n which is independent of the level J so we do at most 6n operations at the root we do at most 6n operations at level one at level two and so on okay it's independent of the level morally the reason this is happening is because of a perfect equilibrium between two competing forces first of all the number of sub problems is doubling with each level of the recursion tree but secondly the amount of work that we do per sub problem is having with each level of the recursion tree since those two cancel out we get an upper bound 6n which is independent of the level J now here's why that's so cool right we don't really care about the amount of work just at a given level we care about the amount of work that merge short does ever at any level but if we have a bound on the amount of work at a level which is independent of the level then our overall bound is really easy what do we do we just take the number of levels we know what that is it's exactly log base 2 of n + 1 remember the levels are zero through log base 2 of n inclusive and then we have an upper Bound 6n for each of those log n plus one levels so if we expand out this quantity we get exactly the upper bound that was claimed earlier namely that the number of operations merge short execute is a most 6n * log base 2 of n plus 6n so that my friends is a running time analysis of the merge sort algorithm that's why its running time is bounded by a constant times n log n which especially as n grows large is far superior to the more simple iterative algorithms like insertion or selection sort