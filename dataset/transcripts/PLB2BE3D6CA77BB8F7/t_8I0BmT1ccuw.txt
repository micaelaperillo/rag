the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu I wanted to give everybody um a more conceptual idea of what Big O notation was as well as hopefully answer any like lingering questions you might have about object-oriented programming um so I have these notes um and I type them up and they're pretty detailed so I'm just going to go through some points kind of quickly so first off who's still kind of unclear about why we even do Big O O notation okay um so who who can explain why we do Big O notation quickly like what what is it yeah upper on like how long something's going to take right exactly so Big O notation gives us an upper bound on how long something is going to take now something that's important to remember is it's not a Time bound okay so something that's often confusing is that people say oh it's you know this is a something that will tell us how long our program is going to run that's actually not the case bigo notation lets us informs of of how many steps something is going to take um and so why is that important well I mean I look at all of you guys a couple of you guys have laptops out um everything everybody's computer runs something at a different speed right but if we say something is Big O of n okay what we're saying here is we're saying that the worst case number of steps your program is going to take is going to be linear with respect to the size of the input okay so if my computer is five times faster than your computer my computer will probably run it five times faster okay but it's still as the size of the input grows I'm going to expect a linear um speed up in the amount of time it's going to take okay um so why is that important um at the bottom of page one I have uh you know big otation we are particularly concerned with the scalability of our functions okay so what Big O notation does is it might not predict what's going to be the fastest for really small inputs you know for a uh an array size 10 something that and you guys know a little bit about graphs right we have a graph of x s and we have a of X cubed right for there's a portion of time where the graph of x s is actually bigger than x cubed right but then all of a sudden there's a point where X Cub just goes whoosh way bigger than x squ okay so if we're in some really small amount of input Big O notation might not tell us what's the best function okay but Big O notation we're not concerned about small inputs we're concerned about really big inputs we're concerned about filtering the genome we're concerned about analyzing data from Hubble you know really huge blocks of data okay so if we're looking at uh a program that analyzes the human genome with like you know three million base pairs some segment that we're looking at and we have two algorithms one runs in order n time and one runs in order n Cube time okay what this means is regardless of the machine that we're running on so this is algorithm one and this is algorithm 2 regardless of the machine that we're running on we'd expect algorithm 2 to run approximately n cubed Over N approximately n s slower okay so with Big O notation you can compare two algorithms by just looking at the ratio of their Big O runtime okay so if I'm looking at something that has a an array of size two million as it's input is it clear that this is going to be a much better choice okay um um so you'll run into that especially a lot of you guys are taking this for the purposes of scientific Computing so you'll run into big or notation a lot it's important to have a a grasp of what it means um it's also uh on the second page of the handout I have some common ones that you'll see um the first one is constant time we denote constant time as order one okay but you'll notice that I H have here is order one is equal to order 10 is equal to order 2 to the 100 hey that's unexpected to a lot of people who are learning about Big O notation why is why is uh why is this true that seems kind of ridiculous this is a really big number this is a really small number but yeah con right yes exactly so if we look at a graph of one and a graph of two to the okay we'll see that even though two to the 100 is much higher much bigger than one okay as the size if this is our input size as the size of our input grows do we see any change in these two graphs no they're completely linear or they're completely constant okay so anything when you're doing Big O notation if you run across an algorithm who that does not depend on the size of the input okay it's always just going to be order one even if it's like you know two to the 100 steps if it's a constant number of times regardless of the size of the input it's constant time um other ones you'll see are logarithmic time um any base for logarithmic time we is about the same uh order so order log base 2 of n is order log base 10 of n um this is the fastest time Bound for search hey does anybody know what type of search we'd be doing in logarithmic time something maybe we' H yeah exactly bisection search is logarithmic time right because we take our input and at every step we cut it in half cut it in half cut it in half and that's the fastest search we can do um Okay order n is linear time um order n log n is uh the fastest time bound uh we have for sort we'll be talking about sort in a couple of weeks um and Order n squ is quadratic time anything that is order n to some variable so order or order n squar order n cubed order N4 um all of that is going to be less than um order something to the power of n okay so if we have something that's order two to the N that's that's ridiculous um that's a that's a really that's a computationally very intensive algorithm Okay so so on page two I have some questions for you 1 two 3 does order 100 n^2 equal order N2 who who says yes all right very good how about does order one qu n cubed equals order n cubed okay uh does order n plus order n equals order n okay yeah the answer is yes to all of those um and the intuitive sense behind this is that Big O notation deals with the limit liting behavior of a function okay so I made some Nifty graphs for you guys to look at um when we're comparing uh order 100 N2 to order n uh 100 N2 N2 n cubed in one4 N cubed okay what people often think of is what I have here in the first figure so um these are the four functions I just mentioned there's a legend in the top leftand corner and the scale of this is up to uh x equals 80 so you see at this scale okay this line right here is 100 x^2 okay so this is I think often a tripping point is that when people are conceptualizing functions they're saying well yeah 100 x^2 is much bigger than x cubed uh which is you know a lot bigger than a quarter X cubed right so for very small inputs yes that's true but what we're concerned about is the Behavior Uh as the input is very very large okay so now we're looking at a time scale or a size of up to 1,000 okay so now we see here x cubed even though was a little bit smaller than 100x S in the beginning it shoots off okay X cubed is much bigger than either of the two x^ s and even one4 X cubed is becoming bigger than 100 X2 right at a th okay so that's an intuitive sense why X cubed no matter what the coefficient is in front of it is going to Domin any term with X squ in it because X Cub is just going to go real big like that okay and if we go out even further let's go out to the input size of 50,000 okay we go out to an input size of 50,000 we see that even 100 x cubed versus 100 or versus just uh 100 X2 versus just X squar all right they're about the same okay the X Cub terms now they're way they're way above the X squ so the two x squ terms you know 100 versus not you know just one as far as the coefficient goes uh they're about the same okay so this is the this is the scale at which we're concerned about when we're talking about Big O notation okay the limiting Behavior as your input size grows very large 50,000 uh is not even that large if you think about the size of the genome right I mean does is anybody hear bio what's like the size of like uh the human genome how many base pairs or even one gene or one chromosome what's this the biggest it's h it's over 50,000 yeah over 50,000 right and we talking about you know uh the amount of data that we get back uh from like the Hubble Space Telescope I mean the resolution on those things are absolutely ridiculous and you run all sorts of algorithms on those images to try and see if you know there's life in the universe um so we're very concerned about the you know big long-term behavior of these functions okay how about page three one last question does order 100 n^2 plus order plus one qu n cubed equal order n cubed who says yes yeah okay and so I have one more graph okay um down here these red dots are 100 x^2 all right these blue circles are one4 X cubed and this line is the sum okay we can see that it's this line is a little bit bigger than the one4 X Cub term but it gener really this has no effect this far out okay so that's why we just going to drop any lower order terms and whenever you're approached with a big O expression that you know has a bunch of constant factors and has all sorts of different powers of N and stuff you're always just going to drop all the constant factors and just pick the biggest thing okay so this is this line right here um is order n cubed does that clear to everybody okay so now we've gone through the basics of how we analyze this and and why are we looking at this um let's look at some code okay so the first example okay um all of these things right here in Python we make the assumption that um statements like this x + One X X Y um all these mathematical operations are all constant time okay that's a that's something that you can just assume okay so for this function down here we have constant time constant time constant time constant time operation so we'd say this function bar is what what's its complexity H constant yeah constant times so the complexity of all of these functions are just 01 okay because it doesn't matter how big the input is right all of the it's all going to run in constant time okay um for this multiplication function here uh we use a for Loop okay often times when we see for Loops that's just going through the input that's a signal to us that it's going to probably contain a factor of o n he why is that okay right what do we do in this for Loop we say for I and range y what does that mean how many times do we execute that for Loop yeah y times so if Y is really small we execute that for Loop just a few number of times but if Y is really large we execute that for Loop a whole bunch of times okay so when we're analyzing this we see this for Loop and we say ah that for Loop all right must be o y okay does that make sense to everybody okay good um let's look at now factorial right can anybody tell me what the complexity of factorial is yeah order n why is it order end it's just it's still for Loop yeah it's the exact same structure we have a for Loop that's going through um Range one to n plus one okay so that's dependent on the size of n so this for Loop is order n and inside the for Loop we just do a constant time operation right that's the other thing just because we have this for Loop doesn't mean that what's inside the for Loop is going to you know be constant but in this case if we have order n times we do a constant time operation then this whole chunk of the for Loop is order n okay the rest of everything else is just constant time okay so we have constant time plus order n times constant time plus constant time is going to be order n okay how about this one factorial two yeah exactly this is also order n the only thing that's different in this code is that we initialize this count variable and inside the for Loop we also increment this count variable okay but both result times equal num and count plus equal one both of these are constant time operations right so if we do n times two constant time operations that's still going to be order in okay so the takeaway from these two examples I'm trying to demonstrate here is a single line of code can generate a pretty complex thing okay but a collection of lines of code might still be constant time okay so you have to be you to look at every line of code and consider that all right um thrown in some conditionals here okay what's the complexity of this guy yeah this one's also linear what's going on here we initialize a variable count that's constant time all right we go through character and a string right this is linear in the size of a string right now we say if character equal equal t Okay this character equal equal T right that's also a constant time operation that's just asking if this one thing equals this other thing all right so we're looking at two characters we're looking at two numbers equal equal or not not equal is generally a constant time operation um the exception to this might be equality of certain types um like if you define a class and you define an equality method in your class and the equality method of your class is not constant time then this equal equal check might not be constant time but on two strings equal equal is constant time okay and this is constant time as well okay so linear in the size of a string um something that's important when you're doing this for exams uh it's a good idea to Define what n is before you give the complexity bound so here I'm saying n is equal to the size of a string so now I can say this function is order n what I'm saying is that it's linear with respect to the size or the length of a string okay because sometimes um you know for like in the one where there was the input X and Y okay but the the in the running time was only linear in the size of Y so you'd want to Define that n was equal to the size of Y to say that it was order n um so always be clear if it's not clear um be sure to explicitly State what n is equal to okay this code's a little more tricky what's going on here yeah that that was perfect so um just to reiterate um the for Loop we know is linear with respect to the size of a string we have to go through every character in a string now the second is if care in b string okay when we're looking at Big O notation we're worried about the worst case complexity in Upper bound right what's the worst case yeah if the character is not in b string we have to look at every single character in BR string before we can return false okay so that is linear this one single line if character in b string that one line is linear with respect the size of b string okay so how do we analyze the complexity of this okay we have I want to be able to touch the screen we have this for Loop this for Loop is executed uh let's call n is the length of a string okay this for Loop is executed n times every time we execute this for Loop we execute this inner body and what's the time bound on the inner body well if we let m equal the length of b string and we say that this check is order M every time we run it then we run an order M operation order n times so the complexity is we use something of size m n times yeah just order NM so we execute an order M check order n time we say this function is order n m okay does does that make sense to everybody okay because you'll see the nested for Loops um nested for Loops are very similar to this um okay while Loops like combine the best of conditionals with the best of uh for Loops right because a while loop has uh the chance to act like a for Loop but a while loop can also have a conditional um it's actually possible to write a while loop that has a complex conditional that also executes a number of times and so you can have one single line of code generating like an order n squ uh complexity um let's look at factorial 3 who can tell the the complexity of factorial 3 yeah it's also linear that is a interesting that factorial is always linear despite its name um we have constant time operations how many times is the while loop executed yeah n times and what's inside the body of the while loop constant time operations okay so we execute a bunch of constant time operations n Times order n um how about this care split example's a this one's a little tricky because you're like well what's the complexity of Len okay um in Python lens's actually a constant time operation I this example is very crafted such that all of the operations that are here are constant time so appending to a list is constant time and indexing a string is constant time okay so what's the complexity of care split constant time who would agree with with constant time right who who would say it's linear time okay yeah very good it is linear time um that's that's a correct intuition okay we say while the length of a string is not equal to the length of the result okay these are two constant time operations what do we do we append a value to the result and then we add up this index so when is this check going to be equal this check's going to be equal when the length of the result is equal to the length of a string and that's only going to happen after we we've gone through the entire a string and we've added each of its characters to result okay so this is linear um with respect to the size of a string now something that's important uh to recognize is that not all string and list operations are constant time um there's a website here um that first off it says cpython if you go to it cpython just means python implemented in C which is actually what you're running is cpython um so don't worry about that there's often two time bound complexities it says the amortized time and the worst case time and so if you're looking for Big O notation you don't want to use the amorti time you want to use the worst case time um and it's important to note that operations like slicing and copying um actually aren't constant time okay if you slice a a list or a string uh the complexity of that operation is going to depend on how big your slice is okay does that make sense is the way that a slice where he says it walks through the list until it gets to the index and then keeps walking until the final index and then copies that and returns it to you okay so slicing is not constant time um copying is similarly not constant time okay um for this little snippet of code all right this is just similar to what we oh yeah yeah with the Big O do you write o and of the string or ah so this is what I was saying um you want to Define what n is so we'd say something like n equals the length of a string and then you can say it's order n okay right um it's important to Define what what you're saying the time the complexity is related to okay um so here I'm saying if we let N equal to the size of Z can anybody tell me what the complexity of this snippet of code is pardon me yeah precisely order N squared Why well because we execute this 4 I for Loop here okay order and times each time through this for Loop the body of this for Loop is in fact another for Loop all right so my approach to problems like this is to step back a all right and ignore the outer loop all right just concentrate on the inner loop what's the runtime of this inner loop yeah this is order n we go through this now go to the outer loop all right just ignore the body since we've already analyzed the body ignore it what's the complexity of the outer loop also order n so now you can combine the analysis you can say for order n times I execute this body this body takes order end times so if I execute something that's order n order n times that is order n squ complexity okay so we just multiply how long it takes the outer body of the loop to take the inner body of the loop and so in this fashion you know I could give you now probably a four or five nested for Loop and you could tell me the complexity of it right um harder sometimes understand is recursion um I don't know how important it is to understand this because I've never actually taught this class before but Mitch did tell me to go over this so I'd like to touch on it um so consider recursive factorial what's the time complexity of this how can we figure out the time complexity of a recursive function right the way we want to figure out the time complexity of a recursive function is just to figure out how many times we're executing said recursive function okay so here I have recursive factorial of n okay when I make a call to this what do I do okay I make a call to recursive factorial n minus one right and then what does this do this calls recursive factorial on a sub problem of size n minus 2 okay so often times when you're dealing with fact with you're dealing with recursive Pro um dealing with recursive problems to figure out the complexity what you need to do is you need to figure out how many times you're going to make a recursive call before a result is returned okay intuitively we can start to see a pattern we can say I called on n and then n minus one and then n minus 2 and I keep calling recursive factorial all right until n is less than or equal to zero okay when is n going to be less than or equal to zero well when I get n - n all right so how many calls is that yeah this is n calls okay so if you it's a good practice to get into being able to draw this out and work yourself through how many times you're running the recursion and when we see we're making end calls we can say oh this must be linear in time okay how about this one this Fu function this one's a little harder to see okay but what are we doing right we call Fu on input of size n right which then makes a pro a call to sub problem of size n/2 which makes a call to a sub problem of size n over4 and so on until we make a call to sub problem of some size okay so this is n this is 2 to the 1 this is 2^2 all right we start to see a pattern 2^2 2 cubed 2 4th right so we're going to keep making calls on an smaller and smaller and small smaller sub problem size but instead of being linear like before okay we're decreasing at an exponential rate okay um there's a bunch of different ways to try and work this out in your head I wrote up one possible description but it's an uh when we're decreasing at this exponential rate what's going to end up happening is that this uh recursive problem where we make a recursive call um in the form to sub problem of size n/ B all right the complexity of that is always going to be log base B of n okay so this is just like bisection search okay where bisection search we essentially do in bisection search we restrict the problem size by half every time and that leads to logarithmic time actually log base 2 of n this problem is also log base 2 of n um if we if we change this recursive call from n/2 to n/ 6 we get a a Time complexity of log base 6 of n okay um so try and work that through um you can read this closer later um definitely ask me if you need more help on that one all right the last one is how do we deal time complexity of something like Fibonacci okay Fibonacci FIB n minus one plus FIB n minus 2 that's initially that kind of looks linear right we just went over the recursive factorial and it made the call to a sub problem of size n minus one and that was linear right uh Fibonacci is a little bit different if you actually draw it out in a tree you start to see like at every level of the tree uh we expand the call by two okay and now imagine this is just for Fibonacci of six and whenever you're doing Big O complexity you want to imagine an input of 100,000 50,000 um and you can imagine how big that tree grows okay um intuitively the point to see here is that they're going to be about n levels all right to get down to FIB uh down to one from your initial input of six okay so to get down to one from an initial input of size n is going to take about n levels right the branching factor of this tree at each level is two okay so if we have n levels and at each level we increase our branching Factor by another two um we can say that a loose bound on the complexity of this is actually 2 to the N hey this isn't this is something that's even less intuitive I think than uh what we did before with the logarithms so try and work through it again um play with it a little bit there's actually a tighter bound on this which is like 1.62 to the N which is a lot more complicated math that you can look up um but for the purposes of this class it's sufficient to say that Fibonacci is order two to the N okay so does that roughly clear up some time complexity stuff for you guys okay awesome does anybody have the time I forgot my watch today okay excellent um that gives us a little bit of time to talk about object-oriented programming does anybody have any specific questions about object-oriented programming oh okay um how about this how many of you guys finished uh the problem set and turned it in already or did any of you guys not turn in the problem set yet or we'll talk Loosely about it then but not too specifically um did anybody have any questions from I guess at least the first part you know we're making some classes making some trigger classes um yeah self. what if you like want to um so in the first part were ah when we have like the celf we have like the getter methods okay so what's important about that I'll tell you what's important about that so we have a class let's say we have a class person okay so we Define our nit method to just take a name all right and so now what the problem had asked you to do was to define a getter Method All Right define a getter method called git name that just Returns the attribute okay so what's the point of this okay because I can just say um Sally equals person okay so here I define a person named Sally and uh you know I I initialize a person with the string Sally all right and if I just look at sally. name that's going to just directly print the attribute okay so why did we need this git name function like what's what's the point of this additional getter method and does anybody know why that is right so that's what it does this git name does return the attribute name but we don't need this this method to just look at the attribute name right let's actually code this up okay so we have class person okay so if we run this code right and over here in the Shell we Define um Sally equals person with an name Sally okay if I just print sally. name it prints the attribute so why did I need to do to provide this getter method called git name that does us the same thing that's the question right like that that seems sort of redundant okay um but there's actually a pretty big important reason for it let's say we set um SN name equal to the attribute sally. name all right if we look at s name we see Sally and but now if I say actually I'm not sure if this is the correct reasoning okay this is not the proper this is going to be better all right let's say Sally equals a person Sally who's taking what okay so now I I can look at the attribute classes to show Sally's classes which are weird floats um and I can also use sally. git classes to look at Sally's classes okay if I set a variable s s classes equal to sally. classes okay this binds this variable s classes to the attribute sally. classes now if I say s classes. append 1401 all right if I now look at the attribute sally. classes it now has 1401 in it okay this is not this is not safe this is not type safe okay because the reason for that is if you define a class and you access the class's attributes directly instead of through a getter method you can then do this and and sometimes it's accidental all right you'll set some variable equal to some attribute of a class then later on in your code you'll alter that variable okay but that variable is not a copy of the attribute okay yes you can make copies of the attribute and stuff but the overall takeaway is that in programming we try to do something called Defensive programming okay this isn't defensive because it is possible if you code it incorrectly to alter the attribute of the class or the the instance of the class um but if we use the getter method If instead of sally. classes instead of directly accessing the attribute here we had set s classes equal to sally. git classes and then we had changed s classes around okay that wouldn't have happened because the getter method it does return self. classes but in the way that python is scoped and when we return something we're not returning the exact same same thing um the reference that we're returning a copy of it okay does that make sense all right cool um other questions about classes I me we have a little class up here if there's like some basic stuff that you'd like explained again um Now's the Time so here I'm setting a just some variable s classes equal to the attribute Sally classes um um it's just like setting you know any sort of variable equal to some other quantity so you apped the variable but it also apped like the attri so what I did here was I said I set the variable s classes equal to this attribute sally. classes um and then because I know this is a list I appended another value to it okay but this is the same as when we have two lists if we have a list called a and we say a is equal to 1 2 3 and I say B is equal to a okay what is B now if I say b. append 1401 what does B look like what does a look like right because they're aliases of each other so what I did here when I set s classes directly equal to the attribute sally. classes I made s classes an alias of the attribute okay but the problem with that is that then I can change them and because they're aliases the attribute itself is changed all right and we don't want to do that in object-oriented programming when you define an object the only way you should be able to change an attribute is through some method of the class that allows you to change that attribute okay so if I wanted to be able to add a class to to Sally's class list I should Define a method called Define ad classes or Define ad class that does self. classes. append new class okay um it's while technically it's possible to directly access an attribute it's really bad practice to do so um simply because this unexpected Behavior can result and also because if you say oh well it's not going to matter for this one time I'll remember how to do the right thing and the problem with that is it's often the case that you're not the only person using your code okay so it's a better practice to provide all the sorts of methods that you would need to do with the class in order to get and access and change attributes um as methods within the class okay does that make sense okay um so yeah this is maybe our one violation if you guys have been attending my recitation um our monra of programmers are lazy um this is less lazy than just directly accessing the attributes but even though we know that programmers are super super lazy um programmers also like to be super super safe so when there's a trade-off between defensive programming and being lazy always pick defensive programming