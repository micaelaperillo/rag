the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu so in the example we looked at we had a list of inss that's actually quite easy to do in constant time time if you think about it an INT is always going to occupy the same amount of space roughly speaking either 32 or 64 bits depending upon how big and int the language wants to support so let's just for the sake of argument assume an INT occupies four units of memory and I don't care what a unit is is a unit 8 Bits 16 bits doesn't matter four units now how would we get to the E element of the list what is the location in memory of L subi well if we know the location of the start of the list and certainly we can know that because our identifier say l in this case will point to the start of the list then it's simply going to be the start + 4 time I my list looks like this I point to the start the first element is here so that's start + 4 * 0 makes perfect sense the second element is here so that's going to be start + 4 * 1 sure enough this would be location four relative to the start of the list Etc so this is a very conventional way to implement lists but what what does its correctness depend upon it depends upon the fact that each element of the list is of the same size in this case it's four but I don't care if it's four if it's two it's two * I if it's 58 it's 58 times I it doesn't matter but what matters is that each element is the same size so this trick would work for accessing elements of lists of floats lists of inss anything that's of fixed size but that's not the way lists are in Python and python I can have a list that contains inss and floats and strings and other lists and dicks almost anything so in Python it's not this nice picture where the lists are all homogeneous in many languages they are by the way and those languages would implement it exactly as I outlined it on the board here but what about languages where they're not like python one possibility and this is probably the oldest way that people used to implement lists is notion of a link list these were used way back in the 1960s when lisp was first invented and effectively there what you do is a list every element of the list is a pointer to the next element and then the value so what it looks like in memory is we have the list and this points to the next element which maybe has a much bigger value field but that's okay this points to the next element now let's say this one maybe has a tiny value field and then at the end of the list I might write none saying there is no next element or nil in list speak well what's the cost here of accessing the nth element of the list of the E element of the list somebody how many steps does it take to find element I I steps exactly so for a link list finding the I element is order I that's not very good right that won't help me with binary search because now if this were the case case for finding an element of a list in Python binary search would not be log length of the list but it would be length of order length of the list right because the worst case is I'd have to visit every element of the list say to discover something isn't in it so this is not what you want to do instead python uses something like the picture in your handout and the key idea here is one of indirection so in Python what a list looks like is it is a list section of memory a list of objects each of the same size because now what each object is is a pointer so we've now separated in Space the values of the I of the members of the list and the pointers to if you will the next one so now it can be very simple this first element could be big second element could be small we don't care now I'm back to exactly the model we looked at here if say a pointer to some place in memory is four units long then to find L sub I I use that trick to find say the E pointer and then it takes me only one step to follow it to get to the object so I can now in constant time access any object into a list even though the objects in the list are of varying size this is the way it's done in all objectoriented programs programming languages that makes sense to every body this concept of indirection is one of the most powerful programming techniques we have it gets used a lot um now my dictionary defines indirection as a lack of straightforwardness and openness and as a synonym uses deceitfulness uh and and it had this pejorative term until about 1950 when computer scientists discovered it and decided it was a wonderful thing and there's something that's often quoted as people who do algorithms they say quote all problems in computer science can be solved by another level of indirection so it's sort of whenever you're stuck you add another level of indirection the caveat to this is the one problem that can't be solved by adding another level of indirection is too many levels of indirection which can be a problem as you look at certain kinds of memory structures the fact that you've separated the pointers from the value Fields can lead to them being in very far apart in memory which can disturb behaviors of caches and things like that so in some models of memory this can lead to surprising inefficiency but most of the time it's really a great implementation technique and I highly recommend it okay so that's how we do the trick so now we can convince ourselves that binary search is indeed order log n and as we saw Tuesday logarithmic growth is very slow so it means we can use binary search to search enormous lists and get the answer very quickly all right there's still one catch and what's the catch there's an assumption to binary search binary search works only when what assumption is true the list is sorted because it depends on that piece of knowledge so that raises the question how did it get sorted or the other question it raises if I asked you to search for something does it make sense to follow the algorithm of one sort L to use binary search does that make sense well what does it depend upon whether this makes sense from an efficiency point of view we know that that's order log length of L we also know if the list isn't sorted we can do it in order l we can always use linear search so whether or not this makes a good idea depends upon whether we can do this fast enough so it has the question is order question mark plus [Music] order log Len of L less than order L if it's not it doesn't make sense to sort it first in some right so what's the answer to this question do we think we can sort a list fast enough as in and what would fast enough mean what would it have to be for this to be better than this we know that we have to be able to sort a list in sublinear time can we do that alas the answer is provably no no matter how clever we are there is no algorithm that will sort a list in sublinear time and if you think of it it makes a lot of sense because how can you get a list in ascending or descending order without looking at every element in the list at least once right logic says you just can't do it if you're going to put something in order you're gonna have to look at it so we know that at we have a lower bound on sorting which is order L and we know that order L plus order log length L is the same as order L which is not better than that all right so why do we care if this is true why are we interested in things like binary search at all and the reason is we're often interested in something called amortized complexity now I know that there are some course 15 students in the class who will know what amortization means but maybe not everybody does the idea here is that if we can sort the list once and end up searching it many times the cost of the search can be allocated a little bit to each of the different time sorry the cost of the sort can be allocated a little bit of it to each of the searches and if if we do enough searches then in fact it doesn't really matter how long the sort takes so if we were going to search this list a million times maybe we don't care about the onetime overhead of sorting it and this kind of amortized analysis is quite common and is what we really end up doing most of the time in practice so the real question we want to ask is if we plan on performing K searches is who knows how long it will take to sort it but we'll take is order of whatever sort of the list is plus K Times log length of L excuse me is that less than K times Len ofl if I don't sort it to do case sort searches will take this much time if I do sort it it will take this much time the answer to this question of course depends upon what's the complexity of that and how big is K does that make sense in practice K is often very big the number of times we access say a student record is quite large compared to the number of times students enroll in MIT so if at the start of each semester we produce a sorted list it's pretty then it pays off to do the searches now in fact we don't do a sorted list we do something more complex but you understand the concept I hope all right so now we have to say how well can we do that that's what I want to spend most of the rest of today on now is talking about how do we do sorting because it is a very common operation so first of all let's let's look at a way we don't do sorting so there was a famous computer science who o scientist who apped on this topic um we can look for him this way well-known technique is bubble sort now actually stop we're going to need sound for this do we have sound in the booth do we have somebody in the booth well we either have sound or we don't we'll find out shortly other way come on should not oh there thank you now it's hard to get a job right as President right and I me you're going through the rigers now it's also hard to get a job at Google right we um we have questions and we ask our candidates questions and uh this one is from Larry Schwimmer what you guys think I'm kidding it's right here what is the most efficient way to sort a million 32-bit integers well uh I'm maybe I'm sorry maybe no I I think I I I think uh I think the uh the bubble sort would be the wrong way to go uh come on who told him this okay I didn't see computer science we've got our spies in there why not let's okay let's ask let's ask a a different interview all right so uh as he sometimes is the president was correct bubble sort though often discussed is almost always the wrong answer so we're not going to talk about bubble sort uh I by the way know Larry Schwimmer and can believe he did ask that question um but yes I'm surprised someone had obviously warned the the president actually the then future president I think all right so let's look at a different one uh that's often used and that's called selection sort this is about as simple as it gets the basic idea of selection sort and it's not a very good way to sort but it is a useful kind of thing to look at because it introduces some ideas like many algorithms it depends upon maintaining establishing and maintaining an invariant and invariance is something that's invariantly true the invariant we're going to maintain here is we're going to have a pointer into the list and that pointer is going to divide the list into a prefix and a suffix and the invariant that we're going to maintain is that the prefix is always sorted we'll start where the prefix is empty it contains none of the list and then each step through the algorithm will decrease the size of the suffix by one element and increase the size of the the prefix by one element while maintaining the invariant and will be done when the size of the suffix suffix is zero and therefore the prefix contains all the elements and because we've been maintaining this in variant we know that we have now sorted the list so you can think about it for example if I have a list that looks like uh 4 2 3 I'll start pointing here and the prefix which contains nothing obeys the invariant I'll then go through the list and find the smallest element in the list and swap it with the first element so my next step list will look like 2 4 3 I'll now Point here my invariant is true the prefix contains only one element so it is in ascending order and I've increased its size by one I don't have to look at this element again because I know by construction that's the smallest so now I I move here and I look for the smallest element in the suffix which will be three I swap three and four and then I'm going to be done does that make sense so it's very straightforward it's in some sense the most obvious way to to sort a list and if you look at the code that's exactly what it does I've stated the invariant here and I just go through and I sort it so we can run it let's do that so I'm going to sort the list 354 Etc or 345 Etc 3545 going to call selection sort and I don't think this is in your handout but just to make it obvious what's going on each iteration of the loop I'm going to print the partially sorted list so we can see what what's happening so the first step it finds four and puts that in the beginning actually find zero puts it in the beginning Etc all right so people see what's going on here it's essentially doing exactly what I did on the board over there and when we're done we have the list completely sorted what's the complexity of this what's the complexity of selection sort well there are two things going on right I'm doing a bunch of comparisons and I'm doing a bunch of swaps since I do at most the same number of comparisons as I do swaps or swaps as I do comparisons I never swap without doing a comparison we can calculate complexity by looking at the number of comparisons I'm doing you can see that in the code as well so how many comparisons might I have to do here well the key thing to notice is each time I look at it each iteration I'm looking at every element in what in the list no every element in the suffix so the first time through I'm going to look at let's just say n equals the length of the list so the first time through I'm going to look at n elements then I'm going to look at n minus one then I'm going to look at nus 2 until I'm done right so that's how many operations I'm doing and what is the order of n + nus1 + nus exactly order n so selection sort is order n is that right somebody said order n do you believe it's n is this really n it's not n what is it somebody raise your hand so I can throw the candy out yeah uh it's not in factorial you said that at the question mark at the end of your voice no like of the numbers is like n * right it's really exactly right it's a little smaller than n squ but it's order n s right I'm doing a lot of these additions so I can't ignore all of these addition all of these extra terms and say they don't matter it's almost as bad as comparing every element to every other element all right so selection sort is order N squared and you can do it by understanding that sum or you can look at the code here and that sort of will also tip you off right okay so now can we do better and there was a while where people were pretty unsure whether you could do better but we can um if we think about it now it was a method invented by John Von noyman a very famous guy and he back in the 40s amazingly enough um and he viewed this as a kind of divide and conquer algorithm and we've looked at divide and conquer before so how what is the general form of divide and conquer a phrase you've heard me use many times uh popular popularized by the way I think by mackelli in in the prince in a not very nice context um so what we do and they're all the kind the same we start with one let me get over here and get a full board for this first we have to choose a threshold size let's call it n zero and that will be essentially the smallest problem so we can keep dividing making our problems smaller this is what we saw with binary search for example until it's small enough that we say oh the heck with it we'll stop dividing it now we'll just solve it directly so that's how small we need to do it how the smallest thing will divide things into the next thing we have to ask ourselves is how many instances at each division so we have a big problem we divide it into smaller problems how many are we going to divide it into so we divide it into smaller problems until we reach the threshold where we can solve it directly and then the third and most important part is we need some algorithm to combine the subsolutions it's no good solving the small problem if we don't have some way to combine them to solve the larger problem all right so we saw that before and now we're going to see it again and we're going to see it in particular in the context of merge sort um if I use this board can people see see it or is the screen going to olude it is there anyone who cannot see this board if I write on it all right then I will write on it okay so let's first look at this problem what Von noyman observed in 1945 is given two sorted lists and amazingly enough this is still the most most popular sorting algorithm or one of the two most popular I should say given two sorted lists you can merge them quickly so let's look at an example I'll take the list uh 1 12 18 19 and 20 that's list one and the try and merge it with the list 2 3 4 and 17 so the way you do the merge is you start by comparing the first element to the first element and then you choose say all right one is smaller than two so that will be the first element of the merge list I'm now done with one and I never have to look at it again the next thing I do is I compare five and two the head of the two remaining lists and I say well two is smaller than five I never have to look at two again I then compare five and three I say three is smaller never have to look at three again I then compare four and five four is smaller I then compare five and 17 five is smaller Etc all right now how many comparisons am I going to do this time well let's first ask the question how many elements am I going to copy from one of these lists to this lists copy each element once right so the number of copies is order Len of the list so that's pretty good that's linear that's sort of at the lower bound but how many comparisons that's a little trickier to think about pardon at most the length of the longer list which would also be we could claim to be order Len of I sort of cheated using L when we have two lists but just think of it as the longer list so youd think that many comparisons so you think we can do this whole thing in linear time and the answer is yes so that's our merge so that's a good thing now that takes care of this step but now we have to ask how many times are we going to do a merge because remember this worked because these lists were sorted and so I only had to compare the front of each list now when I think about how I'm going to do the binary or the merge sort what I'm going to do is take the original list break it up break it up break it up break it up until I have lists of length one well those are all sorted by trivially sorted and then I'll have at the end a bunch of lists of length one I'll merge pairs of those now I'll have sorted lists of length two then I'll merge those getting sorted lists of length four until at the end I'll be merging two lists each half the length of the original list right that Mak sense to everybody so now I have to ask the question how many times am I going to call merge yeah yes I'm going to call merge log length of the list times whoops all right so if each merge is order log n where n is the length of the list and I call merge sorry each merge is order n where n is length of the list and I call merge login times What's the total complexity of the merge sort and log in thank you all right let's see what I have to choose a heavy candy because they carry better not well enough though all right you can relay it back so now let's look at an implementation so here's the implementation of sort and I don't think you need to look at it in detail it's doing exactly what I did in the board actually you do need to look at it in detail but not in real time and then sort now there's a little complication here because I wanted to show another feature to you for the moment we'll ignore the complication which is uh it's it's in principle working but it's in not very bright all right I'll use the mouse what we see here is whenever you do a sort you're sorting by some ordering metric could be less than could be greater than could be anything you want if you're sorting people you could sort them by weight or you could sort them by height you could sort them by God forbid GPA um whatever you want um so I've written sort to take as an argument the ordering I've used this funny thing called Lambda which you don't actually have to be responsible for you're never going to probably need to use it in this course but it's a way to dynamically build a function on the Fly and the function I've built is I've said the default value of LT is X less than y Lambda X lamb Lambda X Y say is X and Y are the parameters to a function and the body of the function is simply return the value X less than y all right nothing very exciting there what is exciting is having a function as an argument and that is something that you will be doing in future problem sets because it's one of the very powerful and most useful features in Python is using functional arguments all right Kevin got past that what we see is we first say if the length of L is less than two that's my threshold uh then I'm just going to return L actually a copy of L otherwise I'm going to find roughly the middle of L then I'm going to call sort recursively with the part to the left of the middle and the part to the right of the middle and then merge them so I'm going to go all the way down till I get to list of length one and then bubble all the way back up merging as I go so we can see that there will be the depth of the recursion will be log in as observed before this is exactly what we looked at when we looked at binary search how many times can you divide something in half log n times and each recursion we're going to call merge so this is consistent with the notion that the complexity the overall algorithm is n log n let's run it and I'm going to print as we go what's getting merged oops um never mind hold on get rid of this one this was our selection sort we already looked at that yeah so what we'll see here is the first example I was just sorting an list of integers may we look at that all by itself I didn't pass it in the second argument so it used the default less than so it was first merged four and five then it had to merge 35 with four and five then 29 with 17 58 and zero and then the longer list 1729 with 04535 with 0 17 29 58 and then we were done so indeed it did a logarithmic number of merges the next piece of code I'm taking advantage of the fact that this function can sort list of different kinds and I'm calling it now with the list of floats and I am passing in the second argument which is going to be well that's for fun wonder what happens if I make this greater than let's see what we get well now you note it's sorted it in the other order because I passed in the ordering that said I want to use a different comparison then less than I want to use greater than and so the same code did the s the other way I can do more interesting things so here I'm assuming I have a list of names and I've written two ordering functions myself one that orders first compares the last names and then the first names and different one that compares the first names and then the last names and uh we can look at those just to avoid cluttering up the screen let me get rid of this so what we can see is we got we did the same way of dividing things initially but now we got different orderings and so if we look at the first ordering I used we start with jezelle Brady and then Tom Brady then Chancellor grimon Etc and if we do the second ordering we see among other things you have me between jizelle and Tom not a bad outcome from my perspective but uh but again a lot of flexibility by using this functional argument I can Define whatever functions I want and using the same sort get lots of different code and you will discover that in fact the built-in sort of python has this kind of flexibility and you'll also find as you write your own programs increasingly you'll want to use functions as arguments because it allows you to write a lot less code to accomplish the same tasks