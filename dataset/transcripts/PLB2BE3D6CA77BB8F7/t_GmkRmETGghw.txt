the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu today we're moving on to a what will be a major unit of the course which is the topic of efficiency thus far we focused our attention on the admittedly more important problem of getting our programs to work I.E to do what we want them to do for the next several lectures I want to talk about how do we get them to work quickly enough to be useful it is in practice often a very important consideration in designing programs the goal is not to make you an expert in this topic uh it's hard to be an expert in this topic I'm certainly not an expert but I want to give you some intuition about how to approach the question of efficiency how to understand why some programs take much longer to run than others uh and how to go about writing programs that will finish before you die and we'll see that if you write things wrong the programs could in principle run longer than you can so why is efficiency so important earlier in the term I started spent some time talking about how really fast computers are and showing you that we could use brute force algorithms to solve fairly large problems the problem the difficulty is that some of the computational problems we're confronted with are not fairly large but enormous so for example in my research group where we work at the intersection of computer science and medicine we have a big database of roughly a billion and a half heartbeats and we routinely run computations that run for two weeks on that data and the only reason they complete in two weeks and not two years is we were really careful about efficiency so it really can matter and increasingly it matters as we see the scale of problems growing the thing I want you to take home to remember is that efficiency is rarely about clever coding it's not about some little trick that saves one instruction here or two instructions there it's really about choosing the right algorithm so the take-home message is that efficiency is about algorithms not about coding details clever algorithms are hard to invent a successful computer scientist might invent maybe one in his or her whole career uh I have to say I have invented zero important algorithms in my whole career therefore we don't depend upon being able to do that instead what we depend upon is problem reducing when confronted with a problem we want to reduce it to a previously solved problem and this is really often the key to taking some problem and thinning it into a useful computation we sit back say well this looks a little bit like this other problem how can I transform my problem to match a problem that some clever person already knows how to solve before I spend time on problem reduction however I want to draw back and look at the general question of how do we think about efficiency when we think about it we think about it in two Dimensions space and time and as we'll see later in the term we can often trade one for the other we can make a program run faster by using more memory or use less memory at the cost of making it run more slow slowly for now in the next few lectures I'm going to focus on time because really that's mostly what people worry about these days when they're dealing with complexity so now suppose I asked you the question how long does some algorithm implemented by a program take to run how would you go about answering that question well you could say all right I'm going to run it on some computer on some input and time it look at my watch it took three minutes I ran this other algorithm and it took two minutes it's a better algorithm well that would be really a bad way to look at it the reason we don't think about computational complexity and that's really what people call this topic in terms of how long a program takes to run on a particular computer and that it's not a stable measure to do that it's influenced by the speed of the machine so a program that took one minute on my computer might take 30 seconds on yours it has to do with the clever Ness of the Python implementation maybe I have a better implementation of python than you do so my computer will my programs will run a little bit faster but most importantly the reason we don't depend upon running programs is it depends upon the input so I might choose one input for which the program took two minutes and another seemingly similar input on which it took an hour so I need to get some way to talk about it more abstractly the way we do that is by counting the number of basic steps so we Define some function say time which Maps the natural numbers to the natural numbers the first n in this case the first natural number the argument corresponds to the size of the input how big an input are we running do we want to run the program on and the result of the function is the number of steps that the computation will take for an input of that size I'll come back to this in a little bit more precise detail momentarily a step is an operation that takes constant time and that's important so steps are not variable but they're constant so we have lots of these for example example an assignment a comparison an array access Etc in looking at computational complexity in this course we're going to use a model of the computer that's known as random access a random access machine frequently abbreviated as RAM in a random access machines instructions are executed one after another that is to say they're sequential only one thing happens at a time and we assume constant time required to access memory so we can access at random any object in memory in the same amount of time as any other object in the early days of computers this model was not accurate because memory was often say a tape and if you wanted to read something at the end of the tape it took a lot longer to read something at the beginning of the tap than something at the beginning of the tape in modern computers it's also not quite accurate modern computers have what's called a memory hierarchy where you have levels of memory the level one cache the level two cache the actual memory and it can differ by say a factor of a hundred how long it takes to access data depending upon whether it's in the cache the cache keeps track of recently accessed objects nevertheless if we start going into that level of detail we end up losing the forest for the trees so almost everybody when they actually try and analyze algorithms typically works with this model we also know in modern computers some things happen in parallel but again for most of us these will be second order effects and the random access model is quite good for understanding algorithms now when we think about how long an algorithm will take to run there are several different ways we could look at it we could think of the best case and as we think about these things as a concrete example we can think about linear search so let's say we have an algorithm that's using linear search we've looked at that before to find out whether or not an element is in the list well the best case would be that the first element is three and I'm searching for three and I find it right away and I stop so that would be my best Pace best Pace best case complexity it's the minimum running time over all possible inputs is the best case I could also look at the worst case what's the worst case for linear search it's not there exactly so I go and I have to look at every element and whoops it's not there so the worst case is the maximum over all possible inputs of a given size the size here is the length of the list and then I can ask what's the expected or average case what would happen most of the time the expected case seems in principle like the one we should care about but the truth is what when we do algorithmic analysis we almost never deal with the expected case because it's too hard if we think about the expected case for say linear search we can't talk about it without some detailed model of what the list itself looks like what elements are in it and what the distribution of queries looks like are we most of the time asking for elements that are not in the list list in which case the expected value is out here somewhere we it's far or are we most of the time looking for things that are in the list in which case the expected value would be somewhere near halfway through the length of the list if we don't know those things we have a tough time modeling expected value and one of the things we know is that frequently we don't when we release a program have a good sense of how people are actually use it and so we don't usually focus on that similarly we don't usually focus on the best case it would be nice but you can imagine that it's not really what we care about what happens when we get really lucky um because we all believe in Murphy's Law right that uh if something bad can happen it will and that's why complexity analysis almost always focuses on the worst case what the worst case does is it provides an upper bound how bad can things possibly get what's the worst that can happen and that's not nice because it means that there are no surprises we say the worst that this thing can do is run in is look at every element of the list once and so if I know that the list is a million elements I know okay it might have to do a million comparisons but it won't have to do any more than a million and so I won't be suddenly surprised that it takes overnight to run um Al last the worst case happens often we do frequently end up asking whether something is in a list and it's not so even though it seems pessimistic to worry about the worst case it is the right one to worry about all right let's look at an example so I've got a little function here f you can see it here it's on the handout as well um first of all what mathematical function is f Computing just to force you to look at it for a minute what's the Computing somebody it is a function that should be familiar to almost all of you nobody pardon exponential exponentiation uh don't think so but I appreciate your trying it's worth some candy not a lot of candy but a little candy yeah factorial exactly it's Computing factorial great grab all right all right so let's think about how long this will take to run in terms of the number of steps well the first thing it does is it computes it executes an assertion and for the sake of argument for the moment we can assume that most instructions in Python will take one step then it does an assignment so that's two steps steps then it goes through the loop each time through the loop it executes three steps the test at the start of the loop and the two instructions inside the loop how many times does it go through the loop somebody right n times so it will be 2 + 3 * n and then it ex Utes a return statement at the end so if I want to write down the function that characterizes the algorithm implemented by this code I say it's 2 + 3 * n + well I could do that but it would be kind of silly let's say n equals 3000 well if n equals 3 ,000 this tells me that it takes uh 9,000 well what is it take 9,000 and3 steps right right well do I care whether it's 9,000 or 903 I don't really so in fact when I look at complexity I tend to I don't tend to I do ignore additive con constants so the fact that there's a two here and a one here doesn't really matter so I say well if we're trying to characterize this algorithm Let's ignore those because what I really care about is growth with respect to size how does the running time grow as the size of the input grows we can even go further do I actually care whether it's 3,000 or 9,000 well I might I might care whether a program takes say three hours to run or nine hours to run but in fact as it gets bigger and we really care about this as things get bigger I probably don't care that much if I told you this was going to take 3,000 years or n or 9,000 years you wouldn't care or probably even if I told you it was going to take 3,000 days or 9,000 days you'd say well it's too long anyway so typically we even ignore multiplicative constants and use a model of asymptotic growth that talks about how the complexity grows as you reach the limit of the sizes of the inputs this is typically done using some notation using a notation we call Big O notation written as a single O So if I write order n o n what this says is this algorithm the complexity the time grows linearly with n with the doesn't say whether it's three times n or 2 * n it's linear in N is what this says but why do we call it Big O well some people think it's because oh my God this program will never end U but in fact no uh this notion was introduced to computer science by Donald kth and he chose the Greek letter Omron because it was used in the 19th century by people developing calculus uh we don't typically write Omron because it's harder to type so we usually use the capital Latin letter o hence life gets simple what this does is it gives us an upper Bound for the asmic growth of the function so formally we would write something like f ofx where f is some function of the input X is order let's say x SAR that would say it's quadratic in the size of X formally what this means is that the function f I should probably write this down the function f grows no faster and the quadratic polom X SAR all right so let's look at what this means uh I wrote a little program that uh talks about some of the I should say probably most popular values we see so some of the most popular orders we see we would write down we often see order one and what that means is constant the time required is independent of the size of the input doesn't say it's one step but it's independent of the input it's constant we often see order log n logarithmic growth that's order n linear one we'll see later this week is n log n this is called log linear and we'll see why that occurs surprisingly often order n to the C where C is some constant this is polinomial a common polinomial would be squared as in quadratic and then if we're terribly unlucky we run into things are order C to the N exponential in the size of the input to give you an idea of what these classes actually mean uh I wrote a little program that produces some plots uh don't worry about what the code looks like in a few weeks you'll be able to write such programs yourself um not only you be able to you'll be forced to um so I'm just going to run this and produce some plots showing different orders of growth why isn't it producing these plocks all right excuse me I see there it goes all right so let's look at the plots so here I've plotted linear growth versus logarithmic growth and as you can see it's quite a difference if we can manage to get a logarithmic algorithm it grows much more slowly than a linear algorithm and we saw this when we looked at the great advantage of uh binary search as opposed to linear search actually this is linear versus log linear what happened to figure one three well we'll come back to it so you'll see here that log linear is much worse than linear so this factor of n log in actually makes a considerable difference in running time now I'm going to compare log linear to quadratic the a small degree polinomial and as you can see it almost looks like log linear is not growing at all so as bad as log linear looked when we compared it to linear we see that compared to quadratic it's pretty great and what this tells us is that in practice even a quadratic algorithm is often impractically slow and we really can't use them and so in practice we work very hard to avoid even quadratic which somehow doesn't seem like it should be so bad but in fact as you can see it gets bad quickly yeah this was the log versus linear not surprising and now if we look at quadratic versus exponential we can see hardly anything and that's because exponential is growing so quickly so instead what we're going to do is I'm going to plot the y axis logarithmically just so we can actually see something and as you can see on input of size a thousand an exponential algorithm is roughly order 10 to the 286th that's an unimaginably large number right I don't know what it compares to the number of atoms in the universe or something ridiculous or maybe more but we can't possibly think of running an algorithm is going to run this take this long it's just not even conceivable so exponential we sort of throw up our hands and say we're dead we can't do it and so nobody uses exponential algorithms for everything yet for anything yet as we'll see there are problems that we care about that in principle can only be solved by exponential algorithms so what do we do as we'll see well we usually don't try and solve those problems we try and solve some approximation to those problems or we use some other tricks to say well we know the worst case will be terrible but here's how we're going to avoid the worst case we'll see a lot of that towards the end of the term the moral is try not to do anything that's worse than log linear if you possibly can all right now some truth and advertising some caveats if I look at my definition of what big o means I said it grows no faster than so in principle I could say well what the heck I'll just write 2 to the X here and it's still true it's not faster than that it's not what we actually want to do what we actually want is a tight bound we'd like to say it's no faster than this but it's no slower than this either to try and characterize the worst case as precisely as we can formally speaking the theorist use something called Big Theta notation for this where they write a Theta instead of an O however most of the time in practice when somebody writes something like f ofx is order x² what they mean is the worst case is really about x squared and that's the way we're going to use it here we're not going to try and get too formal we're going to do what people actually do in practice when they talk about complexity all right let's look at another example now all right here I've written factorial recursively didn't even try and disguise what it was so let's think about how we would analyze the complexity of this well we know that we can ignore the first two lines of code because those are just the additive pieces we don't care about that the first line um and the if and the return so what's the piece we care about we care about the number of times that factorial is called in the first implementation of factorial we cared about the number of iterations of a loop now we've instead of using a loop used recursion to do more or less the same thing and so we care about the number number of times fact is called how many times will that happen well let's think about why I know this doesn't run forever because that's always the way we really think about complexity in some sense I know it doesn't run forever because each time I call factorial I call it on a number one smaller than the number before so how many times can I do that if I start with the number n n times right so once again it's order n so the interesting thing we see here is that essentially I've given you the same algorithm recursively and iteratively not surprisingly even though I've coded it differently it's the same complexity now in practice the recursive one might take a little longer to run because there's a certain overhead to function calls that we don't have with while Loops but we don't actually care about that this overhead is one of those multiplicative constants I said we're going to ignore and in fact it's a very small multiplicative constant it really doesn't make much of a difference so how do I decide whether to use recursion or iteration has nothing to do with efficiency it's whichever is more convenient to code in this case I kind of like the fact the recursive factorial it's a little neater so that's what I would use and not worry about the efficiency all right let's look at another example uh how about G what's the complexity of G well I can ignore the first statement but now I've got two nested Loops how do I go and think about this the way I do it is I start by finding the inner loop and say all right how many times do I go through the inner loop or how long long does each I go through the in Loop end times right so to execute the inner four statement is going to be order n the next question I ask is how many times do I start the in Loop up again that's also order n times so what's the complexity of this somebody yes I think I heard the right answer it's order n sared because I execute the inter Loop n times or each time around is n and then I multiply it by n because I'm doing it the outer loop n times so the inner loop is order n s okay does that make sense so typically whenever I have nested Loops I have to do this kind of reasoning same thing if I have recursion inside a loop or nested recursions I start at the inside and work my way out is the way to do the analysis all right let's look at another example it's kind of a different take how about h what's the complexity of H all right first of all what's H doing it's kind of always a good way to start what is answer going to be yeah the sum of the numbers in X exactly it's going to be the sum of the digits spring training is already underway so all right it's gonna sum of the digits and what's the complexity well we can analyze it uh right away we know we can ignore everything except the loop so how many times do I go through this Loop depends upon the number of digits in the string representation of the int right now if I were careless I would write something like order n where n is the number of digits in s but I'm really am not allowed to do that why not because I have to express the complexity in terms of the inputs to the program and S is not an input s is a local variable so somehow I'm going to have to express the complexity not in terms of of s but in terms of what x so that's noo so what is it in terms of X how many digits yeah it's not constant no because I'll have more digits in a billion than I will in four right log in this case base 10 of X the number of digit decimal digits required to express an integer is the log of the magnitude of that integer you think about when we looked at binary numbers and decimal numbers uh last lecture that was exactly what we were doing so that's the way I have to express this now what's the moral here the moral is not the thing I really care about is not that this is how you talk about the number of digits in an INT what I care about is that you always have to be very careful people often think that they're done when they write something like order n but they're not until they tell you what N means because it can be pretty subtle you know n and in this case was not you know if Order X would have been wrong because it's not the magnitude of the integer X it's this is what controls the growth so whenever you're looking at complexity you have to be very careful what you mean what the variables are this is particularly true now when you look at functions say with multiple inputs okay um let's look at some more examples so we've looked before at search so this is code you've seen before really um here's a linear search and a binary search and in fact informally we've looked at the complexity of these things before and we can run them and we can see how they will grow but it won't surprise you so if we look at the linear search whoops while I'm printing the values here just just shows it works but now the binary search what we're going to look at is how it grows this is exactly the search we looked at before and the thing I want you to notice and as we looked at before we saw it was logarithmic is that as the number of as the size of the list grows doubles I only need one more step to do the search this is the beauty of a logarithmic algorithm so as I go from a 100 which takes seven steps to 200 it only takes eight 1 1600 takes 11 and when I'm all the way up to some very big number and I'm not even sure what that number is it took 23 steps but very slow growth so that's a good thing what's the order of this it's order n where n is what order log n where n is what let's just try and write it carefully well it's order length of the list right we don't care what the actual members of the list are now that's an interesting question to ask let's look at the code for a minute is that a valid assumption well it seems to be when we look at my test but let's look at what I'm doing here so a couple things I want to point out one is I used a very common trick when dealing with these kinds of algorithms you'll notice that I have something called B search and something called search all search does is called B search why did I even bother with search why didn't I just when my code down here called B search with some initial values the answer is really I started with this search and a user of search shouldn't have to worry that I got clever and went from this linear search to a binary search or maybe some more complex search yet I need to have a consistent interface for the search function and the interface is what it looks like to the caller and it says when I call it it just takes a list in an element it shouldn't have to take the high bound and the low bound as arguments because that really is not intrinsic to the meaning of search so I typically will organize my program by having this search look exactly like this search to a CL caller and then it does whatever it needs to do to call binary search so that's usually the way you do these things it's very common with recursive algorithms various things where you need some initial value that's only there for the initial call things like that all right the other thing I wanted let me finish one of wanted to point out is the use of This Global variable so you'll notice down here I defined something called num calls remember we talked about Scopes so this is now an identifier that exists in the outermost scope of the program then in B search I used it but I said I'm going to use this Global variable this variable declared outside the scope of bearch inside V search so it's this statement that tells me not to create a new local variable here but to use the one in the outer scope this is normally considered poor programming practice Global variables can often lead to very confusing programs occasionally they're useful here it's pretty useful because I'm just trying to keep track of the number of times this thing is called and so I don't want a new variable generated each time the it's instantiated all right now do you want a question yeah check the order length L the size of the list is the order of which search I that's the order of the linear search right the order of the binary search is order log base two of L sorry not of L right doesn't make sense to take the log of a list of the length of the list typically we don't bother writing base two if it's logarithmic it doesn't really very much matter what the base is you'll still get that very slow growth log base 10 log base 2 not that much difference so we typically just write log all right people with me now for this to be true or in fact even if we go look at the linear surf there's kind of a assumption I'm assuming that I can extract the elements from a list and compare them to a value in constant time because remember my model of computation says that every step takes the same amount of time roughly and if I now look say at binary search you'll see I'm doing something that apparently looks a little bit complicated up here I am looking at L of low and comparing it to e and l of high of comparing it to e how do I know that's constant time maybe it takes me order length of list time to extract the last element so I've got to be very careful when I look at complexity not to think I only have to look at the complexity of the program itself that is to say in this case the number of recursive calls but is there something that it's doing inside this function that might be more complex than I think as it happens in this case this r complicated expression can be done in constant time and that will be the first topic of the lecture on Thursday