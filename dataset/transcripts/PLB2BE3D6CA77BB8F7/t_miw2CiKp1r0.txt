the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu last Tuesday we were ended up the lecture talking about napsack problems we talked about the continuous napsack problem and the fact that you could solve that optimally with the greedy algorithm then we looked at the 01 knapsack problem and discussed the fact that while we could write greedy algorithms that would solve the problem quickly we have to be careful we mean by solve and that while those while those algorithms would choose a set of items that we could indeed carry away there was no guarantee that it would choose the optimal items that is to say one that would meet the objective function of maximizing the value we looked after that at a Brute Force algorithm on the board only for finding an optimal solution a guaranteed optimal solution but observed the fact that on a even a moderately sized set of items it might take a decade or so to run decided that wasn't very good nevertheless I want to start today looking at some code that implements The Brute Force algorithm um not because I expect anyone to actually run this on a real example but because a bit later in the term we'll see how we could modify this to something that would be practical and there's some things to learn by looking at it so let's look at some code here um I don't expect you to understand in real time all the details of this code it's more I want you to understand the basic idea behind it and then the result we get so you will remember that we looked at the complexity by saying well really it's like binary numbers so the first helper sub routine I'm going to use is something that generates binary numbers so it takes an N some natural number and the number of digits and returns a binary string of that length representing the decimal number n why am I giving it um this number of digits because I need to zero pad it if I want to have a vector that represents whether or not I take items uh if I take only one item say the first one I don't want just the binary string with one digit in it because I need all those zeros to indicate that I'm not taking the other items and so the second argument tells me in effect how many zeros I'm going to need and there's nothing mysterious about the way it does it okay the next helper function generates the power set of the items what is a power set if you take a set you can then ask the question what are all the subsets of the set what's the smallest subset of a set it's the empty set no items what's the largest subset of a set all of the items and then we have everything in between the set that contains the first item the set that contains the second item Etc the set that contains the first and the second the first and the third there are a lot of them and of course how many is a lot well two to the N is a lot but now we're going to generate every possible subset of items and we're going to do this simply using the decimal to Binary function to tell us whether or not we keep each one so we can enumerate them we can generate them all and now we have the set of all possible items one might take irrespective of whether they obey the constraint of not weighing too much the next function is the one that does the work this is the interesting one uh choose best it takes a power set the constraint and two functions one get value that tells me the value of an item and the other get weight that tells me the weight of an item then it just goes through and it enumerates all possibilities and eventually chooses I won't say the best set because it might not be unique there might be more than one optimal answer but it finds at least one optimal answer and then it returns that again it's a very straightforward implementation of the Brute Force algorithm I sketched in the board and then we can run it with test best which is going to build the items using the function we looked at last time it's then going to get the power set of the items it's going to call choose best best and then print the result so let's see what happens if we run it we get an error oh dear I hadn't expected that and it says test oh test best is not defined all right let's try that again sure looks like it's defined to me there it is okay and you may recall that this is a better answer than anything that was generated by the greedy algorithm on Tuesday you may not recall it but believe me it is it happened to have found a better solution and not surprisingly that's CU I contrived the example to make sure that would happen why does it work better in the sense or why does it find a better answer why might it find a better answer well because the greedy algorithm chose something that was locally optimal at each step but there was no guarantee that a sequence of locally optimal decisions would reach a global Optimum what this algorithm does is it finds a global Optimum by looking at all solutions and that's something we'll see again and again as we go forward that there's always a temptation to do things one step at a time finding local Optimum Optima because it's fast it's easy but there's no guarantee it will work well now the problem of course with finding the global Optimum is as we discussed uh it is prohibitively expensive now you could ask is it prohibitively expensive because I chose a stupid algorithm The Brute Force algorithm well it is a stupid algorithm but in fact this is a problem that is what we would call inherently exponential we've looked at this concept before that in addition to talking about the complexity of an algorithm we can talk about the complexity of a problem in which we ask the question how fast can the absolute best solution fastest solution to this problem be and here you can construct a mathematical proof that says the problem is inherently exponential no matter what we do we're not going to be able to find something that's guaranteed to find the optimal that is faster than exponential well now let's be careful about that statement what that means is the worst case is inherently exponential as we will see in a couple of weeks it'll take us a while to get there there are actually algorithms that people use to solve these inherently exponential problems and solve them fast enough to be useful so for example when you go to look at Airline fairs on kayak to try and find the best fair from A to B it is an inherently exponential problem but you get an answer pretty quickly and that's because there are techniques that you can use now in fact one of the reasons you get it is they don't guarantee that you actually get an optimal solution but there are techniques that guarantee to give you an optimal solution that almost all the time will run quickly and we'll look at one of those a bit later in the term before we do that however um I want to leave for a while the whole question of complexity behind and look at another class of optimization problems we'll look at several different kinds of optimization problems as the term goes forward the kind I want to look at today is probably what I would say is the most exciting branch of computer science today and of course I might have a bias and that's machine learning uh it's a word you'll hear a lot about um and it's a technique that many of you will apply you might not write your own code but I guarantee you are either the beneficiary or the victim of machine learning almost every time you log on to the web these days I should probably start by defining what machine learn learning is but that's hard to do I really don't know how to do it superficially you could say that machine learning deals with the question of how to build programs that learn however I think in a very real sense every program we write learn something if I Implement Newton's method it's learning what the roots of the polinomial is uh certainly when we looked at Curve fitting fitting curves to data we were learning a model of the data that's what that regression is um Wikipedia says and of course it's must be true if a Wikipedia says it that machine learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to evolve behaviors based on empirical data I'm not sure how helpful this definition is but it was the best I could find and it doesn't really matter but it sort of gets it the issue that a major focus of machine learning research is to automatically learn to recognize complex patterns and make Intelligent Decisions Based on data this whole process is something called inductive inference the basic idea is one observes actually one doesn't the program observes examples that represent incomplete information about some statistical phenomena and then Tres to generate a model just like with curve fitting that summarizes some statistical property of that data and can be used to predict the future for example give you information about unseen data there are roughly speaking two distinctive approaches to machine learning called supervised learning and unsupervised learning let's first talk about supervised learning it's a little easier to appreciate how it might work in supervised learning we asso associate a label with each example in a training set so think of that as an answer to a query about an example if the label is discreet we typically call it a classification problem so we would try and classify uh for example a transaction on a credit card as belonging to the owner of that credit card or not belonging to the owner as you I.E with some probability a stolen credit card so it's discret it belongs to the owner it doesn't belong to the owner uh if the labels are real valued we think of it as a regression problem and so indeed when we did the curve fitting we were doing machine learning and we were handling a regression problem based on the examples from the training set the goal is to build a program that can predict the answer for other cases before they are explicitly observed so we're trying to generalize from the the statistical properties of the training set to be able to make predictions about things we haven't seen um let's look at an example so here I've got red and blue circles and I'm trying to learn what makes the circle red or what's the difference between red and blue other than the color right think of my information as the XY values and the label as the color red or blue so I've labeled each one and now I'm trying to learn something well it's kind of tricky um what are the questions I need to answer to think about this and then we'll look at how we might do it so a first question I need to ask is are the labels accurate and in fact in a lot of real world examples in most real world examples there's no gu guarantee that the labels are accurate so you have to assume that well maybe some of the labels are wrong how do we deal with that um perhaps the most fundamental question is is the past representative of the future represent we've seen many examples where people have learned things for example to predict uh the price of Housing and it turns out you hit some Singularity which means the past is not a very good predictor of the future and even if all of your learning is good you get the wrong answer so you sort of always have to ask that question question do you have enough data to generalize and by this I mean enough training data if your training set is very small you shouldn't have a lot of confidence in what you learn um a big issue here is feature abstraction extraction as we'll see when we look at real examples the world is pretty complex place and we need to decide what features we're going to use you know if I were to ask 25 of you to come up in the front of the room and then try and separate you based upon some feature you know if I were to say all right I'm going to separate the good students from the bad students but the only features I have available are the closure wearing uh it might not work so well um and very importantly how tight should the fit be so now let's go back to our example here um we can look at two different ways we might generalize from this data and indeed when we're looking at classification problems in supervised learning what we're typically doing is trying to find some way of dividing our training data in this case I given you a two-dimensional projection as we'll see it's not always two dimensional it's not usually two dimensional so I might choose this rather eccentric shape and say that's great and why is that great it's great because it minimizes training error so if we look at it as an optimization problem we might say that our objective function is how many points are correctly classified in the training data as red or blue and this triangular shape has no training error every point is perfectly classified in the training data if I choose this linear separator instead I have some training error this red point is misclassified in the training does that mean that the triangle is better than the line not necessarily right because my goal is to predict future points and maybe that's mislabeled or an experimental error maybe it's accurately labeled but an outlier very unusual and this will not generalize well this is analogous to what we talked about as overfitting when we looked at Curve fitting and that's a very big problem in machine learning is if you overfit to your training data it might not generalize well and might give you bogus answers going forward okay so that's a very quick look at uh supervised learning we'll come back to that I now want to talk about unsupervised learning the big difference here is we have training data but we don't have labels so I just give you a bunch of points it's as if we looked at this picture and I didn't tell you which were the red points and which were the blue points they were just all points so what can I learn what typically you're learning in unsupervised learning is you're learning about regularities of the data so if we looked at this and think away the red and the blue we might well say well at least you know if I look at this there is some structure to this data and maybe what I should do is divide it this way gives me kind of a nice clean separation but maybe I should divide it this way or maybe I should put a circle around each of these four groupings complicated what to do but what we see is there is clearly some structure here and the idea of unsupervised learning is to discover that structure Far and Away the dominant form of unsupervised learning is clustering and that's what I was just talking about is finding the cluster in this data so we'll move forward here there it is with uh everything the same color but here I've labeled the X and Y axis as height and weight um what does clustering means it's the process of organizing the objects or the points into groups whose members are similar in some way a key issue is what do we mean by similar what's the metric we want to use and we can see that here if I tell you that really I want to Cluster people by height say people are similar if they're the same height then it's pretty clear how I should divide this right what my clusters should be my cluster should probably be this group of shorter people and this group of taller people if I tell you I'm interested in weight then probably I want to Cluster it with a divisor here between the heavier people and the lighter people or if I say well I'm interested in some combination of those two then maybe I'll get four clusters as I discussed before clustering algorithms are used all over the place for example in marketing um they are used to find groups of customers with similar Behavior Walmart is famous for using that clustering to find that uh they did a clustering to determine who would buy when people bought the same thing and then they would rearrange their shelves to encourage people to buy things and sort of the most famous example they discovered was there was a strong correlation between people between people who bought beer and people who bought diapers and so there was a period where if you walked in a Walmart store you would find the beer and the diapers next to each other and I leave it to you to speculate on why that was true it just happened to be true in Walmarts um Amazon uses clustering to find people who like similar books so every time you buy a book on Amazon they're running a clustering algorithm to find out who looks like you so oh this person looks just like you so if they buy a book maybe you'll get an email suggesting you buy that book or the next time you log in Amazon or when you look at a book they tell you here are some similar books and then they've done a clustering to group books as similar based on buying habits um Netflix uses that to to recommend movies Etc uh biologists spend a lot of time these days doing clustering um they classify plants or animals based on their features we'll shortly see an example of that as in right after Patriots Day uh but they also will use it a lot in genetics so clustering is used to try and find genes that look alike or groups of genes uh insurance companies use that to decide how much to charge you for your automobile insurance they cluster drivers based upon who and use that to predict who's going to have an accident um document classification on the web is used all the time um used a lot in medicine just used all over the place so what is it exactly well the nice thing is we can Define it very straightforwardly as an optimization problem and so we can ask what properties does a good clustering have well it should have low inra cluster dissimilarity so in a good clustering all of the points in the same cluster should be similar by whatever metric you're using for similarity and as we'll see there are a lot of choices there uh but that's not enough we'd also like to have high intercluster dissimilarity so we'd like the points with in a cluster to be a lot like each other other but the if a points are in different clusters we'd like them to be quite different from each other that tells us that we have a good cluster all right let's look at it how might we model this similarity well using a concept we've already seen variance so we can talk about the variance of some cluster c as equal to the sum of all elements X in C of the mean of C minus x squared or maybe we can take the square root of it if we want but it's exactly the idea we've seen before right that we say what's the average value of the cluster and then we look how far is each point from the average we sum them and that tells us how much variance we have within the cluster makes sense so that's variance now so we can use that to talk about how similar or dissimilar the elements in the cluster are we can use the same idea to compare points in separate clusters and compute various different ways and we'll look at different ways to look at the distance between clusters so combining these two things we could get say a metric we'll call Badness not a technical word and now I'll ask the question is the optimization problem that we're solving in clustering finding a set of clusters Capital C such that Badness of that set of clusters is minimized is that a sufficient definition of the problem we're trying to solve find a set of cluster C such that Badness of C is minimized is that good enough no no why not just imagine case where if you cluster if you make a single cluster every cluster has one element in it the variance is zero exactly so that has a trivial solution which is probably not the one we want of putting each point in its own cluster Badness it won't be bad it'll be a perfect clustering in some sense but doesn't do us any good really so what do we do to fix that what do we usually do when we formulate an optimization problem what's missing I've given you the objective function what have I not given you a constraint so we need to add some constraint here that will prevent us from finding a trivial solution so what kind of constraints might we look at um there are different ways of doing it um a couple ones that usual sometimes you might have as a constraint the maximum number of clusters so all right cluster my data but I want at most K clusters 10 clusters and that would be my constraint like the weight for the knapsack problem or maybe I'll want to put something on the maximum distance between clusters so I don't want the distance between any two clusters to be more than something in general solving this optimization problem is computationally prohibitive so once again in practice what people typically resort to is greedy algorithms and I want to look at two kinds of greedy algorithms two the probably the two most common approaches to clustering one is called K means in K means clustering you say I want exactly K clusters and get find that find the best K clustering we'll talk about how it does that and again it's not guaranteed to find the best and the other is hierarchical clustering we'll come back to that shortly both are simple to understand and widely used in practice so let's first talk about how we do this let's first look at hierarchical clustering so we have a set of n items to be clustered and let's assume we have an N byn distance Matrix that tells me for each pair of items how far they are from each other so we can look at an example so here's an N byend distance Matrix for the airline distance between some cities in the United States that the distance from Boston to Boston is zero miles distance from New York is 206 the distance from Chicago to San Francisco is 2142 Etc all right so my n byn distance Matrix there now let's go through how hierarchical clustering would relate these things to each other so we start by assigning each item to its own cluster so if we have n items we now have n clusters all right that's the trivial solution that you suggested before the next step is to find the most similar pair of clusters and merge them so if we look here uh and we just we start we'll have uh six clusters one for each City and we would merge the two most similar which I guess in this case is New York and Boston hard to believe that those are the most similar cities but at least by this distance metric they're the closest so we would merge those two and then you just continue the process in principle into all items are in one cluster so now you have a whole hierarchy of clusters and you can cut it off where you want if you want to have six clusters you can look at where in the hierarchy you have six you can look where you have two where you have three of course you don't have to go all the way to finish it if you don't want to this kind of hierarchical clustering is called AG glomera why well because we're combining things we're agglomerating them so this is pretty straightforward except for two the complication in step two is we have to Define what it means to find the two most similar clusters now it's pretty easy when the Clusters each contain one element because well we have our metric in this case distance and we can just do that as I did but it's not so obvious what you do when they have multiple elements and in fact different metrics can be used to get different properties so I want talk about some of the metrics we use for that these are typically called linkage criteria all right so one popular one is what's called single linkage it's also called connectedness or minimum method in this we consider the distance between a pair of clusters to be equal to the shortest distance from any member to any other member so we take the two points in each cluster that are closest to each other and say that's the distance between the two clusters people also use something called complete linkage it's also called diameter or maximum where we consider the distance between any two clusters to be the distance between the points that are furthest from each other so in one case essentially single linkage linkage is looking at the best case complete in English not French is looking at the uh worst case then you won't be surprised to hear that you can also look at the average case where you take all of the distances so you take all of the pairwise things you add them up you take the average you can also take the mean the median if you want instead none of these is necessarily best but they do give you different answers and so I want to look at that now with our example here um so let's look at it and run it so the first step is independent of what linkage we're using we get these six clusters all right now let's look at the Second Step well also pretty simple since we only have one element in each one we're going to get that clustering all right now what about the next step what do I get if I'm using the minimal single linkage distance what gets merged here York somebody Boston New York and Chicago Boston New York and Chicago um and it turns out we'll get the same thing if we use other linkages in this case let's continue to The Next Step uh now we'll end up merging San Francisco and Seattle now we get a difference what is the red represent and what does the blue represent which linkage criteria we're saying we could either merge Denver with Boston New York and Chicago or we could merge Denver with San Francisco and Seattle um which is which which linkage Criterion has put Denver in which cluster well suppose we're using single linkage where're would get we go Chicago yes because it's not so far from Chicago even though it's pretty far from Boston or New York but if we use average linkage we see on average it's closer to San Francisco and Seattle and it is to the average of Boston New York and Chicago so we get a different answer and then finally at the last step everything gets merged together so you can see in this case without having labels we have used a feature to produce things and say if we wanted to have three clusters we would maybe stop here and we'd say all right these things are one cluster this is a cluster and this is a cluster and that's not a bad geographical clustering actually for deciding how to relate these things to each other this technique is used a lot it does have some weaknesses um one weakness is it's very timec consuming it doesn't scale well um the complexity is at least order N squared where n is the number of items points to be clustered and in fact in many implementations is worse than n^ S and of course it doesn't necessarily find the optimal clustering even giving these criteria it might never at any level have the optimal clustering because again at each step it's making a locally optimal decision not guarante to find the best solution I should point out that a big issue in deciding to get these clusters or getting these clusters was my choice of features and this is something we're going to come back to in Spades because I actually think it is the most important issue in machine learning is if we're going to say which points are similar to each other we need to understand our feature space so for example the feature I'm using here is Distance by air suppose instead I added Distance by air and distance by Road and distance by train well particularly given the sparsity of railroads in this country we might get very different clusterings depending upon where the trains ran and suppose I throw in a totally different feature like population well I might get another different clustering if you know depending how I use that what we typically need to do in these situations dealing with multi-dimensional data and most data is multi-dimensional is we construct something called a feature Vector that incorporates multiple features so we might have for each City we'll just take something like the distance you know we might or let's say uh instead of distance we'll compute the Distance by having for each City its uh GPS coordinates where it is and on the globe and its population let's say that's how we Define a city and that would be our feature vector and then we would cluster it say using hierarchical clustering to determine which cities are most like which other cities well it's a little bit complicated I have to ask how do I compare feature vectors what distance metric do I use there do I get confused that GPS coordinates and populations are essentially unrelated and I wouldn't like to compare those to each other uh lots of issues there and that's what we're going to talk about when we come back from Patriots Day is how in real world problems we go from the large number of of features associated with objects or things in the real world to feature vectors that allow us to automatically deduce which things are quote most similar to which other things