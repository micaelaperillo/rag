the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu I want to pick up with a little bit of overlap just to bring people remind people where we were we had been looking at clustering and we looked at a fairly simple example of using agglomerative hierarchical clustering to Cluster cities based upon how far apart they were from each other cities so essentially using this distance Matrix we could do a clustering that would reflect how close cities were to one another and we went through agglomerative clustering and we saw that we would get a different answer depending upon which linkage Criterion we used this is an important issue because as you're as one is using clustering one has to be aware that it is related to these things and you choose the wrong linkage Criterion you might get an answer other than the most useful all right I next went on and said said well this is pretty easy because when we're comparing the distance between two cities or the two features we just subtract one distance from the other and we get a number it's very straightforward I then raised the question suppose when we looked at cities we looked at a more complicated way of looking at them than Airline distance so the first question I said well suppose in addition to the Distance by air uh we add The Distance by Road uh or the average temperature pick what you will what do we do well the answer was we start by generalizing from a feature being a single number to the notion of a feature vector where the features used to describe a city are now represented by a vector typically of numbers now if the vectors are all in the same physical units we could easily imagine how we might compare two vectors so we might for example look at the ukian distance between the two just by say subtracting one vector from the other however if we think about that it can be pretty misleading because for example if when we look at a city one element of the vector represents the distance in miles from another city or in fact this case we distance in miles from say each City and another represents temperatures well it's kind of funny to compare a distance which might be thousands of miles with a temperature which might be 5 Dees right a 5 degree difference in average temperature could be significant certainly a 20° difference in temperature is very significant but a 20 mile difference in location might not be very significant and so to equally weight a 20° temperature difference and a 20 mile distance difference might give us a very peculiar answer and so we have to think about the question of how are we going to scale the elements of the vectors even if we're in the same units uh say inches uh it can be an issue so let's look at this example here I've got on the left uh before scaling something which we can say is in inches height and width um this is not from a person but you could imagine if you were trying to Cluster people and you measure their height in inches and their width in inches maybe you don't want to treat them equally right that there's a lot more variance in height than in width or maybe there isn't maybe there isn't so here on the left we don't have any scaling and we see a very natural clustering on the other hand we notice on the Y AIS the values range from not too far from zero to not too far from one whereas on the x axis the dynamic range is much less uh not too far from zero to not too far from a half so we have twice the dynamic range here than we have here therefore not surprisingly when we end up doing the clustering Height plays a very important role or width I guess is this one plays a very important role and we end up clustering it this way dividing it along here on the other hand if I take exactly the same data and scale it and now the x-axis runs from 0 to a half and the Y AIS roughly again from 0er to one we see that suddenly when we look at it geometrically we end up getting a very different look of clustering what's the moral the moral is you have to think hard about how to Cluster your features about how to scale your features because it can have a dramatic influence on your answer we'll see some real life examples of this shortly but these are all the important things to think about and the they all in some sense tie up into the same major Point whenever you're doing any kind of learning including clustering feature selection and scaling is critical it is where most of the thinking and end up going and then the rest gets to be fairly mechanical how do we decide what features to use and how to scale them we do that using domain knowledge so we actually have to think about the objects that we're trying to learn about and what the objective of the learning process is so continuing how do we do the scaling most of the time it's done using some variant of What's called the minowsky metric um it's not nearly as imposing as it looks so the distance between two vectors X1 and X2 and then we use P to talk about the essentially the degree we're going to be using so we take the absolute difference between each element of X1 and X2 raise it to the P power sum them and then take the one over P not very complicated so let's say p is two that's the one your people are most familiar with effectively all we're doing is getting the ukian distance what we looked at when we looked at the mean Square distance between two things between our errors and our measured data between our measured data and our predicted data we used the mean square error that's essentially a meowski distance with P equal to 2 that's probably the most commonly used but an almost equally commonly used sets P equal to one and that's something called The Manhattan distance um I suspect at least some of you have spent time walking around Manhattan a small but densely populated island in New York and Midtown Manhattan has the feature that it's laid out in a grid so what you have is a grid and you have the Avenues running north south and the streets running East West and if you want to walk from say here to here or drive from here to here you cannot take the diagonal because there are a bunch of buildings in the way and so you have to move either left or right up or down that giv that's the Manhattan distance between two points this is used in fact for a lot of problems typically when somebody is comparing the distance between two genes for example they use a Manhattan me metric rather than a ukian metric to to say how similar two things are uh just wanted to show that because it is something that you will run across in the literature when you read about these kinds of things all right so far we've talked about issues where things are comparable and we've been doing that by representing each element of the feature Vector as a floating Point number so we can run a formula like that by subtracting one from the other but we often in fact have to deal with nominal categories things that have names rather than numbers so for cluster ing people maybe we care about eye color blue brown gray green hair color well how do you compare blue to green do you subtract one from the other that's kind of hard to do what does it mean to subtract Green from Blue well I guess we could talk about it in the frequency domain and light and things um typically what we have to do in that case is we convert them to a number and then have some ways to relate the numbers again this is a place where domain knowledge is critical so for example we might convert blue to zero green to 0.5 and brown to one thus indicating that we think blue eyes are closer to green eyes than they to brown eyes I don't know why we think that but maybe we think that red hair is closer to blonde hair than it is to black hair I don't know um these are the sorts of things that are not mathematical questions typically but judgment that people have to make once we've converted things to numbers we then have to go back to our old friend of scale SC in uh which is often called normalization very often we try and contrive to have every feature range between zero and one for example so that everything is normalized to the same dynamic range and then we can compare is that the right thing to do not necessarily because you might consider some features more important than others and want to give them a greater weight and again that's something we'll come back to and look at all right all of this is a bit abstract I now want to look an example let's look at the example of clustering mammals uh there are essentially an unbounded number of features you could use size at Birth gestation period lifespan length of tail uh speed eating habits you name it the choice of features and waiting will of course have an enormous impact on what clusters you get right if you choose size uh humans might appear in one cluster uh if you appear choose uh eating habits they might appear in another um how should you choose which features you want you have to begin by choosing thinking about the reason you're doing the clustering in the first place what is it you're trying to learn about the mammals um as an example I'm going to choose the objective of eating habits I want to Cluster mammals somehow based upon what they eat uh but I want to do that and here's a very important thing about what we often see in learning with any direct information about what they eat typically when we're using machine learning we're trying to learn about something for which we have limited or no data remember when we talked about learning I talked about learning in which it was supervised in which we had some data and unsupervised in which essentially we don't have any labels so let's say we don't have any labels about what people eat or sorry about what mammals eat but we do know a lot about the mammals themselves and in fact the hypothesis I'm going to start with here is that you can infer people or creatures eating habits from their dental records or their dentition that over time we have evolved All Creatures have evolved to have teeth that are related to what they eat we can see so I managed to procure a database of dentition for various mammals uh there's the laser pointer so uh what I've got here is the number of different kinds of teeth so the right top and sizers the right bottom and sizers the molers ETC premolar don't worry if you don't know about teeth very much I don't know very much I'm and then for each animal I have the number of each kind of tooth and uh actually I don't have it for this particular mammal but for these two I do and I don't even remember what they are they're cute um all right so I've got that database and now I want to try and see what happens when I cluster them the code to do this is not very complicated but I should make a confession about it uh last night I I won't say I learned it I was reminded of a lesson that I've often preached in 600 is that it's not good to get your programming done at the last minute so as I was debugging this code at 2: and 3: in the morning today uh I was realizing how inefficient I am at debugging at that hour maybe for you guys that's the shank of the day for me it's too late I think it all works but I was certainly not at my best as I was uh debugging last night all right but at the moment I don't want you to spend time working on the code itself uh I would like you to think a little bit about the overall class structure of the code uh which I've got on the first page of the handout so at the bottom of my hierarchy I've got something called a point and that's an abstraction of the things to be clustered and I've done it in quite a generalized way because as you're going to see the code we're looking at today I'm going to use not only for cluster ing mammals but for clustering all sorts of other things as well so I decided to take the trouble of building up a set of classes that would be useful and in this classes class I can have the name of a point uh its original attributes that's say its original feature Vector an unscaled feature vector and then whether or not I chose choose to normalize it I might have normalized features as well again I don't want you to worry too much about the details of the code and then I have a distance metric and I'm just for the moment using simple ukian distance the next element in my hierarchy it's not yet a hierarchy it's still flat is a cluster and so what a cluster is you can think of it as at some abstract level it's just going to be a set of points the points that are in the cluster but I've got some other operations on it that will be useful um I can compute the distance between two clusters and as you'll see I have single linkage Mac link Max average the three I talked about last week and also this notion of a cent roid uh we'll come back to that when we get to K means clustering uh we don't need to worry right now about what that is then I'm going to have a cluster set that's another useful data abstraction and that's what you might guess from its name just a set of clusters the most interesting operation there is merge as you saw when we looked at hierarchical clustering last week the key step there is merging two clusters and in doing that I'm going to have a function called f closest which given a metric and and a cluster finds the cluster that is most similar to that to self because as you again will recall from hierarchical clustering that's what I merge at each step is the two most similar clusters and then there's some details about how it works which again we don't need to worry about at the moment and then I'm going to have a subclass of Point called mammal in which I will represent each mammal each mammal by the dentition as we've looked at before then pretty simply we can do a bunch of things with it um before we look at the other details of the code I want to now run it and see what we get so I'm just going to use hierarchical clustering now to Cluster the mammals based upon this feature Vector which will be a list of numbers showing how many of each kind of tooth the mammals have let's see what we get so it's doing the merging so what we can see at the first step it merged beavers with groundhogs then it merged gray squirrels and porcupines wolves and bears various other kinds of things things thought Jag Jaguars and cougars were a lot alike eventually it starts doing more complicated merges it merges the cluster containing only the river waterer with one containing a Martin and a wolverine um beavers and groundhogs with squirrels and porcupines Etc and at the end I had it sto with two clusters it came up with these clusters now we can look at these clusters and say all right what do we think have we learned anything interesting um do we see anything in any of these do we think it makes sense remember our goal was to Cluster mammals based upon what they might eat and we can ask do we think this corresponds to that no all right who somebody said why no go ahead um we've got like a deer doesn't eat similar things as a dog um and we've got like one type one type of bat in the top cluster and a different type of bat in the bottom cluster it seems like they'd be closer together oh sorry uh yeah uh a deer doesn't eat what a dog eats and for that matter uh we have humans here and while some humans are by choice vegetarians genetically humans are essentially carnivores we know that we we eat meat and here we are with a bunch of uh herbivores typically uh things are strange uh by the way bats might end up being in ones because some bats eat fruit other bats eat insects but who knows all right so I'm not very happy um why do you think we get this clustering that maybe isn't helping us very much well let's go look at what we did here let's look at Tes zero so I said I wanted two clusters uh I don't want it to print all the steps along the way I'm going to print the history at the end and scaling is identity well let's go back and look at some of the data here what we can see is or maybe we can't see too quickly looking at all of this is some kinds of teeth have a relatively small range other kinds of teeth have a big range and so at the moment we're not doing any normalization and maybe what we're doing is getting something distorted where we're only looking at a certain kind of tooth because it has a larger dynamic range and in fact if we look at the code we can go back up and let's look at build mammal points and read mammal data so build mammal points uh calls read mammal data and then builds the point so read mammal data is the interesting piece and what we can see here is as we read it in this is just simply reading things in ignoring comments keeping track of things and then when we come down here I might do some scaling so point. scale features using the scaling argument where is that coming from if we look at mammal teeth here from the mammal cluster or mammal not cluster class we see that there are two ways to scale it identity where we just multiply every element in the vector by one that's doesn't change anything or what I've called one over Max and here I've looked at the maximum number of each kind of tooth and I'm dividing one by that so here we could have up to three of those here we could have four of those we can have six of this kind of tooth whatever it is and so we can see by dividing by the max I'm now putting all of the number all of the different kinds of teeth on the same scale I'm normalizing and now we'll see does it make a difference well since I'm dividing by six here and three here it certainly could make a difference right it's a significant scaling factor 2x so let's go and change the code or change the test and let's look at Tes zero whoops zero not o with scale set to one over Max you'll notice by the way that rather than using some obscure code like scale equals 12 I've used strings so that I'll remember what they are it's it's I think a pretty useful programming trick whoops did I use the what the wrong name for this uh should be scaling so off it's going now we get a different set of things and as far as I know once we've scaled things we get what I think is a much more sensible pair where I think what we essentially have is the herbivores down here and the carnivores up here here okay um I don't care how much you know about teeth the point is scaling can really matter you have to look at it and you have to think about what you're doing and the interesting thing here is without any direct evidence about what what mammals eat we are able to use machine learning clustering in this case to infer a new fact that we have some mammals that are similar in what they eat and some mammals that are also similar some groups now I can't infer from this herbivores versus carnivores because I didn't have any labels to start with but what I can infer is that whatever they eat there's something similar about these animals and something similar about these animals and there's a difference between the groups in C1 and the groups in c0 I can then go off and look at some points in each of these and then try and figure out how to label them later okay um let's look at a different data set a far more interesting one a richer one now let's not look at that version of it that's too hard to read let's look at the Excel spreadsheet so this is a database I found online of every county in the United States and a bunch of features about that County so for each county in the United States we have its name the first part of the name name is the state it's in second part of name is the name of the county and a bunch of things like the average value of homes uh how much poverty this population density its population changed how many people are over 65 Etc so the thing I want you to notice of course is while everything is a number the scales are very different you know there's a big difference between uh the percent of something which will go between Zer and 100 and the population density which ranges over a much larger dynamic range so we can immediately suspect that scaling is going to be an issue here so we now have a bunch of code that we can use that I've written to process this it uses the same clusters that we have here except I've added a kind of Point called a county looks very different from a mammal but the good news is I got to reuse a lot of my code now let's run a test we'll go down here to test three and we'll see whether we can do hierarchical clust clustering of the counties whoops test three wants the name of of what we're doing so we'll give it the name it's counties. text I just exported the E the spreadsheet as a text file well we can wait a while while for this but I'm not going to let's think about what we know about hierarchical clustering and how long this is likely to take I'll give you a hint there are approximately 3,100 counties in the United States I'll bet none of you could have guessed that number all right how many comparisons do we have to do to find the two counties that are most similar to each other comparing each County each every other County how many comparisons is that going to be yeah it's 3100 choose two right so that will be 3100 squared thank you and that's just the first step of the cluster to perform the next merge we'll have have to do it again so in fact as we've looked at last time it's going to be a very long and tedious process and one I'm not going to wait for so I'm going to interrupt and we're going to look at a smaller example here I've just got only the counties in New England a much smaller number than 3100 and I'm going to Cluster them using the exact same clustering code we use for the mammals it's just that the points are now counties instead of mammals uh and we got two clusters uh middlex County in Massachusetts happens to be the county in which MIT is located and all the others well you know MIT is a pretty distinctive place maybe that's what did it um I don't quite think so someone got a hypothesis at why we got this rather strange clustering uh and I is it because middle sex contains MIT and Harvard both um this really surprised me by the way when I first ran it I said how can this be so I went and I started looking at the data and what I found is that middle sex county has about 600,000 more people than any other County in New England who knew I would have guessed suffk where Boston is was the biggest County but in fact middle sex is enormous relative to every other count in New England and it turns out that difference of six 100,000 when I didn't scale things just swamped everything else and so all I'm really getting here is a clustering that depends on the population middle sex is Big relative to everything else and therefore that's what I get and it ignores things like education level and housing prices and all those other things because the differences are small relative to 600,000 all right well let's turn scaling on to do that I want to show you how I do the scaling um I did not given the number of features and the number of counties do what I did for mammals and count them by hand to see what the maximum was I decided it would be a lot faster even at 2 in the morning to write code to do it so I've got some code here um so I've got build County points just like build mammal points and read County data like read mammal data but the difference here is along the way as I'm reading in each County I'm keeping track of the maximum for each feature and then I'm going to just do the scaling automatically so exactly the one over Max scaling I did for Mammal teeth I'm going to do it for counties but I've just written some code to automate that process because I knew I would never be able to count them all right so now let's see what happens if we run it that way test three of New England and uh scale equals true I'm either scaling it or not is the way I wrote this one all right and with the scaling on again I get a very different set of clusters um what do we got where's middle sex it's in one of these two clusters oh here it is in c0 um but it's with Fairfield Connecticut and Hartford Connecticut and uh Providence Rhode Island um it's a different answer is it a better answer um it's not a meaningful question right it depends what I'm trying to infer what we hope to learn from the clustering and that's a question we're going to come back to on Tuesday in some detail with the counties and look at how by using different kinds of scaling or different kinds of features we can learn different things about the counties in this country before I do that however I want to move away from New England remember we're focusing in New England because it took too long to do hierarchical clustering of 3100 counties but that's what I want to do it's no good to just say I'm sorry it took too long I give up well the good news is there are other clustering mechanisms that are much more efficient we'll later see they two have their own faults but we're going to look at K means clustering which has the big advantage of being fast enough that we can run it on very big data sets in fact it is roughly linear in the number of counties and as we've seen before when n gets very large anything that's worse than linear is likely to be ineffective so let's think about how K means Works uh step one is you choose k k is the total number of clusters you want to have when you're done so you start by saying I want to take the counties and split them into K clusters two clusters 20 clusters 100 clusters a thousand clusters you have to choose K in the beginning uh and that's one of the issues that you have with K means clusterings is how do you choose K we can talk about that later once I've chosen K I choose K points as initial centroids you may remember earlier today we saw this centroid method in class cluster so what's a centroid you've got a cluster and in the Clusters you've got a bunch of points scattered around the centroid you can think of as quote the average point the center of the cluster the centroid need not be any of the points in the cluster so again you need some Metric but let's say we're using L ukian it's easy to see in the board the centroid is kind of there now let's assume that we're going to start by choosing K points from the initial set and labeling each of them as a centroid we often in fact quite typically choose these at random so we now have K randomly chosen points Each of which we're going to call a centroid the next step is to assign each point to the nearest centroid so we've got K centroids we usually choose a small K say 50 and now we have to compare each of the 3100 counties to each of the 50 centroids and put each one in the correct thing in the closest so it's 51 * 3100 which is a lot smaller number 50 * 3100 which which is a lot smaller number than 3100 squared so now I've got a clustering kind of strange because what it looks like depends on this random choice so there's no reason to expect that the initial assignment will give me anything very useful step four is is for each of the K clusters choose a new centroid now remember initially I just chose at random Cas centroids now I actually have a cluster with a bunch of points in it so I could for example take the average of those and compute a centroid and I can either take the average or I can take the point nearest the average it doesn't much matter and then step five is one we've looked at before assign each point to nearest centroid so now I'm going to get a new clustering and then six is repeat four and five until the change is small so each time I do step five I can keep track of how many points I've moved from one cluster to another or each time I do step four I can say how much have I moved the centroids each of those gives me a measure of how much change the new iteration is produced when I get to the point where the iterations are not making much of a change and we'll see what we might mean by that we stop and say okay we now have a good clustering so if we think of the complexity each iteration is order KNN where K is the number of clusters and N is the number of points and then we do that step for some number of iterations so if the number of iterations is small it will converge quite quickly and as we'll see typically for K means we don't need a lot of iterations to get an answer it's typically not proportional to n in particular which is very important all right Tuesday we'll go over the code for K means clustering and then uh do have some fun playing with counties and see what we can learn about where we live all right thanks a lot