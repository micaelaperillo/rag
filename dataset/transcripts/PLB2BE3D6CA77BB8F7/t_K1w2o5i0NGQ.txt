the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer high quality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu all right we'll back up and start again since I just turned on my microphone um I started with the observation that for most of recorded history people thought qualitatively not quantitatively they didn't know what statistics were they must have had some intuitive sense for example that if you're old you're more likely to have bad hearing than if you're young if you're old you're more likely maybe to die than if you're young things like that but they were just anecdotes and they had no careful way to go from statements about individuals to statements about populations or expectations this changed in about the middle of the 17th century changed fairly dramatically when a an Englishman named John Grant published something called the natural and political observations made upon the bills of mortal Al it this was the first work in recorded history that actually used statistics and what he did is he looked at the Fairly comprehensive statistics of when people died in the city of London and from that attempted to produce a model that could be used to predict the spread of the plague and turns out he was pretty good at it and that just changed the way people started to think and since that time people have used statistics both to inform and unfortunately to mislead uh some have willfully used statistics to mislead uh others have merely been incompetent and that gets me to what I want to talk about today which is uh statistics um this is often a attributed to Mark Twain but in fact he copied it from Benjamin Disraeli who said There are three kinds of lies lies damned lies and statistics and uh there is unfortunately a lot of Truth in that um more recently in the 50s Daryl Huff wrote a wonderful book I recommend it called how to lie with Statistics now here's a quote from the book if you can't prove what you want to prove demonstrate something else and pretend that they are the same thing in the days that follows the Collision of Statistics with a human mind hardly anyone will notice the difference alas that seems to be true um so what I want to do today is talk about a few ways in which one can be fooled into drawing inappropriate conclusions from statistical data now I trust that you will use this information only for good and it will make you a better consumer and purveyor of data rather than a better liar but that's up to you all right so let's start with the first way thing I want to talk about and it's important to always remember that no matter how good your stat statistics are statistical measures don't tell the whole story we've seen examples of this already earlier in the term there are an enormous set of Statistics that can be extracted from a data set and by carefully picking and choosing among them it's possible to convey almost any impression you want about the same data set the best antidote of course is to look at the data itself in 1973 uh the statistician John Anum published a paper with this set of data uh four different examples where he gave values for X and Y um nothing very sophisticated the interesting thing is that in many ways the statistics for these four data sets are very similar um they have the same mean uh the same median the same variance um for X and Y both the same correlation between X and Y and even if we use linear regression to get a fit we get very similar things so let's look at it I actually wrote some code to do that so this just reads in the data set that that we just had up and then uh plot some things with it and so let's look at it so what we can see is you have these in your handout we have four graphs four plots and I won't show you them all because they all are identical what they all give me is a mean of 7.5 and a whole bunch of things same to 17 decimal places or so the same median and the same linear fit uh y equal .5x + 3 so I might write a paper in which I would tell you that well from a statistical sense these data sets are all the same every statistical result I ran said these things are indistinguishable well is it true that these data sets are indistinguishable well one way to look at it is to actually plot the points so we can run it with true which says in addition to plotting the data plot put points on them in addition to plotting the statistics and now we see something fairly dramatic so for example figure two and figure one don't look a lot like each other when we actually look at the data figure three looks quite different and figure four is remarkably different compare figure four to say figure one again these are all in your handout so the moral is pretty simple here and it's one we looked at before which is don't ever ignore the data don't just look at statistics about the data try and find a way to look at the data itself of course it's easy enough to look at the data but how do you look at it and the next thing to remember is that pictures can be deceiving there can be no doubt about the utility of graphics for quickly conveying information however when used carelessly or maliciously a plot can be highly misleading so let's look go back to the PowerPoint and look at this plot of housing prices in the Midwest so we've got a plot here and we've got years 2006 2007 and then in 2008 n we have quarters you may remember that there was a event in the housing market in 2008 uh precipitating the global financial crisis and if we look at this what impression do we get about housing prices in the midwest during this period Well I would get the impression that they're remarkably stable you you publish this and you say okay look they really haven't changed very much maybe they've gone down a little but nothing very serious if we compare that to this plot which is exactly the same data and now I ask you about housing prices in the midwest well what you might tell is it's they're remarkably unstable and in fact it was clearly some sort of horrible event here exactly the same data two plots both of which are truthful but which give a radically different impression about what happened right the chart on the right was design designed on the right in your handout was designed to show that they were highly unstable so what's the difference what trick did I use to produce these two plots yeah so that was certainly one trick that I performed that in the first chart I plotted the y- AIS logarithmically which always make things look like they're changing less than if I plot it linearly and in this chart I use a linear plot go ahead scale the second plot so that the magnitude of the difference is much less compared to magnitude gra scale so that's the other thing I did is if you look at it I sort of cheated I had full years here and then I went to quarters so on part of my chart the resolution on the x is pretty wide as a whole year here and then part of it's on a quarter and not surprisingly since we know that housing prices change seasonally they're different in the spring than in the winter once I start plotting quarters even if there had not been a crash it would have looked much less stable in out years because I changed the resolution on the x-axis I didn't lie you can tell reading the legend that I did that but I sure could have fooled a lot of people with these charts here's another nice example of Statistics so this plots uh this is from a paper by uh two professors I think from Yale um shows what you can do if you're in the ivy league uh that plots initials against GPA for students at Yale and so you can see that if your first name starts with the letter A I think it was first initials uh your GPA is considerably higher than if it starts with C or D and if your parents weren't nice enough to give you an a name you could hope they at least gave you a b name and you certainly don't want them to give you a c or a d name so you know if you're Charlene or David you you could have a real problem I have to say my first child was named David his GPA might have been somewhere in there uh all right clearly it matters well what trick did I perform here right so what I did here is I made the range on the Y AIS very small ranging from 3.32 to 3 3.38 not a big difference however because that's the whole thing it looks like it's a big difference you'll often see this when you say look at things in newspapers where in fact someone has manipulated one of the axes to make things look big or small if I had ringed this from a GPA of zero to a GPA of four the highest GPA at Yale uh the this difference would have looked very tiny but I didn't actually it wasn't I I actually copied this from their paper this was the way they presented it in their paper because they were trying to argue in the paper that your name had a big influence on your life and they used many statistics including your grades and so they actually formatted it kind of like this to try to try and give you this impression uh later we'll see they another statistical sin they committed uh in this paper which um basically was designed to show that in your name was Destiny and uh it had many other things if you're a baseball player and your name starts with k you're more likely to strike out because K is the symbol for strikeouts in a baseball scorebook a lot of implausible things all right moving moving right along probably the most serious and common statistical error is the one known as garbage in garbage out and it's so common that people typically refer to it by its acronym Geo garbage in garbage out um a classic example of this occurred and we could look at more recent examples but I don't want to offend any of you uh in 1840 United States census showed that Insanity among free blacks and mados was roughly 10 times more common than Insanity among enslaved blacks or mados and the conclusion from this was obvious I guess uh US senator former vice president and later Secretary of State John C Calhoun concluded from the Census and I quote the data on sanity revealed in this census is unimpeachable from it our nation must conclude that the abolition of slavery would be to the African a curse because after all if you freed them from slavery they would all go insane um that's what the statistics reported he said now never mind it was soon clear that in fact that census was riddled with errors and John Quincy Adams a former vice president and Massachusetts resident uh responded to Calhoun and said no that's a ridiculous conclusion the sentence is full of Errors Calhoun being a very patient person explained to Adams the following there were so many errors that they balanced one another out and led to the same conclusion just as much as if they were all correct there were just enough errors that you could be okay well what was he relying on what should he have said if you wanted to make this statement more mathematically precise what he was basically implying is that the measurement errors are unbiased and independent of each other and therefore almost identically distributed on either side of the mean I see a typo might as well fix it might as well make it big enough to fix it that's interesting if he had made this much more precise statement then you could have had a meaningful discussion assuming it was possible to have a meaningful discussion with John Calhoun which is perhaps dubious uh about whether or not in fact the errors are independent because if they're not if for example they represent bias in the people compiling the data then you cannot rely upon on statistical methods to say that they'll balance each other out you remember way back in gauss's time gaus talked about this when he talked about the normal distribution and said well if we take these astronomical measurements and we assume our errors are independent and normally distributed then we can look at the mean and assume that that's close to the truth well those are important assumptions which have in this case turned out to be not correct uh and in fact it was later shown that the errors did not balance each other out nicely and in fact today you can say that no statistical conclusion could be drawn from that on the other hand recently the US national research Council perhaps the most prestigious academic organization in the United States published a ranking of all universities in the country and it was later shown that it was full of garbagey input and they did an extensive statistical analysis and published it on data that turned out to be just wrong and uh it was very embarrassing now the good news is MIT came out near the top of this this analysis the bad news is we can't conclude that it actually should have been near the top because who knows about the quality of the data but uh kind of embarrassing all right moving right along another very common way to lie with Statistics is to exploit What's called the C Hawk airgo propor Hawk fallacy so anyone here study Latin bunch of tech oh okay well what does it mean boy your Latin is good either that or you just no statistics but uh I have to say that was the most fluent translation I've had in all the years I've asked this question I hit the real a man real a woman on the throat all right yes with this therefore because of this um I don't know why but statisticians like Physicians and attorneys like to show off by phrasing things in Latin um so for example it is a statistical fact that college students including MIT students who regularly attend lectures have higher GPA students who attend lecture only sporadically so that would tell us that those of you in the room are likely to have a higher GPA than the various students in 600 who are not in this room um I hope it's true um now if you're a professor who gives these lectures what you want to believe it's because the lectures are so incredibly informative that we make the students who come much smarter and therefore they do better and so we'd like to assume causality because I give beautiful lectures and you choose to come you will get a better grade in 600 well yes there's a correlation it's unquestionably true but causation is hard to jump to for example maybe it's the point that students who bother to come to lecture also bother to do the problem sets and are just more conscientious and whether they came to lecture or not the fact that they're more conscientious would give them better gpas there's no way I know to separate those two things uh other than doing a controlled experiment right maybe kicking half of you out of lecture every day and just see how it goes but it's dangerous but again you know you can read things like The Faculty newsletter which will talk about how important it is coming it is to come to lecture because you'll do better because whoever wrote that article for the faculty newsletter didn't understand this this fallacy or was just thinking wishfully um another nice example one that was in the news not too long ago has to do with the flu um this was the cases of flu in New York state in recent years and you'll notice that there was a peak in 2009 and that was the famous swan flu epidemic which I'm sure you all remember now if you look at this carefully or even not too carefully you'll notice a correlation between when schools are in session and when the flu occurs that in fact during those months when schools are in session there are more cases of flu than in the months when school is not in session high schools colleges whatever quite a strong correlation in fact this led many to conclude that going to school is an important causitive factor in getting the flu and so maybe you shouldn't have come to lectures because you'll just have gotten the flu by doing so and in fact because of this you had many parents not sending their kids to school during the swine flu epidemic and in fact you had many schools closing in some communities because of the swine flu epidemic um well let's think about it just as you could use this correlation to conclude that going to school causes the swine flu you could have also used it to prove that the flu causes you to go to school because more people are in school when the flu season is at its height and therefore it's the growth of flu that causes people to go to school that's an equally valid statistical assumption from this data kind of a weird thing but it's true right just as we could conclude that having a high GPA causes people to come to lecture you look at your GPA every morning and if it's high enough you come to lecture otherwise you don't you could draw that conclusion from the data as well the issue here that you have to think about is whether or not there's what's called a lurking variable some other variable that's related to the other two and maybe that's the causitive one so for example a lurking variable here is that the school season coincides with or the non-school season maybe I should say coincides with the summer and in fact if you study the flu virus in a lab you will discover that it survives longer in cold weather than in hot and humid weather when it's cold and dry the flu virus will survive for a longer time on a surface than it will when it's warm and humid and so in fact maybe it's the weather not the presence of schools that causes the flu to be more virulent during certain times of the year in fact it's probably likely true so there is a lurking variable that we have to consider and maybe that's the causitive factor now this can actually lead to some really bad decisions in the world um I'm particularly interested in issues related to health care and public health um in 2002 roughly six million American women were taking hormone replacement therapy um in the belief that this would substantially lower their risk of cardiovascular disease it was argued that women over a certain age or of a certain age if you took extra hormones they were less likely to have a heart attack this belief was supported by several published studies in highly reputable journals in which they showed a strong correlation between being on hormone replacement therapy and not having cardiovascular disease and this data had been around a while and as I said by 2002 in the US roughly six million women were on this therapy later that year the Journal of the American Medical Society published an article asserting that in fact being on this therapy increased women's risk of cardiovascular disease it made you more likely to have a heart attack well how could this have happened after the new study came out people went back and reanalyzed the old study and discovered that the women in that study who'd been on hormone replacement therapy were more likely than the other women in the group to have also better diet and be on a better exercise regimen in fact they were women who were more healthc conscious so there were the lurking variables of diet and exercise and other things that were in fact probably the causitive factors in Better Health not the replacement therapy but there was this lurking variable that had not been discovered in the initial analysis of the data so what we saw is that taking hormone replacement therapy and improve cardiac health were coincident effects of a common cause that is to say being health conscious kind of a strange thing but true and sad story all right moving right along another thing to be cautious of is nonre response bias and related problem of a non-representative sample you'll probably recall that when I first started talking about statistics and the use of Randomness I said that all statistics iCal techniques are based upon the assumption that by sampling a subset of a population we can infer things about the population as a whole and that's true typically because if random sampling is used you can make assumptions that the distribution of results from the random sample if it's large enough will be the same as the distribution of results from the whole population and that's why we typically want to sample randomly and so for all the simulations we looked at we used random sampling to try and ensure that a some number of samples would give us something representative of the population and then we use statistical techniques to answer the question about how many random samples we needed but those techniques were only valid if the samples were indeed random otherwise you can analyze it to your heart's content and any conclusions you drawn are likely to be facius unfortunately many studies particularly in the social sciences are based on what has often called a convenience sampling so for example if you look at psychologic Psychology journals you'll find that many psychological studies use populations of undergraduates for their studies why do they do this is it because they believe that undergraduates are representative the population of as a whole no it's because they're captive they have to agree to participate right it's a convenience if you happen to be at a university to do your experiments on undergraduates um and so they do that and then they say well the undergraduates are just like the population as a whole um you may have observed that at least at this institution the undergraduates are probably not representative of the population as a whole um an well-known example of what you can do with this occurred during World War II whenever an Allied plane would return from a bombing run over Germany um the plane would be inspected to see where flak had hit it so the planes would fly over to drop bombs the Germans would shoot Flack at the planes to try and knock them out of the air they'd come back to England they'd get inspected they say well the Flack hit this part of the plane more often than that part of the plane on average and so they would reinforce the skin of those parts of the plane where they had expected the Flack to hit to try and make the plane less likely to be damaged in future runs or the planes in general what's wrong with this yeah exactly what they're not saying sampling is the planes that never made it back from the bombing r o that never made it back from the bombing runs because they weren't there to sample um and in fact maybe it's the case that what they were doing is reinforcing those parts of the planes where it didn't matter if you got hit by Flack because it wouldn't cause the plane to crash and not reinforcing those parts of the plane that were most vulnerable to being damaged by Flack they did a convenient sample they drew conclusions and they probably did exactly the wrong thing in what they chose to reinforce in the airplanes um this particular error is called nonresponse bias where you do some sort of survey for example and some people don't respond and therefore you ignore what they would have said um it's perhaps something we see when we do the underground guide to core six in fact I should point out that it's now online and it would be good if each of you would go and rate this course rate the lectures rate the Tas Etc uh we actually do read them and it makes a difference in how we teach the course in subsequent terms but there's clearly a bias you know maybe only the people who really feel strongly about the course either positively or negatively bother to fill out the survey and we draw the conclusions that there's a bodal distribution and nobody thinks it's kind of mediocre because they don't bother responding or maybe only the people who hate the course respond and we think everybody hates the course who knows it's a big problem um we see it it's a big problem today with telephone polls where you get more convenience sampling non-representative samples where a lot of polls are done using telephones by law these pollsters cannot call cell phones so they only call landlines how many of you have a landline let the record show nobody how many of your parents have a landline that the record show pretty much everybody well that means your parents are more likely to get sampled than you when there's a poll of say who's who should be nominated for president and so any of these polls that are based on telephones will be biased uh and unfortunately their poll may just say a telephone sample and people may not realize the implication of that that a whole part of the population is UND sampled um there are lots of examples of this all right um moving along um another problem we often see is data enhancement it's easy to read much more into Data than it actually applies implies especially when viewed out of context so on April 29th 2009 CNN reported that quote Mexican Health officials suspect that the swine flu outbreak has caused cus more than 159 deaths and roughly 2,500 illnesses it was pretty scary stuff at the time and people got all worried about the swine flu on the other hand how many deaths a year do you think are attributable to the conventional seasonal flu in the US anyone want to Hazard a guess 36,000 so 36,000 people a year on average will die from the seasonal flu which sort of puts in perspective that 159 deaths from the swine flu maybe shouldn't be so terrifying but again people typically did not report both of those uh another great statistic uh and accurate is that most auto accidents happen within 10 miles of home I'm sure many of you have heard that so what does that mean almost nothing most driving is done with 10 miles of within 10 miles of home and besides that what does home mean in this context what home means is the registration address of the car so if I were to choose to register my car in Alaska does that mean I'm less likely to have an accident driving around MIT I don't think so right again it's a kind of a meaningless um another aspect of this is people often extrapolate from data so we can look at an example of internet usage this is kind of a fun one too um so what I've plotted here is the internet usage in the United States as a percentage of population and I've plotted this from uh starting at 1994 and the green line or actually the the Blue Line are the points and the green line is a linear fit if you looked at my code you'd see I was using poly fit with uh one to get a line to fit it you can see it's a pretty darn good fit so people actually looked at these things and used this to extrapolate um internet usage going forward so we can do that now we'll run the same code with the extrapolation turned on and so figure one is the same same figure one as before same data same fit and here's figure two and you'll notice that as of last year about 115% of the US population was using the internet probably not true uh it may be possible in sports to give 110% but in statistics it's not um um again you see this all the time when people are doing these projections is they fit some data they extrapolate into the future without understanding why maybe that isn't a good thing to do we saw that by the way when we were modeling Springs right we could accurately project linearly until we exceeded the constant of elasticity at which point our linear model was totally broken so you always need to have some reason other than just fitting the data to believe that what you're doing makes actual sense all right the final one I want to talk about um is what is typically called in the literature the Texas Sharpshooter fallacy and this is a little bit tricky to understand sometimes um I say is there anyone here from Texas oh good so no one will be offended by this um well imagine that you're driving down some Country Road in Texas and that you see a barn and that Barn has six targets painted on it and in the dead center of each Target you find a bullet hole so you're driving you're pretty impressed and you stop and you see the owner of the barn you say You must be a damn good shot he says absolutely I never miss at which point the farmer's wife walks out and says that's right there ain't a man in the state of Texas who's more accurate with a paint gun what did he do he shot Six Bullets into the barn and he was a terrible shot they were all over the place then he went and painted a Target around each of them and it looked like he was a great shot now you might think that well that's silly no one would do that in practice but in fact it happens all of the time in practice a classic of this genre appeared in the magazine New Scientist in 2001 and it reported that a research team led by John Eagles of the Royal cornhill Hospital in Aberdine had discovered that and I quote anorexic women are most likely to have been born in the spring or early summer between March and June in fact there were more than there were more than 133% there were 133% more anorexics anorexics born on average in those months and 30% more anorexics than average in June now let's look at this worrisome statistic are any of you women here born in June all right well I won't ask about your health history um but maybe you should be worried or maybe not so let's look at how they did this study you may wonder why so many of these studies are FL studies about women's health and it perhaps because they're all done by male doctors um anyway the team studied 446 women who had been diagnosed as anorexic so if you divide that by 12 what you'll discover is that on average there should have been 37 women born in each of those months of the 446 and in fact in June there were 48 anorexic women born so they said well How likely is this to have occurred simply by chance well as I want to do in such occasions um I checked their analysis and I wrote a little piece of code to do that so trying to figure out what's the probability of 48 women being born in June I ran a simulation in which I simulated 446 births and chose a month at random and looked at the probability and let's see what it was when we run it oops well we didn't want these graphs but we'll get the probability of at least 48 births in June was 0.042 so in fact pretty low you might say well what's the odds of this happening by accident pretty small therefore maybe we really on to something maybe it has to do with you know the conditions of the birth in the weather or who knows what well what's wrong with this analysis well one way to look at it is this analysis would have been perfectly valid if the researchers had started with the hypothesis that there are more babies born in June than in any other month more anorex future anorexics born in June than any other month and then run this experiment to test it and validated it so if they had started with the hypothesis and then from the hypothesis conducted what's called a prospective study then they would have perhaps valid reason to believe that the study supports the hypothesis but that's not what they did instead what they did is they looked at the data and then chose a hypothesis that matched the data the Texas Sharpshooter fallacy given that that was the experiment they performed the right question to ask is not what is the probability that you had 48 future anorexics born in June but what was the probability that you had 48 funer future anorexics born in at least one of the 12 months because that's what they were really doing right so therefore we should really have run this simulation similar to the previous one and again these are on your handout but is there at least one month in which there were 48 births and if we run that we'll see that the probability is over 40% not so impressive as 4% so in fact we see that we probably shouldn't draw any conclusion the probability of this happening by pure accident is almost 50% so why should we believe that it's somehow meaningful again an example of the Texas Sharpshooter fallacy uh that appeared in the literature and a lot of people fell for it and uh if we had more time I would give you many more examples but we don't uh I'll see you on Thursday on Tuesday rather um two more lectures to go on Tuesday I'm going to go over some code that I'll be asking you to look at in preparation for the final exam and then on Thursday we'll uh we'll wrap things up e