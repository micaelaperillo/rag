the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu good morning oh it's so nice to get a response thank you I appreciate it uh I have a confession to make I stopped at my usual candy store this morning and they didn't have any so I am beri of anything other than crummy little tootsie rolls for today but I promise I'll have a new Supply by uh by Thursday all right we ended up the last lecture looking at pseudo code for K means clustering and talking a little bit about the whole idea and what it's doing so I want to start today by moving from the pseudo code to some real code uh this was on uh previous handout but it's also on today's handout so let's look at it um not so surprisingly I've chosen to call it K means and you'll notice that it's got some arguments uh the points to be clustered K and that's an interesting question unlike hierarchical clustering where we could run it and get what's called a dendogram and stop it any level and see what we liked K means involves knowing in the very beginning how many clusters we want um so I'm we'll talk a little bit about how we could choose k a cut off and with the cut off is doing you may recall that in the pseudo code K means was iterative and we keep reclustering until the change is small enough that we feel it's stable that is to say the new clusters are that are not that much different from the old clusters the cut off is the definition of what we mean by small enough we'll see how that gets used the type of point to be clustered the maximum number of iterations there's no guarantee that things will converge as we'll see they usually converge very quickly in a small number of iterations but it's prudent to have something like this in just in case things go Ary and to print just my usual trick of being able to print some debugging information if I need it but not getting buried in in output if if I don't all right so let's look at the code and it very much follows the outline of the pseudo code we started with last time we're going to start by choosing K initial centroid at random so I'm just going to go and take all the points I have and I'm assuming by the way I should have written this down probably that I have at least K points otherwise it doesn't make much sense if you have 10 points you're not going to find 100 clusters um so I'll take K random centroids and those will be my initial centroids um there are more sophisticated ways of choosing centroids as discussed in the problem set but most of the time people just choose them at random because at least it if you do it repetitively it guarantees against some sort of systematic error whoa what happened I see come back thank you all right then I'm going to say that the Clusters I have initially empty and then I'm going to create a bunch of Singleton clusters one for each centroid so all of this is just the initialization getting things going I haven't had any iterations yet and the biggest change so far I'm just setting arbitrarily to the cut off all right and now I'm going to iterate until the change is smaller than the cut off while biggest change is at least the cut off and just in case num itas is less than the maximum I'm going to create a list containing K empty list so these are the new clusters and then I'm going to go through for I in range K I'm going to append the empty cluster these are going to be the new ones and then for p in all the points I'm going to find the centroid in the existing clustering that's closest to p that's what's going on here once I found that I'm going to add P to the correct cluster go and do it for the next point then when I'm done I'm going to compare the new clustering to the old clustering and get the biggest change and then go back and do it again all right people understand that basic structure and even some of the details of the code it's not very complicated uh but and if you haven't seen it before it can be a little bit tricky when I'm done I'm going to just get some statistics here um about the Clusters uh I'm going to keep track of the number of iterations and the maximum diameter of a cluster so the cluster in which things are least tightly grouped and this will give me an indication of how good a clustering I have okay does that make sense to everybody any questions about the K means code well before we use it let's look at how we use it so I've written this function test one one that uses it um some arbitrary values for K and the cut off number of Trials is kind of boring here I've only said one as a default and I've sent PR steps to false the thing I want you to notice here because I'm choosing the initial clustering at random I can get different results each time I run this because of that I might want to run it many times and choose the quote Best clustering what metric am I using for best clustering it's a minmax metric I'm choosing the minimum of the maximum diameters so I'm finding the worst cluster and trying to make that as good as I can make it you could look at the average cluster this is like the linkage distances we talked about before so that's the normal kind of thing it's like when we did Monty Carlos simulations a random walks flipping coins you do a lot of Trials and then you can either average over the trials which wouldn't make sense for the clustering or select the trial that has some property you like this is the way people usually use K means typically they may do a 100 trials and choose the best the one that gives them the best clustering so let's look at this um and let's try it for a couple of examples here let's uh start it up and we'll just run test one on our old mammal teeth database and we get some clustering now I'll run it again um we get a clustering I don't know is it the same clustering kind of looks like it is is no reason to suspect it would be um we run it again well you know this is very unfortunate it's supposed to give different answers here because it often does I think they're the same answers though aren't they yes anyone see a different now they're the same well how unlucky can you be every time I ran it at my desk it came up the first two times with different things but take my word for it and we'll see that with other examples it could come out with different answers let's try it with uh some printing on so we get some things here and let's try it what do we got out of this one all right oh well sometimes you get unlucky and sometimes you get unlucky with Randomness all right um so why did we start with k means not because we needed it for the mammals teeth uh the hierarchical worked fine but because it was too slow when we tried to look at something big like the counties so now let's move on and talk about clustering the counties um we'll use exactly the K means code it's one of the reasons we're allowed to pass in the point type as an argument but the interesting thing will be what we do for the counties themselves so this gets a little complicated uh in particular what I've added to the counties is this notion of a filter the reason I've done this is as we've seen before the choice of features can make a big difference in what clustering you get I didn't want to do a lot of typing as we do in through these examples so what I did is I created a bunch of filters for example no wealth which says all right we're not going to look at home value we're giving that a weight of zero uh we're giving income a weight of zero we're giving property Val poverty level a rate of zero but we're giving the population weight of one Etc okay so what we see here is each Filter Supplies a weight in this case either zero or one to a feature this will allow me as we go forward to run some experiments with different features all features everything has a weight of one oh I made a mistake though that should have been a one then I have filter names which I'm just add AR and that'll make it easy for me to run various kinds of tests with different filters then I've got a knit which takes as its arguments the things you would expect plus the filter name so it takes the original attributes the normalized attributes and you will recall that why do we need to normalize attributes if we don't we have something like population which could number in the millions and we're comparing it to percent female which we know cannot be more than a 100 and so the small values become totally dominated by the big absolute values and when we run any clustering it ends up only looking at population or number of farm Acres or something that's big has a big dynamic range you know Manhattan has no Farm acres some County in Iowa has a lot maybe they're identical in every other respect unlikely but who knows uh except I guess there's no baseball teams in Iowa but at any rate uh we always scale or we try and normalize so that we don't get fooled by that um then I go through and if I haven't already this is a class variable attribute filter which is initially set To None not an instance variable but a class variable and what we see here is if that class variable is still none this will mean it's the first time we've generated an in a point of type County then what we're going to do is set up the filter to only look at the attributes we care about so only the attributes which have a value of one and then I'm going to override distance from class point to uh look at the features we care about okay does this basic I structure and idea make sense to people it should I hope it does because the current problem set requires you to understand it uh in which you all be doing some experiments so now I want to do some experiments with it I'm not going to spend too much time even though it would be fun uh because I don't want to deprive you of the fun of doing your your problem set so let's look at an example um so I've got test which is pretty much like test one um runs K means number of times and chooses the best and we can start well let's start by running some examples ourselves so I'm going to start by clustering on education level only so so with I'm going to get 20 clusters 20 chosen just so it wouldn't take too long to run and we'll filter on education and we'll see what we get well I should have probably done more than one cluster just to make it uh work but we've got it and I just for fun I'm keeping track of what cluster middle sex County the county in which MIT is shows up um so we can see that it's similar to a bunch of other counties and it happens have an average income of $ 28,650 or at least it did then and if we look we should also see oh let me go back I foolishly didn't uncomment pylab doow so we better go back and do that well we're just going to Nuke it and run it again because it's easy and I wanted to run it with a couple of Trials anyway so we'll first do the clustering we'll get cluster zero now we're getting a second one it's going to choose whichever is the tightest and we'll see that that's what it looks like so we've now clustered the counties based on education level no other features and we see that uh it's got some interesting properties uh there's uh a small number of counties clusters uh out here near the the right side with high income um and in fact we'll see that we are fortunate to be in that cluster one of the uh clusters that contains wealthy counties and you could look at it and see whether you recognize any of the other counties that uh hang out with U Middle sex um things like a Marin County San Francisco county not surprisingly right remember we're clustering by education and these might be counties where you would expect the level of Education to be comparable to the level of Education uh in Middle sex all right um me get rid of that for now sure um I ran it I don't want didn't want you to have to sit through it but I ran it on a much bigger sample size so here's what I got when I uh ran it asking for 100 clusters and uh I think it was five trials and you'll notice that this case actually we have a much smaller cluster containing middle sex not surprising because I've done a 100 rather than 20 and it should be pretty tight since I chose the best and you can see we have this interesting we have a distribution here now remember that the name of the game here is we're trying to see whether we can infer something interesting by clustering unsupervised learning so one of the questions we should ask is how different is what we're getting here from if we chose something at random now remember we did not cluster on things based on income I happened to plot income here just because I was curious as to how this clustering related to income suppose we had just chosen at random and split the counties at random into a 100 different clusters what would you have expected this kind of graph to look like do we have something that is different obviously different from what we might have gotten if we just done a random division into a 100 different clusters think about it what would you get pardon well a bell curve is a good guess because bell curves occur a lot in nature um and as I said I apologize for the rather miserable quality of the rewards um it's a good guess but I think it's the wrong guess what might what would you expect would you expect the different clusters yeah go ahead uh very sharp bell curve was was one comment um well someone else want to try it that's kind of close I thought you were on the right track in the beginning well take a different example let's take uh students if I were to select a 100 MIT students at random and compute their GPA would you expect it to be radically different from the GPA of all of MIT the average GPA of all MIT students probably not right so if I take 100 counties and put them into a cluster the average income of that cluster is probably pretty close to the average income in the country and so you'd actually expect it to be kind of flat right that each of the randomly chosen clusters would have the same income more or less well that's clearly not what we have here so we can clearly infer from the fact that this is not flat is that there is some interesting correlation between level of income and let and education and for those of us who earn our living education we're glad to see it's positive actually not negative uh as another experiment just for fun I clustered by gender only so this looked only at the female male ratio in the counties and uh here you'll see with middle Sexes with remember we had about 3,000 counties to start with so the fact that there were so few in the cluster on education was interesting right here we have more uh and we get a very different looking picture which says perhaps that the female male ratio is not unrelated to income but it's a rather different relation than we get from uh education this is what would be called a bodal distribution a lot here and a lot here and not much in the middle but again the dynamic range is is pretty is much smaller but we do have some counties where the income is pretty miserable all right um we could play a lot more with this but I'm not going to um I do want to before we leave it because we're about to leave machine learning reiterate a few of the major points that I wanted to make sure were sort of the take-home messages so we talked about supervised learning much less than we talked about unsupervised interestingly because unsupervised learning is probably used more often in The Sciences than supervised and when we did supervised learning we started with a training set that had labels each point had a label and then we tried to infer the relationships between the features of the points and the associated labels between the features and the labels we then looked at unsupervised learning and the issue here was our training set was all unlabeled data and what we try and infer is relationships among points so rather than trying to understand how the features relate to the labels we're just trying to understand how the points or actually the features related to the points relate to one another both of these as I said earlier are similar to what we saw when we did regression where we tried to fit curves to data um you need to be careful and wary of overfitting just as would you did did with regression in particular if the training data is small a small set of training data you may learn things that are true of the training data that are not true of the data on which you will subsequently run the algorithm to test it so you need to be wary of that um another important lesson is that features matter which features matter uh it matters whether they're normalized and in some cases you can even wait them if you want to make some features more important than the others features need to be relevant to the kind of knowledge that you hope to acquire so for example when I was trying to look at the eating habits of mammals I chose features based upon teeth not features based upon how much hair they had or their color or their lengths of the tals I chose something that I had domain knowledge which would suggest that it's probably relevant to the plan at hand question at hand um and then we discovered it was just as here I said well maybe education has something to do with income we ran it and we discovered thank goodness that it does okay so I've probably told you 10 times that features matter if not I should have because they do and it's probably the most important thing to get right in doing machine learning now our forray into machine learning is part of a much larger unit in fact the largest unit of the course really is about how to use computation to make sense of the kind of information one Encounters in the world a big part of this is finding useful ways to abstract from from the situation you're initially confronted with to create a model about which one can reason so we saw that when we did curve fitting we would abstract from the points to a curve to get a model and we see that with machine learning that we abstract from every detail about a county to say the education level to give us a model of the counties that might be useful I now want to talk about another kind of way to build models that's a as popular way as there is probably the most common kinds of models and those models are graph theoretic and there's a whole Rich theory about graph and graph theory that are used to understand these models so suppose for example you had a list of all the airline flights between every city in the United States and what each flight cost suppose also counter counterfactual supposition that for all cities a b and c the cost of flying from a to c by way of B was the cost of a to B and the cost from B to C we happen to know that's not true but we can pretend it is so what are some of the questions you might ask if I gave you all that data and in fact there's a company called ITA software in Cambridge recently acquired by Google for I think $700 million that is built upon answering these kinds of questions about these kinds of graphs so you could ask for example what's the shortest number of hops between two cities if I want to fly from here to Juno Alaska what's the fewest number of stops I could ask what's the least expensive different question flights from here to Juno um I could ask what's the least expensive way involving no more than two stops just in case I don't want to stop too many places I could say I have 10 cities what's the least expensive way to visit each of them on my vacation all of these problems are nicely formalized as graphs a graph is a set of nodes think of those as objects uh nodes are also often called vertices or ver a Vertex for one of them and those nodes are a Connect Ed by a set of edges often called arcs if the edges are unidirectional the equivalent of a one-way street it's called a diaph or directed graph graphs are typically used in situations in which there are interesting relationships among the parts uh the first documented use of this kind of a graph was in 1735 when the Swiss mathematician Leonard Oiler uh used what's we now called graph Theory to formulate and solve the kbur bridges problem so this is a map of hburg which was then the capital of East Prussia a part of what's today Germany and it was built at the intersection of two rivers and contained a lot of islands the islands were connected each other into the mainland by Seven Bridges for some bizarre reason Which history does not record and I cannot even imagine the residents of this city were obsessed with the question of whether it was possible to take a walk through the city that invit involved Crossing each Bridge exactly once could you somehow take a walk and go over each Bridge exactly once I don't know why they cared they seem to care they debated it they walked around they did things um probably would be unfair for me to ask you to look at this map and answer the question but it's kind of complicated um oil's great Insight was that you didn't have to actually look at the level of detail represented by this map to answer the question you could vastly simplify it and what he said is well let's represent each land mass by a point and each Bridge as a line so in fact his map of kingburg looked like that considerably simpler this is a graph we have some vertices and some edges he saidwell we can just look at this problem and now ask the question once he'd formulated the problem this way it became a lot simpler to think about and he reasoned as follows if a walk were to to Traverse each Bridge exactly once it must be the case that each node except for the first and the last in the walk must have an even number of edges so if you were to go to an island and leave the island and Traverse each bridge to the island unless there were an even number you couldn't Traverse each one exactly once right if there were only one Bridge once you got to the island you were stuck if there were two Bridges you could get there and leave but if there were three Bridges you could get there leave get there and you're St again he then looked at and said well none of these edges has an even none of these nodes have an even number of edges therefore you can't do it end of story stop arguing um kind of a nice piece of logic uh and then Oiler later went on to generalize this theorem to uh cover a lot of other situations but but what was important was not the fact that he solved this problem but that uh he thought about the notion of taking a map and formulating it as a graph and this was the first example of that and since then everything has worked that way so if you take this kind of idea and now you extend it to diagraphs you can deal with one way Bridges or oneway streets or suppose you want to look at our Airline problem you can extend it to include weights for example the number of miles between two cities or the amount of toll you'd have to pay on some Road and so for example once you've done this you can easily represent the entire US highway system or any road map by a weighted directed graph more used more often probably the worldwide web is today typically modeled as a directed graph where there's an edge from page a to page B if there's a link on page a to page B and then maybe if you want to care ask the question how often do people go from A to B A very important question say to somebody like Google who wants to know how often people click on a link to get to another place so they can charge for those clicks uh you use a weighted graph which says how often does someone go from here to there and so so a company like Google maintains a model of what happens and uses a weighted directed graph to essentially represent the web the clicks and everything else and can do all sorts of analysis on traffic patterns and things like that there are also many less obvious use of graphs biologists use graphs to measure things ranging from the way proteins interact with each other to more obviously Gene expression networks are clearly graphs physicists use graphs to model phase transitions with typically the weight of the edge representing the amount of energy needed to go to from one phase to another so those are again weighted directed graphs the weight the direction is can you get from this phase to that phase and the weight is how much energy does it require uh epidemiologists use graphs to model diseases Etc and we'll see an example of that uh in a bit all right um let's look at some code now to implement graphs this is also on your handout and I'm going to comment this out just so we don't run it by accident so as you might expect I'm going to use classes to implement graphs so I start with a class node uh which is at this point pretty simple it's just got a name now you might say well why did I even bother introducing a class here why don't I just use strings well because I was kind of wary that sometime later I might want to associate more properties with nodes so you can imagine if I'm using a graph to model the worldwide web I'll want more than the URL for a page you know I might want to have all the words on the page or all who knows what else about it so I just said for safety let's start with a simple note class but let's make it a class so that any code I write can be reused if at some later date I decide nodes are going to be more complicated than just strings good programming practice an edge is only a little bit more complicated uh it's got a source a destination and a weight so you can see that I'm using the most general form of an edge so that I will be able to use edges not only for graphs and digraphs but also weighted directed graphs by having all the potential properties I might need and then some simple things to fetch things the next cluster in the hierarch is a diaph so it's got an init of course I can add nodes to it uh I'm not going to allow myself to add the same node more than once I can add edges edges and I'm going to check to make sure that I'm only connecting nodes that are in the graph then I've got children of which gives me all the descendants of of a node and uh has node and string and then interestingly enough maybe surprising to some of you I've made graph a subass of diagraph now maybe that seems a little odd after all when I started I started talking about diagraphs about graphs and then said and we can add this feature but now I'm kind of going the other way around um why is that why do you think that's the right way to structure the hierarchy what's the relation of graphs to diagraphs diagraphs are more General than graphs a graph is a specialization of a diaph just like a county is a specialization of a point so typically as you design these class hierarchies the more special I something is the further it has to be down more of like a subclass that makes sense I can't really turn this on its head I I can remove I can specialize a diagraph to get a graph I can't specialize a graph to get a diagraph and that's why this hierarchy is organized the way it is um what else is there interesting to say about this um a key question probably the most important question in designing and implementation of graphs is the choice of data structures to represent the diaph in this case there are two possibilities that people typically use they can use an adjacency Matrix so if you have n nodes you have an N byn Matrix where each entry gives in the case of a diaph a weighted diaph the weight connecting the nodes on that edge or in the case of a graph it can be just true or false so this is can I get from A to B is that going to be sufficient suppose you have a graph that looks like this and we'll call that Boston and this New York and I want to model the roads well I might have a road that looks looks like this and a road that looks like this from Boston to New York as in I might have more than one road so I have to be careful when I use an adjacency Matrix representation to realize that each element of the Matrix could itself be somewhat more complicated in the case that there are multiple edges connecting two nodes and in fact in many graphs we will see there are multiple edges connecting the same two nodes it would be surprising if there weren't all right now the other common representation is an adjacency list in an adjacency list for every node I list all of the edges emanating from that node which of these is better well neither it depends upon your application an adjacency Matrix is often the best choice when the connections are dense everything is connected to everything else but is very wasteful if the connections are sparse if there are no Roads connecting most of your cities or no airplane flights connecting most of your cities then you don't want to have a matrix where most of the entries are empty just to make sure that people follow the difference which am I using in my implementation here am I using an adjacency Matrix or an adjacency list adjac I heard somebody say an adjacency list because my candy Supply is so meager they didn't even bother raising their hand so I know who said it but yes it is an adjacency list and we can see that by looking what happens when we add an edge I'm associating it with the source node so from each node and we can see that when we look at the children of here I just returned the edges of that node and that's the list of all the places you can get to from that node all right so it's very simple but it's very useful um next lecture we'll look yeah thank you question I love questions di yeah what makes the more specialized what makes the gra more specialized because good question so the question is what makes the graph more specialized what we'll see here when we look at graph it's not a very efficient implement ation but every time you add an edge I add an edge in the reverse Direction because you can if you can get from node a to node B you can get from node B to node a so I've removed the possibility that you have say one-way streaks in the graph and therefore it's a specialization there are things I can not do with graphs that I can do with digraphs but anything I can do with a graph I can do with a diaph it's more General does that make sense um so that that's a great question and and I'm glad you asked it all right Thursday we're going to look at a bunch of classic problems that can be solved using graphs and uh I think that should be fun