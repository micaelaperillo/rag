the following content is provided under a Creative Commons license your support will help MIT open courseware continue to offer highquality educational resources for free to make a donation or view additional materials from hundreds of MIT courses visit MIT opencourseware at ocw.mit.edu on the uh number two 23 yeah says if the Cod one the value to would be the yeah it was a you were generating random values for that you you were but if you look at um what totes zero is collecting or so if you look at the um where it draws a random number J is you know indexing in totes right so when J is zero your stand deviations which is also being indexed by J is going to be zero yeah so you're always going to get the same value any questions oh okay that was easy um so in lecture we were talking a lot about uh or clustering or we've been talking about clustering the past was it two lectures and we had two different types of clustering methods what were they Hier hierarchical and C Mees so um can someone give me a rundown of like what the steps are in hierarchical clustering so breaks everything down into one so let's say I have a bunch of data points what would be the first step so we're going to first assign each point to a cluster right so each point gets its own cluster and then the next step would be what right so you're going to find the two clusters that are closest to each other and merge them so In This Very contrived example would be these guys and then you're going to keep doing that until you get to a certain number of clusters right so you merge these two then you might merge these two then you might merge these two etc etc right so so you're going to you're going to going to set the number of clusters that you want at the outset so for uh I guess for the mamillion teeth example there was a stopping criteria of two clusters if I'm not mistaken yeah so if we look so let's take a look at the code here that implements the hierarchical clustering so this is just some infrastructure structure code it builds up the number of points uh we have a cluster set class which we'll go over in a second and uh then we just add for each point we're going to create a cluster object and add it to the cluster set so let's take a look at the cluster set cluster set is has one attribute the members attribute and it just has a set of points or a set of clusters actually and the key method in here or the key methods are merge one and merge n merge n is what actually implements the clustering here so what you you give it the distance metric that you're going to use for your points the number of clusters that you want at the end of your clustering uh some a history tracker and then you also tell it if you want to print out some debugging information so which apparently is not used in this method oh no it is merge one um anyway so while we have more clusters than the number of clusters we desire we're going to keep iterating and on each step we're going to call this function or method called merge one and just pass it the distance metric and all merge one is going to do is it's going to if there's only one um if there's only one cluster here then it's just going to return none if there are uh two clusters and it's going to merge them and uh if there are more than two clusters it's going to find the closest according to the distance metric and then merge those two so and then the return value is going to be the two clusters that it merged so let's look at the merge clusters code all it does is it takes the two clusters and for each point in both clusters it adds it to a new list points and it creates a new cluster from those points and then it removes two clusters from members and adds the new newly created cluster um so then the fine cluster or find closest method so what's this bit of code doing here right so and we'll get to the metric in a second so it initially finds it looks at the first two uh uh members in the cluster set and it sets M distance to be that and it sets two merge to be those two members and then it arates through every possible pair of clusters in this cluster set and finds the minimum distance according to the metric so um let's see so let's look at the cluster class all the cluster object does or class does is it holds a set of points it knows the type of point that it's holding because that come becomes important when we talk about the different types of things that we want to Cluster and then it also has something called a centroid all a centroid is is just the middle of the cluster right if you take all of the uh points and you average their uh average their distances or average their location so these different functions these just compute uh metrics about about this particular cluster right so single linkage dist all this is going to do is it finds the minimum uh distance between every pair of points in the cluster and what does Max Max linkage distance do I'm sorry I'm mistaken it finds the minimum distance between this uh a point in this cluster and a point in another cluster I misspoke so what does Max Lage just do the opposite yeah the opposite I have to keep you talking on a bunch of sleep and then average linkage dist same thing this is why having meaning meaningful function names is important because it helps you explain code so it also has this method in here called update and what update does is it takes a new set of points and it sets the uh points that this cluster has to be these new points and then it computes a new centroid for this cluster and the return value is the distance of the old centroid from the new centroid and this becomes important in some of the algorithms all right and then there's just some bookkeeping stuff here like members will just give you all the points in this cluster you all know what yield does right okay so yield returns a generator object which allows you to iterate over elements so this was asked during the quiz review what's the difference between range and X range right so if I have a range of value is it's going to it actually returns a different type so like I can print out this list right in this case it'll print out the type of object it is so this is this is accomplish using yield so if I wanted to write this myself what this is going to do is it's going to return something called a generator object and all it does is instead of holding all the numbers in memory it's going to return them one at a time to me so like when I use range here it constructs a list and it has all of those integers in memory if I use x range it's not going to hold all the integers in memory but I can still iterate overi them one at a time within that function call yield a bunch of times the function finishes right yeah so is that how is that accomplished do it sort of operate different the way you think of functions run right so what this tells python is that when it sees a yield it's sort of like a return except it's telling python that I want to come back to this location at some point so it um a return just take returns out of the function completely what a yield does is it takes the value that it's specified after yield and it returns to the calling uh place in memory in in in the program and that value but then when uh it comes time to get a new value it'll uh return back to where this yield exited so kind of a way of maybe seeing this is if I iterate over my X range each time it needs a new value it's going to go back inside this function and grab it so it's it's it's it looks like a function but what it's actually doing is uh creating a what it's called a generator object and it has these special methods before getting the next value so it's some nice syntactic sugar but it's pretty neat but that's what's going on with this yield statement here is that instead of returning the entire list of points or instead of um uh uh doing it in some other way all it's doing is just yielding each point one at a time so that you can iterate over them all right um um so what else so here's here's a method for computing the centroid um all we are going to do is total up where each point is and then take the average over all the points does that make sense all right so the example we saw was mammal teeth and the way that that's accomplished in this set of code is we're going to define a a subass of a class Point that's called mammal and what point does is it has a name for a given data point it has a set of attributes and then you can also give it some normalized attributes if you don't give it the normalized attributes uh then it'll just use the original attributes so it becomes important when we talk about when we do scaling of our data which we'll do in a in a shortly so there's nothing really special about it uh except for this distance function right it's just defining the ukian distance for a given multi-dimensional point so everyone knows that if you have a point in two dimensions then it's just you know if it if it's an XY point then it's just you know X2 + y^2 square root of it generalizes to higher Dimensions if you weren't already aware so like if I want to find the straight line distance between a point in 3D is just going to be x^2 + y^2 + z^ 2 sare root that's all and then so on and so forth okay um so all this does is it subclasses point and it has this function scale features and what scale features does is it'll take a key and in this case we've to find two ways of scaling this data scaling these points we have the identity which is it's just going to leave every Point alone and then we have this one over Max which is going to scale each attribute by the maximum value uh in this data set and if we look at the data set we have we know that our max value is six right so you could do that automatically but in this case uh we're just we're using U prior knowledge of the data set that we have so so when don't we do a clustering so this is going to do a higher hierarchical clustering right and what we're going to ask if I just specify the default parameters all it's going to do is go look for two clusters it's going to use the identity and it's just going to print out the history like when it's performed the different merges now unless I have extraneous code that I'm already running so what starts off first is we get a lot of merges with just these single element clusters right so have a beaver with a groundhog so I guess they're pretty similar in terms of teeth we have a squirrel with a porcupine wolf with a bear and So eventually though we start finding clusters so like a wolf and a bear I guess are more similar but they're also similar with a dog so we're going to start merging multi-point clusters um so we start seeing like the beaver and groundhog cluster is going to get merged with the squirrel and porcupine cluster so if you were to visualize this so like the reason why it's called hierarchical clustering is what which ones did I say beaver groundhog these guys have been merged into their into a cluster right they started out as their own cluster they've merge into their own into their own cluster and then the gray squirrel and the porcupine same thing they started off in their own clusters at the beginning they got merged and now what this step is saying is that these two clusters get merged so we're building this tree or hierarchy that's where the hierarchical comes from so we actually we use um hierarchical clustering a lot uh in other fields so um like in speech recognition we can do a hierarchical clustering of the speech sounds so if I have um say different vows and maybe a couple of consonants I would expect to see say these kind of clustered together first and so what I might see is uh these would be like um fricatives but then I might have some stops like te and B that get merged first so like it's it's a way of kind of making these generalized groupings at different levels so um uh does anyone have any real questions about ocal clustering so should I move on to K means all right so what's the general idea with K means so I start off with a set of data points what's my first step choose your number right so I'm going to choose a k so let's say for giggles this is we're going to choose k equal 3 and then what's my next step K so we're going to pick K random points from our data set all right and then what do we do then you cluster yeah all right so after we've chosen our three centroids here these become our clusters right and we're going to look at each point and we're going to figure out which which cluster they're closest to so in this case this is going to be a pretty easy clustering so all these points are going to belong here all these points are going to belong here all these points are going to belong here right and then we're going to update our centroid for each of these clusters and there's going to be a distance that the centroid moves each time we update it so in this case the centroid moved quite a bit right then we're going to find the maximum distance that the centroid moved and if it's below a certain cutoff value then we're going to say I've got a good enough clustering if it's above a certain uh cutoff value then what I'm going to say is like this centroid moved quite a bit for this cluster right so I'm going to try another iteration I'm going to say for each one of these points I'm now going to look try and find the closest the cluster that closest cluster that it belongs to based on this new these new centroids and in this case nothing's really going to change so all of the delas all of the centroids are going to stay the same so it's going to be below the cutoff value and it's going to stop so what's an advantage of K means over hierarchical clustering yeah so let's say that I have a million points if I were to hierarchically Cluster these that means that I'd start off with a million clusters and then on each iteration I'm just going to reduce it by one I don't know say three Okay so on each iteration we're just reducing it by one which if that's all we were doing would not be so hard it doesn't take too long to countd down from a million on a computer but on each one of these steps we have to compute the pawise distance between every cluster so it's going to be n * n minus one comparisons on each step which in this first case works out to a lot and it doesn't get much better so approximately right and it doesn't get much better as we go down with K means what happens is we just have to perform on each step if we have a million points and we have K clusters we just have to perform K * 1 million comparisons right because for each point we need to find the closest centroid approximately so the upshot is that K means winds up being a lot more efficient on each iteration which is why if you have a large number of points might want to choose K means over hierarchical clustering okay um what's an advantage though of hierarchical clustering over K means even though it's less efficient what's another thorough what's that more thorough more thorough you can get a lot of different levels you yeah you get a lot of different levels so you can look at the the clusterings at uh different from different perspectives um but the kind of key thing with you don't necessarily know how many clusters actually are yeah hierarchal cluster um just go down Tre right so you could go down different levels of the tree and fig pick whatever however many number of clusters you want but the the big reason that uh or the one of the kind of main advantages that Hier clustering has over K means is that K means is r it's non- deterministic hierarchical clustering is deterministic it's always going to give you the same result with K means because your initial starting conditions are random because you're choosing K random points the end result will be different each time and so uh when we do K means clustering uh this necessarily means that we don't necessarily want to do it once like if we choose k equal 3 we might want to do you know five different K means clusterings and take the best one right um so that's kind of one of the big points with K means uh there's a degenerate condition with K means so if my objective is is to if my stopping criteria is that the centroid doesn't move what's a really easy way to make the CID not move by choosing K wasn't k equals n right so if I have K clusters or if I have n points and I have k equal n then all of my points are going to be their own cluster and every time I update I'm never going to move my centroid this also so in your problem set you're going to be asked to compute a standard error for each of the Clusters or and a total error for the entire clustering so what that is is if I have let's see if I remember this if I have the centroids and I have each point in I don't know the centroids so what I'm going to do is I'm going to take the centroid for each cluster and I'm going to find the distance from each point in the cluster to the centroid and then I'm going to sum up all of those distances over the entire cluster that's going to give me my error not sure if that's equation is totally right but that's that's you know there might be like a division in there but the general idea is that what I'm trying to emphasize is that we can reduce this number by just increasing the number of K and if we make k equal n this is going to be zero so like I was saying with Statistics you never want to trust just one number with K means you never just want to trust one clustering or one measurement of error you want to kind of look at it from mulp perspectives Advantage points so why don't we look at the code and try and match up all of that stuff with what you'll see on your palm set so the big function to look at for K means is apply named K means and it's going to take a set of points it's going to take a number of clusters it's going to take a cutoff value and a point type and a variable named Max ERS so first step we get our initial centroids all we're going to do is we're going to sample our points randomly and choose K of them our clusters we're going to represent as a list and we are going to for every uh for all the points in the the initial uh centroids we are going to add a cluster with just that point and then we get into into our uh biggest into our Loop here so what this is saying is while our biggest change in centroid is greater than the cut off right and we haven't exceeded the number of iterations or maximum number of iterations we're going to keep trying to refine our clustering right so that brings up a point I actually failed to mention why should we have this cutoff Point Max ERS go forever yeah there's a chance that if our cut off value is too small um or we have a point that's like on a border and likes to jump between uh clusters and move the centroid just above the the the the cutoff point that will keep will never converge to our cut off and so we want to set up a secondary break so we have this Max Max it which defaults to 100 um so with with this setup though there's a couple of things you have to consider and that is one you need to make sure that your max it is not too small because if it's too small you're not going to converge and you don't want to make it too large because then your algorithm will just take forever to run right likewise you don't want to make your cut off too small either so sometimes you have to play around with the algorithm to figure out what the best parameters are and that's often times more of an art than a hard science so anyway continuing on for each iteration we're going to set up a new set of clusters and we're going to set them initially to have no point in them and then for all the points in our data set we are going to look for the smallest distance so that means we are going to look we're going to initially set our smallest distance to be the distance from the point to the first centroid and then we're just going to iterate through all the centroids or through all the Clusters and then find the smallest distance is anyone lost by that make sense once we find that we're just going to add that point to the new set the set the new clusters right and then we're going to go through our update we're going to iterate through each of our clusters we are going to update the points in the cluster so remember the update method sets the points of the cluster to be the uh this new set of points you've given it and it also updates the centroid or it it updates the centroid and it Returns the Delta between the old centroid and the new centroid so that's where this change variable is coming from and then we're just going to look for the biggest change right and if at some point in our clustering these centroids have stabilized and our clusters are relatively uh stationary then our cut off then our change will be our Max change will be small and we'll wind up terminating the algorithm [Music] and all this function does is once it's converged or it's gone through the maximum number of iterations it's just going to find the maximum distance uh of a point to its centroid so it's going to look for uh it's going to look for a a point that has a maximum distance from a its uh corresponding centroid and that's going to be the coherence of this clustering and then it's just going to return a tupal containing the Clusters and the maximum distance so it's a it's not a hard algorithm to understand and it's pretty simple to implement there any real questions about what's going on here all right so the example that he went over and lecture was with K Mees was this counties uh clustering example so we had a bunch of data uh of different counties in the us and we just you know played around with clustering them and seeing what we got so if we you know made five clusters and we clustered on all the features wanted to see kind of what the distribution would be uh for say incomes what this function test is going to do is it's going to take a k a cut off and a number of Trials so remember we said that because K Mees is non-deterministic we're going to want to run it a number of times to find to you know maybe we get a bad initial set of points for for for for centroids or for our clusters um and that gives us a bad clustering so we're going to run it a number of times and you know try and prevent that from happening well it can be tricky to be honest um one technique you could use would be to have a uh like a training set and a development set so you you what I mean by that is you you perform a clustering on the training set and then you take the training the the development set and you figure out which clusters they belong to and then you measure the the the error of that development set so it's you know once you've assigned these uh development points to the Clusters you measure the distance to the centroid and you you sum up the square distances and you sum up over all the Clusters then if you do that a number of times what you would do is you choose the clustering that gave you the smallest error on your development set and then you'd say that's probably my best clustering for this data so that's one way of doing it there's multiple ways of kind of uh you know skinning the cat I was trying to think of a good aphorism um and actually that's what's on your problem set or one of the problems on your problem set is to uh cluster based on like a holdout set and see what the error what the effect of the error is on this on this this hold out set so did that answer your question okay um and then choosing K you can there's different methods for doing it too it's a lot of it is you you you run a lot of experiments and you see what you get um this is kind of where the research part comes in for a lot of applications so um you can also try some other automatic methods like entropy or other more complicated measurements of of a but don't worry about those um for our purposes if you get below the cut off value and you run a number of iterations and you've minimized your error on your test set we'll be happy it's just kind of you know we want you to be familiar with K means but not experts in it because it's a useful uh tool for your kit um anyway so all this code is going to do is it's just going to keep it's going to run a number of Trials it's going to perform a cayan clustering and it's going to look for the clustering with the smallest maximum distance so remember the uh return value of K means is the maximum distance from a point to its centroid we're going to Define find our best clustering as being the clustering that gives us the smallest max distance so that's another way you would choose your best clustering um yeah and that's all we're going to do for uh this bit of code here is we're going to find the average income in each cluster and draw a histogram so I think this is actually done so we have five clusters and what they're showing us is that the Clusters if we take the average income of the different clusters they're going to be centered at these uh average incomes so it it let's see what some other plants look like so these set of examples or this set of examples is using a um a point type that's called County and County like the mammal class inherits from point and it defines a uh set of features that can be used or set of it calls them filters but basically if you pass it one of these filter names like all education no education wealth only no wealth what this is doing is it's selecting uh this tupal of tupal and so if I say wealth only it's going to use this Tuple as a filter and for each element in this tupal of tupal it's a it has a tupal that has has the name of the attribute and whether or not it should be used in the clustering so if it has a one it should be used if it has a zero it shouldn't be used so if we look at how that's applied it'll get a filter spec and then it'll just set a uh an a a attribute called ATR filter and if we see where that's used then it's going to iterate through all of the attributes and if the a given attribute has a one then it's going to include it in the set of features that are used in this distance computation so did that make sense at all no so well the idea is that if it's to illustrate that if you use different features when you're doing your clustering you'll probably attain different clusterings so in that first example I showed we used all the features but in this example we are going to look at uh only wealth and that includes the features uh if we look at this set of filters here it's going to include the home Val home value the income the poverty and then all the other fil all the other attributes like population change it's not going to include so it's not going to be those aren't going to be used in the clusterings so this is going to change what our clusters look like so if we look at what we have for a cluster small that's not very clear yeah so let's see what happens I'm not sure if that's really going to show us probably better if I showed them all that one's right so this will take a while so I this one question but there's a method for uh it's like it values or Keys yeah using that actual codee in actual code so you know that if you have a dictionary D you can do like d. keys so remember I was kind of demoing that code the difference between range and X range same thing if you this is going to return an actual list of all the keys right what D do uh iter Keys returns is a generator object that gives you one by one by one each key in the dictionary so if you wanted to so a lot of times you guys in your code will use something like 4K in in d. keys to intergrate through all the keys in the dictionary what this method does though is it creates an actual copy of that list of keys right so it's it's when you call D do keys if you have a lot of keys in your dictionary it's got to go one by one by one by one into in through each of those keys add it to a list and then return it to you what d. it keys does is it will it it skips that you know going one by one by one and adding it to a new list it just gives you a generator object which when you use it in a for Loop when you use it in a for loop it's going to just return the key or yield the key one at a time without creating a separate list that make sense more efficient yeah it's it's generally more efficient and then there's also I think in values which goes through each of the values and then I think there's in items and I think if I'm not mistaken if you do something just like that so you do 4K V in D this is going to return this is going to iterate through uh a tupal that contains the key and the values associated with that key and it's equivalent to doing this make sense so where are we at oh maybe I shouldn't have done this all at once so I don't know if it's going to finish why don't we just look at two why don't we take a look at what the average incomes or what the clustering gives us for average incomes for Education versus no education it's probably not going to be a very good comparison it's doing five trials and two trials for each clustering so K means is the efficient one which means that higher will take a long long time there we go so these are the average incomes if we cluster with k equal 50 on education and then there should be another one I didn't create the new figure so apparently there's a bug in the code I wanted to show the two plots side by side so we could see the differences cuz what you should see is um we would see a different distribution in incomes among the Clusters if we clustered based on no education versus education level but my code is uh buggy and not working the way I expected it to so I apologize