all right welcome back to double06 data structures today we're going to cover a different kind of tree-like data structure called the heap binary heap it's going to let us solve sorting problem in a new way let me first remind you of a portion the problem we're going to be solving today is called priority queue this is the interface we'll see several data structures but one main data structure for today and this is a subset of the set interface and subsets are interesting because potentially we can solve them better faster simpler something and so you'll recognize you should recognize all of these operations except we don't we didn't normally highlight the max operation so here we're interested in storing a bunch of items they have keys which we think of as priorities and we want to be able to identify the maximum priority item in our set and remove it and so there's lots of motivations for this maybe you have a router packets going into the router they have different priorities assigned to them you want to route the highest priority first or you have processes on your computer running trying to run on your single threaded single core and you've got to choose which one to run next and you usually run higher priority processes first or you're trying to simulate uh a system where events happen at different times and you want to process the next event ordered by time all of these are examples of the priority queue interface we'll even see applications within this class when we get to graph algorithms but the general the main two things we want to be able to support are inserting an item which includes a key and deleting the maximum item and also returning it at the same time we'll also talk some about being able to build the structure faster than just inserting it but of course we could implement build by starting empty and repeatedly inserting and also the complexity of just finding the max without deleting it this you could simulate with these two operations by deleting the max and then re-inserting it which works but often we can do faster okay but the two key main operations are insert and delete max and we're going to see a few data structures to do this any suggestions among the data structures we've seen in this class what should we use to solve priority queue interface any possible answers sequence avl oh that's interesting uh sequence fl is a good answer but that's maybe the fancier version yeah set avl sounds good set avl supports these operations and many more all in log n time except for build which takes n log n time because you have to sort first uh so set avl is a good good way to do this we'll talk we'll come back to your sequence avl idea later uh this gets login for operation great i mean this is set avl is our most powerful data structure it does all the operations we care about uh on the set side and the sequence of vl does all the operations on the sequence side but note that this is a set not a sequence we care about keys there are hacks to get around that with sequence avls but let's do that later so great if we wanted to for example speed up find max in a set avl we could add augmentation we could remember subtree property augmentations we can use that to get constant time find max by storing in every node the maximum key item within the subtree and that's a subtree property it's one we mentioned last class so we could even improve that to constant time great um so we're done end of lecture in some sense that's true but what we're going to see today is another data structure called the binary heap which is in some sense a simplification of set avl it achieves basically the same time bounds build will be faster by a log factor but that's not the main reason we care about them the main advantage is that they're simpler and they give us an in-place sorting algorithm so i have up here the three of the operations i've been talking about build insert and delete max uh so we have set avl trees there and log n build log n insert log n delete so along the way to our heap i want to mention two other data structures one is a dynamic but unsorted array and the other is a dynamic sorted array these are simpler data structures we've talked about many times before and they're useful kind of motivations for getting started because a heap is going to be built on top of arrays instead of what's sort of a fusion between arrays and trees so um if i have an unsorted array this is very easy to insert into right i just append to the end this is what we called insert last so insert is fast constant amortized we might have to resize the array but so that's the amortized part but delete max is slow in unsorted array i don't know where the maximum is so i have to scan through the whole array so i scan through the array identify the max is somewhere in the middle and then if i want to delete it i want to delete that maximum element well in a dynamic array all i can really do is delete the last element efficiently so i could for example swap it with the last element so i take this element and put it here and then delete the last element in that array which is pop in python or delete last in our world so overall this is linear time which is bad but i wanted to highlight exactly how it's done for a reason we'll get to in a moment sorted array is sort of the reverse it's very easy to find the max where is it at the end delete max uh the maximum element is always the last element in a increasing sorted array i guess that's constant amortized because then i have to delete it which may incur resizing insert though is going to be linear because maybe i can binary search to find where that item where the added item belongs let's say i just added this item here um i could bind our search to find it but then i'm going to have to do a big shift so i might as well just swap repeatedly until i find the position where the added item x belongs and now i've restored sorted order that takes linear time which is bad and what we want is somehow the best of these two worlds insert is fast for array uh delete is fast for a sorted array we can't get constant time for both but we can get login time for both we already know how with set avl trees but we're going to see a different way to do it today and the main motivation for a different way to do this is sorting so i want to define a priority queue sort so given any data structure that implements a priority queue interface in particular insert and delete max i can make a sorting algorithm what do i do insert all the items delete all the items but because when i delete them they come out largest first i get them in reverse sorted order then i could reverse in linear time and i've sorted my items so we can insert x for x and a or build a and then repeatedly delete max okay how much time does this algorithm take i'm going to introduce some notation here it takes however long it takes to build and items call that t sub build uh n plus uh sorry plus n times the time to do a delete max okay or uh we can write this as n times time to do an insert plus time to do a delete max okay so i'm using these t functions to just abstract what are the running times provided by my data structure that implements this interface the interface says what's correct is and these t functions to give me my performance bounce so if i plug in each of these data structures i get a sorting algorithm i get avl sort i get array sort i get a sorted array sort what do those look like it turns out many of these are familiar uh so set avls take login for operation so we get an n login sorting algorithm out of them which is insert all of the items into the abl tree i don't want to use avl build because that uses sort i'm not allowed to sort in order to implement sort but we saw how to insert into an abl tree and keep the thing balanced so that takes log n each and then we can find the max delete it rebalance and so on total time will be n log n this is an algorithm we call avl sort it's a bit complicated because avl trees are complicated but it gives us optimal comparison bound and login now what about array sort so suppose i use an unsorted array i insert the item so if i insert the items so i'm doing all the insertions here before all the deletions so what's going to happen is i just insert the items in the original array order in other words i just take the array and then what i do is repeatedly extract the maximum item by searching for it moving it to the end of the array and then repeating that process that's unfamiliar that's selection sort from lecture three um so this arrays give us selection sort this is a new way to think about what we were doing way back then with a sorted array what are we doing we insert all the items that's actually where all the work happens because we maintain the sorted array so we start with an empty array it's sorted we add an item okay still sorted we add a second item and we swap if we need to in order to sort in general we add an item we swap it to the left until it's sorted again that is insertion sort okay kind of cool this is a unifying framework for three uh sorting algorithms that we saw before we didn't actually talk about avl sort last time but it was in the notes and so that is the right part of this table so of course uh these array data structures are not efficient they take linear time some for some of the operations so the sorting algorithms are not efficient but they're ones we've seen before so it's neat to see how they fit in here uh they had the selection sort of insertion sort had the advantage that they were in place you just needed a constant number of pointers or indices uh beyond the array itself so they're very space efficient so that was a plus for them but they take n squared time so you should never use them except for like n at most 100 or something avl tree sort is great and then it gets n log end time probably more complicated than merge sort and you could stick to merge sort but neither merge short nor set avl tree sort are in place and so the goal of today is to get the best of all those worlds in sorting to get n log n comparisons which is optimal in the comparison model but get it to be in place and that's what we're going to get with binary heaps we're going to design a data structure that happens to build a little bit faster as i mentioned linear time building so it's not representing a sorted order in the same way that avl trees are but it will be kind of true based it will also be array-based we're going to get logarithmic time for insert and delete max it happens to be amortized because we use arrays but when the key thing is that it's an in-place data structure it only consists of an array of the items and so when we plug it into our sorting algorithm priority q sort our generic sorting algorithm not only do we get n log n performance but we also get it in place sorting algorithm this will be our first and m and only this class and log n in place sorting algorithm cool that's the goal let's do it so uh what we're going to do uh because we're in place basically we have to have an array storing our n items that's sort of the definition of in place just using n slots of memory exactly the size of the number of items in our structure but we're obviously not going to use a regular unsorted array or a regular sorted array we're going to use array just as sort of the underlying technology for how things are stored but we'd really like logarithmic performance which should make you think tree only way to get a log is with the binary tree more or less so tree somehow we want to embed a tree into an array let me grab an example let me draw a tree if i got to choose any old tree i want i would choose a tree that's basically perfectly balanced perfectly balanced to be like this where what's the property that i have all of these levels all of these depths are completely filled with nodes this is depth zero remember this is depth one this is depth two this is depth three so what i'd really like uh is to have uh two to the i nodes at depth i that would be a perfect binary tree uh but that only works when n is one less than a power of two right uh i can't always achieve that for any n and so the next best thing i could hope for is 2 to the i knows a depth i until the very last i the largest depth and in that level i'm still going to restrict things i'm going to force all of the nodes to be as far left as possible okay so i want to say accept at max depth where nodes are i'll call them left justified and these two properties together is what i call a complete binary tree uh why is this interesting because i claim i can represent a tree like this as an array i've narrowed things down enough that i can draw an array down here and what i'm going to do is write these nodes in depth order so write a first because that's step zero then bc that's depth one then uh well they're alphabetical i made it that way d e f g is depth two and then hij is depth three okay this is very different from traversal order of a tree traversal order would have been h d i b j e a f c g okay but this is what we might call depth order do the lowest depth nodes first very different way to lay things out or to to linearize our data and this is what a heap is going to look like so the cool thing is between complete binary trees and arrays is a bijection for every array there's a unique complete binary tree and for every complete binary tree there's a unique array why because the complete constraint forces everything forces my hand there's only if i give you a number n there's one tree shape of size n right you just fill in the nodes top down until you get to the last level and then you have to fill them in left to right what you might call reading order for writing down nodes and the array is telling you which keys go where this is the first node you write down at the root this is the next node you write down at the left child of the root and so on okay so here we have a binary tree represented as an array or an array representing a binary tree the very specific binary tree it has a clear advantage which is it is guaranteed balance no rotation is necessary in heaps because complete binary trees are always balanced in fact they have the best height they possibly could which is ceiling of log n balanced remember just meant your big o of log n this is one times log n so it's the best level of balance you could hope for so somehow i claim we can maintain a complete binary tree for solving priority queues this would not be possible if we were trying to solve the whole set interface and that's kind of the cool thing about heaps is that by just focusing on the subset of the set interface we can do more we can maintain this very strong property and because we have this very strong property we don't even need to store this tree we're not going to store left and right pointers and parent pointers we're just going to store the array this is what we call an implicit data structure which basically means no pointers just an array of the n items how are we going to get away without storing pointers i'd still like to treat it like a tree i'd still like to know the left child of b is d and the right child of b is e we'll see why in a moment we can do this with uh index arithmetic so maybe i should add some labels before i get there so this array naturally has indices right this is index zero this is index one index two index three index four index five index six seven eight nine because there are ten items zero through nine and i can apply those labels up here too these are the same nodes so 0 1 2. this is just depth order but once i have this labeling it's going to be a lot easier to figure things out so if i wanted to know the left child of b is d somehow given the number one i want to compute the number three uh add two they're all sort of multiplied by three there are all sorts of operations that take one and turn it into three but there's only one that's going to work in all cases and the intuition here is well i have 2 to the i nodes at level i if i want to go to the child level there's 2 to the i plus 1 nodes down there right exactly double except the very last one but that won't really matter if there is a left child it will behave the same and so intuitively i have this space of size 2 to the i have to expand it to a spacer size to the i plus 1 so i should multiply by 2. okay and that's almost right but then there's some constants so i'd like to say 2 times i but if we look at the examples here 1 times 2 is 2 which is 1 less than 3. 2 times 2 is 4 which is 1 less than 5. hey we almost got it right it's just off by one off my one is you know index errors are the most common things in computer science what about the right child if the left child's a two i plus one where's the right child i hear lots of mumbles two i plus two one more because we're writing things left to right in depth order the right child is the right sibling of the left child so it's just one one larger okay given those rules we can also compute parent it's just whatever is the inverse of both of these functions which i want to divide by two at some point but be to get i want to get back to i given 2i plus 1 or given 2i plus 2. and so um if i subtract 1 from i then i either get 2i or 2i plus 1 and then if i take an integer division by 2 i get i the original i sorry maybe i'll call this j to be clearer so j is the left or right child then i can reconstruct i which was the parent okay so this is you know constant number arithmetic operations so i don't have to store left and right pointers i can just compute them whenever i need them whenever i'm at some node like e and i want to know what's its left child if sorry i given the node index 4 which happens to contain the item e and i want to know what's this left child i just multiply by 2 and add 1 i get 9. and then i can index into this array at position nine because i don't this is just in my head remember uh this is we're just thinking that there's a tree here but in reality on the computer there's just the array so if we wanna go from e to j we can from four to nine uh if we go try to go to the right child we multiply by 2 8 add 2 10 and we see oh 10 is beyond the end of the array but our race stores its size so we realize oh e does not have a right child this is something you can only do in a complete binary tree in a general binary tree you don't have these nice properties cool so this is basically a heap i just need to add one more property naturally called the heap property so uh there are multiple types of heaps this type of heap is called a binary heap we will talk about others in future lectures i'm going to call it q i should write explicit thing this is an array representing a complete binary tree call the array q and we want every node to satisfy the so-called max heap property which says q of i is greater than or equal to q of j for both children left of eye and right of eye okay so um we have a node i and has two children two i plus one and two i plus two these are our two values of j what we want is a greater than or equal to up uh relation here and here okay so this node should be bigger than both this one and this one which of these is larger we don't know and we don't care very different from binary search trees or set binary trees where we said these guys were less than equal to this this one this one was less than equal to all the nodes in the sum tree here we're just locally saying this node is greater than or equal to this node and this node so this is the biggest is at the top okay so one nice lemma about these heaps this is weird let me give you some more intuition if you are a binary heap if you satisfy this max heap property everywhere then in fact you learn that every node i is greater than or equal to all nodes in its subtree these are what we call descendants and subtree of i okay let me look at this example so i haven't written any numbers here but you can imagine so a here is greater than or equal to both b and c and b is greater than equal to d and e and c is greater than or equal to f and g and d is greater than equal to h and i and e is greater than equal to j that would make this structure a heap not just a complete binary tree so what does that imply it implies that a must be the maximum so you look at any node here like j a is greater than or equal to b is greater than or equal to e is greater than or equal to j and in general what we're saying is that a is greater equal to all nodes in the tree b is greater than or equal to all nodes in its sub tree down here c is greater than or equal to all nodes in its subtree that's what this lemma is saying you can prove this lemma by induction uh but it's really simple if you have two nodes i and j and j is somewhere in the sub tree that means there's some downward path from i to j and you know that for every edge we traverse on a downward path our key is going down non-strictly because every child is less than equal to its parent i is greater than or equal to this is greater than equal to this is greater equal this is greater than or equal to j okay so by uh transitivity of less than or equal to you know that i is in fact greater than equal to j or sorry the key in i is greater than or equal to the key in j this is what we're calling i the index this is what we call q of i this is index j q of j okay very different way to organize um keys in a tree but as you might imagine this is going to be good for priority queues because priority queues just need to find the maximum element okay then they need to delete it that's going to be harder because deleting the root is like that's the hardest node to delete intuitively i'd really prefer to delete leaves but deleting leaves and keeping a complete binary tree is actually kind of hard right if i want to delete h that doesn't look like a binary tree or it doesn't look like a complete binary tree anymore it's not left justified similarly if i want to delete f that's bad because now i don't have four nodes here the one node that's easy to delete is j right if i remove that node i still have a complete tree the last leaf the last position in my array is the one that's easy to delete that's good because arrays are good at deleting the last item but what i've set up here is it's easy to find the max it's going to be up here at the root deleting it is annoying i'd like to somehow take that key and put it at position at the last position at the last leaf because that's the one that's easy to delete okay and that's indeed what we're going to do do in a delete algorithm uh let me first do insert i guess that's a little simpler kind of symmetric [Applause] to what we just said so if i want to insert a key or an item x which has some key uh again the only thing i really can do in an array if i want to add a new item it has to go at the end the only thing we know how to do is insert at the end of an array this is what we called insert last this corresponds to adding no a node containing x the item x at the in the very last level of the binary complete binary tree either it goes to the right of all the existing nodes or it starts a new level but it's always going to be the last leaf after we do the insertion it will be at position size of q minus one okay this is probably not enough though we just insert an arbitrary item in a leaf and now it may not satisfy the max heap property anymore so let's just check if it does and if it doesn't fix it that's what we know how to do but this time we're not even going to need we're not even going to need rotations which is cool so i'm going to define an operation called max heapify up this will make things more like a max heap we're going to start at size of q minus for our value i but it's going to be recursive so what we're going to do is look at a node i in particular the one that just got inserted and where could it violate things well with its parent because we have no idea what key we just put here maybe it's less than our parent then we're happy but if it's greater than our parent we're in trouble and we should fix it so if the item in the parent this key is less than eyes key ah i see i forgot to write key in all these spots this should be dot key and dot key because q of i is an item let's guess it's key so this is the bad case this is if the parent is smaller than the child we wanted the parents to always be greater than or equal to its children so in that case uh what can we do swap them let's swap q parent of i excellent more chuck uh with q of i now they're in the right order okay now we need to think about what about the other child of that node and what about its parent okay so um i have some numbers here let's say this was 5 and this was 10. what do i know about this picture before well i know that 10 is this newly inserted item it's the only one that could have caused violations when i first inserted it so i know that before this before i moved 10 around i knew all the things in this left subtree are less than or equal to 5 and everything up here are greater than or equal to 5. i also know that the nodes in here in fact were less than or equal to five other than this node ten that we just inserted uh this was a correct heap so five was a separator between things above it on the ancestor chain are are greater than or equal to five and things in its sub tree are less than or equal to it okay so after i do this swap which i'm just going to do after i swap the items 5 and 10. 10 is up here 5 is here and now i realize okay great this edge is happy because now 10 is greater than or equal to 5. but also this edge is happy because it used to be happy and we only made its parent larger okay now this edge maybe is bad and so we need to recurse recurse on the parent but that's it okay so we fixed this one edge initially this happens way down at the leaf but in general we're taking our item that we inserted which is x and it starts at the last leaf and it maybe bubbles up for a while maybe it gets all the way to the root if we inserted a new maximum item but in each step it goes up one and so the running time of all this stuff is the height of the tree which is log n okay and because there's only this one item that could potentially be wrong if it ever stops moving we've just checked that it satisfies the maxi property if it gets to the root you can also check it satisfies the maxi property so there's a base case i didn't write here which is if i equals zero we're at the root uh we're done okay and then you can prove this correct by induction there's just one item that's in the wrong spot initially and we put it into a right spot there are many places it could go but we will move it to the i guess unique ancestor position that is correct that satisfies maxi property okay so that's insert delete is going to be almost the same delete man that is sorry delete max thank you you can of course define all of these things for min instead of max everything works the same i just have a hard time remembering which one we're doing um just don't switch you can't use a max heap to do delete min you can't use the mini heap to do delete max but you can use a min heap to do deletemin that's fine so like i said the only le the only node we really know how to delete is the last leaf and the last level which is the end of the array because that's what arrays can delete efficiently and what we what we need to delete is the root item because that's always the maximum one which is at the first position in the array so what do we do swap them our usual trick i think the cool thing about heaps is we never have to do rotations we're only going to do swaps which is something we had to do with trees also binary trees sorry q of 0 with q of the last item great done now we have the last item is the one we want to delete so we do delete last or pop in python and boom we've got we've now deleted the maximum item of course we may have also messed up the maxi property just like we did with insert so with insert we were adding a last leaf now what we're doing is swapping the last leaf with the i'm pointing at the wrong picture let me go back to this tree what we did is swap item j with a so the problem is now and then we deleted this node the problem is now that that root node has maybe a very small key because the key that's here now is whatever was down here which is very low in the tree so intuitively that's a small value and this is supposed to be the maximum value we just put a small value in the root so what do we do heapify down uh we're going to take that item and somehow push it down to the tree until the down in the tree until maxi property is satisfied so this is going to be max keepify down and we will start at position zero which is the root uh and max heapify down is going to be a recursive algorithm so we'll start at some position i but initially that's the root and what we're going to do is look at position i and its two children so let's say we put a very small value up here like zero and let's say we have our children five and ten we don't know maybe i'll swap their order just to be more generic because that looks like well not quite a minor search tree but we don't know their relative order but one of them is greater than or equal to the other because they're in some order and so what would i like to do to fix this local picture yeah i want to swap and i could swap zero is clearly in the wrong spot it needs to go lower in the tree i can swap zero with five or zero with ten which one ten i could draw the picture with 5 but it will not be happy y10 we want to do it with the larger one because then this edge will be happy and also this edge will be happy if i swapped five up there instead the 510 edge would be unhappy it wouldn't satisfy the maxi property so i can do one swap and fix maxi property except that again zero may be unhappy with its children zero was this one item that was in the wrong spot and so it may have to go farther down but five will be five didn't even move so it's happy everything in this subtree is good uh what about the parent well if you think about it because everything was correct a correct heap before we added 0 or before we put 0 too high all of these nodes will be greater than or equal to 10 and so on the ancestor path and all of these nodes were less than equal to 10 before and less or equal to 5 so that's still true but you see this tree is happy this tree still may be unhappy zeros still might need to push down farther that's going to be the recursion okay so we check [Applause] here okay there's a base case which is if i as a leaf we're done because there's nothing below them uh so we satisfy the maxi property at i because there's no children otherwise let's look at the leaf among the left sorry left not leaf among the two children left and right of eye right of i might not exist then ignore it but among the two children that exist find the one that has maximum key value q of dot key okay that's that was 10 in our example and then if these items are out of order if we do not satisfy so greater than would be satisfy less than q of j would be the opposite of the maxi property here if if maxi property is violated then we fix it by swapping q of i with q of j and then we recurse on j okay call max heapify down of j that's it so pretty symmetric uh insert was a little bit simpler because we only have one parent delete min because we're pushing down we have two children we have to pick one but there's a clear choice the bigger one and again this algorithm this whole thing will take order h time the height of the tree which is log n because our node just sort of bubbles down at some point it stops when it stops we know the maxi property was satisfied there and if you check along the way by induction all the other maxi properties will be satisfied because they were before okay so almost forced what we could do here the amazing thing is that you can actually maintain a complete binary tree that satisfies the maxi property but once you're told that the algorithm kind of falls out because we have an array the only thing we can do is insert and delete the last item and so we've got to swap things to there in order to or out of there in order to make that work and then the rest is just checking locally that you can fix fix the property cool uh so that's almost it not quite what we wanted so we now have login amortize insert and delete max in our heap we did not yet cover linear builds right now it's n log n if you insert n times and we did not yet cover how to make this an in-place sorting algorithm so let me sketch each of those i think first is in place so how do we make this algorithm in place i guess i want that but i don't need this so we want to follow priority queue sort do you want that but i don't want to have to uh grow and shrink my array i would just like to start with the array itself so this is in place so what we're going to do is say okay here's my array that i want to sort that's given to me that's the input to priority queue sort and what i'd like is to build a priority queue out of it initially it's empty and then i want to insert the items one at a time let's say okay so uh in general what i'm going to do is maintain that q is some prefix of a that's going to be my priority queue it's going to live in this subarray this prefix okay so how do i insert a new item well i just increment so to do an insert the first step is increment size of q then i will have taken the next item from a and injected it into this queue and conveniently if we look at our insert code which is here the first thing we wanted to do was add an item at the end of the array so we just did it without any actual work just conceptual work we just said oh our q is one bigger boom now this is at the end of the array no more amortization in fact because we're not ever resizing our array we're just saying oh now q is a little bit bigger of a prefix it just absorbed the next item of a similarly delete max is going to at the end decrement the size of q why is that okay because at the end of our delete max operation not quite at the end but almost the end we deleted the last item from our array so we just replaced that delete last with a decrement and that's going to shrink the queue by one it has exact same impact as deleting the last item but now it's constant time worst case not amortized and the result is we never actually build a dynamic array we just use a portion of a to do it so what's going to happen is we're going to absorb all the items into the priority queue and then start kicking them out as we kick them out we kick out the largest key item first and we put it here then the next largest then the next largest and so on the minimum item's going to be here and boom it's sorted this is the whole reason i did max heaps instead of min heaps is that in the end this will be a upward sorted array with the max at the end because we always kick out items at the end we delete the max first okay so that is what's normally called heap sort you can apply this same trick to insertion sort and selection sort and you actually get the insertion sort and selection sort algorithms that we've seen which operate in prefixes of the array okay cool so now we have we've achieved the y up there which is n log n sorting algorithm that is in place so that was our main goal keep sort let me very quickly mention you can build a heap in linear time with a clever trick so if you insert the items one at a time that would correspond to inserting down the array and every time i insert an item i have to walk up the tree so this would be the sum of the depth of each node if you do that you get n log n okay this is the sum over i of log i that turns out to be n log n it's log of n factorial the cool trick is to instead imagine adding all the items at once and not heapifying anything and then heapify up or sorry heapify down from the bottom up so here we're heapifying up now we're going to heapify down and surprisingly that's better because this is the sum of the heights of the nodes and that turns out to be linear it's not obvious but intuitively for depth this is zero this is log n and we've got a whole ton of leaves so right at the leaf level you can see we're paying n log n right because there are n of them and each one costs log n down here at the leaf level we're paying constant because um the height of the leaves are one here the height of the root is log n and this is this is better now we're paying a small amount for the thing of which there are many and it's not quite a geometric series but it turns out this is linear so that's how you can do linear building heap to come back to your question about sequence avl trees turns out you can get all of the same bounds as heaps except for the in-place part by taking a sequence avl tree storing the items in an arbitrary order and augmenting by max which is a crazy idea but it also gives you linear build time and yeah there's other fun stuff in your notes but i'll stop there