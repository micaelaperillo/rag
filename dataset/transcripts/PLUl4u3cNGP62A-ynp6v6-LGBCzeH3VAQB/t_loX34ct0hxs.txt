all right so let's get started with today's lecture we're going to look at a lot more code where we basically uh figure out the complexity class of that given code so first let's remember what we learned at the end of the last lecture so we introduced this Theta notation as a notation to Mark the order of growth of a particular function or a particular piece of code right and the Theta we preferred over Big O notation because the Theta allowed us to get this asymptotic upper bound on the worst case uh runtime of our function so we wanted an ASM totic bound as opposed to an upper bound because that upper bound can be anything that grows faster than our function right so we prefer this Theta as the ASM totic bound so at the end of last lecture we basically said that given some function the Theta for that function is going to be the dominant term of that function so if we have a whole bunch of terms we focus on the one that grows the most we drop any additive constants any multiplicative constants and all the other terms that don't grow as fast as that one as that biggest one okay so we ended up with some classes of algorithms that we're going to go over uh today we're going to see a bunch of codes that fall within those classes of algorithms but before we go into that I wanted to just quickly recap sort of the the the uh the end of last lecture so we saw an example uh that was pretty similar to this one if not the same so we know that given some function we can grab the Theta of that function by focusing on that dominant term but how do we get at that function so given some piece of code the idea to get at that function was to first start by looking at the inputs to the function so we have three inputs in this particular case L L1 and L2 right once we figure out the inputs to this function we we go on and look at everything within the code that depends on these input parameters so they could be direct like a loop that goes over some thing related to the input or it could be indirect as we're going to see in some examples later today but we basically look at just the parts of the function that um that deal with this input if we want to be exact right we start by finding out the exact number of operations that we do within this code right that's something that we did when we counted the number of operations given some function so we're going to count the number of operations given this code in uh relation to L1 L2 and L so we've got this relationship that we can come up with that relates the number of operations run as a function of L L1 and L2 so the one over here is constant because we just have an assignment here for for some variable the next term here is not constant there are five constant things that we're doing assigning I to be a value in range grabbing uh indexing into L at I that's two indexing into l i uh L1 at I that's three checking the equality that's four and then setting in L1 to be true that's five so there's five operations but these are repeated how many times well they're repeated length L1 times because this Loop goes through length L1 right so this term here this for Loop here is length L1 * 5 number of operations then the one here is this assignment over here and then this Loop down at the bottom is exactly uh the same as the loop up at the top except that now this bottom Loop uh repeats length L2 times right right so as L2 gets bigger this Loop will take longer to run right that's how we think about that and then lastly the plus two at the end of that relationship is finding the and of these two variables and then doing a return okay so that leads us to simplify it as 5 * length L1 plus 5 * length L2 plus 3 and this becomes the function that we can then grab the Theta of right so now we just use the regular rules of theta um law of addition and law of multiplication if there's anything to add or multiply in this particular case let's say that L L1 and L2 are all the same length and then we can simplify the above function to 10 length l+ 3 and then the Theta of that becomes just Theta of length L because we dro the three we drop the 10 multi uh the the 10 multiplying L and then we just keep length l so this is how we get at the Theta of a particular function this is when we looked at last time but as we get as we look at more functions today we're going to get better at just identifying the parts of the code that just deal with our inputs right this inl1 equals false this nl2 equals false this return this and those are all constant things that are happening so we don't need to focus in on those we just maybe glance at them really quickly to make sure there's nothing funky going on that's dependent on the length of our lists right but we can just basically say well we've got our inputs we've got one for Loop that goes through the length another for Loop that goes through the length they're in series so we use the law of addition to say that this function is Theta of length L1 plus length L2 right and so then we can quickly tell the Theta of that function just by looking at uh the parts that depend on the input all right so that leads us so at the end of last lecture we ended up with looking at these uh uh sorry deciding that these are the uh complexity classes that we can categorize a lot of our functions in right so Theta of one is constant Theta log n is logarithmic here n is assum assuming n is the input to my function Theta of um uh n is linear Theta of n log n is log linear Theta of n if n is my input to some constant like n s n cubed runs in polinomial time and then Theta of some constant to the N where n is my input is going to be exponential like 2 to the nend 3 to the end those are all considered exponential time algorithms and when we write our algorithms we want to uh be up in this maybe top four maybe top five though polinomial is going to grow pretty quickly as our input gets big right so if we can take our code and just quickly glance at it and classify it within one of these algorithms that can guide us towards writing an towards deciding whether the algorithm we wrote was good or bad right if we glance at it and say hey this algorithm is exponential or this function that that I wrote as exponential maybe we want to rethink our approach to the problem and try to get it into one of the upper complexity classes so what we're going to do the rest of this class is just go through a bunch of these complexity classes and we're going to see some codes that belong to these complexity classes and hopefully uh give you an idea of you know what code looks like that fits in one of these complexity classes so the first one we'll look at is the constant complexity class it's pretty simple it's not really very interesting um if if your code belongs in the in this constant complexity class that means that it does not depend on the input at all it always runs in constant time so your code can have loops or it can have some sort of recursive structure but th that Loop or that recursive structure doesn't depend on the input at all right so it's fine to have loops it's just as long as it doesn't depend on the input it's considered constant so there are some built-in operations that are constant time so if you see any of these operations like indexing into a list appening to a list grabbing uh the value associated with the the dictionary key those are all constant time so if you see them in your code you don't need to account for them at all but we're going to see in a few slides that there are some operations on lists and dictionaries that do add some non-constant complexity so you can't just brush them off all right let's look at a couple um examples of code so so here's a very simple function add X comma y so X and Y are my inputs there is no Loop or nothing recursive nothing that takes time here that nothing that repeats in this code so the complexity of this code is Theta of one okay that's it here's another example this is our kilometer example uh taking in Miles all it does is a multiplication again Theta of one there's nothing interesting going on here no Loop no recursive here's a function that does have a Loop within it first thing we look at though is my input what variable is my input here it's X right so which part of my code here depends on X well there's something that I'm adding here so I'm adding X onto some number and I do have a loop but does the loop depend on X no it depends on some number that is just 100 within my function right if yal X here then this code wouldn't be constant right because this Loop will go through x times but here Y is just 100 so this code is Theta of one complexity there's nothing here that depends on X as X Grows All right so not very interesting uh examples there so let's move on to the next simplest class of uh of functions um the linear complexity class and the these functions will be usually denoted by one Loop um in or maybe many Loops in series or something like that but these Loops all depend just linearly on N you could also have a recursive function that repeats that's also linearly in N so we're going to see an example of a recursive function in a little bit but first we'll start out with just some functions that Loop linearly within um there are some built-in operations though that are linear in time so if we ever see these operations within our code we can't ignore them because they will contribute a Theta of n complexity to our code right so we have to account for them like if we have uh some e Inn within some other loop we can't just say this andn is constant we'd have to use the law of multiplication or something like that to account for it okay so checking if an element is in a list obviously is linear because you have to look at each element uh in the list to determine that that e is in it or not um making a copy of your list is also linear in time even though we're making a copy of half of our list right so the first half of our list it's still linear because copying 0.5 time length L is still Theta of length L right that multiplicative constant on the front of our length L is 0.5 so if we draw drop it that's still Theta of length L um checking for equality between two lists is also constant because you have to look at each element in the list compare them to make sure that they're the same or not and deleting an item in a list is also um is also linear in time sorry this one was constant in time is ALS uh sorry this one was uh linear in time this Del is deletion is also linear in time just because of the way lists are stored in memory so if you delete an item in the end of uh from your list p python will count that as a linear time complexity so let's look at some examples first we'll just start out with just some regular functions with loops and then we'll look at one recursive function so here I've got multiply X by y it Loops through range Y and it just adds X Plus X Plus X Plus X Y times so I've got two parameters here so I need to think about the complexity of this function with regards to both of them so the complexity with respect to Y is Theta of Y right because I've got one Loop that's a function of Y so this Loop will repeat however Big Y is so if y increases this Loop will the time this Loop takes will also increase right so the Theta complexity of this function is Theta of Y with respect to Y but what's the complexity of this function with respect to X I have no looping structure here that's with respect to X right all I'm doing to X is just adding on to some number so the complexity with respect to X is just Theta of one so the overall complexity of this function is just going to be Theta of Y right X does not contribute anything to this uh the runtime of this all right so this uh and the previous sort of loop function from the constant kind of tells us that we need to be careful about what the inputs are right when we report the complexity we have to report it with respect to the inputs to our function we don't always just say Theta of n or Theta of n s or Theta of length n whatever it is we have to relate it to the inputs or a function and if we have more than one input we have to be careful that to to account for all of the inputs that contribute to the complexity okay all right let's look at another example so here's one where you take in a string s we Loop through each character in s we cast each character to to uh an integer and then we add on to some value so we're essentially just adding on all of the characters in s in the string s so this has one Loop that Loops through all the elements in s now if s is a string what's going to make this program slower is it that the string like so so the numerical value of the string is is bigger no because if I'm looping through the string one 0 00 0 it's going to take the same amount of time as if I'm looping through the string 9999 it's the length of the string that matters so that's what this Loop is doing right it's taking into account the the length of the string so if my string is longer then it's going to take longer to run so the complexity of this function is just Theta of length s right because that's the length of the string contributes to my uh to slowing down my function everything else that we do is constant so the overall complexity is Theta of length s or if it's simp simp you can just say Theta of n but then you have to say where n is you know some like length of s all right here's another example this is a factorial uh program that does it iteratively so it's going to use a loop to keep um adding sorry to keep multiplying on I to calculate the factorial of su n so in this case my input is n so now I'm going to look through my function to see what part of my function depends on N so here n is just a number and I'm looping through from two all the way up to n+ one so I'm going to Loop through n minus one times overall since I'm looping through n minus one times there's nothing else really that's contributing to the complexity so Theta of n minus one is just Theta of n right so the complexity of this function is just Theta of n everyone okay so far right so very simple programs that just have one Loop that just depends on the input uh linearly okay I will make a little note about the factorial because this is kind of something important it's going to tell us kind of the difference between Theory which is what this class is mostly about or this set of lectures and the real world so I actually ran the iterative version of factorial on the computer and you can see here I've mult multiply the input by two so 40 80 160 320 and so on so as I'm multiplying the input by two if I'm expecting this function to be linearly related to the input right I'm expecting that the time that this function takes to run is going to be approximately twice as long right if the input increases by two the time it takes for this program to run should just increase by two as well and it does right it does all the way up to somewhere between 640 and 1280 right so if we do the math that's approximately times two each time minus you know because we're just doing times here but then after somewhere within 640 and 1280 the time that it takes to run my program no longer follows this linear pattern in fact it starts to grow faster than linear and from you know at a first glance it looks like it grows squared right polom so instead of you know if you increase uh the input by two it looks like the uh the time it takes for this program to run increases by four after some point and that's because in the real world I've got you know python running on the machine there's only some uh set number of bits that my computer can hold right when it deal when it stores numbers and the factorial of some number within between 640 and 1280 become so large that when Python and the Machine is trying to deal with multiplying these big numbers by these big numbers altogether it's just taking a really long time to run because it can't store these big numbers as efficiently as it could store these smaller numbers and so in the real world what ends up happening is after some you know after I'm trying to store some really large value and doing the operations with some really large values the um the the time complex goes down dramatically right n to n s is a pretty big jump right and so this is kind of shows the difference between Theory and the real world right so in the real world we can't store these values as efficiently as they get bigger yeah so if we used like a machine that had more bits to store values then we'd be able to be uh more efficient Farther Along right yeah exactly and we could I I guess we could uh if we had a language that was maybe doing some smarter things and storing these big values in a much smarter way that could also have an impact in the timing as well but for the purposes of this class we're just interested in the theoretical you know the theoretical happenings here right so as the input increases by X we expect that the uh time that it takes to run the program will be x times as long right because we're looking at values that are really really big in in theory okay so let's look at another example so this is a factorial uh function that does it recursively we've seen this function before um we just looked at the iterative version of factorial now we're looking at the recursive version of factorial so what do we have we have one base case right that our code will eventually get down to and a recursive step which is just n * factorial n minus one so how do we do the analysis of a recursive algorithm because in this recursive algorithm we don't have a loop right in the previous examples we had a loop that we could definitively say hey this Loop will repeat this many times so clearly increasing n will increase this this the lot the time it takes for this Loop to run so when we're dealing with recursive functions we think about the how many times the recursive function is going to be called right because when we call factorial right we have factorial of some you know five or whatever it is and this calls factorial of four and this calls factorial of three right and so we have this chain of function calls where we get down to the base case and once we get down to the base case we start to kick off the um the uh the step that Returns the result one at a time so when we're talking about recursive functions what we really care about is how many times we call the function okay that's our quote unquote Loop for recursive functions right it's just the function calling itself to ask itself to do the work and it does the work with a slightly changed parameter right so what we need to to do is think about how many times does this function call itself and on top of that is there some sort of overhead that's not constant in this particular case when we call factorial recursive we're going to go essentially Theta of n times right because we start with n then we do n minus one n minus two n minus three all the way up down down to zero right so effectively we've called ourselves about n times so Theta of N and the overhead for each one of those calls is is constant because all I'm doing to n is subtracting by one and that's a constant thing right n minus one is Theta of one it's just constant so the overall complexity of this is just Theta of n where uh n is just my input okay so what we notice is that the iterative and the recursive versions of factorial are both Theta of n right which means that generally speaking if we were trying to decide whether to imp implement me factorial recursively or iteratively it won't really matter in the long run because the worst case complexity is Theta of V it's the same for both so it'll be your choice which one to actually use right so then it maybe comes down to readability or you know other factors all right another example so this is compound we saw this last lecture we actually timed it and counted how many actually did we count I don't remember I don't think we counted the number of operations or maybe we did but um we definitely timed it so this function um took in three parameters so we can have to be careful which one of these parameters or which which parameters of these actually contribute to my complexity so this uh function calculates the amount of money I have if I invest some monthly uh uh amount at some monthly interest over some number of months right so the loop here iterates through number of months and then everything else is seems to be constant right I have got one Loop so the inside of the loop is constant I do have to double check that but so far so good it's just it's not looping anything else it's not a a function of anything else the loop itself though is Theta of n months right so the overall complexity of this function is Theta of n months or we could say Theta of n where n is equal to n months none of the other parameters contribute to my complexity and that's exactly what we saw when we ran the code right we ran it by changing each one of the parameters and we saw only n months contributed to a slowing program um if we really wanted to we could have um done this uh analysis in depth right as we've done last lecture to actually count the full number of operations or as we did at the beginning of this lecture so you know total equals z is Theta of one the loop is Theta of n months multiplied by uh four operations so I grabbing a value in range taking multiplication addition and then saving that into total that's four multiplied by Theta uh of n where n is n months plus Theta of one to do the return so that ends up being Theta of 1 plus 4 and plus one which just uh simplifies to Theta of n where n is n months yeah yeah so we're just looking at operations right we're doing calculations with interest and invest and and multiplying it with total right but the fact that interest is bigger like if the interest is $1 if the interest is $1,000 is this going to make that line of code much slower no right because all we're doing is a multiplication between two numbers right so that's why the inside is Theta one right but having a loop where we repeat this over and over again is going to slow the program down yeah okay how about this Fibonacci function so this is an iterative version of Fibonacci um we have I don't know if we've seen this before um again we could do sort of a rough quick analysis where we just briefly glance at every single line and ask ourselves whether it's contributing Theta of one or something worse to our total uh analys to Total runtime analysis so we've got this first part here which is just constant it's St of one right nothing here is is Loopy there's no um recursive going on nothing that depends on the input in a non-constant way in the else we've got this constant again just assigning two parameters we've got a loop so now this Loop is going to be non- constant the stuff inside the loop is constant though right so the loop itself depends on N my input so that's going to be Theta of n but that Theta of n is multiplied by Theta of one like the stuff inside the loop is just constant so that's Theta of n * Theta of one which is just Theta of N and then the return of course is Theta of one so we could do a calculation like this or you could just you know quickly scan and say hey I've just got a loop that looks like that's uh that depends on n and that's Theta of n so the overall complexity of this if we wanted to be detailed is this right Theta of 1 plus Theta of 1 plus Theta of n * Theta of 1 plus Theta of 1 but overall that just gives us Theta of n because that Loop is the only thing that depends on my input okay everyone all write so far okay perfect so now let's move on to the second easiest complexity to kind of identify that's the polinomial complexity so polinomial complexity generally deals with functions that have nested Loops right so if we have two nested Loops that linearly depend on my input that's going to be a function that's uh n s if I've got three nested Loops that all depend on my input linearly that's going to be n cubed right um so let's see some examples so here I have a really simple nested Loop situation I've got a function um called G and it's going to take in an input n so I'm going to look for everything that depends on N well I've got a for Loop here that's going to iterate n times so that's Theta of N and I've got an inner for Loop so for each thing in my outer for Loop I'm going going to do the inner thing n times as well right and then the stuff inside my inner for Loop is constant so that's Theta of N and the stuff outside of my Loops are all sorry the stuff inside my inner for Loop is Theta of one and the stuff outside of any of the four Loops are Theta one as well so they contribute nothing to this complexity so the only thing that I need to uh to account for is my outer loop which is Theta V and law of multiplication says my inner Loop is going to uh be multiplied it's multiply its complexity to my outer Loop's complexity right so the overall complexity of this function is Theta of n^2 because the number of times that I'm going to do this operation and is going to be n^2 times okay perfect all right so now let's look at some examples with lists right we haven't seen those yet um so now we have to think about the in put uh in this case it's going to be two lists and when we're dealing with lists one of the things that or the most common thing we're interested in is what happens to the behavior of the function as the lists get bigger right as we saw sort of in in in last lecture the size of the elements within the list typically don't matter but the fact that I have more elements to do stuff with does matter right so if my list now has twice as many elements this program or most programs will probably be twice as uh slow so here's a function uh called is subset takes in two lists L1 and L2 I've added two little examples up here um to help us figure out what this function does so it's going to tell us whether the elements of L1 are in L2 right so in the first example here elements in L1 are three and five and two and L2 does have the three and the five and the two but it also has other stuff that's totally fine all the elements in L1 are in L2 so this function will return true for those examples those those L1 and L2 and then here's an example where it will return false so the elements of L1 are three and five and two and L2 is missing the three so then that one will return false right the elements of uh L1 are not all in L2 so it's not a subset all right so what's this function doing doing well it's iterating through all the elements in L1 so it's going to first look at the three then the five then the two it's going to look through each element in L2 for every one of those L1 elements so it's going to look at the three and the two the three and the three the three and the five the three and the nine then it's going to look at the five and the two the five and the three five and five five and nine right it's going to keep doing that and it's going to keep track of this uh Boolean uh matched called matched and it's going to as long as it finds this element E1 within my L2 it's going to save matched to be true and it's going to keep doing this until it uh does not find a uh sorry until it keeps finding matches as long sorry until it finds a match as soon as it finds a match it breaks because there's no need for it to keep looking at the remaining elements of L2 it already found one that matches so this code could actually be Rewritten by saying uh kind of the inverse right if E1 is not equal to L2 we can just uh immediately return false because we've already found an element that from L1 that's not an L2 so we could have Rewritten this code in many different ways but the ultimate analysis will be the same so let's look at the uh analysis for this function well we have to so we have to be careful about both of these inputs which parts of this function depend on L1 and L2 well we've got an outer for Loop right so what happens uh to the complexity with regards to this this Loop well if I have more elements in L1 then this code this Loop will go through more times so this Loop will be executed length L1 times so the Theta for this outer loop is going to be Theta of length L1 but there is an inner loop so for each element in my outer loop I'm also going to do everything in this inner loop right so in the worst case I need to look through each element in L2 to find a match so the inner loop will execute at most length L2 times again in the worst case right so the inner loop will be Theta of length L2 so the overall complexity since I've got this nested Loop situation law of multiplication says that it's going to be the Theta of my outer loop multiplied by the Theta of my inner loop so Theta of length L1 times length L2 okay everyone are Yeah question yes so like here in this if yes if the if had something like using an in right which where in is is linear then yes there would be another like it would be like there was another loop at the third level yeah so then it would be uncubed still polinomial but and so if L1 and L2 are the same length which you know sometimes we uh we put on to simplify the complexity uh put this condition on to simplify the complexity then we say that it'sa of length L1 s right it's still polom complexity okay let's look uh sorry question yes if there were not the same length you have to denote it in the terms of the both LS yeah okay let's look at another example so here's a function that uh grabs the intersect of two lists so again I've got a little example uh up here example L1 and L2 so the intersect are going to be the common elements within L1 and L2 but I'm only going to I'm not going to do duplicates so I'm just going to keep the unique numbers so here I've got L1 and L2 contain 352 and 2359 so notice the two and the three and the five both occur all all occur in both lists so the intersect of these two lists is 2 three and five this example here on the right side is going to be a little bit trickier right it's kind of a Unique Edge case but the code still works for that edge case it's if I have L1 that has duplicates of some number and L2 that has duplicates of that same number The Returned list of the of the intersect should just be seven right that one number once okay so how does the code achieve this so you notice the nice little little structure here I've got kind of two blocks of code right I've got something here which is going to actually help us build this list of all of the uh elements that are common within the two lists and then something down here where I'm going to Cull that list to keep only the unique values right so up here this has a nested Loop situation just like in the previous example I have to look at all of the pairs right from L1 and L2 to figure out which are common right so this for Loop uh over L1 is going to go through the three the five and the two and then the inner four Loop through L2 is going to basically match take take a look at does the three match the two does the three match the three does the three match the five does the three match the nine right and then the five match the two five match the three and so on so that's what those Loops are doing and as soon as we find a match we're going to pend it to this temporary list and it's it's okay if we have duplicates in this list so if you look at the example on the right hand side there with the seven uh duplicated many times that's actually going to create a temporary list right that's going to contain nine times that seven so it's going to look at the seven with the seven and it's going to say hey that's a match let me add it then it's look going to look at the seven with the middle seven and L2 and it's going to say let me add that and then it's going to look at the first seven and L1 with the last seven and L2 and it's going to say let me add that right and it's going to do that same thing all over again when it looks at the middle seven and L1 along with each element in L2 so it's going to add the seven three more times and then again when it looks at the last seven in L1 along with each seven and L2 so that's totally fine that's just what this code is doing and then the bottom part down here is going to take this temporary list that we created and it's going to just keep the unique values within it right so it's going to keep that uh create that unique list and it's going to say if I haven't seen this value in unique add it and if I have don't do anything so in the end this code down here is going to take that big list here and just keep the unique values all right so let's do the analysis for this so I've got my outer for Loop and my inner for Loop up in the top half of my code here that generates my temporary long temporary list potentially long temporary list so that we already know from the previous example is Theta of length L1 time Theta of length L2 right pretty simple now what about this bottom half here because we have to be careful about this bottom half this one could also contribute to the complexity right it's looping through a temporary variable a list variable that we created but this list is created by doing something to L1 and L2 right by looking at elements in L1 and L2 so it's actually indirectly related to L1 and L2 so we can't just cast it aside because it could potentially contribute to the complexity of my program right and in the worst case I create this temporary variable that looks like this right so in the worst possible case my temporary variable's length is going to be length L1 time length L2 right I basically added that character every time I compared a value right so I this list at worst case is length L1 time length L2 long so if I'm iterating through that list then the complexity of that second half is also Theta of length L1 time length L2 in the worst case right so the overall complexity of the function is Theta of length L1 time length L2 up here plus Theta of length L1 time length L2 down here so in this particular case the fact that I'm iterating over temp didn't actually increase my complexity but you can imagine code where you know something doing something funky like this where you indirectly reference have some Loop over or something related to the input could affect the complexity so in this case the overall complexity is still Theta of length L1 * length L2 right questions about this one okay yeah why is it not because like you're appending a certain number of them like how do you know this like I this for each problem it varies for each problem right but in the analysis um we're interested in the worst case scenario right the ASM like the ASM totic behavior of the worst case and in the worst case we've added this number length L one times length l two times right most of the time of course it's not going to be this bad right it's just in this one particular case that it is this bad oh I see yeah okay let's look at one more function that's polinomial so here's diameter we saw this last lecture basically if we have a bunch of points in a 2d plane this function tells us the distance sorry the maximum distance between any two points right so I I drew that that that picture in the 2D plane so um this one is going to have nested Loops again so the outer loop iterates through length L times so remember our L is just a list of Two Poles representing these XY coordinates so the outer loop easily goes through length L times but what does the inner loop go through right the inner loop is actually starting at I not zero right if it started at zero the inner loop would be clearly Theta of length L but it it's not right it starts at I on average though how many times does that inner loop go through well the first time it goes through that inner loop it's going to look at um length L um minus one elements next time it's going to look at length L minus 2 elements next time it's going to look at length L minus 3 elements right until we get to the end where it's going to look at one and then zero elements right so if we think about how many times that in Loop actually iterates it's going to be what is it like length L -1 * length L over 2 is that the function I think to add all these together something like that which is basically still something that's a function of length L right like we can simplify it to be 0.5 length L right so it's still a function of length L even because the coefficient in the front of that length L is 0.5 right so the overall complexity of the inner loop is still Theta of length L right everything else within this uh code is constant so the overall complexity is just Theta of length l s yeah sorry where did the one2 come oh it's the formula to add like um like if you add 1 plus 2 plus 3 plus 4 plus all the way up to n like what's the formula to do that I think it's like n * n plus 1/ 2 something like that so this is not exactly half but it's like something on the order plus I don't know something right whatever this calculates to but in effect it's like something that's smaller than length L but it's still a function of length L right and so that front coefficient on on in in right before length l just goes away right even like if it was 10 we would still you know cast it away in this case it's you know 0.5 or whatever it is right so it's still less than one but we still cast it away because we're interested in the Theta of this yeah um uh I mean the inner loop could just not depend on the input at all right like here it's n s because the both of the loops depend linearly on the input but if the inner loop dep like if the outer loop went through range length L squar then the overall complexity would be length L cubed in this case right because it's length L stimes length L um or if one of the loops doesn't depend on the input at all then it contributes nothing constant and it nothing linear so it's it's constant yeah okay let's have you think about this question for a bit so think about the input think about parts of the function that depend on the input and then um what is the complexity okay what's the outer loop Theta of yeah um yes nums is a list so the outer loop is Theta of length of nums correct good um what's the inner loop Theta of yeah is that what you're going to say Theta of one exactly it's the length of digits but digits is not my input right nums is my input so the inner loop will always just iterate through 10 times so in the eyes of the inputs to the function that's just constant right right so the input is nums the outer loop is Theta of nums the inner loop is Theta of one so the overall complexity is Theta of length of nums perfect how about this one what are my inputs do any Loops depend on these inputs all right what's the outer loop complexity yeah L1 yeah Theta of length L1 exactly um what's the inner loop complexity Theta of length of L2 perfect and is there anything else that contributes to the complexity here what's that if the if statement yes what about it is um making you question that the complexity is not constant exactly yes very nice so in iterates through the length of L3 right looking for an element in L3 E1 and L3 is not constant right you have to look through the whole length of L3 to figure out where it's there or not so the this inner bit here right is not constant it's Theta of length L3 in fact it's you know two times length L3 right so the overall complexity of this function is Theta of length L1 time Theta of length L2 times Theta of length L3 right okay cool let's look at the exponential complexity so this is a complexity that grows really really quickly we never want the algorithms that we write to land within this class unfortunately there are just some problems in real life that we have to uh compute that are just naturally part of this complexity class there are some techniques to deal with making these algorithms a little bit faster but inherently there are just exponential lthms that we just can't do any better than exponential uh in in in solving some these problems all right so let's look at Fibonacci again we looked at Fibonacci a few slides ago iterative version and the iterative version was Theta of n but if we look at the recursive version of Fibonacci it's not Theta of n at all in fact as you can see it's in this exponential set of slides the it the recurs ver of version of Fibonacci is actually exponential okay so let's recall what this uh code is doing so there's two base cases right uh Fibonacci of zero and one and then FIB the recursive step is Fibonacci of n minus one plus Fibonacci of n minus 2 so for every level that we go down there's going to be times two more paths that we need to explore to grab the values from right so for some for the very first n we've got just one you know one value to grab for the next n we've got times two that value to grab the next level for the next n we've got two times more values to grab and so on right so the fact that there are two recursive calls in this recursive step leads us to this little inverted tree kind of structure right and we even Drew this when we looked at how many function calls are being run right remember when we're figuring out the um the the complexity with a recursive uh function we need to figure out how many of these function uh how many recursive calls are we actually doing right so because of this tree structure every time we add a new level we basically have two completely separate paths to explore further right and those two paths have their own two paths and so on so this leads us to this tree structure which is actually going to lead to the total number of uh recursive calls uh to be uh exponential so Theta of 2 the N now if we looked at the actual recursive call tree right we looked at this and it looked something like this right a a bunch of lectures ago um you might notice that the tree actually thins out a little bit to the right right it's not a full tree with the leaves nicely all the way down and that's because well the left path calculates FIB five but the right path calculates fib you know four so n minus one of the of the left path but that's fine it's not that we are actually uh going to speed up anything uh um by some sort of order of magnitude right just because the tree thins out a little bit on the right hand side is not going to speed up the overall complexity of this function it's going to be Theta of 2 to the N minus you know some Theta that's less than 2 to the N so that subtraction is not going to really uh decrease the overall complexity of our function so the order of this is still exponential all right here's another example of an exponential code so this is a function uh that is going to generate all the subsets of a list okay so I've again I've added a little example here to help us understand what it's doing so here I've got three numbers a list with three numbers one two and three and to generate subsets what this means is that I'm going to create a new list of all of the possible combinations of numbers within my original list of all the possible lengths right so first one subset of this list could be just the empty list so that's not taking any of my original numbers at all the next one is uh a list with just one of the numbers in it so either the one or the two or the three a next uh part of uh next subset of my uh list could be taking just two of the elements so one and two one and three and two and three and then lastly I can just grab all the elements so one and the two and the three I don't care about the order right I just care that I have all of these different combinations of all of the different lengths in my uh final list right so does everyone understand kind of the goal of this function okay so how do we achieve this well you might not be surprised we're going to do it Rec cursively that's really the only reasonable way to write this code okay um so I'm going to go through this uh slide kind of just explaining what each line does but on the next slide I'll have a little animation that shows step by step how the function creates this uh subset list so first thing it's recursive so I've got my base case up there it's if I have a list of length zero then I the subset of an empty list right is just going to be this list with the empty thing inside it right so if I have no elements there's only one subset that's the empty list then if I have more than one element inside it I'm going to do the same idea that we saw when we worked with lists back in the recursion lectures right I'm going to extract one of my elements I'm going to work on the remaining list and then I'm going to do something by taking that element and tacking it back on to the result so in this particular case the thing that I'm extracting is the last element in my list so if my list is one two and three at a step here I'm going to extract the three and make it into its own list right so that's what that step is doing it extracts the last element in the list then I make a function call to generate subsets on everything except for that last element so if I'm so I say hey function that I'm currently writing right now if you can generate for me the subset of all the elements right the subset for this list then you're going to come up with something that looks like this it's going to be the empty list the one the two and the one and the two together right so the subset of this list is going to be this group of of elements here so that's what this is going to do so this is again us trusting that the function when wew write will generate something that looks like this if we've got to this point then smaller is going to be a list that looks like this so the next part of the code is going to take that little extra thing that I had saved previously and it's going to tack on that three to every element within this list so then I'm going to basically say I'm going to take this three and make a a list with the three in it a list with the one and the three in it a list with the two and the three in it and a list with the one and the two and the three in it so I've just taken that three and added it to everything that resulted from this line of code here from my my function calling itself and then all it does is returns smaller plus new so if I add these two together this is going to generate for me my final subset that I was interested in right I've got the empty thing I've got the one the two and the three by itself I've got the one two the one three and the two three by itself and then the one two three all together so that's the big idea here okay so let's just go through step by step um recursively calling ourselves so this is me finding out uh the my kicking off my function call saying hey generate the subsets for the list one two three I'm going to keep the extra aside I need to make another function call because I'm not at my base case so I'm going to call gen subsets on one comma two this is also not my base case so I'm going to C take my last element put it aside and I'm going to call gen subsets on just the one still not the base case I'm going to take this extra put it aside and I'm going to call gen subsets on the empty list and this is where I reach my base case so far nothing has been returned at all no work has been done at my base case python will say I know what this is It's going to be the list with just the empty thing in it all right cool next that gets returned so this function call goes away so now what is it going to do well it's going to take that extra I set aside take the smaller list that I just returned and basically double that smaller list so this is my smaller list and then I'm going to double that by saying I'm going to put this one to the end of everything in my smaller list maybe this is not so Apparent at this step but let's go one more step and see what happens so now this function also terminates it returns this empty list and one in it and says all right here with this function call I had saved the two separately and said I'm going to now tack on this two to the end of everything that I had just returned right so this is smaller this is smaller over here and all I'm going to do is take this extra thing and Tack it on to the end of everything that was in smaller so I'm going to tack it on to the end of this empty list so it just gives me this two and Tack it on to the end of this one so it gives me the one comma two okay so I've basically doubled my list at this stage one more step this gets returned and now this is my original function call the thing that I had extracted was the three so now we're basically at this step here I extracted the three the function just just below it returned turned this smaller right so that means that this three is going to get appended to the end of everything that was in smaller right so it's going to be appended to the end of this empty list to give me just the three to the end of the one to give me the one and three to the end of the two to give me the two and the three and to the end of the one two to give me the one two three now this is the final answer right so I basically keep what I had returned from the previous function call and concatenate that with the thing that I had just created right where I tacked on my three and this is my final answer it's just sort of out of order to what we intuitively would have written by hand but it hits on all of the elements that I wanted to have anyway right so I've got the empty list everything with just one element in it everything with the two elements in it and everything with all three elements in it so let's look at the complexity analysis of this we've got two things going on here one is how many of these function calls are being done right like with the inverse tree structure how many of those function calls do we need to do to get to the end of our to to our base case and on top of that that sorry that will tell us how many actual elements in the list we will have and on top of that we have actually a Time complexity that's not constant that's to copy our list okay so copying a list is not constant right because it takes some time to take all the elements ments in a list and make a copy of them so if we think about the time it takes to make our list at each step right how many of these sub elements we're creating well at the very base case we have one element at the case just above it we had two elements at the case just above that we had four elements at the case just above that we had eight elements so at each step the number of uh sublists that we were generating was basically twice as much as the previous step so the overall number of subsets was on the order of two to the N but there was also a Time complexity to make a copy of the list within each one of those subsets so we're multiplying the complexity it takes to make all those function calls and generate all those subsets by the time it takes to make a copy of the list so the overall complexity is actually going to be Theta of n * 2 the N because it's a a little bit harder it's a little bit worse than exponential just purely for the fact that we're copying the list at each step okay all right so let's move on to logarithmic complexity this one's going to be a little bit tricky because right off the bat we're not going to be able to see a direct relationship between the input and what Loop we actually have so here I've got a function called digit X it's going to take in a number so um like you know 1 2 3 4 something like that the number 1,234 the code casts it to a string so it takes in a pure numerical value it makes a string out of it and then iterates through the string right so the function here in terms of time complexity is Theta of length s right here we're iterating through the string backward basically four then three then two then one but what's my input it's n it's not s right so the time complexity of this function while it's linear in s s is not linear in N because when my number is 83 my Loop only iterates twice if my number has four digits in it 4,271 my Loop iterates four times right so this relationship is not linear right so what is it exactly well let's think about what that Loop is actually doing right if I have a number with four digits in it right something in the thousands when I iterate through the uh the number by uh uh sort of backward right this number as a string I'm basically taking that one and keeping it in my running sum then it's kind of of like I divided that number by 10 I grabbed the remainder when I divided that number by 10 and that's the thing that I just added the whole number left over when I divided by 10 is this bit here so now think of it like taking this to take this last element here it's like I take this number and divide by 10 again I grab the remainder when I divide by 10 and add it to my running total and the whole number I'm left over when dividing by 10 is just this one more time I take the two the remainder when I divided that 42 what is two and the whole number I was left over with is four and then lastly I can do that last thing again so what's the relationship between the magnitude of n right this 4,000 something or this 80 something to how many times I have to Loop through to get every digit in my number well the trick here is to think about taking my magnitude my n the magnitude of n and dividing it by 10 a bunch of times how many times do I divide by 10 to basically grab every single element every single digit in my n well length s times right that's kind of like taking each character one at a time right to take each character one at a time that's like dividing by 10 to grab the remainder and then I've done that length s times right that's what this Loop is doing so the the relationship between the magnitude of N and how many times I go through the loop is this n divided by 10 some number of times length as times is equal to one that means I've finished going through this entire element uh this entire number all the digits within the number so the relationship between n and length s is length s is equal to log of N and now that I have this nice relationship well I said that this function was was linear in length s so if it's Theta of length s it's going to be Theta of log n i just map those two together questions about this this this trick can work in many different ways what's important to realize is that here there's kind of an indirect relationship between what's actually happening in the code and my input right it's not as as clearcut but there is some relationship which is not constant and not linear okay so the overall complexity of this function is Theta of log n where I don't actually care about the base when I report uh the complexity in terms of log right in this case it's base 10 but if it was base two it would be the same log okay so we saw some uh a bunch of uh examples uh uh just one of logarithmic complexity but we're going to see next that um searching for an element in the list will also be uh logarithmic complexities or complexity okay um before we get to that I'd like to just make a just put this slide up um to remind you that there are several functions built-in functions um with lists and dictionaries that aren't constant right so like that examples the example you guys did um where we use the in operator right we had to be careful if you ever see these operations being done in the code don't just push them aside you have to account for them within the complexity analysis okay so next we're going to look at some searching algorithms okay um these algorithms we're going to see a a bunch of different codes that Implement searching um these will again they'll be very similar to the ones that we actually timed last lecture so we're going to look at searching for an element in a list we're going to look at a bunch of different uh implementations of the plain brute force uh searching element in a list right whether it's sorted or unsorted as long as you just root force your way from the beginning of the list to the end of the list you'll be able to find uh the element you're looking for or say that it's not there so we're going to look at some linear search functions and then we're going to look at the bisection search a couple bsection search implementations um and that's where we divide the list in half and dis scard one of the halves and this those implementations though will um will uh need our list to be sorted right so the brute forceing our way doesn't really matter whether it's sorted or not but the bisection search only gives the correct answer if the list is sorted to begin with all right so first let's look at linear search on an unsorted list um this is code that is going to search for Element e in list L it Loops through the length of the list and and keeps this Boolean flag in mind if it finds the element we're looking for it just sets the flag and at the end of iterating through the whole list it tells us whether it found it or not so the worst case scenario uh analysis says that we have to look through the entire list to determine the element is there or not so the Theta of this particular function is Theta of length L right there's only one Loop depends on the length of L that nothing really special about this uh this function now you might notice that there's something inefficient about this function and that once it finds an element let's say at the beginning of the list this function actually just sets the flag and keeps going through to the end of the list so we can actually do a little bit of a speed up with this bit here and say that hey if we find it just return true right away no need to keep going to the end of the list so what's the analysis for this code well again we're doing worst case analysis so in the worst case the element is not there so we still have to search through every single element in the list beginning to end to determine it's not there so the worst case you know Theta analysis for this function is that we still have to go through to the end of the list to determine it's not there so it's still going to be sorry still going to be Theta of n oh sorry Theta of length L uh time okay so this is on an unsorted list but what if we look at a sorted list okay so we can do a little something clever in our code if the function if the list is sorted we can say we're going to start at let's say it's increasing sorted right we can start at the beginning of the list look through each element if we find it return true if we reach an element that's bigger than the one we're looking for the list is sorted so all the remaining elements in the list are also bigger than the one we're looking for right and then we can just return false right away well we think we're pretty clever but the worst case analysis says that the list is the element is not even in the list at all so we still have to go through and look to the end of the list to figure out that that element is not there so we still have to touch each element in the list to determine it's not there so the Theta worst case Theta complexity and Anis still says that this is Theta of length L right because everything else is constant okay so now let's look at bisection search so as as far as we can tell just doing a linear Brute Force search way is not going to uh give us anything better than Theta of n but when we looked at the timings in last lecture we saw that this binary search or bisection search on an element in a list was actually uh much much faster right it grew at a something a faster rate than linear but not quite constant so let's remember how that code looked so we basically had a list with a bunch of elements in it we looked at the element at the middle of the list and we said are you the one we're looking for in the worst case it's not right so then we have to ask are you bigger or smaller than the one we're looking for if it's bigger then we know we have to look in the lower half of the list if it's smaller we look in the upper half of the list and now that we either look in the lower or the upper half we notice we have the exact same problem to solve so this should ring a little bell that says we should use recursion right as now we have the same problem to solve an element e in a slightly smaller list is it in that list right so that's exactly what we're going to uh Implement so visually speaking this is what we're going to do we're going to uh have an original list with n elements in it we're going to look at the halfway point worst case it's not the one we're looking for so we're going to decide on one of the sides to next search through now we have n/2 elements to look through again it's not there worst case so we have to decide on which half to look through now we have n over four elements to look through we keep doing this we keep sort of having more and more recursive calls until we reach a base case and the base case is that we now have a list with one element in it either that element is the one we're looking for or worst case it's not and we've determined that the element we're looking for is not in these n elements at all right so our base case is down here and we started with n elements over here so the bisection search algorithm will repeat this task of dividing the list in half let's say I times right so this is quote unquote how many it iterations we would have made right but since this is recursion there's no iterations this is how many function calls we have until we reach the base case I function calls so if we take our original n elements and we divide them by two so many times that we have only one element left to search for that's when we found our answer so we now have a a relationship between how many elements we had originally n elements and how many times we had to divide our Loop to get to our answer right how many of these levels we have right and divided 2 to the I equals 1 that's our relationship so in the bis section search algorithm how many times are we calling this recursive function to get to the base case well I times so what is I in terms of n well the relationship between I and N is similar to the one we had over here right where we divided this number by 10 each time except that now we're dividing a list of n Elements by two each time so the relationship is still logarithmic right it relates the number of elements I originally had n with how many times I had to divide my list to get to one element whether it's it's the one I'm looking for or not so the complexity of just the pure bsection search algorithm is Theta of log n where n is the length of the list right that's how many subdivisions I need to do to get to one element to to decide it's not the one I'm looking for so now we're going to look at two different implementations of the code to do bisection search one will be more efficient than the other let's start with the one that's simpler to write but less efficient so this code um you can see here it looks for Element e and list L has two base cases up there those are both constant and one recursive step here right so either we do this one or this one so this one is if we decide decided we need to look in the lower half and this is if we decided we need to look in the upper half for the element so this is just pure bisection search which on the previous slide we decided is Theta of log of length of the list Theta of log n now that's fine but what do we have as a parameter here it's half of my list right so in addition to doing bsection search and just doing the algorithm having a bunch of bisection search calls that take me to that list of one element on top of that each time I make that bisection search call I'm copying my list so this is not constant it's Theta of length L over2 right I grab half of my list so the complexity of that code is Theta of n Times log n Theta of log n for the bisection search bit but Theta of n attacked onto each one of those calls because I have to grab a copy of my list with each function call right so it's not quite that efficient now let's look at a slightly different implementation this particular one is going to use integers to keep track of end points so instead of copying my list let me just keep track of a number for my low end point and a number for my high end point the complexity analysis for the bisection search is going to be exactly the same because even though I'm just keeping track of these high and low end points I'm still dividing the list in half with each call but I'm us I'm doing it by keeping track of integer indices so the size of the problem is still reduced by two at each step I'm keeping track of these integer indices I'm not copying the list at this point I'm just changing an integer value from you know uh you know 10 to five or whatever it is so the complexity analysis of the of theta of the bisection search is Theta of login the code looks a little bit Messier but overall it still does the same sort of things it's Messier because now I want B section search to look for an element e in list L but I'd like my recursive call to keep track of Two end points right these integers low and the integer High the thing that I want to search my list between so I'm going to create another function that I kick off down here which look for looks for an element e in list L but I'm also Al going to keep track of my low and high end points as parameter to by to by to my bisection search function so bisection search helper here is now going to take in these four parameters the rest of the code you know is just details but what's important is everything is constant except for my two bisection search calls right here I'm changing my low uh I'm sorry I'm changing my high if I want to look in the lower half of the list and here I'm changing my low if I want to look in the upper half of the list so the bisection search calls are still going to be Theta of log n but what's the overhead now the overhead is nothing right it's constant this L is the same one I'm not making a copy of it I'm just passing it through e is just a number low is just a number and mid minus one is just a constant operation there's nothing being copied here so the overall complexity of this code while it looks a little bit Messier is just Theta of log n right because the overhead is constant on each one of those function Colum so that brings us to this final question right clearly bsection search on a sorted list is faster it's Thea of login than by than a pure Brute Force search on a list that could be sorted or unsorted so the question is when does it make sense to sort the list first so given an unsorted list when do you sort the list and use this fast binary search versus just using a straight up linear search well that's when the time it takes to do the sort right an initial sort plus the complexity to do binary search is less than doing the straight up linear search right because the list has to be sorted for this to work right well when is that true well this implies that the time it takes to do the sort is less than Theta of n so that means means what can you sort a list without even looking at all the elements once no right like you have to look at all the elements once to even say that hey this list is already sorted so this is actually never true right so what does that mean does that mean we never want to do binary search on a list unless it's already sorted kind of but in fact you know there are various situations when it does make sense to do the sort first first and then use binary search and that's the case where you uh you're given a data set and you want to do a whole bunch of searches on that data set so if you can take that sort do it once and then amortise the cost it took you to do that sort over K different searches then it makes sense to pay the price to do the sort once and then do it over and then do the binary search over all these uh searches all these um yeah all these searches right and so as K gets really big the time it takes for you to do the sort becomes irrelevant right the Theta of doing this thing on the left becomes just the Theta to do the search the search logarithmically than it does to do the search um linearly okay so if you're only doing the search once please do not sort your list and then do a binary search that's going to take longer than just look at the uh elements in your list straight through using Brute Force but if you're going to do a whole bunch of searches makes sense to do the sort and then do uh do the search all right all right that's all I've got next lecture we're going to look at a bunch of different sorting algorithms and we'll have a quiz